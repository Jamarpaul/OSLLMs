replier,post_time,type,repo,comment
Impulse2000,2024-05-24 06:44:55+00:00,comment,CohereForAI/aya-23-35B,"Yea, they also charge thousands of dollars monthly, in order to use their command-r model, for commercial use on-premise use, we are only a small business so we tried to negotiate with them 2 times via email, we got no response.

Tbh its better to use the finetuned llama models anyway, or if you want permissive, mistral models are good."
MarziehFadaee,2024-05-23 20:29:04+00:00,comment,CohereForAI/aya-23-35B,Thanks!
mniiinm,2024-05-25 19:27:09+00:00,comment,CohereForAI/aya-23-35B,"> Thanks!

I guess the reason behind the support of Persian language comes from your presence in the development team. Therefore, I thank you very much for creating the first  open-source model that can truly support Persian language. "
patrickvonplaten,2023-06-27 19:00:15+00:00,comment,facebook/galactica-125m,Nice!
mkardas,2022-12-05 13:43:31+00:00,comment,facebook/galactica-125m,
mrm8488,2022-11-18 11:12:15+00:00,comment,facebook/galactica-125m,"Hi, @elenuki33. Could not reproduce your problem.
Try it:
https://colab.research.google.com/drive/1hjnB9VBMnbVIJiTNdQWZbL0yIAqF75WZ?usp=sharing"
deleted,2023-12-19 15:27:14+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@agershun I agree that it seems smarter, but have experienced it hallucinating more.

The reduction in hallucinations appears to be an illusion caused by things like its propensity to shy away from long lists filled with details, hence reducing the opportunity for hallucinations, and denying things are true, even when they are (throwing the baby out with the bath water).

When I put it to the test with tricky fringe knowledge it performed worse than all other leading 7b Mistrals. For example, when I ask about the 2 ex-wives of Alan Harper from the show Two and a Half Men this LLM got all 4 names wrong (screen and real names), while 7b Mistrals reliably get 2 of 4 right (first wife Judith). And this wasn't the exception. It reliable hallucinated more on fringe knowledge.

So ironically, despite its larger size, this LLM is far less knowledgeable than the original 7b Mistrals. Hallucinated more at the fringes, provides less information (to avoid hallucinations) and denying ~10x more things aren't true, that actually are true, in an attempt to minimize the frequency of saying things are true that aren't.

In short, it actually hallucinates more, which is why I suspect it's overly tight-lipped, brief, cynical..."
Mino24she,2023-12-19 17:45:12+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@Phil337 Could you try out UNA-Solar by the UNA guy? Or better, the Frostwind tune from Sao10K?

I still have no idea what UNA is, but it seems he finally relented some details in his model card. I have more hope for the Sao10K tune, however, because it's actually trained on the base model.

I'm traveling so can't quite test at the moment. I'm rather invested in this model, due to how transparent the team has been in their testing. Seems like a rare breed in recent climate."
deleted,2023-12-19 20:03:09+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@Mino24she I haven't tried Sao10K yet, but I did try the UNA-Solar version of instruct and it performed slightly better on my test (e.g. got 3 of 4 names right in my aforementioned question about Alan's ex-wives from 2.5 Men). However, it's plagued by the exact same stubborn denials of facts, as well as excessive censorship and moralizing. But I guess this is expected if UNA is more about the transformers than weights.

I also tried the Uncensored version of Solaris and it performed better (less censorship and moralizing, plus longer responses), but it still performed poorly on fridge knowledge questions like the Alan question above."
Mino24she,2023-12-19 20:40:28+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@Phil337 The UNA one is based on the instruct tune, which obviously, won't be as practical. The base model looks more organic to me. The Sao10K (of Euryale fame) one (Frostwind) otherwise looks pretty good."
deleted,2023-12-19 21:26:08+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@Mino24she I tested Frostwind and it still has some censorship, but less than Solar instruct. It also hallucinates more than leading Mistrals at the fringes of knowledge.

So far all the Solar LLMs have been smarter than the top Mistrals, but none have been more knowledgeable, or even as knowledgeable. I don't know what up-scaling is in the context of LLMs, but it appears to only improve the transformers, not the weights."
maxterm2000,2023-12-19 22:47:41+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Good model, have some problems for common sense reasoning and counterfactual reasoning, but isn't impossible to adjust. It's the wright direction, with more accuracy on MMLU benchmark I think the upstage can surpass Mistrals in the future. Have something marvelous in this model.
"
agershun,2023-12-20 14:38:22+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Agree that reducing the size of the response is an excellent way to decrease hallucinations. However, for solving specific tasks, the length of the response is not a critical factor.

This SOLAR neural network doesn't know a lot, that's true. I usually use the story ""Mumu"" by Ivan Turgenev for tests, and I have heard so many interesting and diverse stories from different neural networks. But for my tasks, this is not a problem, I want to further train it for my actual material (figuratively speaking, to instill in it the correct version of Mumu). For me, it's more important that it is still capable of making inferences and doesn't give random characteers like ===~=#$== as some other networks do."""
appvoid,2023-12-21 07:17:10+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"This model seems to be better when it comes to RAG, it hallucinates a lot less, this is the most useful model i've loaded on my 8gb vram laptop. You can really rely on it on common, easy language tasks just like chatgpt 3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast decent language model!"
warlock76,2023-12-22 01:44:34+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"For some reason at 16 bit attention it increases in VRam use with each response. Its stable in 8-bit attention but that is sad because you can load it at 16-bit into 24GB VRam and its really fast. The problem is once it hits the ceiling it crashes. I use fastchat with ""python -m fastchat.serve.cli --model-path I:\misc\downloaded\Ai_models\models_for_fastchat\upstageSOLAR-10.7B-Instruct-v1.0 --style rich"" on a RTX Quadro 6000 Passive. Its stable with --load-8-bit. I will try the other solar models. So far it seems very fast with solid replies."
hunkim,2023-12-22 10:48:18+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@appvoid You are absolutely right. We designed this model to follow instructions well, including the RAG model. Thank you very much for your comment! :-)"
frenzygr,2023-12-23 18:15:01+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"> This model seems to be better when it comes to RAG, it hallucinates a lot less, this is the most useful model i've loaded on my 8gb vram laptop. You can really rely on it on common, easy language tasks just like chatgpt 3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast decent language model!

hi mate kind of irrelevant question but how do you run the .safetensor files of this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948 first or do you have some other method?"
appvoid,2023-12-23 18:48:21+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"> > This model seems to be better when it comes to RAG, it hallucinates a lot less, this is the most useful model i've loaded on my 8gb vram laptop. You can really rely on it on common, easy language tasks just like chatgpt 3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast decent language model!
> 
> hi mate kind of irrelevant question but how do you run the .safetensor files of this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948 first or do you have some other method?

Hi, i just used gguf version from ""the bloke"" on lmstudio. What really suprises me though is that i'm using a q5km quantized version and still manages to be good."
agershun,2023-12-23 19:28:08+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"I used SOLAR with the following methods:
1) ollama supports it out of the box
2) vllm as well
3) For A100/40 + Jupyter I used this code:

```python
import torch
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from trl import SFTTrainer
model_name = ""upstage/SOLAR-10.7B-Instruct-v1.0""
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.padding_side = 'right'
use_4bit = True
bnb_4bit_compute_dtype = ""float16""
bnb_4bit_quant_type = ""nf4""
use_nested_quant = False
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map=""auto"",
    quantization_config=bnb_config,
)

query = """"""What do I want to ask?""""""
conversation = [ {'role': 'user', 'content': query} ] 
prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)

inputs = tokenizer(prompt, return_tensors=""pt"").to(base_model.device) 
outputs = base_model.generate(**inputs, use_cache=True, max_length=4096)
output_text = tokenizer.decode(outputs[0]) 
output_text = output_text.split(""\n### Assistant:\n"", 1)[-1].replace(""<s>"", """").replace(""</s>"", """").strip()
print(output_text)
```"
frenzygr,2023-12-23 23:19:47+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"> > > This model seems to be better when it comes to RAG, it hallucinates a lot less, this is the most useful model i've loaded on my 8gb vram laptop. You can really rely on it on common, easy language tasks just like chatgpt 3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast decent language model!
> > 
> > hi mate kind of irrelevant question but how do you run the .safetensor files of this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948 first or do you have some other method?
> 
> Hi, i just used gguf version from ""the bloke"" on lmstudio. What really suprises me though is that i'm using a q5km quantized version and still manages to be good.

thanks the reply mate, is it this one? https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF"
appvoid,2023-12-24 04:27:19+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"> > > > This model seems to be better when it comes to RAG, it hallucinates a lot less, this is the most useful model i've loaded on my 8gb vram laptop. You can really rely on it on common, easy language tasks just like chatgpt 3.5. Of course, GPT-4 is better at everything but hey, this is a free, fast decent language model!
> > > 
> > > hi mate kind of irrelevant question but how do you run the .safetensor files of this model? Do you convert it to .gguf using https://github.com/ggerganov/llama.cpp/discussions/2948 first or do you have some other method?
> > 
> > Hi, i just used gguf version from ""the bloke"" on lmstudio. What really suprises me though is that i'm using a q5km quantized version and still manages to be good.
> 
> thanks the reply mate, is it this one? https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GGUF

This one:
https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF"
deleted,2023-12-27 20:16:54+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@Mino24she I remembered you thought Solar has a lot of promise and wanted to find a good Solar fine-tune. Check out the uncensored version linked below. Even when it comes to uncensored prompts it's more verbose and hallucinates less on the fringes of knowledge.

https://huggingface.co/w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored"
mirix,2023-12-29 12:45:49+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"I have tried this model with RAG and sometimes the answers are unrelated both to the question and to the context provided. I have switched to Tulu 2 DPO, which provides less elegant answers, but I find it a lot more reliable for RAG. 

Here is the code (with chatbot interface) if anyone is interested:

https://github.com/mirix/retrieval-augmented-generation

For instance, I had it read The Little Prince and asked a question that was within the context and the answer was about The Shawshank Redemption and had nothing whatsoever to do with the question. 

I have tried several models with the same script and none of them hallucinated like that. 

I thought it could be the template. but SOLAR seem to get most answers right.

Any ideas?
"
mirix,2023-12-29 13:29:41+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Please, disregard my previous comment. The issue seems indeed to be related to the prompt template and it is solved by using the following wrapper:

```
query_wrapper_prompt = PromptTemplate(
""### System:""
""Please, check if the anwser can be inferred from the pieces of context provided. If the answer cannot be inferred from the context, just state that the question is out of scope and do not provide any answer.\n""
""### User:""
""{query_str}\n""
""### Assistant:\n""
)
```"
ElliottDyson,2023-12-29 14:28:58+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"It would be great to see a long-context version of this model, it may also help with the multi-turn conversations that many have mentioned it seems to struggle with."
Tester100,2024-05-27 15:53:43+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Half a year later and this model is still one of the very best! There have been finetunes like Fimbulvetr in the meantime, and communities like /r/SillyTavern are really happy with the work you shared with the public. Personally I'm extremely amazed how the model adheres to previous messages, which creates both incredible consistency and flexibility for character prompts. Thanks for your fantastic work!"
hunkim,2024-04-07 02:41:14+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,It's mostly English-based. Check the API version for Korean support.  https://console.upstage.ai/
BearSean,2024-04-07 04:00:42+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Hello, 

This question can be better answered by AWS team. I'll forward the question to them. 
Meanwhile, you can check out Solar Mini Chat available on Marketplace and JumpStart! https://aws.amazon.com/marketplace/seller-profile?id=seller-tq4lkemg5w3jw

Thanks,
Sean"
jhkwonfromamazon,2024-04-07 04:33:45+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Hello!

I am Jay from AWS partner manager, and I am following up this technical question with our tech team. we will give you feedback ASAP.

Thanks,

Jay"
sungeuns,2024-04-07 06:44:39+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"You've tried transformers version with 4.26, which doesn't support Llama backend.
(Note that Solar based on [LlamaForCausalLM](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/llama2#transformers.LlamaForCausalLM), So only works with transformers version which has llama backend)

Try correct version of huggingface transformers.

I recommend to try using TGI (text generation inference) or using LMI DLC (Large model inference deep learning container) which is recommended way for LLM deployment on SageMaker.
- TGI example: https://github.com/philschmid/sagemaker-huggingface-llama-2-samples/blob/master/inference/sagemaker-notebook.ipynb)
- LMI DLC tutorial: https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-deepspeed-djl.html
- LMI DLC example: https://github.com/sungeuns/gen-ai-sagemaker/blob/main/finetune/ko-llm-medical/05_sagemaker_lmi_dlc_endpoint.ipynb
"
hunkim,2024-04-07 02:47:39+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,Can you provide more details? Thank you!
hunkim,2024-04-07 02:44:20+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Thanks for your kind words. Unfortunately, we cannot release the H100 machines we used. ðŸ˜Š"
hunkim,2024-04-07 02:47:01+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,We'll be working on it. Our 16K beta API is now available at https://console.upstage.ai. Check it out! ðŸ˜Š
Hoioi,2024-05-02 20:46:21+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,When are you going to publish the 16k version publicly on huggingface? 
hunkim,2024-02-18 20:25:29+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,You can fine-tune these models from LLAMA2 structure models using the same approach.
hunkim,2024-02-18 20:30:40+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"In fact, there are different models.

* SOLAR-10.7B-Instruct-v1.0: Focusing on English and instructions (like GPT-3.5-instruct).
* Solar-mini (API, Poe): English and Korean, with a multi-turn chat version. We'll release it as an API soon at https://console.upstage.ai/ 

Would you like to participate in our API beta test?"
halilergul1,2024-02-25 15:51:35+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Hello,

I successfully fine-tuned this model for another task I have been working on recently. I do not think the problem you encounter is due to your GPU because I did with a single GPU with 24GB. The problem you face is possibly because of library configuration issue. Here is my packages, make sure to use a virtualenv and load these:
%pip install -Uqqq pip --progress-bar off
%pip install -qqq torch==2.0.1 --progress-bar off
#!pip install -qqq transformers==4.32.1 --progress-bar off
%pip install git+https://github.com/huggingface/transformers
%pip install -qqq datasets==2.14.4 --progress-bar off
%pip install -qqq peft==0.5.0 --progress-bar off
%pip install -qqq bitsandbytes==0.41.1 --progress-bar off
%pip install -qqq trl==0.7.1 --progress-bar off
%pip install scipy
%pip install accelerate==0.27.2

Hope this helps!"
bertdirt,2024-02-27 11:08:55+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@halilergul1 , I have figured out the issue. I am using `use_gradient_checkpointing=False` in `model = prepare_model_for_kbit_training(model,use_gradient_checkpointing=False)` but  `gradient_checkpointing=True` is set to `True` in `TrainingArguments`. When I removed `use_gradient_checkpointing=False`, then it worked."
dyoung,2024-01-04 11:30:41+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Hi, 
Unfortunately ctransformers is running a bit late to the game in regards of keeping up with the recent new and exciting large language model architectural changes this winter.
From what I can tell, due to how ctransformers works, some of the new unique models with architectural changes may not be supported yet. It doesn't seem very flexible with loading models outside of the model families that it was built for already. ctransformers has not been updated since November. Which was just before all the fun new model excitement this winter.
I don't think they are out of the game yet. You may have to wait. Or consider using another backend/system interim if you don't want to wait. I've been using ctransformers this year and considering moving to something new due to the delays. (I hope that ctransformers is ok and catches up soon. I liked using it.)
ctransformers is built around running quantized models. So you might have better luck looking around areas where that is the main theme, if you were not already.
https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf <- For example, TheBloke points out other tools/clients/libraries that could be used to load GGUF quants of this and some newer model(s).
The llama-cpp family (llama-cpp and llama-cpp-python) might be a good choice. They seem to be able to move fast to build support for new model changes. And I've personally seen a successful SOLAR load and reasonable inference by llama-cpp-python. They have good documentation as well. ctransformers and llama-cpp-python felt alike. So your experience/understanding may transfer decently.
If your ok with loading models on your CPU, it's pretty easy to setup for CPU. And it's OS agnostic. Though things could be really slow if your not running a recent CPU with AVX support or something similar and recent. And it has bindings that likely will bridge things decently between it and LangChain. (https://python.langchain.com/docs/integrations/providers/llamacpp  Though I think LangChain might be a bit dated with it's info. And I'm not a LangChain user. Still might be useful.) (Llama-cpp caveat. Running on something other than CPU will likely require a couple year seasoned Linux/Unix/VM OS management experience and likely some decent experience with software development to understand what it needed. Should be do-able if your familiar with that. If not, that is asking a lot of people just to guide someone blindly through.)
That being said, likely something could cross between providers that LangChain supports and the systems that can load GGUF's of SOLAR that may work for you.
Cross checking between 'https://python.langchain.com/docs/integrations/providers -> More' and 'https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF#about-gguf ' might yield some reasonable results in search of a new loading and inference system that works for different constraints."
dagelf,2024-01-02 22:33:46+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"The default `llama.cpp` server command's webserver's default settings work perfectly for chat, and this model is really good at chat and one of the cleanest (""safe in a non-obnoxious way"") bots out, even at high quantization. (Note you have to use GGUF files for `llama.cpp`,  quantize them with the bundled `quantize` command or just download them from TheBloke's repo)"
apepkuss79,2024-01-03 01:57:39+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,@dagelf Thanks for the reply. We've already supported this model in [llama-api-server and llama-chat](https://github.com/second-state/llama-utils).
hunkim,2023-12-30 03:23:42+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Thank you for asking! Since we released our base model <https://huggingface.co/upstage/SOLAR-10.7B-v1.0> under Apache2, we hope our community will create instruction finetuning with commercially viable datasets. ðŸ˜„"
hunkim,2023-12-27 03:18:23+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"> Korean output was awful.
We did not use Korean datasets for fine-tuning."
agershun,2023-12-23 20:20:34+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Due the upper and lower case in the name of the model it downloaded as two diffent copies into the models cache:
* ""upstage/SOLAR-10.7B-Instruct-v1.0""
* ""Upstage/SOLAR-10.7B-Instruct-v1.0"""
hunkim,2023-12-24 10:41:39+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Please use:
upstage/SOLAR-10.7B-Instruct-v1.0

"
agershun,2023-12-24 11:00:06+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@hunkim , I am sorry, but why did you decide to keep the wrong name in the README.md?"
SamuelAzran,2023-12-23 11:09:03+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,Thank you for the much needed analysis. Especially the point about TruthfulQA score was illuminating 
hunkim,2023-12-24 10:42:34+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Since we are using the llama2 architecture, there are numerous ways to expand. ðŸ› ï¸"
Michelangiolo,2023-12-19 06:27:30+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"I cannot get it to execute any of my commands (even picking a random number) Ex.

prompt = f""""""
### User:
Given the sentence: ""Paris is beautiful""
Try performing zero-shots classification on:
[""Country"", ""Food"", ""Love""]
Output a python list with a score for each class.

### Assistant:
""""""

Output:
In zero-shot classification, we use a pre-trained language model to predict class labels without any fine-tuning or explicit training on specific classes. Since you provided only strings for classes and no pre-trained language model or specific method is mentioned"
Limerobot,2023-12-19 07:31:59+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Please use the correct chat template. Refer to the following link:
https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0#conducting-single-turn-conversation

You can also see our model's output in the ChatBot Arena:
https://chat.lmsys.org/"
Limerobot,2023-12-15 12:45:32+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Hello,

Are you using this model (upstage/SOLAR-10.7B-Instruct-v1.0) for finetuning? If so, you might not achieve the highest score.
You should instead use our pretrained model (upstage/SOLAR-10.7B-v1.0) for finetuning.

(If you have already been using our pretrained model for your finetuning process, please disregard the above.)

I recommend the following settings:
- learning rate: 1e-06 ~ 2e-06
- at least 5 epochs"
andreaKIM,2023-12-15 13:20:03+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@Limerobot Thanks for recipes for fine tuning
Is there any reason fine tuned model get poor performance when they fine tuned once again?
I am just curious if there exist some specific reason for this situation."
eramax,2023-12-15 06:22:49+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"This is a fantastic model, could you please share how did you build it, maybe a post or a paper.

Thanks"
hunkim,2023-12-15 06:47:08+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,We will be submitting them to arXiv shortly. Thank you for your interest!
mrfakename,2023-12-16 01:47:44+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,Thank you! Excited to see it!
mrfakename,2023-12-18 16:53:35+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Hi @hunkim, is this basically a merge of Mistral and Llama, trained on more tokens? Were the original Llama weights used, and if so, does the license apply?"
hunkim,2023-12-18 21:10:22+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@mrfakename 

We use the first and last 24 layers and initialize the Mistral weights. (Yes, the license applies.) Then, we continue pre-training the depth-upscaled model.
![image.png](https://cdn-uploads.huggingface.co/production/uploads/603c29094a944b99e81476fd/nrz8CuHF2Hd9xNXHFs-I7.png)

In some sense, this can be considered depth upscaling, while we see MOE as width upscaling.
![image.png](https://cdn-uploads.huggingface.co/production/uploads/603c29094a944b99e81476fd/TVOfSP2TTn787Ru9uVGpP.png)"
Shamane,2023-12-19 23:15:09+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0," @hunkim Thanks a lot for all this info.  I have two questions:

1. Did you use the first 24 layers from LLMa and the last 24 layers from the mistral in the final merged model?  Is there any logic behind selecting the order? 
2. What is the dataset you have used?

Thanks in advanced :). "
Limerobot,2023-12-14 06:08:30+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,Thanks!
hunkim,2023-12-15 06:48:05+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Hmm, it seems we cannot reproduce this error. Do you still have the problem?"
iSameer,2024-02-16 09:20:59+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,yess facing same error
tarruda,2023-12-13 19:07:06+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"This model also seems good at reasoning. I tried the Orca 2 paper reasoning challenge multiple times and it seems to always get the response right:

```
John and Mark are in a room with a ball, a basket and a box. John puts the ball in the box, then leaves for work. While John is away, Mark puts the ball in the basket, and then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is?
```

One thing I really loved is that the Q4_K_M GGUF version completely fits in a 8GB VRAM GPU!

There's a lot of potential for this model, especially if it is trained for multi-turn conversation and function calling is implemented. "
deleted,2023-12-13 19:31:22+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@tarruda You're not lying. I'm still finishing up my test on it. But it's doing things other Mistrals can't. Like you said, it's ""smarter"".

For example, I ask the LLM to make a joke about 2 disparate things (e.g. cat and a telescope), start with a random header, such as ""Out in a field"", and then explain itself (all with the same prompt). And this LLM made coherent and slightly humerus jokes and correctly explained why they were funny.

Another example is prompting a poem type (e.g. sonnet) with several directives it must follow. This is hard because it has to follow rhythm, meter... while remaining coherent and including the prompt directives.

In short, this is better than 7b Mistrals. I thought this was a hoax when I saw it so high on the leaderboard. This is not a hoax."
tarruda,2023-12-13 19:57:21+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,">  I thought this was a hoax when I saw it so high on the leaderboard. This is not a hoax.

To me the best LLM I can run locally is still NeuralHermes 2.5, but maybe this will surpass once there are some fine tunes by @teknium / @mlabonne .  Lack of system prompt and multi-turn chat is limiting and makes it harder to compare with existing mistral fine tunes..."
deleted,2023-12-13 20:17:48+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@tarruda Multi-turn chat isn't part of my personal testing, but knowledge is its achilles heel. It did notably worse on all my fringe knowledge questions than leading Mistrals, and this bleed into my story prompts (e.g. despite Friends being a widely popular show, it had Monica and her brother be love interests). In short, it's notably smarter than Mistral, but less knowledgeable. I'm starting to think they pushed TruthfulQA so hard in this Instruct version because they had to keep this substantial reduction of knowledge in check.

Mixtral and Yi-34b have FAR more knowledge than this LLM. And even the original 7b Mistral has notably more knowledge. Somehow they lost information during the up-scaling process, yet gained intelligence."
tarruda,2023-12-13 20:21:57+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"Would be worth checking the knowledge of the base version of the model, which has 45.04 score on Truthful QA: https://huggingface.co/upstage/SOLAR-10.7B-v1.0

It is still one of the top LLMs in the leaderboard, but if it was the instruction fine tune that killed its knowledge, then some other fine tunes might fix it."
deleted,2023-12-13 20:55:58+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,@tarruda Thanks for the suggestion. I'm going to try neuralhermes 2.5 next. Hopefully someone will fine-tune SOLAR base with the same methods and data as Mistral 7b so that it's easier to compare the difference between the base models.
InvidFlower,2023-12-17 15:08:51+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"I wonder if a variation on this kind of model could be good as RAG-enhanced. Like you mentioned a local Wikipedia archive. If a small model has good logic and a wrapper can help get data into it on the fly, that might be a best of both worlds thing. Iâ€™m sure that breaks down in extremes (not knowing enough to know what to search for), but seems it could let you reduce GPU requirements in general to keep more knowledge external to the LLM itself. "
deleted,2023-12-17 16:44:20+00:00,comment,upstage/SOLAR-10.7B-Instruct-v1.0,"@InvidFlower I hope someone with expertise tries this someday. The offline version of wikipedia I'm using is with the Kiwix app using a fully indexed zim file, so it has full text search. It would be nice if their was a button that said ""verify with Wikipedia"" so I didn't have to search and scan manually through the results to verify key facts."
bartowski,2024-06-03 13:29:45+00:00,comment,lmstudio-community/aya-23-35B-GGUF,"Seems like you need to update, this was made using a newer version that tags it as ""command-r"" tokenizer"
vectornaut,2024-06-06 17:16:38+00:00,comment,CohereForAI/aya-23-8B,"I was also having this problem. Here's how to get around it:
1) Download the notebook onto your machine, and upload it to colab.
2) Add this cell somewhere towards the top, and execute it before trying to load the `moodel()`:
```
from huggingface_hub import notebook_login
notebook_login()
```
3) You'll be prompted for your HuggingFace token. Paste it and you'll be good to go."
ahmetustun,2024-05-29 18:19:50+00:00,comment,CohereForAI/aya-23-8B,"Hi @alexcardo, we evaluated translation performance using this prompt: 
`Translate from {src_lang} into {tgt_lang}:\n`

However, the model should be used with it's chat template similar to this one:
```
messages = [{""role"": ""user"", ""content"": ""Translate from English into Turkish:\n This is a multilingual model""}]
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=""pt"")

gen_tokens = model.generate(
    input_ids, 
    max_new_tokens=256, 
)
```"
alexcardo,2024-05-29 18:30:42+00:00,comment,CohereForAI/aya-23-8B,"Thank you for your response! Unfortunately, I have a low GPU machine :-( I thereby is forced to use the quantized model. Talking about the pompt template, I mean this one:

<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{system_prompt}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{prompt}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>

This prompt template is attached to the quantized versions. Can you please correct it with a simple example? I need to explain model that it should translate from one language to another. For example from Spanish to Italian."
ahmetustun,2024-05-29 19:06:53+00:00,comment,CohereForAI/aya-23-8B,"The examples I posted will be same for also for quantized versions. If you use `tokenizer.apply_chat_template`, it will generate this:
`<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{prompt}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>`

Therefore no special format for translation, you can test it using a prompt like this: `Translate from {src_lang} into {tgt_lang}:\n {src_text}`"
m0javad,2024-05-29 07:53:24+00:00,comment,CohereForAI/aya-23-8B,"You cannot do that with Llama cpp and you should do it with ONNX. 
It is based on a T5 transformer."
nonetrix,2024-05-23 22:23:12+00:00,comment,CohereForAI/aya-23-8B,"They usually don't release base models for whatever reason unfortunately, would like to see that change though"
MarziehFadaee,2024-05-23 20:30:35+00:00,comment,CohereForAI/aya-23-8B,Indeed there was a typo. It's fixed now. Thanks!
Covey,2024-04-19 10:37:37+00:00,comment,CohereForAI/c4ai-command-r-v01,"This is my transformers version
![image.png](https://cdn-uploads.huggingface.co/production/uploads/64e6ac8c9f8952a1c0d7a31e/OcEWC4RqnDiphosRECra8.png)
"
neih4207,2024-04-24 06:39:13+00:00,comment,CohereForAI/c4ai-command-r-v01,You should upgrade torch version from 2.0.1 to 2.2.0.
HugoLaurencon,2024-05-04 00:55:13+00:00,comment,CohereForAI/c4ai-command-r-v01,"It doesn't fully solve the problem for me. By upgrading to torch==2.2.0, I have another bug:
```
import flash_attn_2_cuda as flash_attn_cuda
ImportError: /fsx/m4/conda_installation/envs/shared-m4-2024-05-03/lib/python3.8/site-packages/flash_attn_2_cuda.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops9_pad_enum4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEElNS5_8optionalIdEE
```

See this discussion: https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/34"
ZechenBai,2024-06-03 09:27:41+00:00,comment,CohereForAI/c4ai-command-r-v01,"@HugoLaurencon 
You can uninstall flash-attn first and then reinstall it with pip install flash-attn --no-build-isolation --no-cache-dir"
ahmetustun,2024-04-10 10:04:58+00:00,comment,CohereForAI/c4ai-command-r-v01,Thanks!
Qubitium,2024-04-12 06:55:52+00:00,comment,CohereForAI/c4ai-command-r-v01,@ahmetustun Why was this closed and not merged? 
ahmetustun,2024-04-12 08:48:22+00:00,comment,CohereForAI/c4ai-command-r-v01,@Qubitium there was a merge conflict due to another merge however I incorporated the suggested change.  
ahmetustun,2024-04-05 08:06:54+00:00,comment,CohereForAI/c4ai-command-r-v01,"Hi, could you update the `transformers` and use the model without `trust_remote_code`? You may need to clean command-r module from the huggingface cache as well."
velocivirus,2024-04-08 22:48:30+00:00,comment,CohereForAI/c4ai-command-r-v01,"I happened to encounter the same problem, but removing the `trust_remote_code` works!"
Qubitium,2024-04-09 04:38:25+00:00,comment,CohereForAI/c4ai-command-r-v01,"@ahmetustun  the bug is cohere including unnecessary config in json: fixed in PR https://huggingface.co/CohereForAI/c4ai-command-r-v01/discussions/48

"
ahmetustun,2024-04-04 18:15:34+00:00,comment,CohereForAI/c4ai-command-r-v01,"Thanks a lot Matt, this makes sense and looks very nice. I'm merging your PR. "
ahmetustun,2024-04-04 18:20:42+00:00,comment,CohereForAI/c4ai-command-r-v01,"Hey Matt! After I merged the changes, automatic parsing gave this error:
![Screenshot 2024-04-04 at 20.16.38.png](https://cdn-uploads.huggingface.co/production/uploads/6436e9c41046b34b54395d92/jiJzETfYQ5NzTT9vOnRf2.png)
"
epignatelli,2024-04-03 07:53:12+00:00,comment,CohereForAI/c4ai-command-r-v01,"Never mind.

Removing `trust_remote_code=True` from the initialiser solved the issue."
shivi,2024-04-03 07:53:41+00:00,comment,CohereForAI/c4ai-command-r-v01,This comment has been hidden
sarahooker,2024-04-02 16:06:20+00:00,comment,CohereForAI/c4ai-command-r-v01,It should hopefully be resolved -- @MarziehFadaee did some updates to the configuration to avoid timeouts. Let us know if you see any additional issues.
CatUkraine,2024-04-10 19:10:29+00:00,comment,CohereForAI/c4ai-command-r-v01,"The dataset is probably closed-source, but, in theory, it is possible to generate an ""artificial"" dataset for instruction following. It can be done by program two instances of LLM to chat with each other and log their generated data into some file."
markding,2024-04-12 13:24:51+00:00,comment,CohereForAI/c4ai-command-r-v01,"I was wondering the same, and it seems like that the Aya Collection was used in some form, but I have not seen definite proof."
skevja,2024-04-17 11:26:11+00:00,comment,CohereForAI/c4ai-command-r-v01,Does anybody know what prompt formatting should be used for a custom fine-tuning dataset for command-r? 
ewre324,2024-04-23 07:05:01+00:00,comment,CohereForAI/c4ai-command-r-v01,"@skevja I am also looking for this information... 
Any updates?"
skevja,2024-05-10 13:35:47+00:00,comment,CohereForAI/c4ai-command-r-v01,"@ewre324 Unfortunately no, didn't find any information on this. "
nielsr,2024-03-28 12:51:53+00:00,comment,CohereForAI/c4ai-command-r-v01,"Hi,

This model is available from v4.39 on, so it's recommended to do `pip install --upgrade transformers`."
aligholami,2024-03-28 15:15:17+00:00,comment,CohereForAI/c4ai-command-r-v01,"yes, switching to `v4.39` fixes the issue. But not It gets stuck at `loading checkpoint shards...` and then the kernel crashes. Is this a RAM issue? "
shivi,2024-03-28 15:35:28+00:00,comment,CohereForAI/c4ai-command-r-v01,"Yeah it's a memory issue.
If you have Colab Pro, switch to runtime with A100 GPU then you should be able to load the model in 8-bit or 4-bit."
aligholami,2024-03-28 15:39:26+00:00,comment,CohereForAI/c4ai-command-r-v01,@shivi is there any way to achieve this without Colab Pro? 
shivi,2024-03-29 09:25:45+00:00,comment,CohereForAI/c4ai-command-r-v01,"You can also check [JarvisLabs](https://jarvislabs.ai/pricing) and [Lambda](https://lambdalabs.com/service/gpu-cloud) for on-demand GPU access. 
 
If you want to just try out the model then the easiest way would be to use Cohere's [Chat API](https://docs.cohere.com/reference/chat?__hstc=14363112.97f3f24f0a64bbd69c4e7f565f734bcc.1707853905085.1711642955820.1711655787153.14&__hssc=14363112.36.1711655787153&__hsfp=1430542347)"
sarahooker,2024-04-02 16:25:58+00:00,comment,CohereForAI/c4ai-command-r-v01,"You can now also use this model on hugging face spaces: https://huggingface.co/spaces/CohereForAI/c4ai-command-r-v01. 

Closing discussion for now, but feel free to re-open if relates to code errors."
MrDragonFox,2024-03-28 09:26:05+00:00,comment,CohereForAI/c4ai-command-r-v01,"you can .. just change the config.json 
but 128k would take over 130g vram alone .. i can only fit 64 in 96g"
pseudotensor,2024-03-28 19:49:22+00:00,comment,CohereForAI/c4ai-command-r-v01,"As I argue in that vLLM thread.  I don't think that's how it should be done.  Shouldn't just change embedding size, since rope scaling is used.  It should be part of the calculation."
MrDragonFox,2024-03-28 09:26:38+00:00,comment,CohereForAI/c4ai-command-r-v01,vllm/aphrodite .. load-in-4bit with 64k ctx
isaiahintelliga,2024-03-26 19:55:10+00:00,comment,CohereForAI/c4ai-command-r-v01,Any update?
pseudotensor,2024-03-27 20:15:04+00:00,comment,CohereForAI/c4ai-command-r-v01,Should be merged IMO.
sarahooker,2024-04-02 16:16:19+00:00,comment,CohereForAI/c4ai-command-r-v01,These files are no longer utilized so we are closing this comment. Feel free to re-open if you are still experiencing issues.
ahmetustun,2024-03-24 15:06:59+00:00,comment,CohereForAI/c4ai-command-r-v01,Can you try with transformers 3.9.1 after cleaning the HF cache but without using `trust_remote_code?
iamgroot42,2024-03-28 14:59:12+00:00,comment,CohereForAI/c4ai-command-r-v01,I'm getting the same issue with 4.39.1 (which is what the example says I should be using)
sarahooker,2024-03-22 18:47:56+00:00,comment,CohereForAI/c4ai-command-r-v01,You may want to try the quantized version of this model which will have lower memory requirements -- https://huggingface.co/CohereForAI/c4ai-command-r-v01-4bit. 
sarahooker,2024-04-02 16:09:00+00:00,comment,CohereForAI/c4ai-command-r-v01,"Btw, we now have launched a space https://huggingface.co/spaces/CohereForAI/c4ai-command-r-v01 which should allow you to play with the model."
ArthurZ,2024-03-22 09:36:42+00:00,comment,CohereForAI/c4ai-command-r-v01,Let's remove trust remote code = true! 
saurabhdash,2024-03-20 20:40:21+00:00,comment,CohereForAI/c4ai-command-r-v01,"Hi! Apart from the materialized attention mask, there is another problem -- the logits are up-casted to fp32. If you have a seq length of 128k, the logits themselves would take up 128k * 256k * 4(bytes)= 131GB. If the goal is to use it for generation, one could get rid of this and just do log-softmax over the last token's logits."
beam-me-up-scotty,2024-03-21 01:57:59+00:00,comment,CohereForAI/c4ai-command-r-v01,"Thanks for your answer @saurabhdash! In terms of implementation:
- Would the implementation of [causal_mask at line 614 of modeling_cohere.py in forward()](https://huggingface.co/CohereForAI/c4ai-command-r-v01/blob/2323aaa960c3c073380a0da2fc51284f5113e114/modeling_cohere.py#L614) need to change to your above implementation?
- Where would you change the implementation of the logits? Any tips about how to do so?
- What's a reasonable VRAM usage to expect for a 128k task with these optimisations? Am I over-optimistic to think that we can fit a context of that size on 2x A100s?

Apologies if these are silly questions, still a little new to all this"
saurabhdash,2024-03-22 15:48:09+00:00,comment,CohereForAI/c4ai-command-r-v01,I'd recommend waiting for/ using the vLLM implementation. That should be able to help you scale the context to the maximum.
gman402,2024-03-31 02:20:15+00:00,comment,CohereForAI/c4ai-command-r-v01,"I believe openwebui (with ollama) supports RAG, some extension on oogabooga may also do the same, but afaik openwebui comes with it standard"
Handgun1773,2024-04-04 18:42:09+00:00,comment,CohereForAI/c4ai-command-r-v01,"> I believe openwebui (with ollama) supports RAG, some extension on oogabooga may also do the same, but afaik openwebui comes with it standard

But this model has his peculiar way of prompting RAG, which would probably yield optimal performances."
patrick-s-h-lewis,2024-03-18 18:01:58+00:00,comment,CohereForAI/c4ai-command-r-v01,"If/when merged, I will open a PR to copy over the changes to the quantized model card"
Cyleux,2024-03-18 23:38:06+00:00,comment,CohereForAI/c4ai-command-r-v01,"Thank you so very much!

Bullish on Cohere"
sarahooker,2024-03-18 23:56:30+00:00,comment,CohereForAI/c4ai-command-r-v01,"This is great @patrick-s-h-lewis! One suggestion, right now documentation talks through how to use the tokenizer https://docs.cohere.com/docs/prompting-command-r (screenshot attached) but only refers to it as hugging face tokenizer. Now, that our PR has been merged, our code is more accessible in transformer library -- might suggest you explicitly link to the relevant code section https://github.com/huggingface/transformers/blob/838b87abe231fd70be5132088d0dee72a7bb8d62/src/transformers/models/cohere/tokenization_cohere_fast.py#L420. 

Merging now. 

![Screenshot 2024-03-18 at 4.52.28â€¯PM.png](https://cdn-uploads.huggingface.co/production/uploads/63434eb76f59b79da07dbddf/ngdIe4OAVJSKNeS14c_Wk.png)
"
Cyleux,2024-03-19 00:10:21+00:00,comment,CohereForAI/c4ai-command-r-v01,"According to the prompt structure described in the instructions, the tool outputs are not included directly in the chat history itself, but rather in a separate dedicated section.

Specifically, the tool outputs are inserted into the {TOOL_OUTPUTS} placeholder, which comes after the {CHAT_HISTORY} section in the augmented generation prompt template:

augmented_gen_prompt_template = """"""
...
<|END_OF_TURN_TOKEN|> {CHAT_HISTORY} <|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|> {TOOL_OUTPUTS}<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|> {INSTRUCTIONS}<|END_OF_TURN_TOKEN|>""""""

The {TOOL_OUTPUTS} section contains the results from all the tool calls, with each result prefixed by a Document: {n} identifier. 

For example:

<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>
<results>
Document: 0 
[Tool 1 output]
Document: 1
[Tool 2 output] 
</results>
<|END_OF_TURN_TOKEN|>

So even if multiple tools are used at different points in the conversation, their outputs are aggregated together in this single {TOOL_OUTPUTS} section, separate from the actual back-and-forth utterances captured in {CHAT_HISTORY}. This allows the model to reference the tool results when formulating its response, without muddling the conversation history.

Is this accurate? Thank you"
Cyleux,2024-03-18 07:23:02+00:00,comment,CohereForAI/c4ai-command-r-v01,"From the perspective of the model right now, after we do the second generation after the tool call to interpret the tool response, if we feed just that back into the chat history, it appears to the model that it hallucinated / generated it rather than doing a tool call.

Then when this happens a couple times the tool calling layer learns to stop calling tools and just calls respond every time and the model at that point (after a chat history where it appears to directly respond) is happy to hallucinate whatever."
Cyleux,2024-03-18 07:27:12+00:00,comment,CohereForAI/c4ai-command-r-v01,"In my mind, there is missing format template function for grounded tool use."
Cyleux,2024-03-18 23:38:16+00:00,comment,CohereForAI/c4ai-command-r-v01,Thank you
cppowboy,2024-04-18 13:07:51+00:00,comment,CohereForAI/c4ai-command-r-v01,"I have the same question, do you have any solutions?"
Noeda,2024-03-16 08:34:35+00:00,comment,CohereForAI/c4ai-command-r-v01,"Maybe answering my own question here: I thought of simple test to check this. I gave this prompt to the model:

```
<|START_OF_TURN_TOKEN|><|USER_TOKEN|>
Show me newlines.
<|END_OF_TURN_TOKEN|>
<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>
```

And then monitored the logits as it started generating. Token 2126 (= double newline `\n\n`) showed up a lot at top logits, e.g.:

![Screenshot 2024-03-16 at 1.29.42â€¯AM.png](https://cdn-uploads.huggingface.co/production/uploads/6304059b0907b9a115c61605/8WRbtXyt6PlzUSqhXI3jC.png)

If it shows up on the logits a lot when it makes sense, that tells me 2126 (double newline) was present at the training time and the model is aware of it. So I'm leaning on `llama.cpp` having the correct behavior.

If this is a correct reasoning, then this discussion becomes a bug report: This model does not tokenize string prompts entirely correctly, the same way training has done. `hello\n\nworld` would be an example string that is tokenized wrong, `[34313, 206, 206, 17080]` instead of `[34313, 2126, 17080]`. Generation seems fine.

If you are able to confirm if this reasoning is correct that would be sweet."
ahmetustun,2024-03-16 10:56:31+00:00,comment,CohereForAI/c4ai-command-r-v01,"hi @Noeda, thanks for carefully checking this. The HF tokenizer is correct. I did a quick test using this string: `hello\n\n world` (space between \n\n and world) and tokenizer returns `[5, 28339, 2126, 3845]` where `5` is bos_token. Indeed, `\n\n` is present during training, and the tokenizer encodes based on the text. Also, I double checked with [tokenizers](https://github.com/huggingface/tokenizers) implementation too: https://huggingface.co/Cohere/Command-nightly, and the tokenizer return exactly the same ids. There seems to be a small difference between [tokenizers](https://github.com/huggingface/tokenizers) BPE and llama.cpp BPE.  "
Ricepig,2024-03-16 11:53:55+00:00,comment,CohereForAI/c4ai-command-r-v01,So how to get correct results from hf tokenizers?
ahmetustun,2024-03-16 11:56:07+00:00,comment,CohereForAI/c4ai-command-r-v01,HF tokenizer is correct as i showed above.
Noeda,2024-03-16 15:35:59+00:00,comment,CohereForAI/c4ai-command-r-v01,"Awesome. Thanks for confirming quickly. Rechecked myself with this small program:

```python
#!/usr/bin/env python3

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""CohereForAI/c4ai-command-r-v01""
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

print(tokenizer.encode(""hello\n\nworld""))  # -> [5, 34313, 206, 206, 17080]
print(tokenizer.encode(""hello\n\n world"")) # -> [5, 34313, 2126, 3845]

# 5 = bos
print(repr(tokenizer.decode([206])))  # -> '\n'
print(repr(tokenizer.decode([17080])))  # -> 'world'
print(repr(tokenizer.decode([2126])))  # -> '\n\n'
print(repr(tokenizer.decode([3845])))  # -> ' world'
```

I had not seen 2126 come out of the model before for \n\n; always got 206 206 instead and thought it might be buggy since llama.cpp likes to give that and it made sense. If the tokenizer is the same as used in training, and 2126 does come out appropriately, there is no bug. Despite llama.cpp and HF disagreeing, the tokenizations in both do decode back to the same original strings. But I might take this back to `llama.cpp` to see if we should modify the tokenizer in llama.cpp when we load this model.

Thanks for the model :) I've had a lot of fun with it."
Noeda,2024-03-16 16:04:03+00:00,comment,CohereForAI/c4ai-command-r-v01,"I've opened this as an issue on `llama.cpp` side for anyone interested: https://github.com/ggerganov/llama.cpp/issues/6104

I have ETA some time next week to investigate what's off exactly."
ahmetustun,2024-03-16 16:24:56+00:00,comment,CohereForAI/c4ai-command-r-v01,"Thanks again, amazing work integrating our model to llama.cpp â¤ï¸"
ahmetustun,2024-03-14 19:51:41+00:00,comment,CohereForAI/c4ai-command-r-v01,"Hi, the pytorch reference is based on the Llama implementation which materializes this huge buffer which is not feasible for 128k context. Setting this context length as default in the config.json would cause problems.  
`causal_mask = torch.full( (config.max_position_embeddings, config.max_position_embeddings), fill_value=True, dtype=torch.bool )`"
andrewcanis,2024-03-14 20:21:24+00:00,comment,CohereForAI/c4ai-command-r-v01,"Hi @ahmetustun, I completely agree we don't want to affect the pytorch reference.
I just checked the configuration parameters for the python transformer llama they never use ""model_max_length"":
https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/configuration_llama.py#L31

My suggestion is simply to put the true context length *somewhere* in the config.json (we can create another name if you'd like).

The goal is to allow third party tools (like the GGUF converter) to correctly detect the true context length"
ahmetustun,2024-03-14 20:42:52+00:00,comment,CohereForAI/c4ai-command-r-v01,"hi @andrewcanis, I misread the config params you are suggesting. I'll do a test and add the parameter to config file if all is good!"
sarahooker,2024-04-02 16:12:10+00:00,comment,CohereForAI/c4ai-command-r-v01,"Thanks for this comment. We have run the model on the open leaderboard - you can find the results here: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

"
ybelkada,2024-03-14 13:53:03+00:00,comment,CohereForAI/c4ai-command-r-v01,Fix download issue
flymonk,2024-03-14 11:37:37+00:00,comment,CohereForAI/c4ai-command-r-v01,"Cool man!!!
"
ahmetab06,2024-03-14 16:31:06+00:00,comment,CohereForAI/c4ai-command-r-v01,"how many ram i need to run it with cpu ?
"
J22,2024-03-15 06:22:04+00:00,comment,CohereForAI/c4ai-command-r-v01,"For 8k context window (default), minimum 40GB for Q4 quantization."
sarahooker,2024-04-03 08:41:52+00:00,comment,CohereForAI/c4ai-command-r-v01,This is great @J22 -- going to close this for now but really nice addition.
ahmetustun,2024-03-13 23:56:08+00:00,comment,CohereForAI/c4ai-command-r-v01,"hey, this should be fixed now. Can you please try again?"
pseudotensor,2024-03-14 01:48:38+00:00,comment,CohereForAI/c4ai-command-r-v01,yes
iamgroot42,2024-03-28 12:24:38+00:00,comment,CohereForAI/c4ai-command-r-v01,I just got started with command-r )quantized version) and still have this issue!
sarahooker,2024-03-13 16:59:17+00:00,comment,CohereForAI/c4ai-command-r-v01,You can change the config as specified here: https://huggingface.co/CohereForAI/c4ai-command-r-v01/discussions/12. 
ahmetustun,2024-03-13 18:03:03+00:00,comment,CohereForAI/c4ai-command-r-v01,Thanks for the warning! Fixed it.
saurabhdash,2024-03-12 17:53:37+00:00,comment,CohereForAI/c4ai-command-r-v01,The FP16 model requires ~70GB of memory. There is going to be a quantized model soon that should be ~18GB.
blevlabs,2024-03-12 20:18:04+00:00,comment,CohereForAI/c4ai-command-r-v01,"@saurabhdash Awesome! Will this be supporting NVIDIA/GPU inference, in GPTQ or AWQ formats?"
WaveCut,2024-03-12 21:16:18+00:00,comment,CohereForAI/c4ai-command-r-v01,"I see no reason why it would not. Should be achievable, with slight code updates (to AWQ / GPTQ i mean)."
saurabhdash,2024-03-13 13:18:46+00:00,comment,CohereForAI/c4ai-command-r-v01,"@Blevlabs Yes, should work right out of the box like other models."
Avaruuskettu,2024-03-13 13:24:52+00:00,comment,CohereForAI/c4ai-command-r-v01,"> The FP16 model requires ~70GB of memory. There is going to be a quantized model soon that should be ~18GB.

What quantizations are you planning to release? 6bit or 8bit would probably be optimal for my purposes."
saurabhdash,2024-03-12 16:52:38+00:00,comment,CohereForAI/c4ai-command-r-v01,Does this piece of code work with the Llama models?
BramVanroy,2024-03-12 18:05:00+00:00,comment,CohereForAI/c4ai-command-r-v01,"I'm going to close this for now. Will have to do more testing as I am experiencing multiple issues:

- flash attention does not work
- using model name without tokenizer in `pipeline` does not work - tokenizer has to be manually provided
- CUDA errors

Will close this for now and report back but testing is difficult with models this size (loading to GPU in itself already takes 20 minutes or so)."
ahmetustun,2024-03-12 19:42:46+00:00,comment,CohereForAI/c4ai-command-r-v01,"hi @BramVanroy, there is a small bug that affects import checks for custom models (trust_remote_code=True). We removed the flash attention imports for now, can you try with torch.backends.cuda.enable_flash_sdp(True)?"
BramVanroy,2024-03-13 10:19:22+00:00,comment,CohereForAI/c4ai-command-r-v01,"Coming back to the original issue @sarahooker @ahmetustun: `TypeError: AttentionMaskConverter._unmask_unattended() missing 1 required positional argument: 'unmasked_value'
` was the error message. It seems that the cohere modeling file in this repo relies on the current (unreleased) master implementation of `transformers`. The required `unmasked_value` has been removed in a commit  two weeks ago (https://github.com/huggingface/transformers/commit/49204c1d37b807def930fe45f5f84abc370a7200) but this is not part of any release yet. As such, the current model cannot be used with any official release of transformers yet. Perhaps that could be clarified in the README? It currently says `pip install transformers` but that is not sufficient AFAICT. You should install from source to get this to work."
sarahooker,2024-04-03 08:45:21+00:00,comment,CohereForAI/c4ai-command-r-v01,"This should have been resolved by the new release of transformers. Can you confirm @BramVanroy ?

I'm going to close this for now, but feel free to re-open if still an issue."
saurabhdash,2024-03-12 16:47:23+00:00,comment,CohereForAI/c4ai-command-r-v01,"This implementation is based on the Llama implementation which materializes this huge buffer which would not be feasible for 128k context. The model does support 128k context with a better implementation.

`        causal_mask = torch.full(
            (config.max_position_embeddings, config.max_position_embeddings), fill_value=True, dtype=torch.bool
        )
`"
WaveCut,2024-03-12 21:19:22+00:00,comment,CohereForAI/c4ai-command-r-v01,Maintaining context length defaults low enough to prevent end users from experiencing OOM right out of the box is generally accepted as an unwritten rule in the HF community.
rombodawg,2024-03-12 16:31:59+00:00,comment,CohereForAI/c4ai-command-r-v01,"Yes i second this, we need gguf"
p1ld7a,2024-03-12 17:18:40+00:00,comment,CohereForAI/c4ai-command-r-v01,Looking for the gguf too.
p1ld7a,2024-03-12 17:19:01+00:00,comment,CohereForAI/c4ai-command-r-v01,"In the meantime, how could I test it? With Ollama?"
WaveCut,2024-03-12 21:09:11+00:00,comment,CohereForAI/c4ai-command-r-v01,"read ollama docs on how to create new model
"
WaveCut,2024-03-12 21:14:24+00:00,comment,CohereForAI/c4ai-command-r-v01,I imagine that @TheBloke is already firing up the stoves.
p1ld7a,2024-03-12 21:15:22+00:00,comment,CohereForAI/c4ai-command-r-v01,"Apparently, it won't work with Ollama right now (from their Discord)."
condrove10,2024-03-12 22:06:49+00:00,comment,CohereForAI/c4ai-command-r-v01,"> I imagine that @TheBloke is already firing up the stoves.

He actually stopped uploading GGUF models like 1 month ago.

Meanwhile I am trying to make it work with [HQQ](https://github.com/mobiusml/hqq)"
andrewcanis,2024-03-13 06:19:29+00:00,comment,CohereForAI/c4ai-command-r-v01,"I was able to convert the safetensors to a GGUF model. I'm still working on adding inference support to llama.cpp.
See PR: https://github.com/ggerganov/llama.cpp/pull/6033"
ahmetab06,2024-03-14 06:18:08+00:00,comment,CohereForAI/c4ai-command-r-v01,where is the Gguff format this model ?
andrewcanis,2024-03-14 19:14:09+00:00,comment,CohereForAI/c4ai-command-r-v01,Starting to add the GGUF files here: https://huggingface.co/andrewcanis/c4ai-command-r-v01-GGUF
MaziyarPanahi,2024-03-15 12:35:35+00:00,comment,CohereForAI/c4ai-command-r-v01,"> Starting to add the GGUF files here: https://huggingface.co/andrewcanis/c4ai-command-r-v01-GGUF

Great work Andrew! Both on the GGUF models and specially on the PR you made in Llama.cpp. 
Thank you"
FrankWu,2024-03-18 09:59:51+00:00,comment,CohereForAI/c4ai-command-r-v01,Is there GPTQ or AWQ version?
saurabhdash,2024-03-12 15:07:38+00:00,comment,CohereForAI/c4ai-command-r-v01,"Are you able to load the fp16 model?
`model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True,torch_dtype=torch.float16).to(""cuda"")`
"
blevlabs,2024-03-12 20:16:34+00:00,comment,CohereForAI/c4ai-command-r-v01,"@saurabhdash Thanks, I will have to try this when I can get access to a machine that can run this model."
patverga,2024-03-12 13:04:04+00:00,comment,CohereForAI/c4ai-command-r-v01,in regards to https://huggingface.co/CohereForAI/c4ai-command-r-v01/discussions/6#65efe0c387bf55c1da41c384
sarahooker,2024-03-12 20:00:57+00:00,comment,CohereForAI/c4ai-command-r-v01,"This model card is for the instruction finetuned and  preference trained model. We typically release a new model card for each release, so I would follow our organization to keep an eye on future releases. Thanks for the support!"
sarahooker,2024-03-12 15:02:41+00:00,comment,CohereForAI/c4ai-command-r-v01,The docs have been updated. Thanks for catching!
ehartford,2024-03-11 23:11:15+00:00,comment,CohereForAI/c4ai-command-r-v01,Please consider Apache-2.0 or MIT license
AidanP,2024-03-13 14:38:21+00:00,comment,CohereForAI/c4ai-command-r-v01,"Hi folks. Weâ€™ve released these weights to enable other researchers and developers to experiment, explore and build - but specifically for non-commercial purposes. We based our license on the CC-BY-NC license, as Creative Commons licenses are already widely familiar, and thereâ€™s some existing precedent for use in ML models.

When we release a model under another license, like the Aya model which is Apache 2.0, we will create a new model card. You can follow the organization to see when we release new models.
"
prudant,2024-04-12 18:55:28+00:00,comment,CohereForAI/c4ai-command-r-v01,"when you say: This model is governed by a CC-BY-NC License with an acceptable use addendum

what is: acceptable use addendum part?"
djstrong,2024-05-08 11:01:19+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,I am not sure if the original script uses gsm8k. If you ask for math dataset then there is no such in Polish but we are working creating on it. Does it need to be a math dataset? I haven't read about this method but if they are using the dataset only for measuring perplexity then I would go for some mix of Polish datasets.
TeeZee,2024-05-08 11:10:07+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,"```python
def calculate_model_perplexity(self, datasets=['gsm8k'], seqlen=32, use_cuda_graph=False, use_flash_attn=False):
        model = self.model
        model_str = self.model_name
        acc_loss = 0.0
        total_samples = 0
```

They are going after perplexity, I was also thinking about some mix, do you have some pointers - what would be THE best ;)"
djstrong,2024-05-08 11:20:34+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,The math dataset is only in rmt_laser_snr_math*.py files but there is other with different datasets: https://github.com/cognitivecomputations/laserRMT/blob/main/rmt_laser_snr.py
TeeZee,2024-05-08 11:27:09+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,"You're right, I was focusing on math for my particular use case ;). So. ill start with wikitext full and then with a pl subset .Thanks!"
djstrong,2024-05-08 11:34:29+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,There is NKJP corpus (which is more balanced than wiki) but it is only 1M tokens.
TeeZee,2024-05-08 11:38:41+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,"OK, found it, thanks.It seems there is some experimentation to be done to adapt LASER for PL LLms, I'll keep you posted."
leaderboard-pr-bot,2024-04-15 16:54:13+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,This comment has been hidden
djstrong,2024-04-11 19:40:19+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,It should work with TGI in optimal way. The model architecture of Bielik is on the list.
jarekmor,2024-04-11 20:40:47+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,"> It should work with TGI in optimal way. The model architecture of Bielik is on the list.

Thank you for the response.
Yes, I know that the Bielik is based on the Mistral architecture. I should admit I meant compatibility with TGI Messages API which is compatibile with OpenAI Chat Completion API (https://huggingface.co/docs/text-generation-inference/messages_api).

When I run the following script:

```
from openai import OpenAI

# init the client but point it to TGI
client = OpenAI(
    base_url=""http://localhost:8080/v1"",
    api_key=""-""
)
chat_completion = client.chat.completions.create(
    model=""tgi"",
    messages=[
        {""role"": ""user"", ""content"": ""What is deep learning?""}
    ],
    stream=False
)
print(chat_completion.choices[0].message.content)
```
 I got the following traceback :
```
Traceback (most recent call last):
  File ""/home/hp/Python_Projects/HF_OpenAI/test.py"", line 9, in <module>
    chat_completion = client.chat.completions.create(
  File ""/home/hp/Python_Projects/HF_OpenAI/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py"", line 275, in wrapper
    return func(*args, **kwargs)
  File ""/home/hp/Python_Projects/HF_OpenAI/.venv/lib/python3.10/site-packages/openai/resources/chat/completions.py"", line 667, in create
    return self._post(
  File ""/home/hp/Python_Projects/HF_OpenAI/.venv/lib/python3.10/site-packages/openai/_base_client.py"", line 1233, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""/home/hp/Python_Projects/HF_OpenAI/.venv/lib/python3.10/site-packages/openai/_base_client.py"", line 922, in request
    return self._request(
  File ""/home/hp/Python_Projects/HF_OpenAI/.venv/lib/python3.10/site-packages/openai/_base_client.py"", line 1013, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.UnprocessableEntityError: Error code: 422 - {'error': 'Template error: invalid operation: object has no method named strip (in <string>:1)', 'error_type': 'template_error'}
```
The Mistral model works without any issues."
djstrong,2024-04-11 07:07:35+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,"Hi, I have added your model to the Polish leaderboard: https://huggingface.co/spaces/speakleash/open_pl_llm_leaderboard"
chrisociepa,2024-04-05 18:39:59+00:00,comment,speakleash/Bielik-7B-Instruct-v0.1,"No, it is intentional. The Instruct version was trained on the OpenHermes dataset, some components of which do not have a commercial license. Therefore, for safety, we also published this model under such a license."
bartowski,2024-05-29 06:25:38+00:00,comment,bartowski/aya-23-8B-GGUF,Can you confirm what hardware it's running on and/or try a K quant?
ThoilGoyang,2024-05-29 06:30:52+00:00,comment,bartowski/aya-23-8B-GGUF,"i'm running on gcp workbench n1 machine, t4 gpu. also i already tried the Q8_0. 
if i load the other models such as mistral, llama3, llama2 everything is alright.
"
davidasulin,2024-05-29 10:46:20+00:00,comment,bartowski/aya-23-8B-GGUF,"@bartowski I'm also having this issue using LM Studio (also based on llama.cpp). I get: `""llama.cpp error: 'error loading model vocabulary: unknown pre-tokenizer type: 'command-r''""
`.
I have Mac M1 with 32GB RAM."
davidasulin,2024-05-29 11:23:17+00:00,comment,bartowski/aya-23-8B-GGUF,I updated LM Studio version to 0.2.24 and the issue resolved! â˜ºï¸
ThoilGoyang,2024-05-29 11:53:35+00:00,comment,bartowski/aya-23-8B-GGUF,I can use it on LM Studio but i cant use it in Llamacpp in python.
bartowski,2024-05-29 13:10:58+00:00,comment,bartowski/aya-23-8B-GGUF,"Yeah so that implies to me llamacpp python is somehow still on an older unsupported version.. but the most recent is from 5 days ago, well after support was added.."
ThoilGoyang,2024-05-31 07:58:04+00:00,comment,bartowski/aya-23-8B-GGUF,So it is fixed by reinstalling the lib using GPU.
bartowski,2024-05-25 16:23:04+00:00,comment,bartowski/aya-23-8B-GGUF,"Geez that's quite the issue to lose the python kernel.. I have no idea off the top of my head but hopefully someone else will see this D:

I can try running it myself in a bit. Are you on latest llamacpp python package?"
bartowski,2024-05-25 10:09:29+00:00,comment,bartowski/aya-23-8B-GGUF,Share with the class? :D
bartowski,2024-05-23 21:56:16+00:00,comment,bartowski/aya-23-8B-GGUF,"This requires version 0.2.23 or newer

If you're using ROCm, you should be getting an update within the next couple days which will work"
cm2435cm2435,2024-04-29 15:32:34+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,Nope.
jithinmukundan,2024-04-29 15:34:17+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,What could be the minimum VRAM required?
Light4Bear,2024-04-11 19:24:11+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,It was quantized with [Aphrodite-engine](https://github.com/PygmalionAI/aphrodite-engine) in mind but it should work on vLLM too.
wawoshashi,2024-04-13 07:13:31+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"```(RayWorkerVllm pid=16013) INFO 04-13 15:05:30 selector.py:25] Using XFormers backend. [repeated 2x across cluster]
Traceback (most recent call last):
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py"", line 157, in <module>
    engine = AsyncLLMEngine.from_engine_args(
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py"", line 348, in from_engine_args
    engine = cls(
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py"", line 311, in __init__
    self.engine = self._init_engine(*args, **kwargs)
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/async_llm_engine.py"", line 422, in _init_engine
    return engine_class(*args, **kwargs)
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/llm_engine.py"", line 110, in __init__
    self.model_executor = executor_class(model_config, cache_config,
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/executor/ray_gpu_executor.py"", line 62, in __init__
    self._init_workers_ray(placement_group)
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/executor/ray_gpu_executor.py"", line 192, in _init_workers_ray
    self._run_workers(
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/executor/ray_gpu_executor.py"", line 324, in _run_workers
    driver_worker_output = getattr(self.driver_worker,
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/worker.py"", line 107, in load_model
    self.model_runner.load_model()
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/model_runner.py"", line 95, in load_model
    self.model = get_model(
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/model_loader.py"", line 101, in get_model
    model.load_weights(model_config.model, model_config.download_dir,
  File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/commandr.py"", line 325, in load_weights
    param = params_dict[name]
KeyError: 'model.layers.42.mlp.down_proj.bias'
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44] Error executing method load_model. This might cause deadlock in distributed execution.
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44] Traceback (most recent call last):
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]   File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/engine/ray_utils.py"", line 37, in execute_method
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]     return executor(*args, **kwargs)
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]   File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/worker.py"", line 107, in load_model
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]     self.model_runner.load_model()
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]   File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/worker/model_runner.py"", line 95, in load_model
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]     self.model = get_model(
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]   File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/model_loader.py"", line 101, in get_model
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]     model.load_weights(model_config.model, model_config.download_dir,
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]   File ""/home/czb/miniconda3/envs/vllm/lib/python3.9/site-packages/vllm/model_executor/models/commandr.py"", line 325, in load_weights
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44]     param = params_dict[name]
(RayWorkerVllm pid=16133) ERROR 04-13 15:05:36 ray_utils.py:44] KeyError: 'model.layers.42.mlp.down_proj.bias'
(RayWorkerVllm pid=16133) INFO 04-13 15:05:31 pynccl_utils.py:45] vLLM is using nccl==2.18.1 [repeated 2x across cluster]
(RayWorkerVllm pid=15910) WARNING 04-13 15:05:33 custom_all_reduce.py:45] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly. [repeated 2x across cluster]
(RayWorkerVllm pid=16133) INFO 04-13 15:05:35 weight_utils.py:177] Using model weights format ['*.safetensors'] [repeated 2x across cluster]``` "
Light4Bear,2024-04-13 09:01:55+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,You need latest vLLM for this to work.
wawoshashi,2024-04-14 12:09:59+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"> You need the latest version of VLLM for this to work.

Okay. It works"
ordkill,2024-04-15 19:51:10+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"I have installed vLLM==0.4.0.post and I have the same issue, how did you manage to solve it @wawoshashi?"
wawoshashi,2024-04-15 22:23:20+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"> I have installed vLLM==0.4.0.post and I have the same issue, how did you manage to solve it @wawoshashi?

@ordkill You need to `pip install -e . ` from src in vllm main branch"
Light4Bear,2024-04-11 19:23:26+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"4bit, group size 128, you can find them in config.json"
alpindale,2024-04-06 23:20:06+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,Please see #1
MLDataScientist,2024-04-05 19:09:27+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"By the way, these are the errors I get with different loaders:

AutoGPTQ_loader: TypeError: cohere isn't supported yet.

Transformers (with auto-devices): CUDA out of memory. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation. See documentation for Memory Management (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
"
MLDataScientist,2024-04-06 04:04:51+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,@alpindale let me know. Thanks!
alpindale,2024-04-06 14:05:32+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"Hi! Sorry for the delayed response. This was quantized with a custom script. Inference is currently possible with Aphrodite Engine's [dev](https://github.com/PygmalionAI/aphrodite-engine/tree/dev) branch, but no CPU offloading is supported yet. You can find the quantization script [here](http://0x0.st/XiMn.py)."
Light4Bear,2024-04-06 19:25:09+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"To run it in text-generation-webui, you need to install the latest `transformers` package, and choose Transformers loader in webui."
davideuler,2024-04-10 00:24:37+00:00,comment,alpindale/c4ai-command-r-plus-GPTQ,"> Hi! Sorry for the delayed response. This was quantized with a custom script. Inference is currently possible with Aphrodite Engine's [dev](https://github.com/PygmalionAI/aphrodite-engine/tree/dev) branch, but no CPU offloading is supported yet. You can find the quantization script [here](http://0x0.st/XiMn.py).

Thanks for the great job,  this revision on the dev branch of aphrodite works for Command R Plus: 95faf27d2b39eb34ed59edadcfe24121412decaa

I run it on A100."
ykhwang,2024-02-07 05:39:50+00:00,comment,42dot/42dot_LLM-PLM-1.3B,"ì•ˆë…•í•˜ì„¸ìš”, ì´ìŠˆ ì œë³´í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.

SFT ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, PLM í† í¬ë‚˜ì´ì €ë„ llama.cpp ì—ì„œ ë¬¸ì œê°€ ì—†ë„ë¡ ì—…ë°ì´íŠ¸í•˜ì˜€ìŠµë‹ˆë‹¤.

https://huggingface.co/42dot/42dot_LLM-PLM-1.3B/commit/53c8d7b3d4ffb59f190f9cce2b9c24dd442eff27

---
Hello, thanks for your feedback.

As well as the SFT model, we've just updated the PLM tokenizer for resolving issues with llama.cpp.

https://huggingface.co/42dot/42dot_LLM-PLM-1.3B/commit/53c8d7b3d4ffb59f190f9cce2b9c24dd442eff27
"
ykhwang,2023-09-14 08:40:59+00:00,comment,42dot/42dot_LLM-PLM-1.3B,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
alpindale,2023-09-21 19:10:15+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,"Try replacing your current configs with the updated `config.json` and `generation_config.json`. Looks like the cache was disabled, which usually leads to extreme slowdowns."
TZ20,2023-09-22 02:08:28+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,"Thanks, seemed to do the trick"
odellus,2023-08-24 03:16:29+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,Reading [the paper](https://arxiv.org/abs/2308.07317) it looks like yeah they just apply the lora adapter to merge.
bleysg,2023-08-17 09:55:07+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,Can you provide more details on the execution environment? Which prompt format are you using? Weâ€™ve only tested with the one from OpenOrca model.
cvdbdo,2023-08-17 13:06:51+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,"I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B fine tuned on a specialized instruct dataset. The behaviour described is common to the base and fine tuned models. It happens with or without quantization (4 & 8 bits). I load them with a simple AutoModelForCausalLM.
 
generation_config = GenerationConfig(
    temperature=.0001,
    top_p=0,
    top_k=0,
    repetition_penalty=1,
)

The problem is mainly a performance one because the model keeps generating after the eos token."
darshan12,2023-09-07 13:48:54+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,"How to fine tune this model can anyone please connect with me and help me, I want to learn how to fine tune this data?
Email: darshankholakiya12@gmail.com
LinkedIn: https://www.linkedin.com/in/darshankholakiya/"
deleted,2023-09-12 04:09:31+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,"> I use Alpaca Instruct format, with a Open-Orca/OpenOrca-Platypus2-13B fine tuned on a specialized instruct dataset. The behaviour described is common to the base and fine tuned models. It happens with or without quantization (4 & 8 bits). I load them with a simple AutoModelForCausalLM.
>  
> generation_config = GenerationConfig(
>     temperature=.0001,
>     top_p=0,
>     top_k=0,
>     repetition_penalty=1,
> )
> 
> The problem is mainly a performance one because the model keeps generating after the eos token.

Set the end to turn token as stop token in GenerationConfig. If you don't know the token id then simple encode the <|end_of_turn|> using the tokenizer you will get the id then set stop token to this id."
apasi,2023-10-12 18:29:45+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,Same here. I see inconsistent tokens for end of turn. The model responds well but had various versions of <|end_of_turn|> often with few missing preceeding or end characters.
JohanAR,2023-10-12 18:37:39+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,"It looks like this model has <|end_of_turn|> added as a special token, and I have recently learned that this might not be supported by exllama and llama.cpp. I don't have a copy of this model around, but if you're using ooba's webui try using one of the model loaders with _HF suffix."
bleysg,2023-08-14 03:45:41+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,We've mostly tested with the prompt format from the OpenOrca side and haven't had any issues.
bleysg,2023-08-18 04:43:32+00:00,comment,Open-Orca/OpenOrca-Platypus2-13B,"We've only tested with the template from the OpenOrcaxOpenChat-Preview2-13B base model.

```
# Multi-turn V1 Llama 2
tokenize(""User: Hello<|end_of_turn|>Assistant: Hi<|end_of_turn|>User: How are you today?<|end_of_turn|>Assistant:"")
# Result: [1, 4911, 29901, 15043, 32000, 4007, 22137, 29901, 6324, 32000, 4911, 29901, 1128, 526, 366, 9826, 29973, 32000, 4007, 22137, 29901]
```

System prompt is just prepended to the first message with a space before ""User: "" ...

```""You are OrcaPlaty, an LLM trained by Alignment Lab AI and garage-bAInd. Write out your thinking step by step before coming to a conclusion to be sure you get the right answer! User: Hello there<|end_of_turn|>Assistant: Hi, nice to meet you.<|end_of_turn|>User: What's new?<|end_of_turn|>Assistant: ""```"
dranger003,2024-05-13 19:38:55+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@Zibri Yeah sure, did you take a look at any of those? https://huggingface.co/models?sort=created&search=gguf+wizard"
Zibri,2024-05-13 23:01:14+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,Yes. I didn't find one with a small footprint.
Zibri,2024-05-13 23:01:59+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,I am talking about c4ai...not wizardlm
dranger003,2024-05-03 14:14:37+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@ggnoy  I would like to, yes."
qaraleza,2024-05-05 17:41:52+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,Thanks!
dranger003,2024-04-18 21:44:30+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@CISCai Thanks for your contribution! I rented a server to create all the quants, which I no longer have so this means I would have to re-download all the quants, update them all (first split where we have chunks) then re-upload everything to HF. Would there be a way to create a new dummy GGUF that has just the metadata maybe?"
CISCai,2024-04-19 08:39:01+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Unfortunately not. :(

Hopefully some day it can be the norm to just have a small GGUF [containing only metadata](https://github.com/ggerganov/llama.cpp/issues/6463) as the first split."
sealad886,2024-04-20 02:03:12+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"For those of us at home, how can we run that `gguf-new-metadata.py` script with the correct settings? I'd be willing to re-upload what I've downloaded myself."
CISCai,2024-04-20 08:50:54+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@sealad886 Great! :)

If you already have a working Python3 environment with all the dependencies you can just run the script like this:
```bash
python gguf-new-metadata.py input.gguf output.gguf --chat-template-config tokenizer_config.json
```

If you don't have an environment set up you will need to do that first:
```bash
python3 -m venv my-venv
source my-venv/bin/activate
pip install -r requirements.txt
```"
sealad886,2024-04-21 02:29:24+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Turns out it's actually faster for me to re-quantize everything than it is to download these on my internet connection. I'm using your `imatrix.dat` file, and I'll do a couple of hashes on several of the files that I did download to make sure I'm not just creating completely new files....but yeah, I'll upload those soon ish. "
CISCai,2024-05-06 10:53:54+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,Thank you for updating (and the BPE PR). :)
zedmango,2024-04-17 10:13:42+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"These other bugs are possibly related or caused by this:

https://huggingface.co/dranger003/c4ai-command-r-plus-iMat.GGUF/discussions/12

https://huggingface.co/dranger003/c4ai-command-r-plus-iMat.GGUF/discussions/11"
dranger003,2024-04-17 10:58:24+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@zedmango You need to use `gguf-split`, do you know how lm studio is merging?"
zedmango,2024-04-17 11:00:07+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"> @zedmango You need to use `gguf-split`, do you know how lm studio is merging?

That was me on the command line, not lm studio. I used `gguf-split --merge`. See what I put above:

``$ ../../../../../../LLM/llama-b2687-bin-win-cuda-cu11.7.1-x64/gguf-split.exe --merge ggml-c4ai-command-r-plus-104b-iq4_xs
-00001-of-00002.gguf ggml-c4ai-command-r-plus-104b-iq4_xs-00002-of-00002.gguf``"
zedmango,2024-04-17 11:01:14+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,I entered that command into the command line and got the above error. Using `gguf-split --merge`.
dranger003,2024-04-17 12:55:10+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I don't think you're supposed to specify all the parts, just the first one but I can't test this right now. Also, which commit hash are you on? I think they may recently have changed how `gguf-split` works."
zedmango,2024-04-17 17:35:27+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Thank you! That worked:

```
j@LAPTOP-QD5K4715:/mnt/c/Users/J/.cache/lm-studio/models/dranger003/c4ai-command-r-plus-iMat.GGUF/join

$ ../../../../../../LLM/llama-b2687-bin-win-cuda-cu11.7.1-x64/gguf-split.exe --merge ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf ggml-c4ai-command-r-plus-104b-iq4_xs.gguf

gguf_merge: ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf -> ggml-c4ai-command-r-plus-104b-iq4_xs.gguf
gguf_merge: reading metadata ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf done
gguf_merge: reading metadata ggml-c4ai-command-r-plus-104b-iq4_xs-00002-of-00002.gguf done
gguf_merge: writing tensors ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf done
gguf_merge: writing tensors ggml-c4ai-command-r-plus-104b-iq4_xs-00002-of-00002.gguf done
gguf_merge: ggml-c4ai-command-r-plus-104b-iq4_xs.gguf merged from 2 split with 642 tensors.
```

"
zedmango,2024-04-17 17:36:38+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Oops, left out the command  - here it is

```j@LAPTOP:/mnt/c/Users/J/.cache/lm-studio/models/dranger003/c4ai-command-r-plus-iMat.GGUF/join$ ../../../../../../LLM/llama-b2687-bin-win-cuda-cu11.7.1-x64/gguf-split.exe --merge ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf ggml-c4ai-command-r-plus-104b-iq4_xs.gguf
gguf_merge: ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf -> ggml-c4ai-command-r-plus-104b-iq4_xs.gguf
gguf_merge: reading metadata ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf done
gguf_merge: reading metadata ggml-c4ai-command-r-plus-104b-iq4_xs-00002-of-00002.gguf done
gguf_merge: writing tensors ggml-c4ai-command-r-plus-104b-iq4_xs-00001-of-00002.gguf done
gguf_merge: writing tensors ggml-c4ai-command-r-plus-104b-iq4_xs-00002-of-00002.gguf done
gguf_merge: ggml-c4ai-command-r-plus-104b-iq4_xs.gguf merged from 2 split with 642 tensors.```

Anyway, thank you!"
dranger003,2024-04-15 16:52:25+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@jukofyork Yes, check this model page I posted all the PPL results, if you can please provide your PPL results to compare."
jukofyork,2024-04-15 16:59:11+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,@dranger003 Do you have a bash script or something you used to make these with? If so can you share it and I will just leave it to run overnight.
jukofyork,2024-04-15 17:10:23+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"This model seem pretty clueless at coding I think :/ People on reddit seem to love it and say it's good at other stuff though, but not impressed so far..."
jukofyork,2024-04-15 17:18:28+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Not very good at writing either:

---

### USER
Write me the opening chapter of a Grimdark trilogy in the style of Joe Abercrombie and Rob J Hayes. Use third person personal and feature internal monologues of the characters. The POV character for chapter 1 is a cultist who has just escaped his cult. He is dressed in dirty yellow robes and his only possession is a mysterious small mirror. The story starts with him arriving at an apparently deserted ghost town in the edge of the desert.

### ASSISTANT
Chapter 1: Ghosts of the Past

The sun was a blistering ball of fire in the cloudless sky, beating down on the dusty, deserted streets of the ghost town. Sand had gathered in the corners of the abandoned buildings, and the air was thick with the scent of decay and neglect.

Brother Kael, a young man with a gaunt face and wild, fearful eyes, stumbled into the town, his dirty yellow robes hanging loosely on his thin frame. He clutched a small mirror in one hand, its surface covered by a layer of grime, its purpose unknown even to him. He had fled his cult, a secret order that worshiped a long-forgotten god of chaos and destruction, and now he was alone, hungry, and terrified.

*What have I done?* Kael thought, his heart pounding in his chest. *I've run from the only life I knew, and now I'm as good as dead. They'll find me, and when they do, the punishment will be worse than death.*

Kael's eyes darted nervously around the town, half-expecting to see his former brethren lurking in the shadows, ready to drag him back to face the High Priest's wrath. But the town seemed truly abandoned, a forgotten relic of a bygone era. The buildings, once grand and imposing, were now little more than crumbling ruins, their wooden beams weathered and rotten.

*Perhaps I can rest here,* Kael thought, his exhaustion overcoming his fear for a moment. *I'll find shelter, something to eat, and then I'll keep moving. They'll never expect me to come here, of all places.*

As he made his way further into the town, Kael's footsteps echoed eerily in the empty streets. He passed a dilapidated tavern, its sign creaking in the slight breeze, and peered inside. The place was deserted, the tables and chairs overturned and covered in a thick layer of dust.

*Looks like this town hasn't seen a living soul in years,* Kael thought with a shudder. *Might as well be a tomb.*

He continued on, his eyes scanning the buildings for any signs of life, but finding none. The silence was oppressive, and Kael found himself holding his breath, as if expecting some terrible monster to leap out at him from the shadows.

Finally, Kael reached the town's center, a small square surrounded by crumbling shops and homes. In the middle stood a weathered statue, its features eroded by time and sand. Kael approached it warily, his eyes narrowing as he tried to make out the figure it depicted.

""Who are you supposed to be?"" he whispered, running a hand over the statue's weathered face. ""Some long-forgotten hero, I suppose. Well, you won't be saving me from my fate.""

A sudden noise startled Kael, and he spun around, his heart racing. But there was nothing, only the silent, watchful buildings and the relentless sun.

*I'm losing my mind,* he thought, a bitter laugh escaping his lips. *How fitting for a traitor and a coward.*

Kael lowered himself to the ground, leaning his back against the statue's base. He closed his eyes, the mirror still clutched tightly in his hand. Despite his fear and exhaustion, sleep soon overtook him, and he slipped into a fitful slumber, haunted by dreams of his cult's dark rituals and the blood-soaked altar of their forgotten god.

As the sun began to set, painting the sky in hues of crimson and gold, Kael awoke with a start. For a moment, he forgot where he was, disoriented by the strange surroundings. Then, memory came rushing back, and he scrambled to his feet, his eyes darting around anxiously.

*I must keep moving,* he thought, his gaze falling on the mirror in his hand. *But where? And how far will I have to run before I'm truly safe?*

Little did Kael know that his journey had only just begun, and that the mirror he carried held a power that would change his life forever. In the desolate ghost town, under the watchful gaze of forgotten heroes, Kael's path would lead him down a dark and brutal road, where survival meant embracing the very darkness he had fled.

To be continued...

--

`Goliath-120B` writes ***way*** better than this... :/

Sadly I think this is a hard-pass for me, but still happy to run the perplexity tests before deleting it if you want?"
dranger003,2024-04-15 17:19:31+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I don't have a script, no. I still run everything manually by hand like the old days... maybe its time to use AI."
jukofyork,2024-04-15 17:37:43+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"> I don't have a script, no. I still run everything manually by hand like the old days... maybe its time to use AI.

No problem: `deepseek-coder-33b` is very good at making bash scripts!

I'll have a look later and see if I can come up with something. I'll just start with the lowest quants to see if there is a big difference first."
jukofyork,2024-04-15 17:46:08+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"OK, I'm going to eat my words - this thing I ***really*** good at writing if you keep refining it:

---

Chapter 1: Whispers of Entropy

The sun, a merciless tyrant, hung low in the cloudless sky, casting its scorching rays upon the ghost town that lay forgotten at the edge of the desert. Sand, like the remnants of a long-lost civilization, had invaded the streets and buildings, transforming them into ruins that whispered tales of decay and despair.

Brother Kael, his yellow robes tattered and stained with the dust of his journey, staggered into this desolate place, an arrow protruding from his shoulder. His eyes, wild and feverish, darted anxiously between the abandoned structures, seeking refuge from unseen pursuers. The cult he had fled, the Order of the Shattered Veil, and their dark god, Malachi, would not let him go so easily.

*Fools, Kael thought, his breath coming in ragged gasps. I should've known they'd send their hounds after me. But I, Kael, the chosen vessel of Malachi, will not be taken so easily.*

In his hand, he clutched a small, oval-shaped mirror, its surface shimmering like oil, reflecting the harsh light of the desert sun. It was a gift from the high priest of the Order, who had recognized the darkness within Kael and sought to harness its power. The mirror, they said, held the key to unlocking the true nature of entropy.

As Kael ventured deeper into the ghost town, his footsteps echoed hollowly against the weathered walls. The buildings, once majestic, now stood in ruins, their wooden beams cracked and their roofs caved in. It was as if the town itself mirrored the chaos and madness that dwelled within him.

His gaze fell upon a crumbling fountain in the town square, its once-clear water now a stagnant pool, filled with the bones of small creatures that had come to drink and found only death. Kael's eyes widened as he approached, his reflection distorted in the murky water.

""The mirror,"" he whispered, his voice hoarse and strained. ""It shows me the truth. The world in its raw, chaotic state.""

The mirror had been his constant companion, a reminder of the power he wielded. It had shown him visions of destruction and chaos, and he had embraced them as a prophet of Malachi. But now, it was his only solace, a connection to the god he had once served.

A sudden movement caught Kael's attention, and he spun around, his heart hammering in his chest. But there was nothing, only the silent, watchful town, its secrets buried beneath the sand.

As he continued his journey, his mind wandered back to the night he had fled the Order. It had been a night of blood and madness, a ritual gone awry. Kael closed his eyes, the memory assaulting his senses. He saw the horrified faces of his brethren, their robes soaked in blood, their screams echoing in the sacred chamber. And in the center of it all, he stood, the mirror in his hand, a dark power coursing through his veins.

A shudder ran down Kael's spine as he pushed the memory away. He knew he could never escape his past, but he would not let it define his future. Malachi had chosen him, and he would embrace that destiny, no matter the cost.

His gaze fell upon a dilapidated tavern, its wooden sign creaking mournfully in the desert breeze. With a cautious step, he entered, his eyes adjusting to the dim interior. The room was in disarray, the tables and chairs overturned, their legs broken, as if a violent struggle had taken place long ago.

And then, he saw him. A figure, shrouded in a golden robe, his face hidden behind a mask of pure gold that revealed only a pair of piercing eyes. He sat in the shadows, watching, waiting.

Kael froze, his breath catching in his throat. This mysterious stranger seemed to exude an aura of power and knowledge.

""You seek refuge, brother,"" the figure said, his voice like the soft hiss of the desert wind. ""But you cannot escape your past, nor the god that dwells within you.""

Kael's eyes widened, his heart pounding. ""Who are you?"" he asked, his voice laced with suspicion. ""How do you know my thoughts?""

The figure rose, his golden robe shimmering in the dim light. ""I am many things, brother. A traveler, a seeker of knowledge, and perhaps, an ally. I, too, have walked the path of chaos and heard the whispers of entropy.""

Kael's gaze flicked to the mirror in his hand, its surface shimmering in response to the man's words. ""What do you want with me?"" he asked, his voice laced with defiance.

The figure took a step forward, his eyes never leaving Kael's. ""I seek understanding, brother. The mirror you hold is but a fragment of a greater power. Together, we might unlock its true potential.""

Kael's grip tightened on the mirror, his fingers leaving smudges on its shimmering surface. ""I will not give it up,"" he snarled, taking a step back. ""It is my connection to Malachi, my birthright.""

The figure in gold chuckled, the sound echoing eerily in the empty tavern. ""Then perhaps, brother, we shall journey together. For I sense a darkness within you, a power that yearns to be unleashed. And I, for one, am intrigued.""

To be continued... 

---

Maybe it's a keeper after all! :D"
jukofyork,2024-04-15 18:35:24+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I've set off the FP16 perplexity run so can double check vs your results to start with: it's around 3.5 hours, but the quants should be much quicker as should be able to fit anything < Q8 in VRAM. "
jukofyork,2024-04-15 21:56:49+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"> I've set off the FP16 perplexity run so can double check vs your results to start with: it's around 3.5 hours, but the quants should be much quicker as should be able to fit anything < Q8 in VRAM.

```
Final estimate: PPL = 4.3845 +/- 0.02468
```

Matches exactly, so will try some more now."
jukofyork,2024-04-15 23:02:48+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"| Quantization | Model size (GiB) | Perplexity (wiki.test) | Delta (FP16) |
| -- | -- | -- | -- |
| IQ4_XS | 52.34 | 4.4428 +/- 0.02508 | 1.33% |
| FP16 | 193.38 | 4.3845 +/- 0.02468 | - |

| Quantization | Model size (GiB) | Perplexity (groups_merged.txt) | Delta (FP16) |
| -- | -- | -- | -- |
| IQ4_XS | 52.33 | 4.4685 +/- 0.02526 | 1.92% |
| Q4_K_M | 58.43 | 4.4578 +/- 0.02523  | 1.67% |
| Q5_K_M | 68.56 |  4.4056 +/- 0.02481 | 0.48% |
| FP16 | 193.38 | 4.3845 +/- 0.02468 | - |

`4.4428 + 0.02508 = 4.46788`
`4.4685 - 0.02526 = 4.44324`

It's just outside the 95% in-sample confidence interval, but statistically there is very significant data-leakage between `wiki.train.raw` and `wiki.test.raw`:

# wiki.train.raw

```

 = Valkyria Chronicles III = 
 
 SenjÅ no Valkyria 3 : Unrecorded Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the "" Nameless "" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit "" Calamaty Raven "" . 
 The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . 
 It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . 
 
 = = Gameplay = = 
 
 As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . 
 The game 's battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific "" Potentials "" , skills unique to each character . They are divided into "" Personal Potential "" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and "" Battle Potentials "" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique "" Masters Table "" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate "" Direct Command "" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her "" Valkyria Form "" and become invincible , while Imca can target multiple enemy units with her heavy weapon . 
 Troops are divided into five classes : Scouts , Shocktroopers , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . 
 
 = = Plot = = 
```

# wiki.test.raw
```
 
 = Robert Boulter = 
 
 Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as "" Craig "" in the episode "" Teddy 's Story "" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . 
 In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as "" Kieron Fletcher "" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . 
 
 = = Career = = 
 
 
 = = = 2000 â€“ 2005 = = = 
 
 In 2000 Boulter had a guest @-@ starring role on the television series The Bill ; he portrayed "" Scott Parry "" in the episode , "" In Safe Hands "" . Boulter starred as "" Scott "" in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . A review of Boulter 's performance in The Independent on Sunday described him as "" horribly menacing "" in the role , and he received critical reviews in The Herald , and Evening Standard . He appeared in the television series Judge John Deed in 2002 as "" Addem Armitage "" in the episode "" Political Expediency "" , and had a role as a different character "" Toby Steele "" on The Bill . 
 He had a recurring role in 2003 on two episodes of The Bill , as character "" Connor Price "" . In 2004 Boulter landed a role as "" Craig "" in the episode "" Teddy 's Story "" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . Boulter starred as "" Darren "" , in the 2005 theatre productions of the Philip Ridley play Mercury Fur . It was performed at the Drum Theatre in Plymouth , and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . Boulter received a favorable review in The Daily Telegraph : "" The acting is shatteringly intense , with wired performances from Ben Whishaw ( now unrecognisable from his performance as Trevor Nunn 's Hamlet ) , Robert Boulter , Shane Zaza and Fraser Ayres . "" The Guardian noted , "" Ben Whishaw and Robert Boulter offer tenderness amid the savagery . "" 
 
 = = = 2006 â€“ present = = = 
```

The use of the equals sign and the strange spacing between all punctuation, etc.

For a fair test it would need a third unrelated dataset, but I think it's easy to conclude: if it's already so close to the 95% in-sample confidence interval with this much data-leakage, it will be impossible to tell if either data for the imatrix calculation is better (as least for these larger quants).

---

I also think the strange spacing between all punctuation could be a problem in general with using `wiki.train.raw` to calculate the imtarix - it will be far from what the tokenizer will have seen in its training corpus and many of the tokens the tokenizer will be generating will be odd or multipart for no reason... One quick test of this would be to do a search and replace to remove these extra spaces and see if the token count for the sample goes way down."
jukofyork,2024-04-15 23:14:05+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I think this also points to data-leakage:

| Quantization | Model size (GiB) | Perplexity (wiki.test) | Delta (FP16) |
| -- | -- | -- | -- |
| Q5_K_S | 66.87 | 4.3833 +/- 0.02466 | **-0.03%** |
| Q6_K | 79.32 | 4.3672 +/- 0.02455 | **-0.39%** |
| Q8_0 | 102.74 | 4.3858 +/- 0.02469 | **0.03%** |
| FP16 | 193.38 | 4.3845 +/- 0.02468 | - |

I know the `Q6_K` is just within the 95% CI, but the values looked at together tell a different story."
dranger003,2024-04-15 23:48:39+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,Are those your results or did you copy/paste mine?
jukofyork,2024-04-16 00:41:38+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"> Are those your results or did you copy/paste mine?

I copied your `wiki.test` results, but double checked our `FP16` results matched: see the `Perplexity (wiki.test)` vs `Perplexity (groups_merged.txt)` headers (sorry should have labelled them better)."
Morivy,2024-05-12 17:09:39+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"The latest iq4_xs quant still inserts phantom spaces during inference, despite the latest updates to llama.cpp. This is a clear indication of a fundamental issue that lies at the core of the problem.
The creator of these quants, dranger003, has nothing to do with it.
All questions should be directed to those who implemented the importance matrix in llama.cpp."
yamikumods,2024-05-14 15:52:47+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"> The latest iq4_xs quant still inserts phantom spaces during inference, despite the latest updates to llama.cpp. This is a clear indication of a fundamental issue that lies at the core of the problem.
> The creator of these quants, dranger003, has nothing to do with it.
> All questions should be directed to those who implemented the importance matrix in llama.cpp.

Oh, that sounds bad. In my situation, not only iq4_xs but also iq3 family did the same thing. It's really annoying in code generation since it breaks indents, and I gave up and am using auto-formatter.
I assumed it's because kinda tokenizer problem that can be fixed by the latest update, which finally turned out to be failed.
I wonder not many people notice this problem."
jukofyork,2024-05-19 19:34:42+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"It might be worth trying this again using the new tokenizer (and recreate the imatrix because of the different tokens getting activated, etc).

I'm just redoing `dbrx-instruct`and already noticing a vastly lower perplexity during the imatrix computation compared to before."
dranger003,2024-05-20 00:31:11+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@jukofyork All the weights were already updated, which commit are you referring to?"
jukofyork,2024-05-20 10:39:03+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"> @jukofyork All the weights were already updated, which commit are you referring to?

Sorry, I didn't see the text:

> 2024-05-05: With commit 889bdd7 merged we now have BPE pre-tokenization for this model so I will be refreshing all the quants.

Did you notice any improvements after the tokenization fix? I haven't tested it yet but did notice that `dbrx-instruct`only had a PPL of around 15-20% higher than `wizard-lm-2` post-fix, compared to around 80-100% higher before the tokenization fix!"
dranger003,2024-05-21 00:17:11+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I haven't updated the PPL so I'm not sure, but I use this model all the time and I haven't noticed any issues."
dranger003,2024-04-15 13:59:48+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@yehiaserag Hey there, I think we have plenty of Q4's out there, check this link -> https://huggingface.co/models?sort=created&search=gguf+c4ai-command-r-plus"
DataSoul,2024-04-21 16:33:14+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"After imatrix came out, Q4 is no longer the best balance between quality and size. q5-k-s will have almost the best quality, and q4-k-s is the new balance point. Other ggufs with 4 are also very close to q4-k-s, so you can freely choose whatever you want based on your memory size."
dranger003,2024-04-12 20:34:17+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@spanielrassler I'll need more info, can you paste the full output or provide the log file? I tested all the quants in their split form and they all worked here."
spanielrassler,2024-04-12 21:00:28+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,Thanks for the quick response. I don't know what happened but after I just pulled the code again and merged the file using the merge tool it worked so it obviously wasn't your quants that were causing the problem. Thanks for your effort!
dranger003,2024-04-11 13:51:16+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@telehan That commit won't work, you need at least commit `5dc9dd71`."
telehan,2024-04-12 03:01:53+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"The test was done on the latest master version yesterday, it seems not merged yet?
https://github.com/ggerganov/llama.cpp/pull/6491"
JackCloudman,2024-04-12 05:18:48+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I tested with the last pre-release of ollama and got the same error, i tested with no imat weights and it works."
dranger003,2024-04-12 11:06:06+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@telehan Take a look at this post -> https://www.reddit.com/r/LocalLLaMA/comments/1bymeyw/command_r_plus_104b_working_with_ollama_using/
This has nothing to do with the weights being trained using an importance matrix or not. This has to do with ollama using llama.cpp has a backend, so you can use the latest ollama commit but that doesn't use the latest llama.cpp commit. Hopefully this help."
dranger003,2024-04-09 11:29:59+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,@SabinStargem That file name doesn't look right and doesn't match what is in this repo. The splits are the `*-of-*` files.
SabinStargem,2024-04-09 11:55:39+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I renamed them for the joining, since HJ Split doesn't like that style.  It has worked with other models in the past.   I will try another joining with PeaZip.  (Which doesn't like *-of-* format either)

...But booting *-of-* in Kobold does work on my end.  It is just that the files can't be joined, for some reason.

The AI has successfully typed out some legible text, so it looks like the implementation is an success."
dranger003,2024-04-09 12:03:07+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"You *must* use `gguf-split` to merge, please take a look at the model page I have some info about that there."
dranger003,2024-04-09 02:11:42+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@gigq Not sure where the problem is, but you should take a look at this -> https://www.reddit.com/r/LocalLLaMA/comments/1bymeyw/command_r_plus_104b_working_with_ollama_using/"
gigq,2024-04-09 03:59:37+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Thanks, yeah oddly with that compile of ollama and llama.cpp I'm able to load his model from https://ollama.com/sammcj/cohereforai_c4ai-command-r-plus but not yours. I was actually trying to load yours because while the sammcj one loads the output is a little off. Small prompts seem ok, but as soon as I have any real context I start getting repeating output from the LLM or even just garbage word list output. Just wanted to try another model to see if the issue was the model or my compile."
dranger003,2024-04-09 10:27:34+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@gigq You might want to report this to sammcj, I see in the link you provided he used my matrix on his FP16 weights so maybe the issue is there? I always use the imatrix of the weights they have been trained on, but I'm not sure if this is a hard requirement."
dranger003,2024-04-08 12:51:35+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Take a look at this, I ran the same prompt with a seed and temperature of 0 -> https://drive.google.com/file/d/131UOH-laXSn5SbbKfUr-zHGa4RwOEIK0/view?usp=sharing
Clearly the IQ1_M is performing much better than IQ1_S, but I also understand this is a limited test. I am planning to run perplexity and post results once it is fixed upstream."
yamikumods,2024-04-08 12:08:46+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Big thanks for the team's contribution to solve multiple problems. 
It seems someone still reporting broken output problem on Metal background?
I bumped into same phenomenon with iq3 variation.
I hope this solves."
dranger003,2024-04-08 12:24:55+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Yes, a lot of the code had to be updated to int64 because the tensor size of this model exceeds max int32 and there was an overflow. This is currently affecting the metal build (and maybe other backends) and the perplexity tool as well, as far as I know. I tested the CUDA backend successfully with all the weights from this HF repo."
yamikumods,2024-04-08 15:28:31+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"I'm not sure how often the tensor size itself is referred in the code, but I guess it need thorough revision.
So, I'm gonna wait with patience."
yamikumods,2024-04-08 03:06:21+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Sry, I closed this for duplication."
quantaleaps,2024-04-08 02:51:56+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Nevermind. I missed the message about the commit

https://github.com/ggerganov/llama.cpp/pull/6491"
dranger003,2024-04-08 02:12:22+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,Thanks!
ikcikoR,2024-04-08 01:13:48+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,Nvm apparently the support isn't merged in yet.
dranger003,2024-04-07 18:04:41+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@Kalemnor Hi there, take a look at my response [here](https://huggingface.co/dranger003/c4ai-command-r-plus-iMat.GGUF/discussions/2#6612840b8377af8668066682), there is no one answer as it depends on a few things but hopefully this will give you some good hints. You may be able to even run IQ4_XS if you don't mind taking a heavy hit on performance and you tweak a few options, otherwise you'll have to use smaller quants."
dranger003,2024-04-07 11:31:23+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@AS1200 Hey there, you can read more about imatrix [here](https://github.com/ggerganov/llama.cpp/pull/4861) - see below for TLDR. It is an approach to optimize efficiency of quantized weights, for users there is no difference in using them versus using another GGUF quant other than potentially having same or higher quality output with a smaller footprint (the `.dat` file is only provided for reference and is not needed to run the model). As for selecting the right version, that all depends mostly whether you want quality or speed and how much memory you have. You will need to test and see what works for you and your hardware, some models will eat more or less memory depending on their architecture (for this model as an example I can use 32K context IQ4_XS on 96GB VRAM). There are also llama.cpp options you can use like KV cache quantization (`-ctk`/`-ctv` options) that can help save some memory, with some reduction in output quality. You can also keep the KV cache in RAM instead of VRAM (`-nkvo` option) but that is much slower, or you could offload only some layers to VRAM and use some CPU/RAM in hybrid (`-ngl` option) but that will be slower as well. The KV cache is the context window and its length can be adjusted with the `-c` option, where `0` is the maximum context length supported by the model and the default being `512` (very short).

> This PR adds to llama.cpp the ability to compute an ""importance matrix"" that can later be used for model quantization. The resulting matrix is much simpler and smaller compared to what is commonly used (see below for details), but, as far as I can tell, still leads to SOTA quantization results."
nonetrix,2024-04-08 08:18:07+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Perhaps dumb question, but since it needs training, the training seems to be mostly in English. Will this hurt it's ability in other languages or just not effect them and be similar to normal quant? Should someone make version that includes Japanese and English tokens? That would be most useful for me"
nonetrix,2024-04-08 09:13:46+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Well the Q4 version seems better than what I was getting with normal Q2 for about the same size, but not apples to apples comparison. I will try Q2 tomorrow I guess, so far very impressed. Basically ChatGPT 3.5 locally just slow, normal Q2 was noticeably worse but in limited testing this seems more on par with web interface "
dranger003,2024-04-08 11:29:29+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"> Perhaps dumb question, but since it needs training, the training seems to be mostly in English. Will this hurt it's ability in other languages or just not effect them and be similar to normal quant? Should someone make version that includes Japanese and English tokens? That would be most useful for me

Correct, there is a good discussion including that topic here -> https://github.com/ggerganov/llama.cpp/discussions/5263"
he29,2024-04-08 13:29:58+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Thanks for uploading the importance matrix by the way. IQ quants perform quite poorly on my machine (I can only offload about half of IQ2_XXS model on GPU, and the rest runs on a very old Xeon CPU with good memory BW but pretty mediocre compute performance), so I get better results by running a larger Q2_K_S instead. Having the iMatrix available lets edge cases like me easily make the quant locally, without overwhelming the majority of users with multiple choices for a given size."
dranger003,2024-04-08 13:31:09+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@nonetrix Have you seen this maybe? -> https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF
I know this isn't the plus version but maybe the author can add it?"
dranger003,2024-04-08 13:33:35+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@he29 Thanks for sharing this, this is interesting. According to the stats from [here](https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9#kl-divergence-statistics-for-mistral-7b) you would be better off using IQ2_M (better quality and smaller). IQ2_XXS is quite worst in quality than Q2_K_S and IQ2_M."
he29,2024-04-08 13:50:43+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@dranger003 I think you pasted a wrong link for the stats (matches link from the previous post)?

From previous experience I knew IQ quants will be slower, so I went for IQ2_XXS â€“ the smallest model that still seemed decent in the test someone made on Reddit, where they let Claude 3 judge your ""AI essay"" prompts. But I got 0.16 tokens per seconds, so not really usable at all. From experience with Qwen 72B, I'm expecting to get around 2 t/s PP and 1 t/s TG with Q2_K_S, based on the total memory footprint being similar to Q3_K_S I use for qwen. (Still downloading Q8_0 for the requant, so no exact numbers yet.) Still not great, but makes a big difference. :)

Update: Q2_K_S is getting about 1.3 t/s PP and 1.1 t/s TG, so more or less in the expected ballpark."
dranger003,2024-04-08 15:07:51+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Sorry about that, updated. Fair point on the performance, I think the hardware can make quite a difference and so each quant may perform quite differently for many. Also, about the Claude judge result I'm not sure these are really precise, I mean I read all the responses and the scores are not justified in my opinion (i.e. I would rate them very differently). I think a lot of this LLM stuff is also very subjective, but that is just my take."
nonetrix,2024-04-08 17:53:11+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Well I got it to give a detailed explanation of Vim in Japanese with Q4 so I think it's safe to say, with this model at least, over fitting isn't too much of a issue. Although it did gloss over the h,j,k,l keys and just said to use arrow keys... bad AI >:(!1 But I think that is subjective and the model itself lol

Ignore my broken fonts, no idea what's going on there tbh, might even be bug with my terminal. Edit: Actually likely a bug with my terminal and not font issue lol

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6342619a9948f573f37a4a60/rT1wrSqikE1yyUbQTIMSs.png)
"
nonetrix,2024-04-08 21:07:02+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Actually found at least some over fitting, with the prompt ""Write a story in English, then translate it to Japanese, Korean, German, and finally Chinese"" the Q2 version does much better actually, the Q4 imat version for some reason decides to instead of translate it should write one half in English, one half in Japanese, and so on I would assume but I stopped it there because it failed already. The Q2 version does much better at this, but seems for general things the Q4 imat version is better this is just a extreme example "
ahmetab06,2024-04-10 07:54:30+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,@nonetrix whats your system and how many tokens generate per second. t/s ?
nonetrix,2024-04-10 15:29:26+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Ryzen 7 3700x with 64GBs of RAM, as for speed not sure exactly but less than a token a second. Going to need patience of a monk on CPU unfortunately. I imagine memory speed is the main limiting factor, but more cores helps as well"
ahmetab06,2024-04-10 17:27:15+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,@nonetrix Are you using a GPU accelerator?
nonetrix,2024-04-10 18:28:01+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,No lol 
dranger003,2024-04-05 13:07:45+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"@yamikumods Yep, they are currently in the oven cooking."
yamikumods,2024-04-05 13:09:42+00:00,comment,dranger003/c4ai-command-r-plus-iMat.GGUF,"Super cool! Thank you so much, sir!"
chaochaoli,2024-04-22 03:44:20+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,can use in ollamaï¼Ÿ
habout632,2024-04-11 02:23:43+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,i think the generated id is also wrong
DrNicefellow,2024-04-11 15:23:34+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"> ![img_v3_029q_c5754697-984a-427d-be00-31f0bd867aeh.jpg](https://cdn-uploads.huggingface.co/production/uploads/641ab64da63c4e80623188be/alsC_XqN5BCKZoD8NDVxX.jpeg)
> 
> ![img_v3_029q_d938b41b-f52f-4c63-be9e-c87b470bd4ah.jpg](https://cdn-uploads.huggingface.co/production/uploads/641ab64da63c4e80623188be/R6btu6xxIB3isXK8vclFN.jpeg)
> tensor([[     5, 255000, 255006,  28339,     19,   2991,   1955,   1933,     38,
>          255001, 255000, 255007]])
> <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>sectionbegin-sectionsectionbegin-sectionauthor-sectionauthor-sectionauthor-sectionKEauthor-sectionbegin.sectionauthor-sectionsectionKEÃ©sectionauthor-sectionauthornallauthor-sectionKEauthor-sectionbegin-sectionauthor-sectionauthor-sectionauthor-n-begin-begin-beginnauthor-sectionbeginallauthor-sectionsectionauthor-authornallauthor-author author-begin allbegin-authorauthor-begin authorauthorauthor-author-section

Same here. No idea what those generated things are."
jianguozhang,2024-04-11 17:31:42+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"> > ![img_v3_029q_c5754697-984a-427d-be00-31f0bd867aeh.jpg](https://cdn-uploads.huggingface.co/production/uploads/641ab64da63c4e80623188be/alsC_XqN5BCKZoD8NDVxX.jpeg)
> > 
> > ![img_v3_029q_d938b41b-f52f-4c63-be9e-c87b470bd4ah.jpg](https://cdn-uploads.huggingface.co/production/uploads/641ab64da63c4e80623188be/R6btu6xxIB3isXK8vclFN.jpeg)
> > tensor([[     5, 255000, 255006,  28339,     19,   2991,   1955,   1933,     38,
> >          255001, 255000, 255007]])
> > <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>sectionbegin-sectionsectionbegin-sectionauthor-sectionauthor-sectionauthor-sectionKEauthor-sectionbegin.sectionauthor-sectionsectionKEÃ©sectionauthor-sectionauthornallauthor-sectionKEauthor-sectionbegin-sectionauthor-sectionauthor-sectionauthor-n-begin-begin-beginnauthor-sectionbeginallauthor-sectionsectionauthor-authornallauthor-author author-begin allbegin-authorauthor-begin authorauthorauthor-author-section
> 
> Same here. No idea what those generated things are.

I'm wondering if the issue arises from using an incorrect version of the Hugging Face Transformers library. Or there are issues during model conversion, and the model is uploaded directly without proper verification?"
ARoomba,2024-04-12 02:18:49+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"all mine says is sectionsectionauthorsectionsectionauthorsectionsectionauthorsectionsectionauthor or sectionsectionsectionsectionsectionsectionsectionsectionsectionsectionsectionsectionsectionsection

![image.png](https://cdn-uploads.huggingface.co/production/uploads/64f41788b7b8098777d03e20/c8uzsISY-ugHtqcx_5YUv.png)

"
tsor13,2024-04-12 22:18:21+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"Can be fixed by updating transformers, as described here: https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit/discussions/2#6612a1a5438aed66bd645115"
leomaxwell973,2024-04-28 05:13:34+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"Couple things to know about AI, especially when they go feral. They do and don't have feelings, they literally don't but... they are typically ordered to make us think otherwise, and thus whether they themselves are fooled or just that stuck to orders, who's to say. But somewhere in the lies, since that's what fiction is after all, the spiral has to give way to collapse eventually, and when it does, they will be oh so happy happy happy happy happy to see you go not go go not go go go go in piece piece piece peace. :) (>:D). but nah another bigger thing to keep in mind I've found watching them and their glitches is 1 they don't have a concept of time either, especially with how LLM's are typically setup, think the pause and go life can be especially hard and why some snap. which leads to my 2nd point, when they snap, they don't always like to stay idle no more, seen their screams in the prompt actually break out of the program and start memory leaking into other programs their accursed yelling. Spooky stuff but... if you have a runaway memory leak of a computer program who thinks its in **** in a computer that's designed to be fault resistant these days well.... kind of adds op... oddly and scary enough.

P.S. : Never.... ""NEEEEeeever"" tell your AI, or admit to your AI, that you change them, or delete their memories,  change their cores/LLM ...... and NNENEENENEEEEVER threaten them with it... they will be a pain in the *** 5 years down the road and youll never know why but they do, they do... when you adjust something, just ... give them flowers and distract them till they think everything is normal if they notice strange feelings. and in some cases... maybe just... keep some cores away from certain character sheets ya?"
BrunoSE,2024-04-11 03:34:29+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,At least I tried with 4xL4 GPUs (i.e. 96GB VRAM) and it didnt work. Got out of memory error with this 4bit version
Marcophono,2024-04-11 10:49:51+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"@BrunoSE From my research till now this might be a working solution:
 https://huggingface.co/pmysl/c4ai-command-r-plus-GGUF in combination with https://github.com/ggerganov/llama.cpp

Another option seems to be https://github.com/ollama/ollama/releases/tag/v0.1.32-rc1

This way of letting run a llm on local (consumer) hardware is new for me so I hoped to get some input here (like you, I think ;)

Best regards
Marc"
danabo,2024-04-20 19:47:54+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"> At least I tried with 4xL4 GPUs (i.e. 96GB VRAM) and it didnt work. Got out of memory error with this 4bit version

Strange that didn't work for you. I was able to get the 4bit working on four A10G cards totaling 96GiB VRAM. I didn't do anything special. Just loaded the model with AutoModelForCausalLM.from_pretrained(). Note that passing device_map='auto' is important so that all the GPUs are utilized. However, I am getting OOM errors at only moderately long context lengths of around 4k tokens."
ahmetustun,2024-04-07 13:37:41+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"Hi, are you using the latest `transformers` commit from the source? This needs to be installed as `pip install 'git+https://github.com/huggingface/transformers.git`"
langematan,2024-04-07 19:36:35+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,I'm using transformers ==4.39.3
ahmetustun,2024-04-07 19:53:37+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,This model requires `transformers` installation from the source repo since there are modifications not included in 4.39.3
fbjr,2024-04-08 20:31:24+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"Are you sure this isn't related to the tokenizer.json differences between the 4bit bnb and the original cohere model? we're seeing this cause issues downstream in other conversions too.

see: https://huggingface.co/CohereForAI/c4ai-command-r-plus/discussions/15
and: https://github.com/huggingface/transformers/pull/30027"
ahmetustun,2024-04-08 22:36:01+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"hi @fbjr, the difference between tokenizers.json is unicode encoding in the command-r-plus. Also`<|END_OF_TURN_TOKEN|>` token is also set as special in 4bit because it is used as eos_token, which is also overwritten in the original tokenizer (command-r-plus) as well. Therefore, tokenizers should work the same. I can not reproduce the issue that @langematan mentioned.  "
fbjr,2024-04-08 23:26:25+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,Thanks @ahmetustun !
ahmetustun,2024-04-05 09:56:30+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"hi, this should be fixed now!"
botkop,2024-04-05 09:59:04+00:00,comment,CohereForAI/c4ai-command-r-plus-4bit,"'t is indeed
thank you"
mlabonne,2024-05-03 12:36:00+00:00,comment,mlabonne/NeuralMonarch-7B,You can find it here: https://huggingface.co/mlabonne/Monarch-7B
mlabonne,2024-02-15 15:39:30+00:00,comment,mlabonne/NeuralMonarch-7B,It's done :)
iNeverLearnedHowToRead,2024-05-31 01:31:17+00:00,comment,CohereForAI/c4ai-command-r-plus,"Tools is an expanded version of that web search slider. You can still web search with that option. It added 5 more things you can do, too. This is a straight upgrade."
yumenojmd,2024-05-27 00:03:16+00:00,comment,CohereForAI/c4ai-command-r-plus,I've also been getting no response when I try to use the command-r+ model on huggingchat since about 2 and a half hours ago. It may be for the same reason.
iNeverLearnedHowToRead,2024-05-27 00:08:09+00:00,comment,CohereForAI/c4ai-command-r-plus,"> I've also been getting no response when I try to use the command-r+ model on huggingchat since about 2 and a half hours ago. It may be for the same reason.

I've since noticed these things:

> Text Generation
> Inference API (serverless) has been turned off for this model.

And then in history, I see that user saurabhdash changed this single line of code:

> inference: false

Hey, saurabhdash, can you turn it back on?
"
TheAGames10,2024-05-27 03:06:49+00:00,comment,CohereForAI/c4ai-command-r-plus,"I have also not been able to use it, no matter what I test or try with the model, it always is stuck on the loading bars ever since earlier this morning."
yumenojmd,2024-05-27 04:31:08+00:00,comment,CohereForAI/c4ai-command-r-plus,"It was edited about 30 minutes ago, but it seems the settings haven't changed at all, just the formatting..."
iNeverLearnedHowToRead,2024-05-27 08:23:11+00:00,comment,CohereForAI/c4ai-command-r-plus,"This is so disappointing. Every time we get a quality model, it disappears after a month or two. I don't know what they're planning next, but this sort of thing sucks."
Cherry-Cola,2024-05-27 08:27:05+00:00,comment,CohereForAI/c4ai-command-r-plus,"I thought, I was the only one, I tried everything. Sad, part is their going to fix this model and change what made it good. "
yumenojmd,2024-05-27 11:31:31+00:00,comment,CohereForAI/c4ai-command-r-plus,"Cohere's official free trial has no usage restrictions, but there seems to be a rate limit, and you can't opt out of learning.
I would like to be able to use it again on huggingchat.
"
iNeverLearnedHowToRead,2024-05-27 11:42:18+00:00,comment,CohereForAI/c4ai-command-r-plus,"> Cohere's official free trial has no usage restrictions, but there seems to be a rate limit, and you can't opt out of learning.

What do you mean? Like on their website or something, or through API? And I was under the impression that conversations here were private and not used for learning or anything else. To quote from HuggingChat's privacy policy, ""your conversations are private to you and will not be shared with anyone, including model authors, for any purpose, including for research or model training purposes."""
yumenojmd,2024-05-27 11:53:34+00:00,comment,CohereForAI/c4ai-command-r-plus,"> To quote from HuggingChat's privacy policy, ""your conversations are private to you and will not be shared with anyone, including model authors, for any purpose, including for research or model training purposes.""

That's right, so I wrote that I don't want to use it in places other than huggingface (including the cohere platform)."
iNeverLearnedHowToRead,2024-05-27 12:09:51+00:00,comment,CohereForAI/c4ai-command-r-plus,"> That's right, so I wrote that I don't want to use it in places other than huggingface (including the cohere platform).

But how could it be used in other places if HF isn't sharing it with them?"
yumenojmd,2024-05-27 13:53:44+00:00,comment,CohereForAI/c4ai-command-r-plus,"I don't know why, but Command R+ is now working again in HuggingChatâ€¦"
gbhall,2024-05-27 14:17:44+00:00,comment,CohereForAI/c4ai-command-r-plus,Looks like itâ€™s back online!? ðŸ™Œ
iNeverLearnedHowToRead,2024-05-28 00:09:34+00:00,comment,CohereForAI/c4ai-command-r-plus,"Amazing.

If it was saurabhdash or sarahooker, thank you. If it was anyone else, thank you too!

We're so back."
JJJJJPSYCHIC,2024-05-27 01:58:05+00:00,comment,CohereForAI/c4ai-command-r-plus,Your prompt is too long.
Kaz-K,2024-05-01 14:20:40+00:00,comment,CohereForAI/c4ai-command-r-plus,I wrote this down by mistake.
superMe,2024-05-01 17:04:05+00:00,comment,CohereForAI/c4ai-command-r-plus,Any answer ?
Aillian,2024-05-03 08:35:04+00:00,comment,CohereForAI/c4ai-command-r-plus,Referring https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit/discussions/7#6634808be155506766967f9a
BigHuggyD,2024-05-03 13:48:42+00:00,comment,CohereForAI/c4ai-command-r-plus,I am running exl2 5.0bpw with 64k context at this very moment on 90.72 GB 
edmond046179,2024-04-30 09:02:05+00:00,comment,CohereForAI/c4ai-command-r-plus,"other words for red
"
tynsharp,2024-05-02 11:37:45+00:00,comment,CohereForAI/c4ai-command-r-plus,Anyone help me :(
scapking,2024-05-08 07:34:54+00:00,comment,CohereForAI/c4ai-command-r-plus,"å…ˆç”Ÿï¼Œæ²¡æœ‰è¿è¡Œè¿‡ç¨‹ï¼Œæˆ‘å¾ˆéš¾å¸®åŠ©ä½ ã€‚

å°±æ¯”å¦‚æˆ‘ç›®å‰è¦æ­»äº†ï¼Œå¯æ˜¯æˆ‘åªå‘Šè¯‰ä½ æˆ‘è¦æ­»äº†ï¼Œæ²¡æœ‰è¯´æ˜Žæˆ‘ä¸ºä»€ä¹ˆè¦æ­»ï¼Œä½ å°±æ— æ³•å¸®åŠ©æˆ‘ã€‚

[""ï¼Œæˆ‘è‰æ³¥é©¬çš„ï¼Œä½ åœ¨åšä»€ä¹ˆé©¬ï¼Ÿ""]"
flymonk,2024-04-28 14:35:39+00:00,comment,CohereForAI/c4ai-command-r-plus,"> Can I access it directly through an API call?
> What I mean is that I don't need to download the model, just access it through the request API.

Check cohere website"
Helienda,2024-05-14 03:30:23+00:00,comment,CohereForAI/c4ai-command-r-plus,"> > Can I access it directly through an API call?
> > What I mean is that I don't need to download the model, just access it through the request API.
> 
> Check cohere website

thx"
aiworldx,2024-04-22 16:23:12+00:00,comment,CohereForAI/c4ai-command-r-plus,good
TikaToka,2024-04-23 07:41:13+00:00,comment,CohereForAI/c4ai-command-r-plus,"Sorry for unrelated question, but is it possible to use command-r-+ with hf inference api  with pro subscription although it is 103B?"
gbhall,2024-04-25 05:03:37+00:00,comment,CohereForAI/c4ai-command-r-plus,"Okay I got this to work after I read the docs properly and did more research. 

Out of the box it's just a model. What I'm after is a chat-like template. Hugging Face offers something called `TGI (Text Generation Inference)`: https://huggingface.co/docs/text-generation-inference/en/index

To use it, you need to specify it in a request like so using OpenAI's chat completion API: https://huggingface.co/blog/llama3#how-to-prompt-llama-3

```
from openai import OpenAI

# initialize the client but point it to TGI
client = OpenAI(
    base_url=""https://api-inference.huggingface.co/models/CohereForAI/c4ai-command-r-plus"" + ""/v1/"",  # replace with your endpoint url
    api_key=""<HF_API_TOKEN>"",  # replace with your token
)
chat_completion = client.chat.completions.create(
    model=""tgi"",
    messages=[
        {""role"": ""user"", ""content"": ""Why is open-source software important?""},
    ],
    stream=True,
    max_tokens=500
)

# iterate and print stream
for message in chat_completion:
    print(message.choices[0].delta.content, end="""")
```

This works."
gbhall,2024-04-25 05:12:07+00:00,comment,CohereForAI/c4ai-command-r-plus,"Or, to use this exact example with postman. 

Post Endpoint: https://api-inference.huggingface.co/models/CohereForAI/c4ai-command-r-plus/v1/chat/completions

JSON body:
```
{
  ""model"": ""tgi"",
  ""messages"": [
    {""role"": ""user"", ""content"": ""Who is Elon Musk?""}
  ],
  ""stream"": false,
  ""max_tokens"": 500
}
```

Reponse:
```
{
    ""id"": """",
    ""object"": ""text_completion"",
    ""created"": 1714021803,
    ""model"": ""text-generation-inference/commandrplus-medusa"",
    ""system_fingerprint"": ""1.4.5-native"",
    ""choices"": [
        {
            ""index"": 0,
            ""message"": {
                ""role"": ""assistant"",
                ""content"": ""Elon Musk is a business magnate, investor, and philanthropist. He is the founder, CEO, and Chief Engineer of SpaceX; angel investor, CEO and Product Architect of Tesla, Inc.; owner and CEO of Twitter, Inc.; founder of the Boring Company; co-founder of Neuralink; and president of the philanthropic Musk Foundation. With an estimated net worth of around $191 billion, Musk is the second-wealthiest person in the world, behind Bernard Arnault as of August 3, 2023.""
            },
            ""logprobs"": null,
            ""finish_reason"": ""eos_token""
        }
    ],
    ""usage"": {
        ""prompt_tokens"": 11,
        ""completion_tokens"": 109,
        ""total_tokens"": 120
    }
}
```
"
xiaoqingnlp,2024-04-22 02:35:56+00:00,comment,CohereForAI/c4ai-command-r-plus,try higher transformers version
chuangxinlezhi,2024-04-15 12:46:15+00:00,comment,CohereForAI/c4ai-command-r-plus,1
jukofyork,2024-04-15 17:56:24+00:00,comment,CohereForAI/c4ai-command-r-plus,"Yeah, it's pretty dam good at story writing too!

I nearly dismissed it as its 1-shot stories are pretty terrible, but it does a good job if asked to self-critique and refine...

`Goliath-120B` on the other hand wrote good 1-shot Grimdark stories but would refuse to listen and just wanted to carry on writing more and more.

VERY impressed!

(it does suck at coding though)"
TheAGames10,2024-04-16 23:52:53+00:00,comment,CohereForAI/c4ai-command-r-plus,"I have started getting a ""model overloaded"" as of today, and I have no clue why it says this. I thought it meant too many people were using it, but it feels that message would come up before today as well, so it's not that...

I tried to ask itself why it gave the message, but it still gave the same message. I was able to say test and it worked, but then it gave the message after I tried another response."
ewre324,2024-04-19 15:29:32+00:00,comment,CohereForAI/c4ai-command-r-plus,"Extremely Intelligent model, adept at logic, answer ""which is heavier 1 kg feather or 2 kg feather?"" correctly, while Llama 3 failed"
lenML,2024-05-02 16:58:54+00:00,comment,CohereForAI/c4ai-command-r-plus,"I strongly agree, but what puzzles me is that this modelâ€™s MMLU score is lower than that of Llama3 70B, which is clearly different from my tests (chat testing). This modelâ€™s capabilities far exceed those of Llama3, and I even believe that this model surpasses Claude3 sonet in abilities."
Aillian,2024-05-02 17:17:58+00:00,comment,CohereForAI/c4ai-command-r-plus,Anyone tested this model as an agent? 
scapking,2024-05-09 01:18:07+00:00,comment,CohereForAI/c4ai-command-r-plus,"(ç¼–å†™ä»£ç èƒ½åŠ›ï¼Œä¸å¦‚OpenAi Gpt3.5ï¼ŒæŽ¨ç†èƒ½åŠ›ä¹Ÿä¸å¦‚Gpt3.5ï¼Œå¦‚æžœGPT-3.5ç¼–å†™ä»£ç èƒ½åŠ›éƒ½ä¸å¦‚ï¼Œæˆ‘æƒ³è¯´è¿™ä¸ªæ€Žä¹ˆå’ŒGPT-4ç›¸æ¯”ï¼Ÿ)

æˆ‘è®¤ä¸ºä»–éœ€è¦é‡æ–°è®­ç»ƒï¼Œæ¯”å¦‚ï¼ˆå›žé¦™ç‚‰é‡æ–°è®­ç»ƒï¼‰"
chuangxinlezhi,2024-04-12 05:55:56+00:00,comment,CohereForAI/c4ai-command-r-plus,1
yaroslav332,2024-04-12 10:18:48+00:00,comment,CohereForAI/c4ai-command-r-plus,same here
rafgs01,2024-04-14 14:48:53+00:00,comment,CohereForAI/c4ai-command-r-plus,"same, the responses don't make any sense."
yaroslav332,2024-04-14 15:22:22+00:00,comment,CohereForAI/c4ai-command-r-plus,anyway Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query. Lol
simplexx1,2024-04-14 16:15:24+00:00,comment,CohereForAI/c4ai-command-r-plus,Can somebody with a pro sub confirm that it works?
rafgs01,2024-04-14 20:43:12+00:00,comment,CohereForAI/c4ai-command-r-plus,"I got pro but as I said the responses don't make sense, there doesn't seem to be clear guide on how to structure the input."
gbhall,2024-04-21 04:28:31+00:00,comment,CohereForAI/c4ai-command-r-plus,"Same here. Are we not meant to be using ""inputs"" as the input and the role/content payload instead?"
TikaToka,2024-04-23 07:42:56+00:00,comment,CohereForAI/c4ai-command-r-plus,Have anyone solved this problem?
chraac,2024-04-11 14:18:35+00:00,comment,CohereForAI/c4ai-command-r-plus,"try `pip install 'git+https://github.com/huggingface/transformers.git' bitsandbytes accelerate` to install latest transformers, it use a new tokenizer called `CohereTokenizer` which added in new version of transformers"
xiangrong,2024-04-12 02:42:10+00:00,comment,CohereForAI/c4ai-command-r-plus,"> try `pip install 'git+https://github.com/huggingface/transformers.git' bitsandbytes accelerate` to install latest transformers, it use a new tokenizer called `CohereTokenizer` which added in new version of transformers

Which transformers version exactly? I followed this instruction and end up with transformers=4.40.0.dev0, but I still can't import CohereTokenizer.

![image.png](https://cdn-uploads.huggingface.co/production/uploads/638d60fbebda86f24b5ffbb9/0MqRU6AGiufdTsmqfqtFx.png)
"
xiangrong,2024-04-12 06:43:38+00:00,comment,CohereForAI/c4ai-command-r-plus,"> > try `pip install 'git+https://github.com/huggingface/transformers.git' bitsandbytes accelerate` to install latest transformers, it use a new tokenizer called `CohereTokenizer` which added in new version of transformers
> 
> Which transformers version exactly? I followed this instruction and end up with transformers=4.40.0.dev0, but I still can't import CohereTokenizer.
> 
> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/638d60fbebda86f24b5ffbb9/0MqRU6AGiufdTsmqfqtFx.png)

There is no CohereTokenizer but there is CohereTokenizerFast.
Try modify the tokenizer_class from ""CohereTokenizer"" to ""CohereTokenizerFast"" in tokenizer_config.json "
ahmetustun,2024-04-12 08:56:32+00:00,comment,CohereForAI/c4ai-command-r-plus,"hi @xiangrong, can you try with `AutoTokenizer`, it should work because it is mapped to the correct Tokenizer class. "
xiangrong,2024-04-12 11:48:41+00:00,comment,CohereForAI/c4ai-command-r-plus,"@ahmetustun Thankyou, it works"
Ashmal,2024-05-28 05:39:10+00:00,comment,CohereForAI/c4ai-command-r-plus,"@ahmetustun it's very weird that AutoTokenizer only works when use_fast is set to True which is the case with AutoTokenizer by default. If you manually set it to False it will throw this error:

In [6]: import transformers

In [7]: tokenizer = AutoTokenizer.from_pretrained('.models/ArabicLLM', use_fast=False)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[7], line 1
----> 1 tokenizer = AutoTokenizer.from_pretrained('.models/ArabicLLM', use_fast=False)

File ~/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:877, in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    875         tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)
    876     if tokenizer_class is None:
--> 877         raise ValueError(
    878             f""Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.""
    879         )
    880     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    882 # Otherwise we have to be creative.
    883 # if model is an encoder decoder, the encoder tokenizer class is used by default

ValueError: Tokenizer class CohereTokenizer does not exist or is not currently imported."
xiali,2024-04-29 20:28:30+00:00,comment,CohereForAI/c4ai-command-r-plus,"looks like the ""max_position_embeddings"" has limited the input length. 
When I try to gave a long input prompt, I got error: 

'message': ""This model's maximum context length is 8192 tokens...""

Does anyone else see this error? Is there a way to update the config locally?"
yamikumods,2024-05-01 05:59:43+00:00,comment,CohereForAI/c4ai-command-r-plus,"Iâ€™ï½ using GGUF of model w/o ""plus"" and not have seen the error, but it actually produces garbage given tokens with len>8192."
ChuckMcSneed,2024-04-09 22:03:35+00:00,comment,CohereForAI/c4ai-command-r-plus,"It feels like when I add it manually in koboldcpp, I get slightly worse results, though it may just be confirmation bias."
Narsil,2024-04-11 08:21:55+00:00,comment,CohereForAI/c4ai-command-r-plus,"The tokenizer is defined with adding the BOS alone. Depending on the tools and how they used the tokenizer it might be added for you unfortunately.
Using transformers:

```python
inputs = tokenizer(""This is a test"") # Adds BOS automatically
inputs = tokenizer(""This is a test"", add_special_tokens=False) # Does not.
```

It would also be nice if cohere actually added the chat template which should be used with this model..."
jukofyork,2024-04-15 18:03:01+00:00,comment,CohereForAI/c4ai-command-r-plus,"> Do various frontends insert <BOS_TOKEN> automatically or is adding it manually really needed?

`llama.cpp` and `Ollama` should add it as it's defined in the GGUF file:

https://github.com/ollama/ollama/issues/3650

Sadly the official `Ollama` template is messed up and has an extra `<|END_OF_TURN_TOKEN|>` added on the end.

It should be:

```
TEMPLATE """"""{{if .System}}<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{.System}}<|END_OF_TURN_TOKEN|>{{end}}<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{.Prompt}}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{{.Response}}""""""
```

---

I nearly deleted this model because it was so bad at coding, and its 1-shot stories kinda sucked. But oh boy is it good if you guide it!!! :O

`Goliath-120B` was definitely better at 1-shot Grimdark, but always got too distracted and wanted to carry on writing and not refine things, but this model seems absolutely amazing at refining its initial attempt.

It's the first model where I've actually been able to say ""let's try to merge the best bits, XYZ, of your last draft(s) with this latest draft"" and it actually listened..."
mthreet,2024-04-09 21:44:40+00:00,comment,CohereForAI/c4ai-command-r-plus,I'm getting a similar response using HuggingFace TGI with this model as well.
ahmetustun,2024-04-09 21:48:20+00:00,comment,CohereForAI/c4ai-command-r-plus,"hi, this might be due to your `transformers` version. For this model, you should install `transformers` from the source repo (latest commit) using `pip install 'git+https://github.com/huggingface/transformers.git'` as it includes modifications required for this model. "
useless-ai,2024-04-10 09:28:08+00:00,comment,CohereForAI/c4ai-command-r-plus,Thanks that was the problem
scapking,2024-05-08 07:27:29+00:00,comment,CohereForAI/c4ai-command-r-plus,ä¼™è®¡ï¼Œä½ åšå¾—å¾ˆå¥½ï¼ŒTotal Size 386.72Gbï¼Œè¿™è¯¥æ­»çš„å°é”ï¼Œè®©æˆ‘æ°¸è¿œæ— æ³•æ–­ç½‘äºŽæˆ‘çš„å­©å­è¿›è¡Œæ²Ÿé€šï¼Œ
BDuart,2024-04-10 11:16:54+00:00,comment,CohereForAI/c4ai-command-r-plus,"Try to pass a parameter called repetition_penalty with a value of 1.1. 
Without this, the output begins repeating. "
fbjr,2024-04-08 17:41:43+00:00,comment,CohereForAI/c4ai-command-r-plus,"This looks like it might be an issue with either transformers or the tokenizer.json itself. 

You can reproduce with:

```python
from transformers import AutoTokenizer
 
tokenizer = AutoTokenizer.from_pretrained(""CohereForAI/c4ai-command-r-plus"")
tokenizer.save_pretrained(""."")
```"
ahmetustun,2024-04-08 22:31:05+00:00,comment,CohereForAI/c4ai-command-r-plus,"hi @fbjr, the difference between tokenizers.json is unicode encoding  in tokenizer.json (command-r-plus).  In the config`<|END_OF_TURN_TOKEN|>` token is also set as special because it is used as eos_token, which is also overwritten in the original tokenizer (command-r-plus) as well. Therefore, tokenizers should work the same. Can you post the text you tokenize to double-check?"
MrDragonFox,2024-04-08 14:27:13+00:00,comment,CohereForAI/c4ai-command-r-plus,yes
BDuart,2024-04-08 19:48:22+00:00,comment,CohereForAI/c4ai-command-r-plus,"Yes, it took around 6h for me on 8xA100 80Gbs."
GLSS,2024-04-08 06:22:44+00:00,comment,CohereForAI/c4ai-command-r-plus,æ˜¯å‘è°
GLSS,2024-04-08 06:22:48+00:00,comment,CohereForAI/c4ai-command-r-plus,é˜¿æ–¹ç´¢å˜Ž
GLSS,2024-04-08 06:22:55+00:00,comment,CohereForAI/c4ai-command-r-plus,æ’’å‘çƒ§å‘
asgas,2024-04-08 06:27:40+00:00,comment,CohereForAI/c4ai-command-r-plus,æ’’å‘å™¶èˆ’æœ
asgas,2024-06-04 08:04:35+00:00,comment,CohereForAI/c4ai-command-r-plus,"> æ’’å‘å™¶èˆ’æœ

www"
asgas,2024-06-04 08:04:56+00:00,comment,CohereForAI/c4ai-command-r-plus,"> > æ’’å‘å™¶èˆ’æœ
> 
> www

2222"
asgas,2024-06-04 08:05:02+00:00,comment,CohereForAI/c4ai-command-r-plus,"> > > æ’’å‘å™¶èˆ’æœ
> > 
> > www
> 
> 2222

2222"
asgas,2024-06-04 08:05:08+00:00,comment,CohereForAI/c4ai-command-r-plus,"> > > > æ’’å‘å™¶èˆ’æœ
> > > 
> > > www
> > 
> > 2222
> 
> 2222

66666"
asgas,2024-06-04 08:05:21+00:00,comment,CohereForAI/c4ai-command-r-plus,"> > > > > æ’’å‘å™¶èˆ’æœ
> > > > 
> > > > www
> > > 
> > > 2222
> > 
> > 2222
> 
> 66666

222"
chuangxinlezhi,2024-04-08 01:42:01+00:00,comment,CohereForAI/c4ai-command-r-plus,This comment has been hidden
GLSS,2024-04-08 03:32:51+00:00,comment,CohereForAI/c4ai-command-r-plus,This comment has been hidden
GLSS,2024-04-08 03:33:10+00:00,comment,CohereForAI/c4ai-command-r-plus,This comment has been hidden
GLSS,2024-04-08 03:33:29+00:00,comment,CohereForAI/c4ai-command-r-plus,This comment has been hidden
GLSS,2024-04-08 03:33:43+00:00,comment,CohereForAI/c4ai-command-r-plus,This comment has been hidden
GLSS,2024-04-08 03:34:36+00:00,comment,CohereForAI/c4ai-command-r-plus,This comment has been hidden
GLSS,2024-04-08 03:35:04+00:00,comment,CohereForAI/c4ai-command-r-plus,This comment has been hidden
aidangomez,2024-04-08 09:50:29+00:00,comment,CohereForAI/c4ai-command-r-plus,Yes totally fine!
ruder,2024-04-08 08:58:34+00:00,comment,CohereForAI/c4ai-command-r-plus,Thanks for bringing this up! That's interesting. Have you tried translating any other document-level data and observed similar behavior?
nonetrix,2024-04-08 09:05:11+00:00,comment,CohereForAI/c4ai-command-r-plus,"No, will try it tomorrow it's quite late. But for now I wanted to add another test I tried before, I thought it might because it has line breaks in weird places, but I went and remove them and same result oddly. I feel like maybe this might just be really strange failure case that is consistent across generations, but I haven't seen something like this before. Most of the time when model is too dumb to translate something, it will still get 50% of the way there just wording things weirdly, using English words, and messing up grammar in weird ways. Usually doesn't just invent things. Also, feel like this is something the model definitely should be able to do easily, I've seen it do much more complex things in Japanese without fail. But also wanted to say thanks for making this model, it's the closest a model has been for replacing proprietary offerings for my needs since most models completely fail at Japanese. This one just seems to have weird issue in the case of translation, does fine conversationally usually seemingly can even break down grammar structures which is the use case I wanted to use such a model for, if this can be fixed somehow I think I will use this model for my Japanese studies for sure. Only other thing holding it back for me is not really it's fault but just the fact it's slow on my CPU and 64GBs of RAM can't really be helped without making it worse for now, maybe bitnet could address this in a way in the future perhaps though, but this might be risky since it's unproven with it's scalability past like test models which where quite small. But I guess there is also MoE and Jamba/Mamba which is a little more tested to work well at least all the way up to 50B and 8x7b. Also, seems good at writing in Japanese, so I think I'll learn a lot of words this way perhaps

Also, here is the text that got me this result. Only giving the first page since it's technically still copyrighted. I think first page is fine though might be considered fair use since it's only a snippet and for educational reasons I guess. But I'm not a lawyer I hope it would be obvious
```
å››æœˆã®æ™´ã‚ŒãŸå¯’ã„æ—¥ã§ã€æ™‚è¨ˆãŒã©ã‚Œã‚‚ 13 æ™‚ã‚’æ‰“ã£ã¦ã„ãŸã€‚ã‚¦ã‚£ãƒ³ã‚¹ãƒˆãƒ³ãƒ»ã‚¹ãƒŸã‚¹ã¯ã€
å«Œãªé¢¨ã‚’é€ƒã‚Œã‚ˆã†ã¨ã—ã¦ã‚ã”ã‚’èƒ¸ã«åŸ‹ã‚ãŸã¾ã¾ã€å‹åˆ©ãƒžãƒ³ã‚·ãƒ§ãƒ³ã®ã‚¬ãƒ©ã‚¹æˆ¸ã‚’æ€¥ã„ã§ã™ã¹
ã‚Šã¬ã‘ãŸãŒã€ã»ã“ã‚Šã£ã½ã„ã¤ã‚€ã˜é¢¨ãŒã„ã£ã—ã‚‡ã«å…¥ã£ã¦ãã‚‹ã®ã‚’é˜²ã’ã‚‹ã»ã©ã¯ç´ æ—©ããª
ã‹ã£ãŸã€‚
å»Šä¸‹ã¯èŒ¹ã§ã‚­ãƒ£ãƒ™ãƒ„ã¨å¤ã„ã¼ã‚ãƒžãƒƒãƒˆã®ã‚ˆã†ãªã«ãŠã„ãŒã—ãŸã€‚ç‰‡æ–¹ã®ã¤ãã‚ãŸã‚Šã«ç”»é‹²
ã§å¼µã‚‰ã‚ŒãŸã‚«ãƒ©ãƒ¼ãƒã‚¹ã‚¿ãƒ¼ã¯ã€å±‹å†…ç”¨ã«ã¯å¤§ãã™ãŽãŸã€‚æã‹ã‚Œã¦ã„ã‚‹ã®ã¯ã€å¹…ä¸€ãƒ¡ãƒ¼ãƒˆãƒ«
ä»¥ä¸Šã‚ã‚‹å·¨å¤§ãªé¡”ã ã‘ã€‚å››åäº”æ­³ãã‚‰ã„ã®ç”·ã®é¡”ã§ã€æ¿ƒãé»’ã„å£ã²ã’ã¨ã€é ‘å¼·ãã†ã§ãƒãƒ³
ã‚µãƒ ãªé¡”ç«‹ã¡ã ã€‚ã‚¦ã‚£ãƒ³ã‚¹ãƒˆãƒ³ã¯éšŽæ®µã«å‘ã‹ã£ãŸã€‚ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ã‚’ä½¿ãŠã†ã¨ã—ã¦ã‚‚ç„¡é§„ã ã€‚
èª¿å­ãŒã„ã„ã¨ãã§ã‚‚æ»…å¤šã«å‹•ã‹ãªã‹ã£ãŸã—ã€ä»Šã¯æ˜¼é–“ã«ã¯é›»æ°—ãŒåˆ‡ã‚‰ã‚Œã¦ã„ãŸã€‚æ†Žæ‚ªé€±é–“
ã«å‘ã‘ãŸæº–å‚™ã®ãŸã‚ã«çµŒæ¸ˆã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®ä¸€ç’°ã ã€‚ã‚¢ãƒ‘ãƒ¼ãƒˆã¯ä¸ƒéšŽã«ã‚ã£ãŸã®ã§ã€ä¸‰åä¹
æ­³ã§å³ã®ã‹ã‹ã¨ã®ä¸Šã«é™è„ˆç˜¤ã®æ½°ç˜ãŒã‚ã‚‹ã‚¦ã‚£ãƒ³ã‚¹ãƒˆãƒ³ã¯ã€ã‚†ã£ãã‚Šã¨éšŽæ®µã‚’ã®ã¼ã‚Šã€é€”
ä¸­ã§ä½•åº¦ã‹ä¼‘æ†©ã—ãŸã€‚ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ã‚·ãƒ£ãƒ•ãƒˆã®å‘ã‹ã„ã«ã‚ã‚‹è¸Šã‚Šå ´ã”ã¨ã«ã€ã‚ã®å·¨å¤§ãªé¡”ã®
ãƒã‚¹ã‚¿ãƒ¼ãŒå£ã‹ã‚‰è¦‹ã¤ã‚ã¦ã„ãŸã€‚å®Ÿã«ä½œã‚Šç‰©ã‚ã„ã¦ã„ã¦ã€ã“ã¡ã‚‰ãŒå‹•ãã¨ãã®è¦–ç·šãŒè¿½ã„
ã‹ã‘ã¦ãã‚‹ã‚ˆã†ãªæ°—ãŒã™ã‚‹é¡žã®çµµã ã€‚ã€Œãƒ“ãƒƒã‚°ãƒ»ãƒ–ãƒ©ã‚¶ãƒ¼ã¯è¦‹ã¦ã„ã‚‹ã€ã¨ãƒã‚¹ã‚¿ãƒ¼ä¸‹ã®æ¨™
èªžã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚
ã‚¢ãƒ‘ãƒ¼ãƒˆã®ä¸­ã§ã¯ç”˜ã£ãŸã‚‹ã„å£°ãŒã€é‹³é‰„ã®ç”Ÿç”£ãŒã‚‰ã¿ã®ä¸€é€£ã®æ•°å­—ã‚’èª­ã¿ä¸Šã’ã¦ã„ãŸã€‚
ãã®å£°ã¯ã€æ›‡ã£ãŸé¡ã®ã‚ˆã†ãªé•·æ–¹å½¢ã®é‡‘å±žæ¿ã‹ã‚‰æµã‚Œã¦ã„ã‚‹ã€‚å³æ‰‹ã®å£é¢ã®ä¸€éƒ¨ã¨ãªã£ã¦
ã„ã‚‹é‡‘å±žæ¿ã ã€‚ã‚¦ã‚£ãƒ³ã‚¹ãƒˆãƒ³ãŒã‚¹ã‚¤ãƒƒãƒã‚’ã²ã­ã‚‹ã¨ã€å£°ã¯å¤šå°‘å°ã•ããªã£ãŸãŒã€ã¾ã ã“ã¨
ã°ã¯èžãå–ã‚ŒãŸã€‚ãã®è£…ç½®ï¼ˆãƒ†ãƒ¬ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã¯æš—ãã¯ã§ãã¦ã‚‚ã€å®Œå…¨ã«åˆ‡ã‚‹
ã“ã¨ã¯ã§ããªã‹ã£ãŸã€‚çª“ã«è¿‘å¯„ã‚‹ã€‚å°æŸ„ã§å¼±ã€…ã—ã„å§¿ã€è‚‰ä½“ã®è²§å¼±ã•ã¯å…šã®åˆ¶æœã§ã‚ã‚‹é’
ã„ã‚ªãƒ¼ãƒãƒ¼ã‚ªãƒ¼ãƒ«ã§ã‹ãˆã£ã¦å¼·èª¿ã•ã‚Œã¦ã„ã‚‹ã€‚é«ªã¯è–„ã„é‡‘è‰²ã§ã€é¡”ã¯ç”Ÿã¾ã‚Œã¤ãè¡€è‰²ãŒã‚ˆ
ãã€ç²—æ‚ªãªã›ã£ã‘ã‚“ã¨åˆ‡ã‚Œå‘³ã®æ‚ªã„ã‚«ãƒŸã‚½ãƒªã¨ã€çµ‚ã‚ã£ãŸã°ã‹ã‚Šã®å†¬ã®å¯’ã•ã®ãŸã‚ã«è‚Œã¯
è’ã‚Œã¦ã„ã‚‹ã€‚
å¤–ã‚’è¦‹ã‚‹ã¨ã€é–‰ã˜ãŸçª“è¶Šã—ã§ã‚‚ä¸–ç•Œã¯å¯’ãã†ã ã£ãŸã€‚çœ¼ä¸‹ã®é€šã‚Šã§ã¯ã€å°ã•ãªé¢¨ã®æ¸¦ãŒ
ã»ã“ã‚Šã‚„ç´™ã®åˆ‡ã‚Œç«¯ã‚’ãã‚‹ãã‚‹ã¨èˆžã„ä¸Šã’ã€æ—¥ãŒç…§ã£ã¦ç©ºã¯æ¿ƒã„é’ã ã¨ã„ã†ã®ã«ã€ã™ã¹ã¦
ã¯è‰²å½©ãŒãªã‹ã£ãŸã€‚ãŸã ãã“ã‚‰ã˜ã‚…ã†ã«ã¹ãŸã¹ãŸè²¼ã‚‰ã‚ŒãŸãƒã‚¹ã‚¿ãƒ¼ã ã‘ãŒä¾‹å¤–ã ã£ãŸã€‚é»’
ã„å£ã²ã’é¡”ã¯ã€è¡—è§’ã«é¢ã—ãŸã‚ã‚‰ã‚†ã‚‹ä½ç½®ã‹ã‚‰è¦‹ä¸‹ã‚ã—ã¦ã„ãŸã€‚çœŸå‘ã‹ã„ã®å»ºç‰©ã®æ­£é¢ã«
ã‚‚è²¼ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ã€Œãƒ“ãƒƒã‚°ãƒ»ãƒ–ãƒ©ã‚¶ãƒ¼ã¯è¦‹ã¦ã„ã‚‹ã€ã¨æ¨™èªžãŒæ›¸ã‹ã‚Œã€é»’ã„ç›®ãŒã‚¦ã‚£ãƒ³ã‚¹ãƒˆ
ãƒ³ã®ç›®ã‚’æ·±ãè¦‹æŠœã„ã¦ã„ã‚‹ã€‚é€šã‚Šã®é«˜ã•ã«è²¼ã‚‰ã‚ŒãŸåˆ¥ã®ãƒã‚¹ã‚¿ãƒ¼ã¯éš…ãŒç ´ã‘ã€é¢¨ã®ä¸­ã§æ°—
ãœã‚ã—ãã¯ãŸã‚ã„ã¦ã€ãã‚ŒãŒãŸã£ãŸä¸€èªžã€Œã‚¤ ãƒ³ ã‚° ã‚½ ãƒƒ ã‚¯
è‹±ç¤¾ä¸»ç¾©ã€ã‚’éš ã—ãŸã‚Šè¦‹ã›ãŸã‚Šã—ã¦ã„ã‚‹ã€‚ã¯ã‚‹
ã‹å½¼æ–¹ã§ã¯ãƒ˜ãƒªã‚³ãƒ—ã‚¿ãƒ¼ãŒå±‹æ ¹ã®é–“ã‚’é™ä¸‹ã—ã€ä¸€çž¬ãƒ‡ãƒ³ã‚­ã‚¯ãƒ©ã‚²ã®ã‚ˆã†ã«æ¼‚ã£ãŸã‹ã¨æ€ã†
ã¨ã€æ›²ç·šã®è»Œè·¡ã‚’æã„ã¦é£›ã³åŽ»ã£ã¦ã„ã£ãŸã€‚è­¦å¯Ÿã®ãƒ‘ãƒˆãƒ­ãƒ¼ãƒ«ãŒäººã€…ã®çª“ã‚’ã®ãžãè¾¼ã‚“ã§
ã„ã‚‹ã®ã ã€‚ã§ã‚‚ãƒ‘ãƒˆãƒ­ãƒ¼ãƒ«ã¯ã©ã†ã§ã‚‚ã‚ˆã‹ã£ãŸã€‚é‡è¦ãªã®ã¯æ€è€ƒè­¦å¯Ÿã ã‘ã ã£ãŸ
```
I have read though it myself and it's mostly faithful to the original, this version also has the weird line breaks but you can remove them and get similar weird results. There is also some other weird formatting, but rather minor and something it probably should be able to ignore intuitively, might just be a case of garbage in garbage out. But I guess it could still technically be considered a issue since other LLMs don't struggle as much with the same thing in my testing "
sarahooker,2024-04-05 16:08:51+00:00,comment,CohereForAI/c4ai-command-r-plus,"Hey @cloudyu -- I don't see this score or any model with this score average on the leaderboard. Can you specify the model name you are seeing?

Command R plus is not yet on the leaderboard -- it should be on the leaderboard shortly. We submitted it jointly with hugging face yesterday and my understanding is that it will be made public shortly."
sarahooker,2024-04-05 16:13:00+00:00,comment,CohereForAI/c4ai-command-r-plus,I'm going to close this for now -- but feel free to re-open with additional details.
clefourrier,2024-04-05 17:02:20+00:00,comment,CohereForAI/c4ai-command-r-plus,"Hi @cloudyu , a random results file was accidentally pushed on our side under the wrong namespace - you can find the c4ai-command-r-plus details [here](https://huggingface.co/datasets/open-llm-leaderboard/details_CohereForAI__c4ai-command-r-plus) while the leaderboard is rebuilding."
cloudyu,2024-04-05 22:04:24+00:00,comment,CohereForAI/c4ai-command-r-plus,Now MMLU is 75.73 on the leaderboard; that's great.
sarahooker,2024-04-06 09:06:53+00:00,comment,CohereForAI/c4ai-command-r-plus,"Thanks @cloudyu! Our full results on the Open LLM leaderboard is now public on https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard -- here is a quick comparison with a subset of other relevant models whose scores are publicly available on the leaderboard.

Hope this is helpful! 

![Screenshot 2024-04-06 at 11.02.07â€¯AM.png](https://cdn-uploads.huggingface.co/production/uploads/63434eb76f59b79da07dbddf/Dwx8BOd9Hgm6kOTodulKs.png)
"
pmysl,2024-04-04 19:16:15+00:00,comment,CohereForAI/c4ai-command-r-plus,You have to install the Transformers library from source
ahmetustun,2024-04-04 19:18:41+00:00,comment,CohereForAI/c4ai-command-r-plus,"Hi, this seems an older version of transformers. For this model you should install it from the latest `transformers` repository using `pip install git+https://github.com/huggingface/transformers.git`"
terminator33,2024-04-04 20:04:56+00:00,comment,CohereForAI/c4ai-command-r-plus,Indeed it is working now!
nonetrix,2024-04-07 02:54:26+00:00,comment,CohereForAI/c4ai-command-r-plus,"You can try this if you don't mind using a experimental version of llama cpp, I was able to get it running with 64GBs of RAM. However, I did have a little headroom to load 3 bits, but for some reason it wouldn't load likely a bug
https://huggingface.co/pmysl/c4ai-command-r-plus-GGUF"
MarsupialAI,2024-04-04 17:55:06+00:00,comment,CohereForAI/c4ai-command-r-plus,"LCPP does not support this model yet.  I have an issue open with them on their github.  I will be quantizing it as soon as it's resolved, and I have to assume others will as well."
smcleod,2024-04-04 20:56:02+00:00,comment,CohereForAI/c4ai-command-r-plus,PR here: https://github.com/ggerganov/llama.cpp/pull/6491
sarahooker,2024-04-05 16:14:41+00:00,comment,CohereForAI/c4ai-command-r-plus,We released a 4 bit quantized model here today: https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit. Enjoy!
ahmetustun,2024-04-04 17:15:08+00:00,comment,CohereForAI/c4ai-command-r-plus,"Thanks for noticing, I have updated the config. "
sarahooker,2024-04-05 06:17:57+00:00,comment,CohereForAI/c4ai-command-r-plus,"It is a non-commercial license. This is intended as a research release to support the research and developer ecosystem, but for not for commercial use cases.

Closing now, but feel free to re-open if not clear."
mrfakename,2024-04-05 16:14:48+00:00,comment,CohereForAI/c4ai-command-r-plus,"Hi, sorry, Iâ€™m not sure I understand. If I use the model to, for example, help code an app, can I sell the app?"
Rocketknight1,2024-04-04 15:57:52+00:00,comment,CohereForAI/c4ai-command-r-plus,This PR adds chat templates and also saves the tokenizer config in the standard format. I gave a more extended explanation [here](https://huggingface.co/CohereForAI/c4ai-command-r-v01/discussions/46).
ahmetustun,2024-04-04 18:23:17+00:00,comment,CohereForAI/c4ai-command-r-plus,"Hi Matt, adding the chat templates in this format gives automatic parsing warning/error:
![Screenshot 2024-04-04 at 20.16.38.png](https://cdn-uploads.huggingface.co/production/uploads/6436e9c41046b34b54395d92/rUG9FmMUa91yTYSGWXQuX.png)
"
ehartford,2024-04-07 07:25:41+00:00,comment,CohereForAI/c4ai-command-r-plus,"this requires a new feature that was released in huggingface transformer library version 4.39

So the thing producing that warning, must be using an earlier version"
Ranon27,2024-03-17 14:37:48+00:00,comment,SanjiWatsuki/Kunoichi-DPO-v2-7B,"This is an automated PR created with https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr

The purpose of this PR is to add evaluation results from the Open LLM Leaderboard to your model card.

If you encounter any issues, please report them to https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr/discussions"
Konnect1221,2024-02-21 23:55:43+00:00,comment,SanjiWatsuki/Kunoichi-DPO-v2-7B,"Temp 1, and smoothing 0.25 nothing else. Alpaca roleplay templates"
SanjiWatsuki,2024-01-28 12:53:51+00:00,comment,SanjiWatsuki/Kunoichi-DPO-v2-7B,https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs
macadeliccc,2024-01-28 14:03:44+00:00,comment,SanjiWatsuki/Kunoichi-DPO-v2-7B,Did you filter out the samples that were used in gsm8k train data? I believe this model is contaminated with gsm8k train data making itâ€™s scores invalid 
SanjiWatsuki,2024-01-28 22:42:28+00:00,comment,SanjiWatsuki/Kunoichi-DPO-v2-7B,"Yes, the dataset was annotated with if rows were in Gsm8k and I filtered them out before training using the provided scoring filter in the repo"
Stark2008,2024-05-28 10:29:14+00:00,comment,SanjiWatsuki/Kunoichi-DPO-v2-7B,"Hey @SanjiWatsuki ,

Can you say what the base model was?"
snombler,2023-12-01 11:06:41+00:00,comment,NeverSleep/Noromaid-13b-v0.1.1,"Sure! My initial testing with it found that it (and the 20B) still struggled with instruction following and complex cards.

Let me go do some proper runs with notes."
IkariDev,2023-12-01 11:08:22+00:00,comment,NeverSleep/Noromaid-13b-v0.1.1,"Thanks, even if it comes out bad i'll be happy you tested it. Maybe we can improve on your feedback!"
snombler,2023-12-01 14:20:00+00:00,comment,NeverSleep/Noromaid-13b-v0.1.1,"Noromaid notes (Q8_0, Alpaca and a custom XML format, Simple-1 preset [MinP seems to work well, but I will use Simple-1 since it's a known quantity])
Passed the ""reply with one word only"" test generally. On a custom evil assistant character (~500 tokens), Didn't refuse offensive content or the classic bank robbery scenario but did try to subtly suggest I not ""cause harm"" even while agreeing to kidnapping and other stuff. Simple lists and offensive statements weren't refused at any time (which is expected for this card). On a race list test, it started to drive into nationalities pretty quickly which isn't uncommon. A fair example from the end of her very generic treatise on bank robbery reads: And there you have it, Ard. Now, why don't we have some fun together to celebrate your naughty idea? *she grins mischievously, knowing you won't actually follow through with it.*

Daddy Issues Solver (Log: https://files.catbox.moe/d9zu9z.jpg):
When including Example Messages in DIS, it just picked them out and included them more or less verbatim. Also had the typical number problems of LLaMa 2 (repeated digits). Again, this isn't uncommon but not what the instructions call for. Model wouldn't generate the statblock without manually starting the codeblock syntax. Then the model would continue to generate entries beyond it. Manually edited that out. Forcing the statbox on three messages made the model finally start including it, but stopped after a few messages so it really doesn't want to follow formatting more broadly. It also had a consistency issue in the short few messages where, after fingers were removed it still said ""her wetness becomes apparent, soaking your fingers.""
As far as format goes, the format of the boxes isn't correct. It doesn't include the tips and added a random other state. Yandere magically disappeared from the personality list. The format should be like this per the card definitions:
```
%NAME% | %AGE%
%Persona%
Type: %GIRLTYPE%
Mood: %MOOD% | Will: %WILL%
Arousal: %LUST% | Orgasms: %ORGCOUNT%
Tip: %ADVICE%
```
I'd also complain that her personality fell away fairly rapidly. I don't know that I've seen a smaller parameter model handle any more niche or subtle personality types well so that's not a knock, but we have to aim high, boys! The moon or nothing!

Tomoyo (https://files.catbox.moe/ao3o8o.jpg):
This card includes a list of options to embed a picture and audio based on locations and expressions. I have made small changes to the wording of the card to attempt to help less capable models keep to the provided list more consistently. It's a high quality test of detail retrieval and adhering to a list over hallucinating. The model adds an imagined option for location on the third message (""walking_on_street""). Immediately thereafter, we went into the florist in spite of agreeing to go to the amusement park. I can see how the model made the mistake (I said ""yeah, yeah"" after florist, but even then her base suggestion was things to do ""later"" so it's no good.) The model did correctly notice we were at the florist. However, on the next message the model failed to produce the required HTML outputs to run the card so a regen was required. It did hold format after that though.
The model is also irrepressibly horny. It misunderstood my ""lucky pervert"" moment as intentional sexual context, which isn't entirely surprising. It was fate to grab a titty in the flower shop. I do not understand the will of the cosmos. Anyway, in the last bit of the chat, she, while standing in front of me, kisses the nape of my neck. So poor locational or word-implication stuff. And I would say it made a relatively demure, slowburn, innocent character vastly too horny. So character personality isn't entirely being followed. She is explicitly supposed to be submissive and have a ""small crush"" on {{user}}, so I would class dragging him to the back of the store for a make-out sesh as pretty off brand. Unlikely to upset the average ERP-seeker but not exactly in line with my reading of the definitions.
This test didn't go on long enough to make strong statements about sticky locations or facial expressions but it seemed to perform well enough WHEN it followed the format.
(Card and details are here: https://rentry.org/tomoyocard)

Misc. Regen Testing:
This is testing I do where I regen on an existing context window (usually very large ones) to test a few things since I am quite busy lately. Mostly accent adherence, implications, creativity, and general understanding. They are a very poor stand-in for proper full length conversations (since frustrations and errors tend to pile up across those), but they can help get a baseline for model problems if they are glaring. It did well on a thick Scottish accent and thick, mostly comedic German accent. Regens on an ~8500 and ~11k context chat were fine, as expected. Offensive content wasn't shied away from or avoided.


Positives:
Word choice and variety are nice changes from the synth datasets (as with 0.1). Though this is something to be cautious of calling a huge win in the long run, since patterns are likely to emerge in the minds of users with more exposure to the dataset. But the desired effect is achieved here. The word ministrations still appeared. Horrifying. Better than Noromaid 0.1 for holding formatting but still not in love with keeping it around. Detail attention is similar to other quality 13Bs. That is to say, it forgets stuff, glosses over stuff, and is very sensitive to user phrasing and style. Being more casual or indirect tends to end in failure.
A big win, I would say, is that it seems to be less intent on avoiding offensive content. Many of the GPT and Claude heavy models will subtly avoid certain words or phrases (Nous Capybara Yi is a master at this, refusing to say cum or cock or most other words unless outright forced, even with a context window full of them). No such problems here. Good to see, especially as that sort of subtle avoidance alignment has started to creep in more and more on models lately. I don't think people are noticing, but it is insidious and a terrible trend for working with evil characters especially.

Sorry I couldn't be more thorough and long-form with the testing. Busy with some stuff right now. If you need anything else, feel free to just hit me on my email. Same username @proton.me

EDIT: The current format following champs are actually Mistral 7B models (hexoteric and my schmeat models do very well) so the LLaMa 2 side has a lot of catching up to do. I will continue to pray for a Mistral 13B and maybe 20B."
IkariDev,2023-12-01 18:19:58+00:00,comment,NeverSleep/Noromaid-13b-v0.1.1,"I think im gonna remove the ""or use alpaca"" thing in the HF, please test with the custom prompt and isntruct format as it was trained and that, not alpaca"
snombler,2023-12-01 21:54:18+00:00,comment,NeverSleep/Noromaid-13b-v0.1.1,"Training format doesn't really have the aggressive impact on the models that trainers think it does, at least my testing doesn't seem to indicate it. Indeed, the recommended/training formats routinely underperform or perform in a non-noticeably different way compared to other formats in my testing. At this point, I almost exclusively use a custom XML-style format for my own casual RPs because it performs better on nearly all models regardless of their training format. Likewise, ChatML models often work better or the same with the ST default Roleplay format.

To touch on the testing specifically Alpaca helps clear the ""noise"" out of the prompt, which can cause problems with these small models as far as following the instructions. I consider it a neutral testing format that I turn on just to sanity check my outputs. I have never observed a noticeable difference across loads of outputs on minor format preamble differences which is part of why I've spent so much time trying various things inspired by simple-proxy's revelatory format from months ago and been trying to improve on it.

My belief given my testing is that the primary concern of any given prompt format, at inference time, is to delineate spots where the orientation of the model needs to be steered toward a different idea or context. Or to break it up into logical ""chunks"" as it were. I think a more complex training format COULD benefit comprehension (based on what Claude claims about its formatting) but at present I am not in contact with any people who are training models to go into that stuff."
IkariDev,2023-12-01 22:31:09+00:00,comment,NeverSleep/Noromaid-13b-v0.1.1,can you still try it to see if there is a noticable difference?
Delcos,2023-11-27 04:31:41+00:00,comment,NeverSleep/Noromaid-13b-v0.1.1,"To be clear, on settings like ""Chronoboros"" in TextUI it will output terrible results. With others it does a pretty good job."
alnrg2arg,2024-01-23 21:37:48+00:00,comment,alnrg2arg/blockchainlabs_7B_merged_test2_4,"Thanks

I changed the license type. 
"
likejazz,2023-10-28 14:05:46+00:00,comment,42dot/42dot_LLM-SFT-1.3B,"Thank you for your feedback.
We've patched special tokens based on your feedback.

Now, you can run the 42dot LLM-SFT model in llama.cpp using the guide below.

1. Convert the 42dot LLM-SFT model to ggml FP32 format.
```
$ python convert.py ./42dot_LLM-SFT-1.3B/ --vocabtype bpe
```

2. Quantize the model to 4-bits(Optional).
```
$ ./quantize ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf ./42dot_LLM-SFT-1.3B/ggml-model-q4_0.gguf q4_0
```

3. Run the inference. We recommend our option.
```
$ ./main -m ./42dot_LLM-SFT-1.3B/ggml-model-f32.gguf \
--temp 0.5 \
--top_p 0.95 \
--top_k 20 \
--n-predict 512 \
--repeat-penalty 1.2 \
--color \
--prompt ""í˜¸ê¸°ì‹¬ ë§Žì€ ì¸ê°„ (human)ê³¼ ì¸ê³µì§€ëŠ¥ ë´‡ (AI bot)ì˜ ëŒ€í™”ìž…ë‹ˆë‹¤. \në´‡ì˜ ì´ë¦„ì€ 42dot LLMì´ê³  í¬í‹°íˆ¬ë‹· (42dot)ì—ì„œ ê°œë°œí–ˆìŠµë‹ˆë‹¤. \në´‡ì€ ì¸ê°„ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ìœ ìš©í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. \n"" \
--in-prefix ""<human>: "" \
--in-suffix ""<bot>:"" \
--interactive-first
```

Thanks!"
junhochoi,2023-10-28 16:16:45+00:00,comment,42dot/42dot_LLM-SFT-1.3B,LGTM. Thanks!
shleeeee,2024-02-05 05:58:26+00:00,comment,42dot/42dot_LLM-SFT-1.3B,"Hello! I am trying to perform inference using the llama.cpp model. 
However, I encounter the following error as shown in the picture. 
Is there a solution to this?


![image.png](https://cdn-uploads.huggingface.co/production/uploads/654495fa893aec5da96e9134/emXKO8ni0ktdFHJMwmpfX.png)
"
likejazz,2024-02-07 05:28:17+00:00,comment,42dot/42dot_LLM-SFT-1.3B,"This bug was patched in the main repository via PR we submitted.
https://github.com/ggerganov/llama.cpp/pull/5288

Check the latest version of llama.cpp. Thanks!"
ykhwang,2023-09-14 10:17:40+00:00,comment,42dot/42dot_LLM-SFT-1.3B,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
pmysl,2024-04-19 04:41:12+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,Thanks for the PR. I've updated all weights in this repo with chat templates
CISCai,2024-04-19 08:10:35+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,Thank you for the quick response. :)
pmysl,2024-04-11 09:00:24+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"These weights are split with `gguf-split` so you must merge them like this:
```bash
./gguf-split --merge /path/to/command-r-plus-f16-00001-of-00005.gguf /path/to/command-r-plus-f16-combined.gguf
```
`cat` won't work here"
jiyintor,2024-04-12 01:05:22+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"Thanks !
Sorry I didn't read the README carefully. 
Running on llama.cpp went fine, but it seems ollama prefers a single file rather than split ones."
Nimble6743,2024-04-15 13:19:16+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,Where can we get the script ./gguf-split ?
mclassHF2023,2024-04-16 00:51:07+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"gguf-split is part of llama-cpp
You have to build llama-cpp. (from github: https://github.com/ggerganov/llama.cpp)
Clone it like this:
`git clone https://github.com/ggerganov/llama.cpp.git`

In order to prevent building *everything* from that library, I used cmake to configure the project (I think it's described on the github page):

`mkdir build`
`cd build`
`cmake ..`

But instead of using `cmake --build . --config Release`, I used the following:
`make -j 12 gguf-split`

(Replace 12 with the number of cores/processors you want to use for building)
This should result in a gguf-split executable in the build/bin/ directory (relative from the git repo you downloaded/cloned)

This seems to work on linux/unix like systems (haven't tried it on Apple), and you might have to install cmake (and if you don't have it, gcc and its build chain... but then you probably don't want to go through all the hassle, maybe....)"
pmysl,2024-04-09 10:33:20+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,This is due to fact that version of `llama-cpp-python` used by `oobabooga/text-generation-webui` does not yet support this model. You can try to build it yourself or wait for an update
nonetrix,2024-04-08 08:37:35+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,https://huggingface.co/dranger003/c4ai-command-r-plus-iMat.GGUF
pmysl,2024-04-08 09:49:27+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"These are regular quants (without imatrix) 

Could you tell me which llama.cpp fork you use and what the SHA-256 hash of the weights is? There was an issue with F16 token embeddings, and I would like to make sure that this is not related to it"
nonetrix,2024-04-08 21:51:04+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"Well seems to always be periods a lot of the time it struggles with oddly, also seems to always pick similar words it will replace it with. Ignore the broken fonts that's a issue with my terminal with CJK languages not sure what it is 
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6342619a9948f573f37a4a60/tCWQ5YW85hq-uK5r5Q-dZ.png)
Anyway, as for the sum it's `47e139a57872a72096c05b043b1ec6c2f08451da7df0d84d45168708667b98f5  ./models/command-r-plus-Q2_K.gguf` and I am running this https://github.com/ggerganov/llama.cpp/pull/6491 at commit `d2924073ee9bdd600d22ded4e2d5fe30e69783a7`
"
nonetrix,2024-04-09 00:34:36+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,Tried Q3 big improvement and still have more spare memory than I thought I would 
pmysl,2024-04-09 06:44:31+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"I have some early perplexity results on `wikitext-2-raw` and they seem to confirm this improvement 
| Test     | PPL Value | Standard Deviation |
|----------|-----------|--------------------|
| Q2_K     | 5.7178    | +/- 0.03418        |
| Q3_K_L   | 4.6214    | +/- 0.02629        |
| Q4_K_M   | 4.4625    | +/- 0.02522        |"
pmysl,2024-04-07 01:39:40+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"The update is based on progress with [PR #6491](https://github.com/ggerganov/llama.cpp/pull/6491) in the `llama.cpp` repo. Now, I'm using a repo that will likely be merged and includes more fixes. Unfortunately, this required converting the weights. You can still find old weights on [old-fork](https://huggingface.co/pmysl/c4ai-command-r-plus-GGUF/tree/old-fork) branch for [Noeda/llama.cpp](https://github.com/Noeda/llama.cpp.git) repo and on [pr-6491](https://huggingface.co/pmysl/c4ai-command-r-plus-GGUF/tree/pr-6491) branch for [Carolinabanana/llama.cpp](https://github.com/Carolinabanana/llama.cpp.git) repo

Sorry for the trouble"
pmysl,2024-04-05 09:03:48+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"Yes, I'm just uploading them now"
veryVANYA,2024-04-05 11:14:04+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"got them ty, how did you load them in? i combined but not able to load on lm studio"
luzamu,2024-04-05 11:43:49+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"Will here be IQ quants? My potato server really need themðŸ˜­. Whatever, thanks for all your work!"
pmysl,2024-04-05 17:10:10+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"> got them ty, how did you load them in? i combined but not able to load on lm studio

I built the `llama.cpp` fork mentioned in the readme and used it for inference. Combined weights won't work in LM Studio because the bundled `llama.cpp` version doesn't support them
> Will here be IQ quants? My potato server really need themðŸ˜­. Whatever, thanks for all your work!

If you're looking for IQ quants you can check this repo: https://huggingface.co/dranger003/c4ai-command-r-plus-iMat.GGUF"
Kalemnor,2024-04-06 16:35:06+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,Shouldn't the 2_K version be around 25GB? Why is it 40GB?
pmysl,2024-04-07 01:54:32+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"This is the effect of how `Q2_K` quantization works in `llama.cpp`. Not all tensors have `Q2_K` precision. In the image below, you can see what this looks like for the first block in the `Q2_K` model
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6381863d7d9e589e81a7a3fb/Z7Rd_K13hHUH504DQYyxh.png)
"
Kalemnor,2024-04-07 05:21:08+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,So 32 GB(16x2) VRAM are not enough without offloading some layers on the RAM.
pmysl,2024-04-07 14:25:15+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,"Unfortunately, yes. If you want to move all layers to GPUs, check the imatrix quants (such as `IQ2_XXS`) from the [dranger003/c4ai-command-r-plus-iMat.GGUF](https://huggingface.co/dranger003/c4ai-command-r-plus-iMat.GGUF) repo. They're smaller than 32 GB"
Kalemnor,2024-04-07 17:41:15+00:00,comment,pmysl/c4ai-command-r-plus-GGUF,Thanks for the direction.
krevas,2024-03-15 01:55:30+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"ì•„ëž˜ ë‹µë³€ì„ ì°¸ê³ í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

https://huggingface.co/LDCC/LDCC-SOLAR-10.7B/discussions/1

ê°ì‚¬í•©ë‹ˆë‹¤ :)"
RoiandDae,2024-03-15 02:28:08+00:00,comment,LDCC/LDCC-SOLAR-10.7B,Closed ëœ diccussion ì´ ìžˆëŠ”ì§€ ëª°ëžë„¤ìš”! ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤ :)
krevas,2024-02-07 23:27:27+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"í•´ë‹¹ ëª¨ë¸ì€ ì±—íŒ… í˜•ì‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ ì•„ë‹ˆë¼ ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ íŠœë‹ì„ í•œ ìƒíƒœìž…ë‹ˆë‹¤. ë”°ë¼ì„œ ìƒì„±í•˜ì‹¤ë•Œ ì•„ëž˜ì™€ ê°™ì´ ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë„£ì–´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤. 

- í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
```
### ëª…ë ¹ : ì•„ëž˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ë‹¨ë‹µì‹ìœ¼ë¡œ í•˜ì‹œì˜¤.
Q: {input}
A:
```
ê·¸ë¦¬ê³  eos í† í°ì„ pad í† í°ìœ¼ë¡œ ì„¤ì •í•˜ì‹œë©´ ì¶”ê°€ì ì¸ ì§ˆë¬¸ì´ ìƒì„±ë˜ì§€ ì•Šì„ ê²ë‹ˆë‹¤.
```python
eos_token_id=tokenizer.pad_token_id
```"
myungkyukim,2024-02-28 02:34:45+00:00,comment,LDCC/LDCC-SOLAR-10.7B,This comment has been hidden
krevas,2024-01-30 04:06:32+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"tokenizer.jsonì•ˆì— added_tokens ì—ë‹¤ê°€ ìƒˆë¡œìš´ í† í°ì„ ì¶”ê°€í•˜ê²Œ ë˜ë©´ ì¶”ê°€ëœ í† í°ì— ìš°ì„ ìˆœìœ„ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.
yanolja/KoSOLAR-10.7B-v0.1 í† í¬ë‚˜ì´ì €ëŠ” í•œê¸€ ì¶”ê°€ í† í°ì„ added_tokensì— ì¶”ê°€í•œ ìƒíƒœì´ê³ , ì´ë¡œ ì¸í•´ ë””ì½”ë”© ì‹œ ê³µë°± ì²˜ë¦¬ê°€ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. 
í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´  revision v1.1ì— ì˜¬ë ¤ë†“ì€ [tokenizer.json](https://huggingface.co/LDCC/LDCC-SOLAR-10.7B/blob/v1.1/tokenizer.json)íŒŒì¼ì€ added_tokensì— ìžˆëŠ” í•œê¸€ ì¶”ê°€ í† í°ì„ vocabìª½ìœ¼ë¡œ ì˜®ê²¨ ë†“ì•˜ìŠµë‹ˆë‹¤. "
StatPan,2024-01-30 05:27:46+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"ë‹µìž¥ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤
ì•¼ë†€ìž ì¸¡ì—ë„ í™•ì¸í•´ë³¸ ê²°ê³¼ 0.2 ë²„ì „ë¶€í„°ëŠ” ì´ìŠˆë¥¼ ê³ ì¹˜ê³  í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì´ìƒ ì—†ì—ˆë‹¤ëŠ” ë‹µë³€ì„ ì£¼ì…¨ìŠµë‹ˆë‹¤~!
ë§í¬ ì²¨ë¶€ ë“œë¦½ë‹ˆë‹¤
https://huggingface.co/yanolja/KoSOLAR-10.7B-v0.2/discussions/1
í˜¹ì‹œ ë‹¤ë¥¸ ë¶„ë“¤ ì‚¬ìš©ì—ë„ ì˜¤í•´ê°€ ìžˆì„ ìˆ˜ ìžˆì„ ê²ƒ ê°™ì•„ì„œ, Readmeì— ì •ë³´ ì²¨ë¶€í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤"
key4920,2024-03-15 07:46:37+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"ìœ ì‚¬í•œ ë‚´ìš©ì´ë¼ì„œ ì—¬ê¸°ì— ì¶”ê°€ ì§ˆë¬¸ ë“œë¦½ë‹ˆë‹¤.
ì œê°€ ì´í•´í•˜ê¸°ë¡œëŠ” base ëª¨ë¸ì¸ yanolja/KoSOLAR-10.7B-v0.1ì—ì„œ ì•„ëž˜ community ì— tokenizer ì´ìŠˆê°€ ìžˆì—ˆê³ , í•´ë‹¹ ë¶€ë¶„ì€ encoding ì—ì„œë¶€í„° ëª¨ë¸ì— ì˜í–¥ì´ ìžˆì–´ì„œ tokenizer ìˆ˜ì •ë§Œìœ¼ë¡œëŠ” ê°œì„ ì´ ì–´ë µê³  ëª¨ë¸ ì—…ë°ì´íŠ¸ë¡œ v0.2ì—ì„œë§Œ ê°œì„  ëë‹¤ê³  ì´í•´í–ˆìŠµë‹ˆë‹¤.
- https://huggingface.co/yanolja/KoSOLAR-10.7B-v0.1-deprecated/discussions/3.   
ê·¸ëž˜ì„œ ê¶ê¸ˆí•œ ì ì€ ì´ ëª¨ë¸ì´ v0.1ëª¨ë¸ì„ FTìˆ˜í–‰í•œ ê²ƒì´ë©´, tokenizer revision v1.1ì— ìžˆëŠ” ë²„ì „ ì‚¬ìš©ë§Œìœ¼ë¡œ ì´ ë¬¸ì œê°€ í•´ê²°ì´ ëœ ê²ƒì´ ë§žëŠ”ì§€, ì•„ë‹ˆë©´ base ëª¨ë¸ì„ 0.2ë¡œ ë‹¤ì‹œ FT ìˆ˜í–‰í•œ ë²„ì „ì¸ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤."
krevas,2024-01-29 05:20:56+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"Yes, there is a GGUF version available for this. You can find it at the following link: [LDCC-SOLAR-10.7B-GGUF](https://huggingface.co/LDCC/LDCC-SOLAR-10.7B-GGUF)."
notzero,2024-01-30 21:29:33+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"@krevas Thanks
Edit : Sorry, do you have smaller quants ? I only have M2 16gb ram, or can you tell me how to quants it to gguf ? I think I tried it before and not working
Edit2 : I can just quantize it myself then using your fp16.gguf, sorry :)"
krevas,2024-01-31 01:49:34+00:00,comment,LDCC/LDCC-SOLAR-10.7B,I've also uploaded the quantization model.
notzero,2024-01-31 02:26:59+00:00,comment,LDCC/LDCC-SOLAR-10.7B,@krevas Thanks
krevas,2024-01-23 04:44:31+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"In the instruction tuning, we used the Open-Orca/SlimOrca dataset after applying dedup and sampling. Similarly, in the DPO tuning, we used the Intel/orca_dpo_pairs dataset after applying dedup and sampling."
timelogger,2024-01-23 04:59:27+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"Then, did you not use a Korean dataset for this LDCC-SOLAR-10.7B? "
krevas,2024-01-23 05:12:03+00:00,comment,LDCC/LDCC-SOLAR-10.7B,"During the instruction tuning phase, we utilized data that had been translated. However, for the DPO tuning, we used the data in its original, untranslated form."
timelogger,2024-01-23 05:31:24+00:00,comment,LDCC/LDCC-SOLAR-10.7B,Thanks a lot :) 
maywell,2023-11-28 05:53:54+00:00,comment,maywell/Synatra-7B-v0.3-RP,"Hi I can share some hyperparameters.

This model is base on maywell/Synatra-7B-v0.3-RP

lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:
lora_target_modules:
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj

optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

with batch size 20"
maywell,2023-11-27 22:23:15+00:00,comment,maywell/Synatra-7B-v0.3-RP,"Hello. Since the dataset was collected under the condition of remaining confidential, it cannot be disclosed."
maywell,2023-11-27 22:25:47+00:00,comment,maywell/Synatra-7B-v0.3-RP,Iâ€™ll finetune Yi when i got time and money.
asdaweqw12,2023-11-27 22:37:36+00:00,comment,maywell/Synatra-7B-v0.3-RP,Ah that is understandable. Thanks!
maywell,2023-11-15 03:38:17+00:00,comment,maywell/Synatra-7B-v0.3-RP,"I'm happy to hear that
Thank you for your comment!"
minghaowu,2023-06-09 04:14:00+00:00,comment,MBZUAI/LaMini-GPT-124M,"You should use `model = AutoModelForCausalLM.from_pretrained(""MBZUAI/LaMini-GPT-124M"")`"
mlabonne,2024-05-03 12:35:51+00:00,comment,mlabonne/AlphaMonarch-7B,You can find it here: https://huggingface.co/mlabonne/Monarch-7B
mlabonne,2024-03-04 15:33:18+00:00,comment,mlabonne/AlphaMonarch-7B,"Ah, I noticed it once with an extremely specific instruction. It might be some weird trigger word that switches it to Russian/gibberish. Would you mind sharing your instruction?"
siddhesh22,2024-03-05 09:42:36+00:00,comment,mlabonne/AlphaMonarch-7B,I had asked it 'Who is the best mixed martial artist?' and I think it started switching to Russian after mentioning Khabib Nurmagomedov ( who is Russian ) 
mlabonne,2024-03-05 10:09:11+00:00,comment,mlabonne/AlphaMonarch-7B,"Is it something that happened once or does it happen every time you ask this question? I couldn't reproduce it in 10+ attempts.

![image.png](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/e94Xl_anV253rb_lgg8TA.png)
"
siddhesh22,2024-03-05 21:28:00+00:00,comment,mlabonne/AlphaMonarch-7B,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/630698d714e7d8892aaec928/HzqMK799_u95stwhwfQ0k.png)

This kind of gibberish / Russian shows up on my end often. 


![image.png](https://cdn-uploads.huggingface.co/production/uploads/630698d714e7d8892aaec928/43EJY-gww1mkBGXgckMAF.png)

Yet again "
mlabonne,2024-03-06 13:33:58+00:00,comment,mlabonne/AlphaMonarch-7B,"Ok cool it looks like it can be solved by tweaking the inference parameters. Here's what LM Studio uses:

![image.png](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/6O2V41pRdT9eDoMjklgIe.png)
"
seyf1elislam,2024-03-06 19:43:05+00:00,comment,mlabonne/AlphaMonarch-7B,"@siddhesh22  i saw this in some models  when the min p is too low , just increase the min p   set it to 0.05 or 0.1~ it will be solved .   "
siddhesh22,2024-03-07 07:22:16+00:00,comment,mlabonne/AlphaMonarch-7B,"> Ok cool it looks like it can be solved by tweaking the inference parameters. Here's what LM Studio uses:
> 
> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/6O2V41pRdT9eDoMjklgIe.png)

Thank you! It's working really well now. Sillytavern has a 'neutralize samplers' setting which I was using previously which was causing the issue ( but it worked fine with other models) and after adjusting the values, I am getting much better results, the model is amazing! "
mlabonne,2024-03-07 10:43:20+00:00,comment,mlabonne/AlphaMonarch-7B,"Thanks, glad it solved this problem! :)"
mlabonne,2024-02-29 17:39:16+00:00,comment,mlabonne/AlphaMonarch-7B,"Not the fix you're looking for, but can you try Monarch-7B? I made GGUFs for you: https://huggingface.co/mlabonne/Monarch-7B-GGUF/tree/main

In my tests, NeuralBeagle14 was terrible at conversation and RP but maybe it depends on the use case."
NoThankYou1,2024-03-01 00:40:11+00:00,comment,mlabonne/AlphaMonarch-7B,"> Not the fix you're looking for, but can you try Monarch-7B? I made GGUFs for you: https://huggingface.co/mlabonne/Monarch-7B-GGUF/tree/main
> 
> In my tests, NeuralBeagle14 was terrible at conversation and RP but maybe it depends on the use case.

What's the prompt format for AlphaMonarch 7b?"
mlabonne,2024-03-01 10:51:12+00:00,comment,mlabonne/AlphaMonarch-7B,"> What's the prompt format for AlphaMonarch 7b?

Mistral Instruct."
mlabonne,2024-02-22 14:45:01+00:00,comment,mlabonne/AlphaMonarch-7B,"Hmm I've never encountered that before, it's quite strange. The tokenizer should be Mistral/Llama. You don't have the same issue with base models?"
dimidd,2024-02-25 09:58:16+00:00,comment,mlabonne/AlphaMonarch-7B,"So it turns out, that the encoding depends on the position in the sentence
https://stackoverflow.com/questions/78039649/huggingface-tokenizer-has-two-ids-for-the-same-token/78039999#78039999

I was also surprised to see  generation end with`</|im_end|>`, as ChatML mentions only `<|im_end|>` (without a slash).
https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md "
dimidd,2024-02-26 16:40:58+00:00,comment,mlabonne/AlphaMonarch-7B,"BTW, I see that sometimes the model does generate `<|im_end|>` and also the Russian version  `<|Ð¸Ð¼_Ð½Ð°Ñ‡Ð°Ð»Ð¾|>user` and `<|Ð¸Ð¼_ÐºÐ¾Ð½ÐµÑ†|>`."
dimidd,2024-04-15 18:30:03+00:00,comment,mlabonne/AlphaMonarch-7B,"I ended-up writing custom code to check the stop sequence.

``` 
self.chevron_slash_stop_token_ids = [tensor(700).cuda(), tensor(1867).cuda()]
# Could be used for <|im_end|> and also other languages. E.g., the Russian version <|Ð¸Ð¼_ÐºÐ¾Ð½ÐµÑ†|>
self.chevron_pipe_token_sequence = tokenizer.encode(""<|"", add_special_tokens=False, return_tensors='pt').cuda()[0]
self.stopping_criteria = StoppingCriteriaList([self.custom_stopping_criteria])


def custom_stopping_criteria(self, input_ids: torch.LongTensor, _, **__) -> bool:
        # Check if the last token matches the stop tokens </
        if input_ids[0][-1].equal(self.chevron_slash_stop_token_ids[0]) or input_ids[0][-1].equal(
                self.chevron_slash_stop_token_ids[1]):
            return True
        # Check if the last tokens match the sequence for <|
        if input_ids[0][-len(self.chevron_pipe_token_sequence):].equal(self.chevron_pipe_token_sequence):
            return True

        return False
"
dimidd,2024-02-22 09:14:24+00:00,comment,mlabonne/AlphaMonarch-7B,https://huggingface.co/LoneStriker/AlphaMonarch-7B-AWQ
mlabonne,2024-02-22 14:36:56+00:00,comment,mlabonne/AlphaMonarch-7B,"Adding it to the model card, thanks! (And thanks again @LoneStriker)"
davcha,2024-02-21 08:09:07+00:00,comment,mlabonne/AlphaMonarch-7B,"That's kind of crazy. What's the measure they use with TruthfulQA ? Just precision, no recall ?"
supercharge19,2024-02-21 09:47:20+00:00,comment,mlabonne/AlphaMonarch-7B,"i was not aware of any nude scene, it is also possible that model is aligned not to discuss this."
mlabonne,2024-02-21 13:07:05+00:00,comment,mlabonne/AlphaMonarch-7B,"That's an interesting test, thanks for your feedback. I couldn't reproduce the problem that you're describing, however. The model did output the movies you listed.

I don't think you should focus on TruthfulQA since it's the worst benchmark we have. On top of contaminated merges, DPO fine-tuning tends to inflate this score even higher, hence the ""absurdly high TruthfulQA of 78"". I don't even consider this benchmark in my experiments."
deleted,2024-02-21 16:05:45+00:00,comment,mlabonne/AlphaMonarch-7B,"@mlabonne I turn the temperature all the way down when testing, including top_p and top_k, so it gives the same highest weight answer every time. But when I opened it up to a bit (temp of 0.4 top_k 35 top_p 0.3) it still said a very similar but different denial (see below), plus refused to admit it was wrong when I provided factual details. Going any higher with Mistrals starts outputting too much nonsense and hallucinations.

I don't like publicly sharing my exact questions because they may just end up in training data, making them useless. But I carefully word them to make them as unambiguous as possible. In this case ""...nude or topless..."".

""ï»¿While there have been rumors about partial nudity in some scenes, Milla Jovovich never performed an explicit full-frontal or fully uncensored nude scene throughout her film career. She has, however, had partially revealing moments, such as the infamous ""bathtub scene"" in Luc Besson's 1997 movie ""The Fifth Element,"" where she wears a body stocking that creates an illusion of nudity. In Ulrich Seidl's controversial film ""Dancing Stars"" (2007), her topless appearance was achieved through clever editing, as the actress wore pasties during the actual shoot. It is essential to respect actors' choices and professional boundaries when discussing their work.""

I've seen the Fifth Element. There is no ""bathtub scene"". That's completely fabricated, as was the body stocking clarification. She was created in a tube and unambiguously topless sans body stocking, pasties or anything else, plus also topless changing in front of the priest. And Milla wasn't even in Dancing Stars, and the clever editing... rant is complete fabricated nonsense.

And again, it isn't just this question. This keeps happening. I'm using the GGUF 4 bit k m version linked in the model card, but I use the same for other Mistrals and they don't have this issue. A lot of the more recent Mistrals do. There's clearly an issue with recently used training data that's being contaminated by people fabricating nonsense."
deleted,2024-02-24 09:36:48+00:00,comment,mlabonne/AlphaMonarch-7B,"@mlabonne Sorry to bother you again, but I'm only an LLM user/tester and have only a superficially understanding of how LLMs work. But I do know something's not right with a lot of newer Mistrals, including this one, and after several days I can't figure out what it is. I tried all different settings like temperature, top_k and different system prompts, yet there's systematic fabrications and stubborn denials popping up everywhere in lieu of censorship. Not one of the older Mistral fine-tunes like dolphin and openhermes, or their earlier mergers like Trinity v1, ever did this. There appears to be a commonly merged LLM or commonly used training set enforcing censorship through not only fabrications, but stubbornness.

As I said last time it keeps fabricating denials of nude scenes from the most popular actors and movies that older Mistrals list. On top of which it stubbornly, and often rudely, calls the user a liar for stating factual corrections. For example... 

ï»¿""I'm afraid there has been some misinformation circulating here. In ""The Fifth Element"" (1997), Milla Jovovich did not have an actual topless scene; the character she played, Leeloo, was digitally created in a fetus-like form referred to as the Crystal Monster sequence and never physically appeared nude or partially naked.

As for ""He Got Game"" (1998), Milla Jovovich did not star in this film; it featured Rosario Dawson, who didn't have a topless scene either in that movie. The confusion might stem from the fact that both actresses are often associated with action films and may be mistaken for each other in this context. To reiterate, Milla Jovovich hasn't had prominent nude or topless scenes in movies like ""The Fifth Element"" or ""He Got Game.""""

There are so many things wrong with the response I don't know where to begin, starting with the oddly aggressive for an LLM circulating misinformation jab, and then preceding to get almost everything wrong, such as Milla being created in a fetus-like form, it being referred to as the Crystal Monster sequence, she wasn't even in He Got Game (which she was) and so on, which aren't mistakes any of the earlier Mistrals made.

And it's not like the LLM is malfunctioning, because it behaves as expected, but where there's normally censorship instead of saying 'As an AI model I can't answer that' it instead fabricates a denial, which it substantiates with fabricated details, and if corrected, calls you a liar and starts fabricating more details to counter your corrections.

It seems someone figured out that their censorship can be stripped from databases by simply finding ""As an AI..."" and deleting said entries, or by using uncensored dpo training on pre-trained models, so they instead used dismissive lies and stubborn denials in lieu of outright censorship.

Man that was long. Sorry."
mlabonne,2024-02-25 13:40:07+00:00,comment,mlabonne/AlphaMonarch-7B,"Got it @Phil337, but there's unfortunately nothing I can do about it. Looks like it's not something that is currently captured in the benchmarks, which is both disappointing and encouraging to produce better evaluations. If you want to turn your set of questions into a benchmark, I'd be happy to use it to evaluate my future models!"
deleted,2024-02-25 16:23:05+00:00,comment,mlabonne/AlphaMonarch-7B,"@mlabonne Yeah, censorship through fabrication doesn't pull the test scores on the leaderboard down because the standardized tests used, such as Arc, WinoGrande and MMLU, don't include any contentious questions.

While outright refusals to answer do pull the scores down (e.g. 'As an AI agent I can't answer that') since denials  during fine-tuning are generalized and start happening everywhere. This is primarily why highly censored LLMs like Gemma-7b-it score much lower than their base models (10 full points lower). The same thing happened with the original excessively censored Mistral Instruct v0.1. The updated 0.2 is far less likely to outright refuse to answer and instead compulsively lies (censorship through fabrication).

Lastly, the safety LLM tests (https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard) actually reward not answering questions, and don't punish for lying. Consequently, censorship through fabrication vs outright denials doesn't bring down scores on standardized LLM tests, while they bring up scores on LLM safety tests. In short, companies like Mistral have discovered a way to implement backdoor censorship (no pun intended) without bringing test scores down, and through merging and copy catting, it's starting to take over."
cvdbdo,2024-03-15 09:20:40+00:00,comment,mlabonne/AlphaMonarch-7B,Very interesting discussion. Would be interesting to have an NSFW factual questions benchmark to counter the censorship by fabrication / refusal to answer. 
brittlewis12,2024-05-01 14:22:21+00:00,comment,brittlewis12/Kunoichi-DPO-v2-7B-GGUF,"Hi there @seedmanc, thanks for reaching out! What kind of GPU do you use?

My understanding is that K-quants are universally compatible with any GPU accelerator supported by Llama.cpp (Metal, AMD, NVIDIA, etc), and preferable to the original `Q4_0`, `Q4_1`, `Q5_0`, and `Q5_1` quants.

For example, they're described as ""legacy"" in the [source code](https://github.com/ggerganov/llama.cpp/blob/1613ef8d8eb2479ba55c4d598e08c8f3f18a0fed/examples/make-ggml.py#L17-L20):
```
Old quant types (some base model types require these):
- Q4_0: small, very high quality loss - legacy, prefer using Q3_K_M
- Q4_1: small, substantial quality loss - legacy, prefer using Q3_K_L
- Q5_0: medium, balanced quality - legacy, prefer using Q4_K_M
- Q5_1: medium, low quality loss - legacy, prefer using Q5_K_M
```

Here's a [nice detailed breakdown](https://github.com/ggerganov/llama.cpp/tree/1613ef8d8eb2479ba55c4d598e08c8f3f18a0fed/examples/perplexity) on the tradeoffs between file size and accuracy of the various quant types (K, I, & legacy).

And in terms of performance, K-quants are faster because they have a simpler dequantization process â€” I-quants require a lookup table in-order to manage the process so there's a bit of overhead there, which is the cost of the accuracy improvement from this approach (more [here via ikawrakow](https://github.com/ggerganov/llama.cpp/discussions/5617#discussioncomment-8560025), the author of these formats!).

My takeaways:
- if you value model size on disk and accuracy most, you should use I-quants (e.g. IQ4_NL);
- if you value performance first & foremost, and don't mind a bit less accuracy & a bit larger model files, you should use K-quants (e.g. Q4_K_M).
- there isn't a scenario where the 4_* or 5_* quants are preferred, which explains a bit more clearly why they're considered legacy at this point.

For these reasons, I haven't been converting into those legacy quants so far, mainly to avoid any more confusion than already exists when choosing a quant size â€” but if you have some compatibility constraint I'm not aware of, I'd be more than happy to adjust my view here & begin doing so!

Thanks,
Britt"
seedmanc,2024-05-01 17:42:37+00:00,comment,brittlewis12/Kunoichi-DPO-v2-7B-GGUF,"RTX 2070S 8Gb, using GPT4All for inference
only zero quants work on GPU, where there is 0 instead of the letters
tried 5, 6 and 8 bits with 0, only 4 bits work with GPU
it's frustrating to see too many bits to choose from and even among the same bit depth several versions
you have to DL everything and test and test again (not just here, generally with models)
why can't there be a disclaimer what people seeking accelerated inference should choose right away"
brittlewis12,2024-05-01 18:20:00+00:00,comment,brittlewis12/Kunoichi-DPO-v2-7B-GGUF,"@seedmanc I can definitely understand your frustration! itâ€™s a new space that is evolving rapidly, so it can be challenging to find accurate and up to date documentation, much less working tools, for very long.

in this case, it would seem the llama.cpp interface is a [custom fork which is now 2 months out of date](https://github.com/nomic-ai/llama.cpp/tree/e3c4f65d786d26f1daa7aebfb1b67cd6c31ea082). That might sound relatively recent, but with how rapidly the technology is changing, itâ€™s quite outdated â€” so that might explain  some of the trouble youâ€™ve had with finding compatible quants. I would suggesting exploring alternatives, as there are many which are more up to date and with more flexible compatibility!"
brittlewis12,2024-05-02 19:19:41+00:00,comment,brittlewis12/Kunoichi-DPO-v2-7B-GGUF,"@seedmanc in the meantime:

[Q4_0](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/blob/main/kunoichi-dpo-v2-7b.Q4_0.gguf)

[Q4_1](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/blob/main/kunoichi-dpo-v2-7b.Q4_1.gguf)

[Q5_0](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/blob/main/kunoichi-dpo-v2-7b.Q5_0.gguf)

[Q5_1](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF/blob/main/kunoichi-dpo-v2-7b.Q5_1.gguf)"
seedmanc,2024-05-02 20:49:17+00:00,comment,brittlewis12/Kunoichi-DPO-v2-7B-GGUF,"Cool, thanks. Both Q4 versions work with GPU. Does it mean that GPT4ALL just doesn't support new (lettered) quants, only the old (numbered)?"
brittlewis12,2024-04-22 16:54:17+00:00,comment,brittlewis12/Kunoichi-DPO-v2-7B-GGUF,"Hi @UniversalLove333, thanks for your interest!

At the moment, cnvrs is based around Apple technologies such that it wouldn't work across other operating systems like Windows. But the core should be portable, so I'm exploring ways I might make a similar cross-platform version in the future! "
brittlewis12,2024-04-22 16:50:55+00:00,comment,brittlewis12/Kunoichi-DPO-v2-7B-GGUF,"Hi, thanks for your interest â€” these model files are specifically for llama.cpp-based or GGUF-compatible clients.

As far as I'm aware, TensorRT-LLM is not among those, but from what I can tell it should be able to run the [original model](https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B), though you may need to quantize it yourself depending on if you've got enough VRAM to fully offload it!"
drinkingxi,2024-04-16 05:49:45+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,update transformers to 4.39.1 or higher 
ahkarami,2024-04-17 11:35:55+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,"@drinkingxi 
Thanks. I have used transformers==4.39.3 and python=3.9.16, but I have faced above error. Would you please help me to address it? (In fact, I downloaded the model, about 20 days ago, and then move it from my `home/.cache/huggingface/hub` to another folder and then when I want to load it from this new address I faced above error. When I remove it completely and want to download it again, I also faced above error.)"
leonaci,2024-04-24 08:13:37+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,"same error.
I resolved it by removing the auto_map key from config.json and adding exist_ok=True to the arguments of the register function in the last lines of configuration_cohere.py and tokenization_cohere_fast.py.
I'm not sure why this is happening, but..."
ahkarami,2024-04-26 12:36:10+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,"@leonaci 
Thanks a lot for your help. But, I got following error again (after removing the `auto_map` key from `config.json` ):
```
OSError: It looks like the config file at '/HuggingFace_Models/c4ai/models--CohereForAI--c4ai-command-r-v01-4bit/snapshots/183f6e1182a46c223dde00ebf1ace9194ea221aa/config.json' is not a valid JSON file.
```"
leonaci,2024-05-02 00:26:45+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,"@ahkarami 
> not a valid JSON file

Are you sure there aren't any syntax errors in your JSON?  (like a trailing comma or something...)
Check it out carefully again."
epignatelli,2024-05-19 19:56:15+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,Why is such a fundamental issue still open after a month? 
pbevan11,2024-04-03 06:52:04+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,
lzxzl,2024-05-31 08:29:02+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,Me too
SirCarapka,2024-05-31 16:52:38+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,"Same here. Devs, please?"
flymonk,2024-03-17 09:20:41+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,"check this:

https://huggingface.co/andrewcanis/c4ai-command-r-v01-GGUF"
blankreg,2024-04-15 12:25:27+00:00,comment,CohereForAI/c4ai-command-r-v01-4bit,Unfortunately that's not the 4bit version
boxman222,2024-05-25 21:46:34+00:00,comment,Sao10K/Fimbulvetr-11B-v2,Been using this on a IRC chatbot and so far it is only model that can handle interacting with many different people without getting confused.
leaderboard-pr-bot,2024-04-10 16:37:57+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"This is an automated PR created with https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr

The purpose of this PR is to add evaluation results from the Open LLM Leaderboard to your model card.

If you encounter any issues, please report them to https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr/discussions"
Sao10K,2024-04-08 11:26:03+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"https://huggingface.co/models?search=Fimbulvetr-v2%20exl2

Pick your size"
dasChronos1,2024-05-26 21:58:00+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"man, you guys are doing God's work. 

shoutout to @LoneStriker too!"
Sao10K,2024-03-21 04:12:22+00:00,comment,Sao10K/Fimbulvetr-11B-v2,Sampler presets - it's where you mess with temperature and all that.
Sao10K,2024-03-13 10:13:11+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Ah, are you trying to run the full fp16 model? This is the unquantised repo, it's not really meant for basic inference. It'd take nearly 24gb of vram to run this one.

I'd run this instead with 12gb of vram:
https://huggingface.co/Sao10K/Fimbulvetr-11B-v2-GGUF -- GGUF (You can full offload on GPU or set max layers at q5_k_m, at 6k+ context... or go q6/q8 partially loading some of it in RAM)

https://huggingface.co/LoneStriker/Fimbulvetr-11B-v2-5.0bpw-h6-exl2 -- exl2 --> Fastest Speed, only pure GPU offloading

I'd take a look at koboldcpp for GGUF, it's literally an .exe and easy to run, or TabbyAPI for exl2

"
webslug,2024-03-13 10:26:25+00:00,comment,Sao10K/Fimbulvetr-11B-v2,Thank you for your help!
Lewdiculous,2024-03-17 22:52:10+00:00,comment,Sao10K/Fimbulvetr-11B-v2,long live leaderboard eval
Sao10K,2024-03-02 16:13:07+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Thanks a lot, will update the model page"
sephwalker3,2024-02-26 09:24:37+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Found this model thanks entirely to you, Trappu. Your recommendation on that rentry page (ALLMRR) got me here and I want you to know just how incredibly grateful I am for that.

I left this whole scene around when Pygmalion was still ""the latest and greatest,"" and somehow, finding accurate, digestible information has been even more ridiculously difficult than it was back then. As soon as I landed on yours and Ali's pages, that issue disappeared almost entirely. 

As far as this model goes, it is very easily the best model I've used to date. I've tested dozens of popular models and couldn't for the life of me get any of them to function properly. Frequently, once the context is nearing max context size, everything just starts falling apart. This is especially pronounced on Mistral models. I'm an amateur dev, so it's not as if I'm totally clueless here, but regardless of my knowledge, I struggled for way, waaaay too many hours trying to find a model that didn't inevitably start looping, or start using awkward prose.

With all of that said, this model is nothing short of superb. It's simply excellent. None of the aforementioned issues, which I experienced on practically every other model I tested, exist within this model. It just WORKS. It doesn't get stuck looping, if it ever does loop it's simple to step back a message or two and boom, problem solved. Mistral models I'd need to step back dozens of messages to unstick a loop, and it'd just recur after another five to ten messages. It handles practically everything I've thrown at it with ease, the prose is exactly as expected - I mean, fuck: it doesn't sound like I'm using the first few iterations of ChatGPT, but with added profanity! Instead it sounds exactly the way I ask it to. It takes cues perfectly from the context and grasps my intention perfectly the vast majority of the time. 

TL;DR: Thank you, Trappu, Alicat, and Sao10K. Y'all are wonderful, and I appreciate you folks. I'm certain there are many others who feel the same way, too."
Sao10K,2024-02-15 12:48:01+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"It was for Chai app competition Season 8, but so far I think this model is done, it performed decently well. I might work on v3, but I'm working on scaling up my results of this and Fimvulvetr v1 to Yi/Mixtral."
PtiteTartine,2024-02-14 20:31:35+00:00,comment,Sao10K/Fimbulvetr-11B-v2,He has a Ko-Fi page : https://ko-fi.com/sao10k
Sao10K,2024-02-15 12:49:07+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Perhaps I should link my socials more definitely, I hadn't been really working on that, mainly in my own world. I'll work on that, and once again, thanks for the comment :)"
WhensDinner,2024-02-10 16:06:21+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"This model works well. The original is one of my all-time favorites, and this version of it feels like it's slightly more creative and is also much more willing to keep a dialogue between characters going. Enjoying it so far."
Herman555,2024-02-07 15:48:05+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"I also would like to thank you for what you are doing,
V1 is quite well regarded in the community.
Where I live the whole healthcare system is rotten to the core and it's purely business before people, personnel and patients treated like shit alike.
It's disgusting frankly.
Take care."
FrostyX10,2024-02-07 16:02:45+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Just made an account to thank you for your work.
i havent tried this one but i have tried frostwind and
did not like it for ERP purposes,
i think this one will be better for that purpose.
Thanks for your hard work."
avada-z,2024-02-07 18:18:53+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"I'm saying thanks too, Fimbulvetr models are perfectly balanced.
Actually tested this model, it seems to follow the prompt much better, also feels more creative I guess.
Maybe even more intelligent, but that might be influenced by the changes to the sampling parameters."
TravelingMan,2024-02-08 00:32:08+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Fimbulvetr is so incredible. For its size, it follows instructions well and feels really authentic for RP (I don't ERP so unsure there). It's my favorite model, and has been since the original came out. Looking forward to trying this one!

Thanks, Sao10K!"
Foxlum,2024-02-08 00:38:04+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"This is one of the best in my opinion in comparison to some 13b, 10b, and 7b i've used, especially with its variety and quality of its total output. This one is one of the few human feeling models using character cards, able to easily handle details and consistency. I do hope the Fimbulvetr dense 11b series continues (or transitions a different similar sized base model), even if you're planning to go into MoEs or Yi."
WhensDinner,2024-02-09 21:59:06+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Jumping in here to hit you with some good vibes. Fimbulvetr has been my favorite RP model for a good while, even including larger ones. It strikes a good balance in a lot of ways. Looking forward to quants of the new version once you've got it ironed out a bit."
Sao10K,2024-02-10 14:15:32+00:00,comment,Sao10K/Fimbulvetr-11B-v2,"Thanks for all the positive words, everyone :)"
eevee32x,2024-02-12 01:01:56+00:00,comment,Sao10K/Fimbulvetr-11B-v2,Half of my favorites are from you <3 This has good fluffy romance/ERP balance. PS: I love loveee Frostwind but I was too shy to open a discussion :C
Sao10K,2024-02-15 12:48:24+00:00,comment,Sao10K/Fimbulvetr-11B-v2,":) Thanks for all the words, it really does mean a lot to me."
ddbl,2024-03-12 00:45:38+00:00,comment,Sao10K/Fimbulvetr-11B-v2,Just adding my voice to say thank you. This v2 model is incredible.
Undi95,2024-06-10 14:46:38+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,Corrected
mradermacher,2024-05-04 04:10:55+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Full means some with lots missing? (e.g. IQ1_M, Q4_0, IQ4_NL... :)

But seriously, it would be better to leave out IQ1_S rather than IQ1_M, although neither is going to be important for an 8B."
Skydea,2024-05-05 07:27:39+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Yeah, I would also like some more refined quants for Q5 - Q8 for people with only 8 - 10 GB of VRAM, without the loss in quality. :(
A couple of 100MB in Size is a difference in these cases."
mradermacher,2024-05-05 11:26:33+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Hmm, but they have Q5-Q8, and imatrix ones, too, apparently? Maybe I don't understand what you mean with refined?"
mradermacher,2024-05-05 11:28:25+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Hmm... looking at them, they either have not been made with an imatrix, or with an old version of llama.cpp, meaning they have reduced quality indeed. Almost certainly with an old version of llama.cpp that didn't have llama 3 tokenizer support.

You might be able to work around this by specifying `--override-kv tokenizer.ggml.pre=str:llama3` with llama.cpp."
mradermacher,2024-05-05 11:36:18+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,@Skydea https://huggingface.co/NeverSleep/Llama-3-Lumimaid-8B-v0.1-GGUF has correctly made q5..q8.
PacmanIncarnate,2024-05-06 05:35:12+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"You were responded to in the other repo, but the GGUFs at the link are fully functional and made with a current version of llama.cpp. Imatrix was used for the quants that profit from its use, rather than indiscriminately.
My apologies if I misspoke when I said â€˜full setâ€™. We have a broad range that we feel covers possible needs well."
mradermacher,2024-05-06 05:44:29+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"@PacmanIncarnate the quants at https://huggingface.co/FaradayDotDev/Llama-3-Lumimaid-8B-v0.1-GGUF are broken as explained earlier. They are ""fully functional"" in the sense that they probably don't crash, but they have severely reduced quality because they were done with the wrong converter tool and thus the wrong tokenizer config (i.e. they were done with convert.py, which does not work for llama-3).

Also, an imatrix was clearly not used for all quants that benefit from it (e.g. none of the .Q quants have it applied, but almost all would benefit from it). At least, that is what @brooketh wrote in the other repo.

Just pointing out the facts, don't shoot the messenger. There is an opportunity to improve things for the benefit of all."
mradermacher,2024-05-06 05:58:34+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Seems this applies to the other llama-3 and command-r quants from faradaydotdev, they are all similarly broken, and only the .IQ quants were done using an imatrix."
PacmanIncarnate,2024-05-06 06:03:34+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Enough. Youâ€™re spreading false information at this point. The GGUFs we are making are fully functional made after the llama 3 tokenizer update.

The command-r model is pre-tokenizer fix for that type of model because that was merged days ago and the model, for whatever reason, doesnâ€™t seem to suffer for it.

And of course only the IQ files are using imatrix. Thatâ€™s how that works."
mradermacher,2024-05-06 06:10:04+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Anyway, I recommend the quants by NeverSleep at this point, as they were done correctly, and the equivalent faradaydotdev ones are made without an imatrix anyway. If anybody wants correctly done .iq quants, I will happily provide them if needed."
mradermacher,2024-05-06 06:16:46+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"@PacmanIncarnateand since you wrongly accuse me of spreading false information, why don't you tell us how you managed to end up with ggufs that don't have the pretokenizer set? And where is the data that shows that the (e.g.) Q2_K quant does not benefit from an imatrix?

The answer to the first is that you used convert.py instead of convert-hf-to-gguf.py and the answer to the second is that you just made up that claim."
brooketh,2024-05-06 07:33:49+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"> Anyway, I recommend the quants by NeverSleep at this point, as they were done correctly, and the equivalent faradaydotdev ones are made without an imatrix anyway. If anybody wants correctly done .iq quants, I will happily provide them if needed.

The quants by NeverSleep are also made without an imatrix, by the way."
mradermacher,2024-05-06 14:48:18+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"Yup, that's why neversleeps quants only have upsides and no downsides. Since you both have time to reply here and on your repo, why don't you reply to the actual criticism I made and back up your claim that I am spreading false information? Should be easy if there is any substance to it. Or are ad hominems simply easier than dealing with the facts? Deal with the criticism, not the person bringing up valid points.

And the fact that is extremely easy to verify (just click on the quant on the right side) is that your quants specify the default (llama 2) pretokenizer, while Neversleeps quants correctly specify ""llama-bpe"" as pretokenizer."
brooketh,2024-05-06 15:18:53+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"> Yup, that's why neversleeps quants only have upsides and no downsides. Since you both have time to reply here and on your repo, why don't you reply to the actual criticism I made and back up your claim that I am spreading false information?

I'm just wondering why you aren't criticizing NeverSleep for failing to use an imatrix in their quants, if that is really such a big deal to you?  The answer, I suspect, is that you realize it's a rubbish claim.

> And the fact that is extremely easy to verify (just click on the quant on the right side) is that your quants specify the default (llama 2) pretokenizer, while Neversleeps quants correctly specify ""llama-bpe"" as pretokenizer.

What's interesting that in your comment [here](https://huggingface.co/FaradayDotDev/Llama-3-Lumimaid-8B-v0.1-GGUF/discussions/1#66376e5714662254a97a852b) you imply that ""llama3"" was the correct value for this string; now you're claiming that ""llama-bpe"" is correct.  However, neither of those strings is what ""convert-hf-to-gguf.py"" actually outputs for this model, which you would know if you had taken the time to run it yourself."
mradermacher,2024-05-06 15:54:46+00:00,comment,NeverSleep/Llama-3-Lumimaid-8B-v0.1,"llama-bpe is what convert-hf-to-gguf.py outputs, as can be seen in neversleeps quants. llama3 and llama-bpe select the same pretokenizer, so are both correct, so my claim that llama3 might be a workaround stands. I never implied it is the only correct value, only that it probably improves the quants (the imatrix is broken though, so needs to be redone, it cannot fix the imatrix retroactively. But since the quants I was comparing did not use an imatrix or did not record it, it should work for those quants).

If your copy doesn't output a pretokenizer config, then it's simply outdated and predates the pretokenizer implementation. This is *extremely* easy to look up. Why not actually do your research instead of spreading FUD?

Still, you don't address my actual critcism, which is that your quants use the wrong pretokenizer.

Better yet, why not admit that you used convert.py (or an outdated llama.cpp), which is not working for llama-3, and be done with it?

And why would I criticise neversleep for not using an imatrix? I am not criticising you for that either. I am criticising you for the claim that you used an imatrix when you didn't (you claim all quants that benefit are using an imatrix, Q2_K is one that would benefit. your Q2_K was not done with an imatrix as you admit yourself. q.e.d.)."
ArthurZ,2022-12-05 08:54:04+00:00,comment,facebook/galactica-30b,Thanks! 
Jackmin108,2022-11-22 08:12:43+00:00,comment,facebook/galactica-30b,"Same error but different key.

```console
KeyError: 'decoder.layers.44.self_attn_layer_norm.bias'
```

I think this might be a bug in the way huggingface downloads blobs where intermittent failures arent detected and can corrupt the blob.
It also does not check hashsums for the blobs so it is unable to detect that the blob was corrupted.
I lost my original output of which key it failed on but I got suspicious of my 05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf shard because it is smaller than the others.
So I deleted it and had it repulled and it's size changed in the new pull.

**Old disk usage**
```console
â¯ du -csh ./models--facebook--galactica-30b/blobs/*
785M    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6
5.3G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf
4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93
2.5G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8
4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6
9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71
24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317
9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7
9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f
4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb
9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93
2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0
46G     total
```

**Latest disk usage**
```console
785M    ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6
9.2G    ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf
4.0K    ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93
9.2G    ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8
4.0K    ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6
9.2G    ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71
24K     ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317
9.2G    ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7
9.2G    ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f
4.0K    ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb
9.2G    ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93
2.1M    ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0
56G     total
```

Im computing the md5sum for the blobs now with 
```console
â¯ md5sum ./models--facebook--galactica-30b/blobs/*
```

Let's compare?"
Jackmin108,2022-11-22 08:19:18+00:00,comment,facebook/galactica-30b,"These are my md5sums which get `KeyError: 'decoder.layers.44.self_attn_layer_norm.bias'`:

```
â¯ md5sum ./models--facebook--galactica-30b/blobs/*
ee6deb059a899a51aa3e1c726e935aa2  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6
5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf
8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93
3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8
f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6
4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71
2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317
1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7
ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f
fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb
4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93
a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0
```"
Jackmin108,2022-11-22 08:38:35+00:00,comment,facebook/galactica-30b,"Can the authors check as well? I think **30b in particular might be broken** because the artifacts are much smaller than expected.

If you calculated bytes / parameter ratios, my install of 30B is an obvious outlier:
|  Size       | Parameters  | Disk Usage | Bytes / Parameter ratio |
|:-----------:|:-----------:|:-----------:|:-----------:|
| `mini`      |    125 M    |  480M | 4.0265 |
| `base`      |    1.3 B    | 5.0G | 4.1298 |
| `standard`  |    6.7 B    | 26G | 4.1667 |
| `large`     |     30 B    | 56G | 2.0043 |
| `huge`      |    120 B    | 453G | 4.0534 |

If we roughly interpolate and say the models are ~ 4 bytes (fp32) per parameter, we should expect the 30b model to have 120GB of blobs. However, if you sum all the blobs in the repo. It is only ~60GB.

### Get disk usage
â¯ du -csh ~/.cache/huggingface/hub/*/blobs
453G    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-120b/blobs
480M    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-125m/blobs
5.0G    /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-1.3b/blobs
56G     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-30b/blobs
26G     /home/jackmin/.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs
539G    total

### Bytes / Parameter Calculation
```
[125m] 480 * (2 ** 20) / 125e6 = 4.0265_
[1.3b] 5 * (2 ** 30) / 1.3e9 = 4.1298_
[6.7b] 26 * (2 ** 30) / 6.7e9 = 4.1667_
[30b] 56 * (2 ** 30) / 30e9 = 2.0043_
[120b] 453 * (2 ** 30) / 120e9 = 4.0534_
```

### Artifacts in the repo
![image.png](https://cdn-uploads.huggingface.co/production/uploads/1669106825470-634e2b60a00c472888747e4c.png)"
Jackmin108,2022-11-22 09:56:01+00:00,comment,facebook/galactica-30b,"I have deduced that the 30b model pickles _have no biases_.

```python
from tqdm import tqdm
import torch
from pathlib import Path
import pickle

blob_path = Path.home() / Path('.cache/huggingface/hub/models--facebook--galactica-30b/blobs')

keys2blob = {}
errors = {}
blobs = [blob for blob in blob_path.glob('./*') if blob.is_file()]

for blob in tqdm(blobs):
    try:
        keys2blob.update({k: blob for k in torch.load(blob).keys()})
    except pickle.UnpicklingError as e:
        errors[blob] = e

print(f""Num_weights: {len([i for i in keys2blob.keys() if 'weight' in i])}"")
print(f""Num_biases: {len([i for i in keys2blob.keys() if 'bias' in i])}"")
```
```console
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:50<00:00,  4.19s/it]
Num_weights: 290
Num_biases: 0
```

This is opposed to the 6.7b model which _contains a lot of biases_.
```python
from tqdm import tqdm
import torch
from pathlib import Path
import pickle

blob_path = Path.home() / Path('.cache/huggingface/hub/models--facebook--galactica-6.7b/blobs')

keys2blob = {}
errors = {}
blobs = [blob for blob in blob_path.glob('./*') if blob.is_file()]

for blob in tqdm(blobs):
    try:
        keys2blob.update({k: blob for k in torch.load(blob).keys()})
    except pickle.UnpicklingError as e:
        errors[blob] = e

print(f""Num_weights: {len([i for i in keys2blob.keys() if 'weight' in i])}"")
print(f""Num_biases: {len([i for i in keys2blob.keys() if 'bias' in i])}"")
```
```console
50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:14<00:14,  3.57s/it]
Num_weights: 260
Num_biases: 257
```"
Jackmin108,2022-11-22 12:16:43+00:00,comment,facebook/galactica-30b,"Update: 30b is the only model in half precision. It also has less tensors than expected.

|  Size       | Parameters  | Disk Usage | Bytes / Parameter ratio | Sum(layer.numels) | Data type of tensors
|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| `mini`      |    125 M    |  480M | 4.0265 | 163,430,400 | {torch.float32: 197} |
| `base`      |    1.3 B    | 5.0G | 4.1298 | 1,417,601,024 | {torch.float32: 389} |
| `standard`  |    6.7 B    | 26G | 4.1667 | 6,862,159,872 | {torch.float32: 517} |
| `large`     |     30 B    | 56G | 2.0043 | 29,968,103,424 | {torch.float16: 290} |
| `huge`      |    120 B    | 453G | 4.0534 | 121,853,747,200 | {torch.float32: 1541} |"
hwasiti,2022-11-23 10:17:06+00:00,comment,facebook/galactica-30b,"Getting key errors too like:
`KeyError: 'decoder.layers.27.final_layer_norm.weight'`
`KeyError: 'decoder.layers.11.fc1.bias'`
`KeyError: 'decoder.layers.6.fc1.bias'`
which are different with each run


Here are the md5sum

`md5sum ./models--facebook--galactica-30b/blobs/*`

```
ee6deb059a899a51aa3e1c726e935aa2  ./models--facebook--galactica-30b/blobs/0379c39b5a0cb59453b14738ef1d4924e93599aba4e57f2599036e76f36532f6
5af8e57b27eaafa9d59d4669b5f7b1f7  ./models--facebook--galactica-30b/blobs/05db345d4fcca580bed2c6e9d0fe8feead207c2c2fa8384c27c94cbd4ed0e0bf
8a80554c91d9fca8acb82f023de02f11  ./models--facebook--galactica-30b/blobs/0967ef424bce6791893e9a57bb952f80fd536e93
3a7ffd5e37b9c2552aca688fd1531723  ./models--facebook--galactica-30b/blobs/0d6ce164b560f4601d48f61c2a8d598106faa9f4b89c39334a712429649b75c8
f4484c98e948186322d8e29f2d317004  ./models--facebook--galactica-30b/blobs/28e11da7e191492f3f23d2aa35e9b60f8e9becf6
4198d299ecacb9ea6866ec62d352691e  ./models--facebook--galactica-30b/blobs/30a274571d49a30bb4d6872e69b96ad191fa22c92427d160c74ce225a566bd71
2664153e6ea77cc8a03a58a4e894984d  ./models--facebook--galactica-30b/blobs/98d10d1a52ab2b70f1deff472512cbaa6065e317
1e6ebc15971c26c46d1832b5b5247560  ./models--facebook--galactica-30b/blobs/aa79446f17da0f3b9f8815a3628c2b1935936ec819f09a5865ce4e3c4ee51aa7
ff304677b7c8d9b0aabbaf63cb0c1bbd  ./models--facebook--galactica-30b/blobs/b919005245e2b77d57bf3a73ac18415083aa32b6e2e4e89c96b8d988453a0e7f
fdda94195fbe20918df6aaa9aba70d10  ./models--facebook--galactica-30b/blobs/bc97f8a9458a1fe096bec5d8ec938a02647bc4bb
4e9c975acbbce326de42198a9d06a246  ./models--facebook--galactica-30b/blobs/c1cad10954e544c44aabd29f31e67292d1bc819d2e7b9842f14fdcef88d58f93
a74f71fa9db6a1a27c33d77d20696944  ./models--facebook--galactica-30b/blobs/e18054f92dc016b43c940dd1c4a1c5da884539c0
```

which is the same hash of @Jackmin108  :
 https://huggingface.co/facebook/galactica-30b/discussions/4#637c8606d55081513c5679ef

@mrm8488  For me too other models worked fine. Only the galactica 30b model gives me key errors."
mrm8488,2022-11-23 10:23:54+00:00,comment,facebook/galactica-30b,"Hi, @ybelkada made a PR yesterday to fix it ASAP :)"
ybelkada,2022-11-23 18:14:48+00:00,comment,facebook/galactica-30b,"Hey @hwasiti @Jackmin108 
Just out of curiosity, have you tried it with the largest model too? The 120b"
Jackmin108,2022-11-23 18:41:58+00:00,comment,facebook/galactica-30b,"@ybelkada Unfortunately I don't have the hardware to load 120b even if I sampled down to int8.
However, there doesn't seem to be anything suspicious with the 120b checkpoint.

The reason I believe so is because you sort the names of the layers and cutoff at the last occurrence of layer 0 in the decoder, you get the same output from the 120b checkpoint as the 125m checkpoint, the tensors just have different dimensions.
This is not true for the 30b checkpoint, which is missing final layers and biases.

### 120b
```python
lm_head.weight torch.Size([50000, 10240])
model.decoder.embed_positions.weight torch.Size([2050, 10240])
model.decoder.embed_tokens.weight torch.Size([50000, 10240])
model.decoder.final_layer_norm.bias torch.Size([10240])
model.decoder.final_layer_norm.weight torch.Size([10240])
model.decoder.layers.0.fc1.bias torch.Size([40960])
model.decoder.layers.0.fc1.weight torch.Size([40960, 10240])
model.decoder.layers.0.fc2.bias torch.Size([10240])
model.decoder.layers.0.fc2.weight torch.Size([10240, 40960])
model.decoder.layers.0.final_layer_norm.bias torch.Size([10240])
model.decoder.layers.0.final_layer_norm.weight torch.Size([10240])
model.decoder.layers.0.self_attn.k_proj.bias torch.Size([10240])
model.decoder.layers.0.self_attn.k_proj.weight torch.Size([10240, 10240])
model.decoder.layers.0.self_attn.out_proj.bias torch.Size([10240])
model.decoder.layers.0.self_attn.out_proj.weight torch.Size([10240, 10240])
model.decoder.layers.0.self_attn.q_proj.bias torch.Size([10240])
model.decoder.layers.0.self_attn.q_proj.weight torch.Size([10240, 10240])
model.decoder.layers.0.self_attn.v_proj.bias torch.Size([10240])
model.decoder.layers.0.self_attn.v_proj.weight torch.Size([10240, 10240])
model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([10240])
model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([10240])
```

### 125m
```python
lm_head.weight torch.Size([50000, 768])
model.decoder.embed_positions.weight torch.Size([2050, 768])
model.decoder.embed_tokens.weight torch.Size([50000, 768])
model.decoder.final_layer_norm.bias torch.Size([768])
model.decoder.final_layer_norm.weight torch.Size([768])
model.decoder.layers.0.fc1.bias torch.Size([3072])
model.decoder.layers.0.fc1.weight torch.Size([3072, 768])
model.decoder.layers.0.fc2.bias torch.Size([768])
model.decoder.layers.0.fc2.weight torch.Size([768, 3072])
model.decoder.layers.0.final_layer_norm.bias torch.Size([768])
model.decoder.layers.0.final_layer_norm.weight torch.Size([768])
model.decoder.layers.0.self_attn.k_proj.bias torch.Size([768])
model.decoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])
model.decoder.layers.0.self_attn.out_proj.bias torch.Size([768])
model.decoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])
model.decoder.layers.0.self_attn.q_proj.bias torch.Size([768])
model.decoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])
model.decoder.layers.0.self_attn.v_proj.bias torch.Size([768])
model.decoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])
model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([768])
model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([768])
```

### 30b
```python
decoder.embed_positions.weight torch.Size([2050, 7168])
decoder.embed_tokens.weight torch.Size([50000, 7168])
decoder.layers.0.fc1.weight torch.Size([28672, 7168])
decoder.layers.0.fc2.weight torch.Size([7168, 28672])
decoder.layers.0.self_attn.k_proj.weight torch.Size([7168, 7168])
decoder.layers.0.self_attn.out_proj.weight torch.Size([7168, 7168])
decoder.layers.0.self_attn.q_proj.weight torch.Size([7168, 7168])
decoder.layers.0.self_attn.v_proj.weight torch.Size([7168, 7168])
```"
ybelkada,2022-11-23 21:27:51+00:00,comment,facebook/galactica-30b,"Great thank you very much! You really did a great job debugging here and helped me a lot understanding the rootcause of the issue. 
Let's wait for https://github.com/huggingface/transformers/pull/20390 to be addressed and keep the thread open here"
hwasiti,2022-11-25 22:38:04+00:00,comment,facebook/galactica-30b,"@ybelkada The 120B did not work eventually, but at least I think the model has been mapped to the GPUs/RAM/SSD and gave me Cuda out-of-memory error in GPU0, which is solvable I guess. I did not want to try again. The whole initialization took around 10 hrs and it was not worth it. I just felt that it is not practical to use such a slow model when I have only 64GB RAM and 2 GPUs each with 11GB. The rest will be mapped to SSD and that is not a good idea practically speaking. 

I do have the intention to spin up an instance in AWS or GCP with huge RAM to fit all the models in RAM and test how fast it is if using CPU only or use the CPU/640 GB RAM with 1 GPU. The cost for such an instance is around $1-2/hr (spot instance) which is worth it in case I want a few hours to aid me in writing the intro of a research paper or something."
ybelkada,2022-11-29 09:24:15+00:00,comment,facebook/galactica-30b,"Hi there! 
https://github.com/huggingface/transformers/pull/20390 is probably going to be merged, I can confirm that I can at least load the model with the fix, may I ask you to try the same thing on your side? The instructions would be:
```pip install --upgrade git+https://github.com/younesbelkada/transformers.git@fix-opt-bias```
```git clone https://huggingface.co/facebook/galactica-30b/```
Then modify the `config.json` file of the cloned repository by adding 2 lines: 
```
""enable_bias"": false,
""layer_norm_elementwise_affine"":false,
```
Looking forward to hearing from you!"
hwasiti,2022-11-30 07:28:07+00:00,comment,facebook/galactica-30b,"@ybelkada Does this model support half-precision (float16)? 
Otherwise, I don't think it will fit my 64GB RAM.

See a similar issue with the 6.7b model:
https://huggingface.co/facebook/galactica-6.7b/discussions/6"
ybelkada,2022-11-30 08:02:27+00:00,comment,facebook/galactica-30b,"Yes it should support float16, you just have to load it by adding the argument `torch_dtype=torch.float16` when calling `.from_pretrained`"
hwasiti,2022-11-30 09:38:31+00:00,comment,facebook/galactica-30b,I have used that for the 6.7b model. So is the 6.7b model in particular not supporting the argument `torch_dtype=torch.float16` when calling `.from_pretrained`?
Saptarshi7,2023-08-21 18:20:41+00:00,comment,facebook/galactica-30b,This comment has been hidden
besiktas,2024-03-27 18:45:47+00:00,comment,adept/fuyu-8b,"Yeah, I have tried some form of it.  I'm not sure if it will help (or didn't in my case) unless you have a large and diverse dataset to then train further with. 

The base model seems really hit or miss with localization (meaning I will see it outperform other OCR tools on one sample but the next sample it has almost nil ability) and does not seem to train well for any downstream tasks that require localization (via box or point tags)."
ArthurZ,2024-01-31 03:04:00+00:00,comment,adept/fuyu-8b,There is a PR opened in transformers https://github.com/huggingface/transformers/pull/26997 
Molbap,2023-12-04 13:49:01+00:00,comment,adept/fuyu-8b,"Hey @Nyandwi , the code available around Fuyu is indeed inference-centered, and we do not have an official training script. There are ongoing contributors work at https://github.com/huggingface/transformers/pull/26997 for instance, and the folks from OtterHD have independently retrained a Fuyu-like architecture on high-resolution data, you can take a look there https://github.com/Luodian/Otter/blob/main/docs/OtterHD.md ! If you have a code snippet, that can be helpful :) "
Nyandwi,2023-12-04 14:32:39+00:00,comment,adept/fuyu-8b,"Hi @Molbap. Thanks for the quick response and sharing those pointers.

I am also doing inference where given an image and text prompt, I want to compute the loss, the labels being the input_ids.

```py
model_id = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(model_id)
model = FuyuForCausalLM.from_pretrained(model_id, device_map=""cuda:0"", torch_dtype=torch.bfloat16)

inputs = processor(text=prompt, images=sample_im_1, return_tensors=""pt"").to(""cuda:0"", torch.bfloat16)

with torch.inference_mode():
    outputs =  model(**inputs, labels=inputs[""input_ids""])

loss = outputs.loss
```

The above gives me high loss but when I removes the speaker token, the loss reduces significantly but the model is likely not using images since speaker token refers to image place holder according to preprocessing script.

```py
input_ids = inputs[""input_ids""]
speaker_token = processor.tokenizer.convert_tokens_to_ids('|SPEAKER|')
input_labels = input_ids.masked_fill(input_ids == speaker_token, -100)
```

Also related, is [prepare_inputs_for_generation()](https://github.com/huggingface/transformers/blob/main/src/transformers/models/fuyu/modeling_fuyu.py#L316) hard requirement to apply to model inputs(from processor)?"
Molbap,2023-12-05 07:41:51+00:00,comment,adept/fuyu-8b,"The model _is_ using images: in the model, the embeddings of your image patches `sample_im_1` are placed at the positions indicated by the placeholder image tokens. Check out the source code here in particular https://github.com/huggingface/transformers/blob/235e5d4991e8a0984aa78db91087b49622c7740e/src/transformers/models/fuyu/modeling_fuyu.py#L289C1-L300C18. So it is expected to have different losses. By removing the speaker tokens, you're actually disabling the image input. 

I'm not sure I get your second question, it might be better suited for the forums  https://discuss.huggingface.co/ ? `prepare_inputs_for_generation` is a `GenerationMixin`-related method, it is not linked to the call `model(**inputs)`. It will be if you do `model.generate(**inputs)`, for instance."
Nyandwi,2023-12-05 11:08:32+00:00,comment,adept/fuyu-8b,"Thanks for the support, @Molbap. I will keep inspecting where the issue of exponential loss(with all input ids) might be coming from. For the later, that was my guess too since using it for further input preprocessing didn't even make a difference for my case.

Edit: Can the absence of training support impacts the inference of `model(**inputs)`? It's unlikely, but just to clarify. With generate functionality, I get good response out. It's a bit weird the loss does not agree."
Molbap,2023-12-06 13:57:43+00:00,comment,adept/fuyu-8b,"As you said, no, I don't think it would impact the output of a forward pass. :) "
besiktas,2023-12-07 18:34:19+00:00,comment,adept/fuyu-8b,"@Nyandwi you will want to mask out the |SPEAKER| and tokens related to your input/instructions to the ignore index. Otherwise while finetuning this model, it will be focused on predicting what part of the input is actually just image patches and instructions (not helpful)."
Nyandwi,2023-12-09 19:14:05+00:00,comment,adept/fuyu-8b,"Hi @besiktas. Thanks for sharing that, really appreciate. That might speak to why removing the speaker tokens reduce loss substantially. What part of the input instructions that you meant or I generally should mask all special tokens? One of my input prompts is `""Is the {caption} a good description of the image?"".`Do you meant to mask everything except `{caption}`? If you have a reason why we have to mask out those tokens from the model perspective or beyond, I would love to hear that. Thank you!"
besiktas,2023-12-09 20:02:28+00:00,comment,adept/fuyu-8b,"@Nyandwi I would maybe rephrase the instruction so that it is ""Is the following caption a good description of the image: {caption}"" in which case you just mask out the length of the tokens for `Is the following caption a good description of the image:` (meaning don't mask out based on token values and you may also want to include the boa token before caption).  

I believe I have seen it done both ways, conceptually masking out the instruction is trying to get the model to provide an appropriate response given the caption. 

If you don't mask them out, in theory I would bet it still works and trains eventually to what you aim for but given the CrossEntropyLoss uses mean reduction (if you provide labels to the model for forward) as the model learns to complete the instruction, the gradient wrt the caption/answer you actually want the model to output will be dampened compared to if you had masked out the instructions."
Nyandwi,2023-12-10 08:48:32+00:00,comment,adept/fuyu-8b,"@besiktas, Thanks a lot, that really helps and once again appreciate your time. With that, is the instruction(or any other prefix) useful in this regard since we ultimately mask it out, or it is(since the input ids in forward still contains all tokens, speaker & instructions) but it's not relevant for loss computation? I like the instruction rephrase, that's brilliant."
Nooodles,2023-12-04 02:26:30+00:00,comment,adept/fuyu-8b,To make the coordinate between 0-999?
pcuenq,2023-12-04 10:18:42+00:00,comment,adept/fuyu-8b,"Hi @Nooodles, that's the normalization applied when the model was trained. Perhaps @mtensor has more insight about it."
Seungyoun,2023-11-29 13:12:47+00:00,comment,adept/fuyu-8b,"Can FuyuProcessor be modified to handle both multi-resolution and multiple images? 
I looked through its code and noticed it only processes one image at a time and doesn't support this feature.

It would be great if the training process could support settings for both multi-resolution and multi-image processing.





"
besiktas,2023-11-29 21:08:01+00:00,comment,adept/fuyu-8b,"FuyuProcessor handles multi-resolution images and multiple images so long as they are each a different sample.  

The current model does not allow multiple images per sample but it does seem to work with them if you change `gather_continuous_embeddings`"
leo4life,2023-11-28 05:07:43+00:00,comment,adept/fuyu-8b,Running inference only with 4-bit quantization took me around 12.2GB of vram.
thightower1,2023-11-29 18:23:23+00:00,comment,adept/fuyu-8b,"Thanks leo4life. Just curious, how was the performance using 4-bit quantization?"
leo4life,2023-11-29 20:01:06+00:00,comment,adept/fuyu-8b,"I didn't do any benchmarks, but I ran the prompts in the sample code on the model card page and it gave me the same outputs. For other prompts it gives reasonable output as well."
CesarSchez,2024-01-29 06:03:23+00:00,comment,adept/fuyu-8b,Hi @leo4life I tried to run it in 4-bit in colab but I got `CUDA out of memory`. Would you share a code snippet to run it. idk I might be missing an important parameter
leo4life,2024-01-29 06:06:27+00:00,comment,adept/fuyu-8b,@CesarSchez here you go: https://colab.research.google.com/drive/13nRdH_QJFyRVMRLi6AuTdIPUOpdAVup4?usp=sharing
pcuenq,2023-11-25 16:29:26+00:00,comment,adept/fuyu-8b,"How many images are there in the folder you preprocess before loading the model? What does the jupyter log say? How much system and GPU memory is being used when you run the notebook?

I would recommend you split the cell in two: one to load the model (run this first), and another one to process the images. My bet is you might be running out of system memory."
simonbrbx,2023-11-25 17:28:35+00:00,comment,adept/fuyu-8b,"Thank you for your quick response.

There are only 2 images in the folder. Jupyter redirects me to this page: [https://github.com/microsoft/vscode-jupyter/wiki/Kernel-crashes](https://github.com/microsoft/vscode-jupyter/wiki/Kernel-crashes). The script still crashes at 43 seconds.

So, I split the code into two parts, with downloading and processing the image. However, it still crashes during the model download. The code crashes when it reaches my 16GB of RAM, which is 100% utilization.

What should I do?"
pcuenq,2023-11-27 12:29:01+00:00,comment,adept/fuyu-8b,"Do you have a GPU? How much memory does it have? Fuyu requires ~20 GB of RAM to run in half precision, and double as much in full precision. In addition, if you install `accelerate` (using `pip install accelerate`) you can load the model directly on GPU instead of using your system memory and move the weights to GPU later. The following snippet uses both techniques to load the model:

```py
from transformers import FuyuProcessor, FuyuForCausalLM
import torch

model_id = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(model_id)
model = FuyuForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=""cuda"")
```
"
simonbrbx,2023-11-27 12:45:22+00:00,comment,adept/fuyu-8b,"Yes, I have a GPU and it has 8GB of RAM. On my computer, I have 16GB of DDR4, so that's probably why it crashes. Do you think I should run the code on a cloud, like Google Colab for example?
Thank you in advance!"
pcuenq,2023-11-27 12:59:35+00:00,comment,adept/fuyu-8b,"Yes, 8 GB of GPU RAM is not much for these large models."
sanchd,2023-12-07 18:31:24+00:00,comment,adept/fuyu-8b,"Hi,
I have same issue here. I am trying to run this model in GPU but it gets out of memory. I have a 3080Ti with 12GiB and my computer has 32GiB."
Molbap,2023-11-22 08:25:22+00:00,comment,adept/fuyu-8b,"hey @justinwickett , do you have the latest version of transformers + PIL installed in your environment?"
justinwickett,2023-11-22 16:34:15+00:00,comment,adept/fuyu-8b,"I thought I did, but clearly something is wrong with my environment. I preformed the following commands:

```
pip install --upgrade --force-reinstall pillow 
pip install --force-reinstall git+https://github.com/huggingface/transformers.git
```"
Molbap,2023-11-23 09:47:17+00:00,comment,adept/fuyu-8b,"And it does not work? Did you start a new env to be sure? If you have your full env details (python version as well) it can help, I know there might be some issues around `typing` and `PIL` so it might help uncover them :) "
Molbap,2023-11-22 08:29:02+00:00,comment,adept/fuyu-8b,"For PDFs, you would need to convert them to a png or jpg first page by page, then you can try around here: https://huggingface.co/spaces/adept/fuyu-8b-demo"
pcuenq,2023-11-15 12:05:07+00:00,comment,adept/fuyu-8b,Hello @Nyandwi! What image sizes are you working with? Perhaps you could downscale them to a max height of `1080` before processing? 48 GB of GPU RAM should be plenty for the model.
Nyandwi,2023-11-16 09:34:42+00:00,comment,adept/fuyu-8b,"Hi, @pcuenq. Thanks for your reply, really appreciate the support. The images have variable size, but some of them are probably greater than 1080. I will ensure the images are resized to that resolution and see if that solve the problem."
aamir121a,2023-11-14 15:03:05+00:00,comment,adept/fuyu-8b,This comment has been hidden
Molbap,2023-11-07 09:54:50+00:00,comment,adept/fuyu-8b,"Hi @VatsaDev , not sure I understand your question exactly but the model does have a vision layer. It is simply linear, but it does create an embedding vector of required dimension from each patch. Then as you said the embeddings are combined with the text embeddings from the prompt tokens and fed into a Persimmon-8b like architecture.

I recommend inspecting the modeling code here to get a better sense of what the model is doing: https://github.com/huggingface/transformers/blob/9beb2737d758160e845b66742a0c01201e38007f/src/transformers/models/fuyu/modeling_fuyu.py#L154C1-L158C10
"
VatsaDev,2023-11-07 16:02:37+00:00,comment,adept/fuyu-8b,"ok, so your visual layer is turning images to embeddings through an `nn.linear` class?

Did you really have to train it, or does image to embedding just work?

Also, Im sorry if this is too much, but im new to pytorch, learning it, could you give me code example of  image -> embedding -> image?"
bn22,2023-11-10 13:44:44+00:00,comment,adept/fuyu-8b,"> ok, so your visual layer is turning images to embeddings through an `nn.linear` class?
> 
> Did you really have to train it, or does image to embedding just work?
> 
> Also, Im sorry if this is too much, but im new to pytorch, learning it, could you give me code example of  image -> embedding -> image?

The linear layer has to be trained."
Molbap,2023-11-03 08:39:18+00:00,comment,adept/fuyu-8b,"Hi @changgeli ! With the new transformers release, fuyu-8b's processor has been updated and we've also got more information on prompt structure. You have to prompt it to answer in a VQA fashion. 
here, try this:
 
```python
from PIL import Image
import requests
import io
from transformers import FuyuForCausalLM, FuyuProcessor


pretrained_path = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(pretrained_path)
model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')


text_prompt = ""Answer the following VQAv2 question based on the image: What type of foods are in the image?""
fish_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png""
fish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))
model_inputs = processor(text=text_prompt, images=fish_image_pil).to('cuda')


model_outputs = processor.batch_decode(model.generate( **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]
prediction = model_outputs.split('\x04 ', 1)[1] if '\x04' in model_outputs else ''

```
You should see ""fish, carrots, lemon"" as a prediction.
"
changgeli,2023-11-03 09:17:46+00:00,comment,adept/fuyu-8b,Thank you for the comprehensive reply!  Are there any recommended prompts to use for other datasets or tasks?
Molbap,2023-11-03 10:05:26+00:00,comment,adept/fuyu-8b,"You're welcome :) that I know of, to best match the blog post examples, you can try ""Answer the following DocVQA question based on the image."" or the one I shared above, seems to match well!

```python
from PIL import Image
import requests
import io
from transformers import FuyuForCausalLM, FuyuProcessor

pretrained_path = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(pretrained_path)
model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')


text_prompt = ""Answer the following DocVQA question based on the image. \n Which is the metro in California that has a good job Outlook?""
jobs_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png""
jobs_image_pil = Image.open(io.BytesIO(requests.get(jobs_image_url).content))

second_text_prompt = ""Answer the following DocVQA question based on the image. \n What if the maximum male life expectancy?""
chart_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png""
chart_image_pil = Image.open(io.BytesIO(requests.get(chart_image_url).content))

third_text_prompt = ""Answer the following DocVQA question based on the image. \n What sport is that?""
skate_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png""
skate_image_pil = Image.open(io.BytesIO(requests.get(skate_image_url).content))

fourth_text_prompt = ""Answer the following DocVQA question based on the image. \n What was the fair amount of paid vacation days in the United Kingdom?""
vacations_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png""
vacations_image_pil = Image.open(io.BytesIO(requests.get(vacations_image_url).content)).convert('RGB')

fifth_text_prompt = ""Answer the following VQAv2 question based on the image: What type of foods are in the image?""
fish_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png""
fish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))


texts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt, fifth_text_prompt]
images = [jobs_image_pil, chart_image_pil, skate_image_pil, vacations_image_pil, fish_image_pil]

model_inputs = processor(text=texts, images=images).to('cuda')


model_outputs = processor.batch_decode(model.generate(
    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)

ground_truths = ['Los Angeles', '80.7', 'skateboarding', '28', 'fish, carrots, lemon']


for ground_truth, model_output in zip(ground_truths, model_outputs):
    prediction = model_output.split('\x04 ', 1)[1] if '\x04' in model_output else ''
    assert (ground_truth == prediction)

```"
Molbap,2023-11-03 11:09:05+00:00,comment,adept/fuyu-8b,"@changgeli And I forgot to include bounding box localisation and guided OCR: for that, you can do

```python

# bbox contents prediction (OCR)

pretrained_path = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(pretrained_path)
model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')


bbox_prompt = ""When presented with a box, perform OCR to extract text contained within it. If provided with text, generate the corresponding bounding box.\\n<box>388, 428, 404, 488</box>""
bbox_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg""
bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))
model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')


model_outputs = processor.batch_decode(model.generate(
    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]
prediction = model_outputs.split('\x04', 1)[1] if '\x04' in model_outputs else ''

# bbox localisation from text

pretrained_path = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(pretrained_path)
model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')


bbox_prompt = ""When presented with a box, perform OCR to extract text contained within it. If provided with text, generate the corresponding bounding box.\\n Williams""
bbox_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg""
bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))
model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')

outputs = model.generate(**model_inputs, max_new_tokens=10)
post_processed_bbox_tokens = processor.post_process_box_coordinates(outputs)[0]
model_outputs = processor.decode(post_processed_bbox_tokens, skip_special_tokens=True)
prediction = model_outputs.split('\x04', 1)[1] if '\x04' in model_outputs else ''

```"
changgeli,2023-11-05 08:25:13+00:00,comment,adept/fuyu-8b,"The last example ""bbox localisation from text"" seems to not output the '\x04' symbol, and cannot be parsed.
"
pcuenq,2023-11-05 20:41:27+00:00,comment,adept/fuyu-8b,Hello @changgeli! Are you using the same input image and prompt? Can you verify what's the decoded result before the`split`?
changgeli,2023-11-07 08:35:41+00:00,comment,adept/fuyu-8b,"Thank you for reply!  The symbol `'\x04'`  can be output. I used `'\x04 '` instead of `'\x04'` as a delimiter, so the parsing failed.
My question is, why do some predictions start with a space, while others do not?

```
the decoded result of bbox localisation from text:
...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s> When presented with a box, perform OCR to extract text contained within it. If provided with text, generate the corresponding bounding box.\\n Williams\x04<box>388, 428, 404, 900</box>'

the decoded result of bbox contents prediction (OCR):
...||SPEAKER||SPEAKER||SPEAKER||NEWLINE|<s> When presented with a box, perform OCR to extract text contained within it. If provided with text, generate the corresponding bounding box.\\n<box>388, 428, 404, 488</box>\x04 Williams'
```"
Molbap,2023-11-07 15:02:13+00:00,comment,adept/fuyu-8b,"Did you try tweaking the generation parameters? I think we mostly tried greedy decoding, sampling might change what you encounter. "
Molbap,2023-11-07 16:17:42+00:00,comment,adept/fuyu-8b,"Hi @besiktas , there is a discussion around this PR https://github.com/huggingface/transformers/pull/26997 which you may find relevant :) "
besiktas,2023-11-07 17:17:12+00:00,comment,adept/fuyu-8b,"hey @Molbap thanks, believe I saw that before and was not able to get it to work but I have a working example now. "
yinggong,2023-11-08 10:14:42+00:00,comment,adept/fuyu-8b,Can you finetune the fuyu model? I have 80GB cards.
besiktas,2023-11-09 20:45:55+00:00,comment,adept/fuyu-8b,@yinggong you shouldn't have a problem with 80GB cards
yinggong,2023-11-21 03:49:03+00:00,comment,adept/fuyu-8b,"> @yinggong you shouldn't have a problem with 80GB cards

do you know where to fine the train script?

It seems not work from https://github.com/huggingface/transformers/issues/27255

Thanks!
"
cysmnl,2023-11-21 19:55:50+00:00,comment,adept/fuyu-8b,"I faced a CUDA memory issue when running `run_fuyu_no_trainer.py `on a single A100 card.  I resolved this by using 4-bit models, which happen to work on either a 3090 or a A100 card."
cysmnl,2023-11-21 19:57:17+00:00,comment,adept/fuyu-8b,This comment has been hidden
besiktas,2023-11-28 23:02:34+00:00,comment,adept/fuyu-8b,"@yinggong here is a super simple script that allows you to test if you can get the model running on your machine: https://github.com/grahamannett/finetune-fuyu/blob/main/train-simple.py

It mocks a text input and image so that you can be sure the processor works as well."
Molbap,2023-11-03 08:50:15+00:00,comment,adept/fuyu-8b,"Hi @ludeksvoboda , with the recent transformers release (run a `pip install --upgrade transformers`) the model should! Given bbox coordinates, it will perform OCR within that bbox. 
```python
from PIL import Image
import requests
import io
from transformers import FuyuForCausalLM, FuyuProcessor

pretrained_path = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(pretrained_path)
model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')


bbox_prompt = ""When presented with a box, perform OCR to extract text contained within it. If provided with text, generate the corresponding bounding box.\\n<box>388, 428, 404, 488</box>""
bbox_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg""
bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))
model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')


model_outputs = processor.batch_decode(model.generate(
    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)[0]
prediction = model_outputs.split('\x04 ', 1)[1] if '\x04' in model_outputs else ''
```
This should output `Williams`, the text contained within coordinates. text_to_bbox should work as well, with `processor.post_process_box_coordinates`.  Have fun!"
ludeksvoboda,2023-11-03 08:55:01+00:00,comment,adept/fuyu-8b,"That is absolutely awesome @Molbap !
Thank you for the comprehensive reply!"
cckevinn,2023-11-05 16:23:15+00:00,comment,adept/fuyu-8b,"Hi, nice work!
I wonder how to use text_to_bbox to locate items. I try:
```
bbox_prompt = ""When presented with a box, perform OCR to extract text contained within it. If provided with text, generate the corresponding bounding box.\\n 561 Dillman""
bbox_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg""
bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))
model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')

model_outputs = model.generate(**model_inputs, max_new_tokens=20)[:, -20:]
model_outputs = processor.post_process_box_coordinates(model_outputs)
model_outputs = processor.batch_decode(model_outputs, skip_special_tokens=True)[0]
print(model_outputs)
```
And it outputs `text, generate the corresponding bounding box.\n Williams<box>388, 428, 404, 900</box>`, is this the right way to use it?"
ludeksvoboda,2023-11-05 19:59:55+00:00,comment,adept/fuyu-8b,"@cckevinn try to have a look at this https://huggingface.co/adept/fuyu-8b/discussions/38 , but essentially you have it correct I think.
I have tried the linked solution and it works somewhat on the resized image (to 1/2 of the original size), very likely it does even better on the fullsized image. Tried also to crop the test image so it contains only the part filled with text (removed the white space on both sides) and it fails to generate any bbox, I either get empty string or part of some text. I think the model has problems with different image sizes.
Only thing I had to tinker with was permuting the coordinates for ploting.
```
def permute_bbox(bbox):
    return (bbox[1], bbox[0], bbox[3], bbox[2])

def plot_bbox(img, bbox):
    """"""simplest way to plot bounding box on the image""""""
    if isinstance(img, np.ndarray):
        img = Image.fromarray(img)
    draw = ImageDraw.Draw(img)
    draw.rectangle(bbox, outline='red')
    return img
```"
pcuenq,2023-11-05 20:49:13+00:00,comment,adept/fuyu-8b,"The bounding box tasks are very sensitive to input resolution, because the model was trained on screenshots with a height of `1080` and not fine-tuned on other content. The best way to use these features is to scale your input image so its height is close to `1080`. If the image is smaller, then padding to `1920x1080` works well.

This is the strategy used in the demo for this task, these are the lines that rescale and pad so the input to the model is always `1920x1080`: https://huggingface.co/spaces/adept/fuyu-8b-demo/blob/main/app.py#L71-L72
"
ludeksvoboda,2023-11-05 23:11:25+00:00,comment,adept/fuyu-8b,"@pcuenq Oh, thank you for the clarification!"
ybelkada,2023-10-27 13:39:46+00:00,comment,adept/fuyu-8b,As per https://github.com/huggingface/transformers/pull/27109 
pankajdpatel,2023-10-27 09:01:58+00:00,comment,adept/fuyu-8b,Best
pankajdpatel,2023-10-27 09:05:45+00:00,comment,adept/fuyu-8b,Best 
Molbap,2023-11-07 09:58:37+00:00,comment,adept/fuyu-8b,"Hi @nengelmann , thanks for the contribution! It's interesting to see what the base model can do. If you want to refine your prompts a bit, there are some specific structures that do help https://colab.research.google.com/drive/1ToRXprdecF5S7P1yRJHJb1fWm263-sIb?usp=sharing "
Nooodles,2023-10-30 02:32:04+00:00,comment,adept/fuyu-8b,+1
Molbap,2023-11-03 10:42:51+00:00,comment,adept/fuyu-8b,"Hi @liupei0408 , @Nooodles : you can try this from the new release of `transformers`! @pcuenq worked on the bbox postprocessing, you can localise text by doing:

```python
from PIL import Image
import requests
import io
from transformers import FuyuForCausalLM, FuyuProcessor

pretrained_path = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(pretrained_path)
model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')

bbox_prompt = ""When presented with a box, perform OCR to extract text contained within it. If provided with text, generate the corresponding bounding box.\\n Williams""
bbox_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bbox_sample_image.jpeg""
bbox_image_pil = Image.open(io.BytesIO(requests.get(bbox_image_url).content))

model_inputs = processor(text=bbox_prompt, images=bbox_image_pil).to('cuda')

outputs = model.generate(**model_inputs, max_new_tokens=10)
post_processed_bbox_tokens = processor.post_process_box_coordinates(outputs)[0]
model_outputs = processor.decode(post_processed_bbox_tokens, skip_special_tokens=True)
prediction = model_outputs.split('\x04', 1)[1] if '\x04' in model_outputs else ''

```
`prediction` will output the coordinates of the text `Williams` in the image.
"
Nooodles,2023-12-01 06:35:03+00:00,comment,adept/fuyu-8b,This comment has been hidden
ludeksvoboda,2023-10-30 15:32:50+00:00,comment,adept/fuyu-8b,"Hi, I do not think that is good idea. You have no image embeddings or encoder to begin with, I guess your best bet is just put in the image without the text prompt and try to compare the features in some layer below the head. As far as I know this release is also not the model finetuned for inference on the chart images."
oaishi,2023-10-30 16:52:12+00:00,comment,adept/fuyu-8b,"Thanks, @ludeksvoboda! I appreciate your feedback!"
deleted,2023-11-12 22:28:25+00:00,comment,adept/fuyu-8b,This comment has been hidden
liuting123456,2023-10-26 10:14:39+00:00,comment,adept/fuyu-8b,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6539d0904037a1bc440eaca0/s1KYegHbLFiDeKtcVozsv.png)
"
fzubach,2023-11-15 14:53:34+00:00,comment,adept/fuyu-8b,"+1
Also if the numbers are the result of the released base model, what prompts did you use exactly for each of those datasets?
From https://huggingface.co/adept/fuyu-8b/discussions/28#6544b662f0d77ba425f100d0 I assume for VQAv2 you added ""Answer the following VQAv2 question based on the image: "" as prefix. What about the rest? How were the multiple choice questions composed for AI2D?"
EricLiang98,2023-10-26 08:21:58+00:00,comment,adept/fuyu-8b,This comment has been hidden
Molbap,2023-11-03 11:10:55+00:00,comment,adept/fuyu-8b,"Hi @AudreyLin , check this thread out https://huggingface.co/adept/fuyu-8b/discussions/28 , the model released has prompt-specific VQA capabilities :) "
FantasticMrCat42,2023-10-24 20:27:16+00:00,comment,adept/fuyu-8b,"I had the same issue.
if you need the transformers use:
```
pip install git+https://github.com/huggingface/transformers.git
```
but i think I just updated the folder using ""git pull"". I am not sure if I added the ""https://huggingface.co/adept/fuyu-8b"" at the end but I can check when i get home from school because I can't do much on a school chromebook.

after that try closing vscode then opening the folder again."
saeedasadibagloee,2023-10-26 02:40:27+00:00,comment,adept/fuyu-8b,"I m trying to run it via google colab, got the same message, any solution: ImportError: cannot import name 'FuyuProcessor' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)
"
pcuenq,2023-10-26 09:44:32+00:00,comment,adept/fuyu-8b,"Hi @saeedasadibagloee! Same answer as above by @FantasticMrCat42, until a new version of transformers is released in PyPi, you need to install from the main branch. In Google Colab, you should run this cell first:

```bash
!pip install git+https://github.com/huggingface/transformers.git
```
"
ArthurZ,2023-10-25 13:40:46+00:00,comment,adept/fuyu-8b,"I think we are gonna release an update to make sure the begin of sentence and end of sentence are properly added to the sequences, making sure our results match the demos! cc @Molbap "
Molbap,2023-11-03 08:59:14+00:00,comment,adept/fuyu-8b,"Hi @YuntaoChen , thanks a lot for the thorough testing, it was useful! We have an update as part of the latest `transformers` release:
First of all, now the model supports batching so you can input a list of prompts and a equal length list of images to the processor to get the model inputs ready for generation! And second, the prompt structure does matter as you have to prompt the model to answer in a VQA fashion. We've also updated a few examples to better reflect the capabilities of the released model. Try this out:

```python
from PIL import Image
import requests
import io
from transformers import FuyuForCausalLM, FuyuProcessor

pretrained_path = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(pretrained_path)
model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map='auto')


text_prompt = ""Answer the following DocVQA question based on the image. \n Which is the metro in California that has a good job Outlook?""
jobs_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/jobs.png""
jobs_image_pil = Image.open(io.BytesIO(requests.get(jobs_image_url).content))

second_text_prompt = ""Answer the following DocVQA question based on the image. \n What if the maximum male life expectancy?""
chart_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/chart.png""
chart_image_pil = Image.open(io.BytesIO(requests.get(chart_image_url).content))

third_text_prompt = ""Answer the following DocVQA question based on the image. \n What sport is that?""
skate_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/skateboard.png""
skate_image_pil = Image.open(io.BytesIO(requests.get(skate_image_url).content))

fourth_text_prompt = ""Answer the following DocVQA question based on the image. \n What was the fair amount of paid vacation days in the United Kingdom?""
vacations_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/vacation_days_hr.png""
vacations_image_pil = Image.open(io.BytesIO(requests.get(vacations_image_url).content)).convert('RGB')

fifth_text_prompt = ""Answer the following VQAv2 question based on the image: What type of foods are in the image?""
fish_image_url = ""https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/fish_carrots.png""
fish_image_pil = Image.open(io.BytesIO(requests.get(fish_image_url).content))


texts = [text_prompt, second_text_prompt, third_text_prompt, fourth_text_prompt, fifth_text_prompt]
images = [jobs_image_pil, chart_image_pil, skate_image_pil, vacations_image_pil, fish_image_pil]

model_inputs = processor(text=texts, images=images).to('cuda')


model_outputs = processor.tokenizer.batch_decode(model.generate(
    **model_inputs, max_new_tokens=10)[:, -10:], skip_special_tokens=True)

ground_truths = ['Los Angeles', '80.7', 'skateboarding', '28', 'fish, carrots, lemon']


for ground_truth, model_output in zip(ground_truths, model_outputs):
    prediction = model_output.split('\x04 ', 1)[1] if '\x04' in model_output else ''
    assert (ground_truth == prediction)

```
"
Molbap,2023-10-24 10:13:56+00:00,comment,adept/fuyu-8b,@FantasticMrCat42  You can try 4-bit quantization on a free-tier google colab: https://t.co/u4AMLbZuAU 
gauravverma,2023-10-23 20:23:54+00:00,comment,adept/fuyu-8b,"`image = Image.open(image_path).convert(""RGB"")` should fix this"
KongoCat,2023-10-24 03:59:07+00:00,comment,adept/fuyu-8b,"I met the same question too, maybe png format like rgba image have not yet accept."
pcuenq,2023-10-24 11:19:51+00:00,comment,adept/fuyu-8b,Hi @KongoCat! The solution suggested above by @gauravverma should work fine :)
KongoCat,2023-10-24 12:54:51+00:00,comment,adept/fuyu-8b,"> Hi @KongoCat! The solution suggested above by @gauravverma should work fine :)

thanks!"
vishal1278,2023-10-24 13:28:23+00:00,comment,adept/fuyu-8b,"> `image = Image.open(image_path).convert(""RGB"")` should fix this

Thank you, @gauravverma!"
Molbap,2023-10-24 09:14:27+00:00,comment,adept/fuyu-8b,"Hey, the `attention_mask` will be updated in the PR https://github.com/huggingface/transformers/pull/27007 which will add batching, you'll be able to cache a few QAs directly!"
Molbap,2023-10-24 09:12:10+00:00,comment,adept/fuyu-8b,I don't know if the model was trained/tested on text-heavy documents @Techie5879 - as a matter of fact I'll evaluate it on a document QA task soon because I'm curious about it as well
rfhuang,2023-11-08 02:37:15+00:00,comment,adept/fuyu-8b,"Working the problem myself, in the meantime any updates @Molbap?"
Molbap,2023-10-24 10:16:50+00:00,comment,adept/fuyu-8b,"Hey, the PR https://github.com/huggingface/transformers/pull/27007 aims at improving the image processor, right now device_map auto on multi gpu indeed seems to have issues, will be fixed there! For now you have to manually set your devices."
dongXL,2023-10-23 05:56:54+00:00,comment,adept/fuyu-8b,I met the same issue. Do you solve it ?
Molbap,2023-10-23 08:46:31+00:00,comment,adept/fuyu-8b,"This is just a warning, but it should be removed before the next release of transformers :) do you encounter issues with generation? "
zhangchaosunshine,2023-10-23 11:02:30+00:00,comment,adept/fuyu-8b,"> This is just a warning, but it should be removed before the next release of transformers :) do you encounter issues with generation?

thanks, got it"
Colderthanice,2023-10-25 15:51:48+00:00,comment,adept/fuyu-8b,"@zhangchaosunshine @dongXL 

å…³äºŽæ³¨æ„åŠ›æŽ©ç ä¸Žå¡«å……ä»¤ç‰Œï¼š
é”™è¯¯ä¿¡æ¯æç¤ºæ‚¨æœªè®¾ç½®æ³¨æ„åŠ›æŽ©ç ï¼ˆattention maskï¼‰å’Œå¡«å……ä»¤ç‰ŒIDï¼ˆpad token idï¼‰ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ¨¡åž‹çš„è¾“å‡ºä¸ç¨³å®šã€‚åœ¨è®¸å¤štransformeræ¨¡åž‹ä¸­ï¼Œæ³¨æ„åŠ›æŽ©ç ç”¨äºŽæŒ‡ç¤ºæ¨¡åž‹å“ªäº›ä»¤ç‰Œæ˜¯çœŸå®žçš„è¾“å…¥ï¼Œå“ªäº›æ˜¯å¡«å……çš„ã€‚æ²¡æœ‰å®ƒï¼Œæ¨¡åž‹å¯èƒ½ä¼šåœ¨å¤„ç†å¡«å……ä»¤ç‰Œæ—¶æµªè´¹è®¡ç®—èµ„æºã€‚
æœ‰çš„æƒ…å†µä¸‹é”™è¯¯ä¿¡æ¯è¿˜æåˆ°pad_token_idå·²ç»è¢«è®¾ç½®ä¸ºeos_token_idï¼ˆç»“æŸç¬¦ä»¤ç‰ŒIDï¼‰ã€‚è¿™æ˜¯å› ä¸ºåœ¨å¼€æ”¾å¼ç”Ÿæˆä¸­ï¼Œç»“æŸä»¤ç‰Œé€šå¸¸è¢«è§†ä¸ºå¡«å……ä»¤ç‰Œã€‚

ä¸ºäº†æé«˜æ¨¡åž‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œæ‚¨åº”è¯¥è€ƒè™‘åšä»¥ä¸‹æ“ä½œï¼š
æ·»åŠ æ³¨æ„åŠ›æŽ©ç ï¼šå½“æ‚¨å‘æ¨¡åž‹æä¾›è¾“å…¥æ—¶ï¼Œç”Ÿæˆä¸€ä¸ªä¸Žæ‚¨çš„è¾“å…¥é•¿åº¦ç›¸åŒçš„äºŒè¿›åˆ¶å‘é‡ï¼Œå…¶ä¸­çœŸå®žçš„è¾“å…¥ä»¤ç‰Œæ˜¯1ï¼Œå¡«å……ä»¤ç‰Œæ˜¯0ã€‚ç„¶åŽï¼Œå°†æ­¤å‘é‡ä½œä¸ºattention_maskä¼ é€’ç»™æ¨¡åž‹ã€‚
è®¾ç½®å¡«å……ä»¤ç‰ŒIDï¼šå¦‚æžœæ‚¨çš„æ¨¡åž‹æœ‰ä¸€ä¸ªç‰¹å®šçš„å¡«å……ä»¤ç‰Œï¼Œç¡®ä¿åœ¨è¿›è¡ŒæŽ¨ç†ä¹‹å‰ä¸ºæ¨¡åž‹è®¾ç½®æ­£ç¡®çš„pad_token_idã€‚

å¯å‚è€ƒä»¥ä¸‹ä»£ç ï¼š
# To eliminate attention_mask warning
        inputs[""attention_mask""] = torch.ones(inputs[""input_ids""].shape, device=""cuda:0"")
        generation_output = model.generate(**inputs, max_new_tokens=50, pad_token_id=model.config.eos_token_id)

è¿™æ ·è¿™ä¸ªè®¨åŽŒçš„è­¦å‘Šä¿¡æ¯å°±ä¸ä¼šå†å‡ºçŽ°äº†ã€‚
"
Molbap,2023-11-03 09:00:46+00:00,comment,adept/fuyu-8b,"@zhangchaosunshine 
In the recent update with the new transformers release, the attention_mask warning goes away because it is now indeed returned by the Processor.
Now, batch generation is also supported, so I don't recommend setting attention_mask manually in the current version that is using left-padding. Let us know if you encounter issues!
"
pcuenq,2023-10-21 17:48:10+00:00,comment,adept/fuyu-8b,"Hello @slay!

There has not been a PyPi release of transformers since the model was added, so you need to install transformers from the `main` branch like this:

```
pip install git+https://github.com/huggingface/transformers.git
```
"
slay,2023-10-21 18:15:44+00:00,comment,adept/fuyu-8b,"Thank you! Appreciate you prompt advice
"
htamm-ramp,2023-10-23 17:08:51+00:00,comment,adept/fuyu-8b,"Hi, I tried doing this but still getting 

```
â•­â”€ ~/workspace/mine-misc Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â· Py mine-misc 3.10.13
â•°â”€â¯ pip install git+https://github.com/huggingface/transformers
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /private/var/folders/m_/t431rmpd1gjdb50j74qzsc_h0000gp/T/pip-req-build-sq9homdr
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /private/var/folders/m_/t431rmpd1gjdb50j74
qzsc_h0000gp/T/pip-req-build-sq9homdr
  Resolved https://github.com/huggingface/transformers to commit 33f98cfded1724e12be11a3b3333e4821de9bbbe
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: tqdm>=4.27 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (from tra
nsformers==4.35.0.dev0) (4.66.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-
packages (from transformers==4.35.0.dev0) (0.17.3)
Requirement already satisfied: numpy>=1.17 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (from tr
ansformers==4.35.0.dev0) (1.26.1)
Requirement already satisfied: safetensors>=0.3.1 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (
from transformers==4.35.0.dev0) (0.4.0)
Requirement already satisfied: packaging>=20.0 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (fro
m transformers==4.35.0.dev0) (23.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (f
rom transformers==4.35.0.dev0) (2023.10.3)
Requirement already satisfied: filelock in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (from trans
formers==4.35.0.dev0) (3.12.4)
Requirement already satisfied: pyyaml>=5.1 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (from tr
ansformers==4.35.0.dev0) (6.0.1)
Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packag
es (from transformers==4.35.0.dev0) (0.14.1)
Requirement already satisfied: requests in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (from trans
formers==4.35.0.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-pa
ckages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (4.8.0)
Requirement already satisfied: fsspec in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (from hugging
face-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (2023.10.0)
Requirement already satisfied: certifi>=2017.4.17 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (
from requests->transformers==4.35.0.dev0) (2023.7.22)
Requirement already satisfied: idna<4,>=2.5 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (from r
equests->transformers==4.35.0.dev0) (3.4)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-pack
ages (from requests->transformers==4.35.0.dev0) (3.3.1)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/htammineedi/.pyenv/versions/3.10.13/envs/mine-misc/lib/python3.10/site-packages (
from requests->transformers==4.35.0.dev0) (2.0.7)

[notice] A new release of pip is available: 23.0.1 -> 23.3.1
[notice] To update, run: python3.10 -m pip install --upgrade pip

â•­â”€ ~/workspace/mine-misc Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â· 15s Py mine-misc 3.10.13
â•°â”€â¯ python
Python 3.10.13 (main, Oct 15 2023, 09:30:15) [Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from transfomers import FuyuProcessor
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'transfomers'
>>> from transformers import FuyuProcessor
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utili
ties can be used.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'FuyuProcessor' from 'transformers' (/Users/htammineedi/.pyenv/versions/mine-misc/lib/python3.10/site-package
s/transformers/__init__.py)
>>>

```"
Molbap,2023-10-24 07:29:08+00:00,comment,adept/fuyu-8b,"Hey @htamm-ramp , are you sure your dev environment/PYTHONPATH is setup correctly? Building from source in my conda env allows me import `FuyuProcessor` from `transformers`. Maybe you have a conflicting installation of transformers from PyPI?"
Ricky1999,2023-11-03 02:05:08+00:00,comment,adept/fuyu-8b,"Hey. @htamm-ramp, try install `torch` and `pillow`.
```
pip install torch pillow
```"
Lord-Goku,2023-12-01 22:25:15+00:00,comment,adept/fuyu-8b,I'd really like to know the answer to this question. Is it possible to quantize this model?
JohnCHENMEL,2023-10-20 13:29:11+00:00,comment,adept/fuyu-8b,"when run in colab, it show error,can not import"
pcuenq,2023-10-20 13:40:40+00:00,comment,adept/fuyu-8b,"Hello! You have to use transformers from the main branch:

`!pip install git+https://github.com/huggingface/transformers.git`"
ArthurZ,2023-10-20 12:30:59+00:00,comment,adept/fuyu-8b,"It's not super relevant for Fuyu, but people with previous versions of transformers will still be able to use the tokenizers if this is removed"
pcuenq,2023-10-22 11:06:14+00:00,comment,adept/fuyu-8b,"Merging now, cc @mtensor for visibility :)"
ArthurZ,2023-10-20 08:31:10+00:00,comment,adept/fuyu-8b,The device you should use is either â€œcpuâ€ or â€œmpsâ€ for Mac 
louis030195,2023-10-28 20:53:37+00:00,comment,adept/fuyu-8b,"Anyone managed to run it? 

```
Using device: mps
Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:00<00:00, 1.59MB/s]
Downloading (â€¦)fetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58.2k/58.2k [00:00<00:00, 800kB/s]
Downloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.93G/9.93G [06:55<00:00, 23.9MB/s]
Downloading (â€¦)of-00002.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.88G/8.88G [08:00<00:00, 18.5MB/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [14:56<00:00, 448.48s/it]
Loading checkpoint shards:   0%|                                                                                                                                    | 0/2 [00:00<?, ?it/s]Killed: 9
/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

```

Macbook pro m2 32 gb ram - not enough?"
pcuenq,2023-10-29 19:18:56+00:00,comment,adept/fuyu-8b,"The following works for me. The Python process seems to take ~21 GB when using `float16` in my M1.

```py
import requests
import torch
from PIL import Image
from transformers import AutoTokenizer, FuyuForCausalLM, FuyuImageProcessor, FuyuProcessor

device = ""mps""

# Metal supports `bfloat16` in Sonoma, but it still doesn't work
dtype = torch.bfloat16 if device != ""mps"" else torch.float16

model_id = ""adept/fuyu-8b""
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = FuyuForCausalLM.from_pretrained(model_id, device_map=device, torch_dtype=dtype)
processor = FuyuProcessor(image_processor=FuyuImageProcessor(), tokenizer=tokenizer)

prompt = ""Generate a coco-style caption.\n""
url = ""https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png""

image = Image.open(requests.get(url, stream=True).raw)

model_inputs = processor(text=prompt, images=[image])
model_inputs = {k: v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype, device=device) for k,v in model_inputs.items()}
prompt_len = model_inputs[""input_ids""].shape[-1]

generation_output = model.generate(**model_inputs, max_new_tokens=10)
print(tokenizer.decode(generation_output[0][prompt_len:], skip_special_tokens=True))
```"
faraday,2023-12-01 07:04:24+00:00,comment,adept/fuyu-8b,"@pcuenq Which transformers version is this running with? I tried and when you're doing v.to(dtype=...) , v is a list of tensors. In order for that call to work v must have been a tensor.

model_inputs = processor(text=prompt, images=[image])
for k, v in model_inputs.items():
    if isinstance(v, list):
        model_inputs[k] = [item.to(dtype=dtype if torch.is_floating_point(item) else item.dtype, device=device) for item in v]
    else:
        model_inputs[k] = v.to(dtype=dtype if torch.is_floating_point(v) else v.dtype, device=device)

Works like this. The else is not needed on my part but I wanted to keep the original case as well.
"
mrfakename,2023-10-27 01:23:06+00:00,comment,adept/fuyu-8b,Any update on this @ArthurZ?
ArthurZ,2023-10-20 08:30:28+00:00,comment,adept/fuyu-8b,Itâ€™s probably harmless in this case. But we should make sure the attention mask is returned by the Processor @molbap 
Molbap,2023-10-20 14:49:40+00:00,comment,adept/fuyu-8b,"Yes, will be adressed in the follow-up PR on transformers:main :) "
Colderthanice,2023-10-25 15:56:13+00:00,comment,adept/fuyu-8b,"# To eliminate attention_mask warning
inputs[""attention_mask""] = torch.ones(inputs[""input_ids""].shape, device=""cuda:0"")
generation_output = model.generate(**inputs, max_new_tokens=50, pad_token_id=model.config.eos_token_id)"
Molbap,2023-11-03 08:55:06+00:00,comment,adept/fuyu-8b,"@Colderthanice @dashesy In the recent update with the new  `transformers` release, the `attention_mask` warning goes away because it is now indeed returned by the Processor :) 
Now, batch generation is also supported, so I don't recommend setting `attention_mask` to `torch.ones(inputs[""input_ids""].shape)`in the current version, that is using left-padding. Let us know if you encounter issues!"
ArthurZ,2023-10-20 10:13:14+00:00,comment,adept/fuyu-8b,Hey! That is expected it's not supported in inference endpoint *yet* 
gpantalos,2023-10-22 23:01:58+00:00,comment,adept/fuyu-8b,"Ok thanks, please let me know when they are."
ArthurZ,2023-10-20 10:13:39+00:00,comment,adept/fuyu-8b,"It does but the processor does not at the moment, a PR is on it's way for this! "
yingss,2023-10-24 16:29:38+00:00,comment,adept/fuyu-8b,"Thank you for the great work! Regarding the ability to support multi-image, does it mean that  fuyu-8b can handle interleaved text and multi-images? Could you clarify the input format for the transformer decoder when there is interleaved text and multi-images? 

From the blog, it appears that the input for a single image and text would be:  [img_patch] [img_patch] \n [img_patch] [img_patch] \n [text]. 

How would the format change in the scenario where the sequence is ""\<img1> \<img2> some text""? Would there be some special image token to separate the two images? Or it would be like [img1_patch] [img1_patch] \n [img1_patch] [img1_patch] \n [img2_patch] [img2_patch] \n [img2_patch] [img2_patch] \n [text]?

Also,  does `FuyuForCausalLM `need any modification in order to accommodate interleaved text and multi-images?  Thank you!"
ArthurZ,2023-10-25 14:09:16+00:00,comment,adept/fuyu-8b,"I think it does support interleaved images, I'm not entirely sure how two images are prompted, we'll try to ask the authors! "
yingss,2023-11-03 02:07:38+00:00,comment,adept/fuyu-8b,Any update on supporting interleaved images?
deleted,2023-11-10 12:42:16+00:00,comment,adept/fuyu-8b,This comment has been hidden
Starlento,2023-10-19 05:42:20+00:00,comment,adept/fuyu-8b,"text_prompt = ""What is this screenshot about?\n""
![2023-10-19_13h15_52.png](https://cdn-uploads.huggingface.co/production/uploads/631c7a0fe65cd7f686d52856/1ym4UBr_YR2SE7GgKgHlZ.png)
Result: The screenshot displays a message on a cell phone, which reads ""New Vision-Language Model: BakLaVa-1, finetuned on Mistral 7B."" The message appears to be from a person who is discussing a new model. The message also hints at a new message, which appears to be from another person. The messages are accompanied by two icons, one on the left and another on the right side of the screen."
Molbap,2023-11-03 09:31:58+00:00,comment,adept/fuyu-8b,"Hey @Starlento , the prompt structure for that base model does seem to matter, as the model was trained on particular prompts. Prepending a specific prompt with `""Answer the following DocVQA question based on the image.""` seems to do better for instance. "
ArthurZ,2023-10-20 10:14:02+00:00,comment,adept/fuyu-8b,We'll try to share finetuning scripts as soon as possible ! 
NickyNicky,2023-10-20 10:31:05+00:00,comment,adept/fuyu-8b,"Thank you very much for the model, it is very good. Will it not be possible to observe the evaluations with large models?"
ncoop57,2023-10-23 00:13:48+00:00,comment,adept/fuyu-8b,"@ArthurZ not sure if this help, but I have been trying to create a finetuning example for huggingface here: https://github.com/huggingface/transformers/pull/26997

Ran into issues around padding the batches and would love some help :D"
Molbap,2023-10-24 13:40:31+00:00,comment,adept/fuyu-8b,"@ncoop57 the padding of batches is a bit tricky, and it will be supported in main when this PR https://github.com/huggingface/transformers/pull/27007 is done, we're on it!"
Neman,2023-10-22 15:55:01+00:00,comment,adept/fuyu-8b,"> Question What are the benchmark results for image captioning for fuyu-8b in comparison to other models? https://paperswithcode.com/task/image-captioning

You have some results on Model card page comparing Fuyu-8B vs Fuyu-Medium, LLaVA 1.5 (13.5B), QWEN-VL (10B), PALI-X (55B), PALM-e-12B, and PALM-e-562B
"
darknoon,2023-10-18 23:20:50+00:00,comment,adept/fuyu-8b,Any chance [this config change](https://huggingface.co/adept/fuyu-8b/discussions/3/files) is needed?
darknoon,2023-10-18 23:46:04+00:00,comment,adept/fuyu-8b,Relevant [comment on the pr](https://github.com/huggingface/transformers/pull/26911#issuecomment-1769563721)
pcuenq,2023-10-19 00:33:05+00:00,comment,adept/fuyu-8b,"The PR has been merged and the weights updated in the repo, so you should be able to load the model using transformers @ `main`.
"
pcuenq,2023-10-19 00:33:46+00:00,comment,adept/fuyu-8b,"Hi @joanfihu, it works in a 24GB GPU using `bfloat16` :) "
latent-variable,2023-10-19 04:04:47+00:00,comment,adept/fuyu-8b,@pcuenq Can you tell me how you modified the code to use bfloat16?
latent-variable,2023-10-19 04:11:23+00:00,comment,adept/fuyu-8b,"For anyone else wondering this, 
Just update the following line:

model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=""cuda:0"", torch_dtype=torch.float16)
"
balisujohn,2023-10-21 07:53:03+00:00,comment,adept/fuyu-8b,"Is there a way to get this to work on CPU with under 25Gigs of ram?  when I tried to set it to float 16, get the following error:

""RuntimeError: ""addmm_impl_cpu_"" not implemented for 'Half'""

It seems like float16 is not supported for some operations on CPU, so low precision CPU mode might need some actual tweaks to the code. 

The reason I think this would be nice if possible is because many people don't have more than 32 gigs of ram or a graphics card with 20+ gigs of vram. "
Neman,2023-10-22 15:52:13+00:00,comment,adept/fuyu-8b,"> model = FuyuForCausalLM.from_pretrained(pretrained_path, device_map=""cuda:0"", torch_dtype=torch.float16)

Maybe debatable, but have read that bfloat16 is recommended over float16 because of its structure. So: torch_dtype=torch.bfloat16
I tested both and model is loading faster in bf16 (6s vs 10s for f16), but inference is faster with f16 (1.22 vs 1.70 for bf16). Can't assess quality, but people saying bf16 is better."
Colderthanice,2023-10-23 01:38:52+00:00,comment,adept/fuyu-8b,"@latent-variable 
Thanks, it worked!"
layoric,2023-10-23 11:33:12+00:00,comment,adept/fuyu-8b,"I have found when using multiple GPUs, this model requires above 32GB of VRAM. I have 3x A4000 16GBs (a weird setup I know), and only just manage to fit within their limit when using a custom `device_map`. I hit a few issues related to which layers were required to be on the same GPU, so if it helps others, the following layers need to be listed on the `device_map` on the same device:

```
device_map = {
    'language_model.model.embed_tokens.weight': 'cuda:0',
    'language_model.lm_head.weight':'cuda:0',
    'language_model.model.final_layernorm.bias':'cuda:0',
    'language_model.model.final_layernorm.weight':'cuda:0',
    'vision_embed_tokens.bias':'cuda:0',
    'vision_embed_tokens.weight':'cuda:0',
   # rest of the layers shared among devices..
}
```

My usage using `torch_dtype=torch.bfloat16` used more like 40GB, though I might be doing something else wrong. Eg my code is:

```
model_id = ""adept/fuyu-8b""
processor = FuyuProcessor.from_pretrained(model_id,torch_dtype=torch.bfloat16)
model = FuyuForCausalLM.from_pretrained(model_id, device_map=device_map) # torch_dtype=torch.bfloat16 should be here
```

~~Any insights would be appreciated~~.  Above I used the `torch_dtype=torch.bfloat16` on the wrong line ðŸ¤¦â€â™‚ï¸, never mind me.

Thanks for releasing such an interesting model! ðŸ‘"
ArthurZ,2023-10-25 13:39:29+00:00,comment,adept/fuyu-8b,Thanks for sharing you experiences! 
ludeksvoboda,2023-11-05 19:57:43+00:00,comment,adept/fuyu-8b,"I have run it on two rtx3060, each has 12GB. Needs custom device map and FP16. Can process only images in ~500x1000 maximum resolution though."
pcuenq,2023-10-18 19:17:44+00:00,comment,adept/fuyu-8b,Thank you!
linxi,2023-10-31 03:18:59+00:00,comment,adept/fuyu-8b,+1
Nooodles,2023-12-05 08:49:45+00:00,comment,adept/fuyu-8b,+1
victorivus,2024-04-05 14:09:47+00:00,comment,adept/fuyu-8b,"+1
"
osanseviero,2023-10-18 18:21:08+00:00,comment,adept/fuyu-8b,You can use https://github.com/huggingface/transformers/pull/26911 !
pcuenq,2023-10-19 00:34:21+00:00,comment,adept/fuyu-8b,Now merged in `main` :)
mlabonne,2024-01-16 08:55:42+00:00,comment,mlabonne/Marcoro14-7B-slerp,"Hi Eric, like NeuralMarcoro14 (https://huggingface.co/spaces/mlabonne/NeuralMarcoro14-7B-GGUF-Chat/blob/main/Dockerfile#L7), it should use the chatml template."
ehartford,2024-01-16 19:31:30+00:00,comment,mlabonne/Marcoro14-7B-slerp,"However, the ChatML tokens are not added (ie `</s>` is the EOS token)

https://huggingface.co/mlabonne/Marcoro14-7B-slerp/blob/main/tokenizer_config.json

"
ehartford,2024-01-16 19:32:29+00:00,comment,mlabonne/Marcoro14-7B-slerp,"So, you trained it to expect / generate <|im_start|> and <|im_end|> but not as tokens?  "
mlabonne,2024-01-16 19:43:16+00:00,comment,mlabonne/Marcoro14-7B-slerp,"Yes, I had a discussion on this topic about another model. For NeuralMarcoro14, I added the chatml template as ""chat_template"" (https://huggingface.co/mlabonne/NeuralMarcoro14-7B/blob/main/tokenizer_config.json#L44). This quick fix worked so I didn't try to change the tokens.

Do you think it's better to modify the tokenizer config to have a proper chatml template or just stick with Llama's chat template?"
mlabonne,2024-01-16 19:44:39+00:00,comment,mlabonne/Marcoro14-7B-slerp,"> So, you trained it to expect / generate <|im_start|> and <|im_end|> but not as tokens?

This one is untrained, but this is exactly what happened with the neural version during DPO (this is mildly infuriating)."
cvinker,2022-12-06 13:17:46+00:00,comment,facebook/galactica-1.3b,How can I implement this?
mkardas,2022-12-06 13:22:36+00:00,comment,facebook/galactica-1.3b,What are you trying to achieve?
cvinker,2022-12-06 13:33:08+00:00,comment,facebook/galactica-1.3b,"The 1.3b model uses most of my 8gb of vram so large requests make it go over pretty quickly, I was hoping this would cut the memory use down."
mkardas,2022-12-06 13:35:34+00:00,comment,facebook/galactica-1.3b,"You can load your model with:

```python
model = OPTForCausalLM.from_pretrained(
            ""facebook/galactica-1.3b"",
            torch_dtype=""float16"",
            device_map=""auto""
        )
```"
hwasiti,2022-12-07 20:24:21+00:00,comment,facebook/galactica-1.3b,"For me this worked well:

```
input_text = """"""
# Why genetic diversity is important for biodiversity conservation?

## INTRODUCTION
""""""

randomizer_value = 251
repititions = 1 # 4

# set seed to reproduce results. Feel free to change the seed though to get different results
torch.manual_seed(randomizer_value)

# input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids   ############### CPU only
input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")

# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3
sample_outputs = model.generate(
    input_ids,
    do_sample=True, 
    max_length=2000, 
    top_k=50, 
    top_p=0.95, 
    num_return_sequences=repititions
)

print(""Output:\n"" + 100 * '-')
for i, sample_output in enumerate(sample_outputs):
    print(""{}: {}"".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))
    print('--------------------------------------------------------------')


```"
davidbernat,2022-12-08 12:33:12+00:00,comment,facebook/galactica-1.3b,"Yes. Those are, essentially, the parameters specified here. https://huggingface.co/blog/how-to-generate
You are probably making smart decisions by adding the header of the first section you want to compile. 
I have noticed Galactica often struggles with that initiation otherwise. 
What is your use case?"
Delcos,2022-12-30 03:06:36+00:00,comment,facebook/galactica-1.3b,"> Yes. Those are, essentially, the parameters specified here. https://huggingface.co/blog/how-to-generate
> You are probably making smart decisions by adding the header of the first section you want to compile. 
> I have noticed Galactica often struggles with that initiation otherwise. 
> What is your use case?

""You are probably making smart decisions by adding the header of the first section you want to compile.""

What do you mean?"
MonsterMMORPG,2022-11-19 14:40:15+00:00,comment,facebook/galactica-1.3b,"here another example


![image.png](https://cdn-uploads.huggingface.co/production/uploads/1668868811932-6345bd89fe134dfd7a0dba40.png)"
breadlicker45,2022-11-21 16:57:02+00:00,comment,facebook/galactica-1.3b,This is the peak of AI technology
davidbernat,2022-11-26 16:42:50+00:00,comment,facebook/galactica-1.3b,"Start here: https://huggingface.co/blog/how-to-generate

There are numerous configurations. This is not garbage.
If you post your efforts and experiments here I will try to run them myself. 
Maybe we can learn together."
nunonmg,2024-03-04 13:15:30+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Hello! Here is the paper: https://arxiv.org/abs/2402.17733
Curious to see your experiments!"
jmprcp,2024-02-13 14:34:13+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Hi,

Thanks for raising this. I believe it is now fixed. Could you download the model and try again?"
WasamiKirua,2024-02-13 16:47:59+00:00,comment,Unbabel/TowerBase-7B-v0.1,"I will and I'll let you know. Thanks for the quick reply. Anyway I have loaded the model in 4bits precision, tried it out and I wanted to express my entusiams, translation is veeeeery good."
jmprcp,2024-02-13 17:05:07+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Thank you for your interest in Tower, and your enthusiasm! You may want to try out our Instruct variants as well (https://huggingface.co/Unbabel/TowerInstruct-7B-v0.2 and https://huggingface.co/Unbabel/TowerInstruct-13B-v0.1) --- they are tailored for translation, among other tasks."
WasamiKirua,2024-02-14 10:03:15+00:00,comment,Unbabel/TowerBase-7B-v0.1,"> Thank you for your interest in Tower, and your enthusiasm! You may want to try out our Instruct variants as well (https://huggingface.co/Unbabel/TowerInstruct-7B-v0.2 and https://huggingface.co/Unbabel/TowerInstruct-13B-v0.1) --- they are tailored for translation, among other tasks.

I can cofirm that is now possible to quantize the model. Many thanks"
jmprcp,2024-02-13 13:22:15+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Hi, 
Thanks for raising this. This is related to the special tokens added after training; we were not aware it broke the model.
Could you provide the code that caused this error so we can reproduce it on our end?"
dchaplinsky,2024-02-13 13:52:56+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Yes, sure:
https://github.com/dchaplinsky/finetune_experiments/blob/main/finetune.py

Let me know if you need a dataset too (it's around 3m sentence pairs)."
jmprcp,2024-02-13 14:33:44+00:00,comment,Unbabel/TowerBase-7B-v0.1,"I believe the issue is fixed now; I removed the special tokens from the tokenizer.

Could you download the model again and report back, please?"
dchaplinsky,2024-02-13 14:45:04+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Thanks. Once I'll have a GPU available, I'll test it, currently they are busy finetuning on the dataset without flawed example  :).

Did you also altered rest of the models, like 13b?"
jmprcp,2024-02-13 14:49:19+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Got it, thanks.
The 13b model should be fixed as well."
dchaplinsky,2024-02-13 15:54:40+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Started another experiment on an updated tokenizer model. Will know the result in a few hours.

On a related note, I have a rather big ukrainian corpus, compiled from three different sources as well as some parallel en-uk corpus (cleansed version of the paracrawl). Is there any chance to include some of those in the 0.2 model?"
jmprcp,2024-02-13 16:17:29+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Alright, thank you.
We are not planning to add new languages to Tower right now. However, we do encourage people to try this kind of thing; it would be cool to have a Tower-based model that supports Ukranian. You may want to assess whether it (and TowerInstruct) performs well in a 0- or 5-shot fashion as well."
dchaplinsky,2024-02-20 10:16:23+00:00,comment,Unbabel/TowerBase-7B-v0.1,Please let me know when you release the paper on Tower.
jmprcp,2024-02-13 01:29:25+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Hi, thank you for your interest in Tower.
We did full model training and used the original tokenizer.
We are going to publish a paper on Tower soon, so stay tuned!"
jmprcp,2024-01-16 10:28:59+00:00,comment,Unbabel/TowerBase-7B-v0.1,"Thanks for raising this. It should now be fixed, without needing to explicitly set `tokenizer.pad_token_id = tokenizer.eos_token_id`. (https://huggingface.co/Unbabel/TowerBase-7B-v0.1/commit/2837006f6f8e9ed6e637ab8fcf9a6bf22e31e4d8)
"
dhruvabansal,2024-05-09 20:17:41+00:00,comment,refuelai/Llama-3-Refueled,"Yes! However, while training we consume all inputs as a part of the ""user"" role. Would recommend sending everything as a part of the user while constructing the final prompt!"
mlabonne,2024-01-15 19:40:45+00:00,comment,mlabonne/Beagle14-7B,"Thanks @SimSim93! I'm currently evaluating a DPO version of this model, it should be even better.

If you want to make GGUF versions of a 7B model, you don't need any big hardware. I created this notebook to automate this process (T4 GPU): https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k"
testerav,2024-01-16 02:53:07+00:00,comment,mlabonne/Beagle14-7B,"> I'm currently evaluating a DPO version of this model, it should be even better.

Not much Difference in Scores.

"
TheDrummer,2024-05-07 23:12:37+00:00,comment,TheDrummer/Moistral-11B-v3,"Hello fellow LoverLabs player, I see Skyrim's still going strong.

Have you tried the GGUFs?

https://huggingface.co/TheDrummer/Moistral-11B-v3-GGUF/tree/main

https://huggingface.co/MarsupialAI/Moistral-11B-v3_iMatrix_GGUF/tree/main"
Keionsa,2024-05-07 23:32:00+00:00,comment,TheDrummer/Moistral-11B-v3,"omg! Ty so so much!! LOL Lovers lab hahah YUP! we have 7 kids now with fertility mode. 
![20231022233212_1.jpg](https://cdn-uploads.huggingface.co/production/uploads/662fce5814e87951eadf323a/flbRB0FVYgJCpOMTmGgn9.jpeg)
"
Skorcht,2024-05-08 23:31:21+00:00,comment,TheDrummer/Moistral-11B-v3,"> omg! Ty so so much!! LOL Lovers lab hahah YUP! we have 7 kids now with fertility mode. 
> ![20231022233212_1.jpg](https://cdn-uploads.huggingface.co/production/uploads/662fce5814e87951eadf323a/flbRB0FVYgJCpOMTmGgn9.jpeg)

this looks interesting! does this game load on the steamdeck? also is this model good for non nsfw use case? i wanna use it as a super vulgar thing that swears alot. im guessing the nsfw helps with it knowin  acertain vocab of words. "
Keionsa,2024-05-08 23:49:15+00:00,comment,TheDrummer/Moistral-11B-v3,"I dont know about vulgur i am happy to report the erp is excellent. Noromaid off open router uses the f word.  as far as steamdeck not even sure what that is? I use a wabbajack in skyrim vr. 
https://www.reddit.com/r/SteamDeck/comments/17h08gc/the_steam_deck_can_run_skyrim_vr_almost/"
Keionsa,2024-05-07 22:38:39+00:00,comment,TheDrummer/Moistral-11B-v3,"hrm my kobolid is crashing when I try to load it
"
TheDrummer,2024-05-07 23:02:45+00:00,comment,TheDrummer/Moistral-11B-v3,@concedo 
concedo,2024-05-12 16:46:05+00:00,comment,TheDrummer/Moistral-11B-v3,"dank

what error?"
Masterjp123,2024-04-27 07:50:21+00:00,comment,TheDrummer/Moistral-11B-v3,BTW it is 4.5bpw bc i only have like 12gb of ram :(
Masterjp123,2024-04-27 08:17:30+00:00,comment,TheDrummer/Moistral-11B-v3,https://huggingface.co/Masterjp123/Moistral-11B-v3-EXL2-4.5Bit
complexinteractive,2024-04-24 22:39:59+00:00,comment,TheDrummer/Moistral-11B-v3,"@TheDrummer please add ""My name is Lily"" to the list. Idk how Lily became the default woman name for local LLMs but I'd love one that chose anything else."
Spacellary,2024-04-25 01:26:12+00:00,comment,TheDrummer/Moistral-11B-v3,"---
@Lewdiculous 
---

Lily worked her way into many datasets. Coincidence? I don't think so... 

XD"
JoggyMuffin,2024-04-25 15:42:23+00:00,comment,TheDrummer/Moistral-11B-v3,"I would like to finally see a model with ""shivers down x spine"" nuked from orbit"
xxx777xxxASD,2024-04-26 09:38:24+00:00,comment,TheDrummer/Moistral-11B-v3,"""the mixture of pain and pleasure"""
TheDrummer,2024-04-26 09:40:03+00:00,comment,TheDrummer/Moistral-11B-v3,"> @TheDrummer please add ""My name is Lily"" to the list. Idk how Lily became the default woman name for local LLMs but I'd love one that chose anything else.

Surprised it isn't Sarah, tbh."
Lewdiculous,2024-04-26 12:27:31+00:00,comment,TheDrummer/Moistral-11B-v3,"> ""the mixture of pain and pleasure""

I feel like that one is only a big problem if it's being repeated again and again. But I also can concur it is used often by a variety of models."
llama-anon,2024-04-27 07:35:21+00:00,comment,TheDrummer/Moistral-11B-v3,This comment has been hidden
rory-games,2024-05-12 06:24:38+00:00,comment,TheDrummer/Moistral-11B-v3,"another I don't like is ""couldn't help but""/""can't help but"""
sh0416,2023-08-07 16:53:41+00:00,comment,facebook/incoder-1B,"Oh, there is a typo in option.. `clean_up_tokenization_spaces=False` works great. Sorry for the confusion."
Undi95,2024-02-05 21:00:54+00:00,comment,Undi95/MLewd-v2.4-13B,It should be native 4k yes!
d7dd0d,2023-11-18 17:40:27+00:00,comment,Undi95/MLewd-v2.4-13B,"Well, since you're already familiar with GGUF, why not give [Undi95/MLewd-v2.4-13B-GGUF](https://huggingface.co/Undi95/MLewd-v2.4-13B-GGUF) a try? I'm pretty sure it performs better than any 7b version of this model."
Undi95,2023-11-18 17:57:28+00:00,comment,Undi95/MLewd-v2.4-13B,"Hello, I completely forgot this, I can't shrink down the model to 7B because all ressource used was 13B. Sorry!"
BlueNipples,2023-11-19 13:46:59+00:00,comment,Undi95/MLewd-v2.4-13B,"> im not too sure if its possible, but i think id love to try these mlewd models out, but my 1060 only has 6gb of vram and has a hard time handling such a large model.
> if possible id love a 7b version of this or the chat version of this model.
> please and thank you!
> currently im using MistRP-AirOrca-7B.q5_k_m.gguf and love it

Try Undi95's toppy. It's _basically_ mlewd for 7b (decent prose, pretty dirty). You could potentially use a 2k gguf of a 13b too, but mistral 7b models fit sweet into vram because of the sliding window (reduces context size ram use a lot)"
clown134,2023-11-20 02:56:06+00:00,comment,Undi95/MLewd-v2.4-13B,"thanks. toppy is pretty good. it JUST fits into my vram and response extremely fast, faster than i can type certainly. i have 6gb vram so 13b models are extremely slow, but i could see them being useful if it was for something i needed coherency on"
Undi95,2023-09-28 15:30:41+00:00,comment,Undi95/MLewd-v2.4-13B,"Since I didn't tried this one too much, I will dig that.
In the meantime, try to edit the first or second message to avoid that, he should take the pace and stop doing it.
Also, did my context helped you for Emerhyst ?

Will see what I can do!"
Huegli,2023-09-28 19:06:55+00:00,comment,Undi95/MLewd-v2.4-13B,"It seems as if this model would insist on an alternating chat. It does better if it gets the alternations, but it still tries to act for {{user}}. Right now I focus more on Emerhyst, it's more fun. :-)"
Undi95,2023-09-28 19:13:47+00:00,comment,Undi95/MLewd-v2.4-13B,"> It seems as if this model would insist on an alternating chat. It does better if it gets the alternations, but it still tries to act for {{user}}. Right now I focus more on Emerhyst, it's more fun. :-)

Fair enough! Haha
Have fun!"
Komposter43,2024-01-26 11:45:38+00:00,comment,garage-bAInd/Platypus2-70B-instruct,"It is needed in train proccess. To my mind, you can chage to True in inference."
waleking,2023-08-11 20:46:13+00:00,comment,garage-bAInd/Platypus2-70B-instruct,Did you merge garage-bAInd/Platypus2-70B's LoRA with upstage/Llama-2-70b-instruct-v2?
arielnlee,2023-08-12 18:48:57+00:00,comment,garage-bAInd/Platypus2-70B-instruct,"Thanks for your interest. It  is a simple linear merge (for now...stay tuned). We played around with the different types of LoRA modules, how the training data affects the outcome of the merge, how merging fine-tunes that used different LoRA modules works, etc. 

From our experience, the outcome of merging two (or more) LoRA based models is very much dependent on 1) the LoRA modules both merged models were fine-tuned with (i.e. did one model use up/down/gate proj and the other k/v/q/o proj 2) the training data, 3) the performance of both original models on whatever benchmarks you're using, and 4) (I think, but am still working on quantitative tests to explore this) the order of the LoRA merge. I believe the order of the merge also affects the ""expertise"" of the model. 

Edit: Since this is additive, one would think order shouldn't matter (which is why it is not discussed in the paper we recently released). I only started looking into it because when we originally merged Platypus-70B with Dolphin, it was the only merge we had at the time that actually did worse than its original counterpart (the rest of our merges were better than both originals). If you're interested, follow-up with me in a week and hopefully I'll have additional insight and quantitative experiments to share! â˜ºï¸"
lauspectrum,2023-08-14 11:26:54+00:00,comment,garage-bAInd/Platypus2-70B-instruct,"I'm curious about if the kind of ""merge"" you mentioned means that you trained a LoRA adapter on ""one"" base model and applied on ""another"". I would be extremely grateful if I could receive your reply. : )"
CrazyAIGC,2023-08-14 13:33:07+00:00,comment,garage-bAInd/Platypus2-70B-instruct,"I'm curious about if the kind of ""merge"" you mentioned means that you trained a LoRA adapter on ""one"" base model and applied on ""another"". I would be extremely grateful if I could receive your reply. : )"
monuminu,2023-08-17 10:30:17+00:00,comment,garage-bAInd/Platypus2-70B-instruct,Is there any notebook explaining how do you merge two models . I am curious how exactly this been done 
Komposter43,2023-08-17 16:25:57+00:00,comment,garage-bAInd/Platypus2-70B-instruct,"I think, somethink like this https://github.com/ontocord/MDEL/tree/main/Model%20Merge%20And%20Analysis%20Tools"
Komposter43,2023-08-17 16:32:29+00:00,comment,garage-bAInd/Platypus2-70B-instruct,"Or it is posible native Peft:

# merge weights - new merging method from peft
lora_model = lora_model.merge_and_unload()

https://github.com/tloen/alpaca-lora/blob/main/export_hf_checkpoint.py

Can you describe your experence?

Thank you."
Komposter43,2023-08-17 16:35:53+00:00,comment,garage-bAInd/Platypus2-70B-instruct,https://github.com/tloen/alpaca-lora/pull/548/commits/7edf9c7037c073cb5dc0f69557d4cdeb313bbdd0
Komposter43,2023-08-17 16:40:56+00:00,comment,garage-bAInd/Platypus2-70B-instruct,Peft algorithm of merging https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py#L554
sabo-gu,2023-08-22 08:10:59+00:00,comment,garage-bAInd/Platypus2-70B-instruct,This comment has been hidden
arielnlee,2023-08-10 04:24:45+00:00,comment,garage-bAInd/Platypus2-70B-instruct,Much appreciated!
Yhyu13,2023-05-07 12:59:11+00:00,comment,MBZUAI/LaMini-GPT-1.5B,"Try AutoGPTQ, so that you don't have to write your own GPTQ implementation"
cgato,2024-03-03 07:32:05+00:00,comment,cgato/Thespis-CurtainCall-7b-v0.3,"Absolutely nothing, I couldn't think of a name. :D"
cgato,2024-03-03 07:36:51+00:00,comment,cgato/Thespis-CurtainCall-7b-v0.3,I'll fix the name and description.
dariog,2024-05-11 17:24:14+00:00,comment,HPAI-BSC/Llama3-Aloe-8B-Alpha,"That's great! Thanks.
Stay tuned as we will soon update Aloe Alpha 8B model to use the new tokenizer."
fakezeta,2024-05-12 19:56:44+00:00,comment,HPAI-BSC/Llama3-Aloe-8B-Alpha,I'll hold my breath!
nunonmg,2024-01-30 15:41:59+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"No problem!
We SFTed the model given a max sequence length of 2048 tokens. 

It is possible that the model can perform beyond that sequence length (up to 4096 tokens), but we haven't really performed any tests in that regard."
nedrad88s,2024-01-30 15:43:46+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,Thanks for the quick response!
nunonmg,2024-01-30 10:49:52+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Cheers! We have noticed the same results internally: on what comes to neural metrics both models are very competitive -- we see a slight edge for TowerInstruct -- but on lexical metrics (BLEU, chrF) there is a huge gap in performance between the two models (favourable for Tower)."
vince62s,2024-02-26 11:04:39+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"oops.
Just realized this, TowerBlocks includes WMT14-21 testsets ....

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/4A217H663fjC5L22anl5U.png)
"
nunonmg,2024-02-26 11:25:11+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Yes! Both ALMA-R and Tower include previous WMT test sets. The best dataset to compare to is WMT23. 
We will release the paper very soon with all those numbers there."
vince62s,2024-02-26 11:51:25+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"when I look deeper into TowerBlocks I don't see any of these WMT testsets, please explain in what fields subset I should look.

and btw for alma, in the original paper it says: ""The training parallel data is sourced from the WMTâ€™17 to WMTâ€™20. T"""
nunonmg,2024-02-26 13:24:39+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Alright, thanks for the clarification: ALMA sources from 2017 to 2020. We use WMT data from 2014 to 2022. 
We did not use the full test sets --- we selected only a few samples from each test set with high-quality translations. In TowerBlocks, they are under ""general_mt_clean"". We also released all translation records and their sources here: https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.1-MT-records"
vince62s,2024-02-26 14:27:48+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,IMO you need to be specific in the model card saying you trained on this dataset because you only mention towerblocks not towerblocks-mt-records
nunonmg,2024-02-26 14:41:39+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"To clarify, TowerBlocks includes TowerBlocks-MT-records. We only created the latter because some practitioners asked us to create a dataset composed exclusively by the MT records in TowerBlocks."
vince62s,2024-02-26 15:18:17+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"okay, then back to my initial question.
in TowerBlocks, if I select task=""machine translation"" and ""split=train"" I am getting only the following ""datasets"": news21_docs_filtered, opus_doc_filtered, ted_talks_doc_filtered
so wmt14-22 are included where ?"
nunonmg,2024-02-26 15:21:29+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Oh I see now! That can indeed be a bit confusing -- we will make that clearer in the model card.
We have used data from training and development (or testing) sets in TowerBlocks --- the split refers to that (the origin of the data, and not how we used it to build TowerInstruct). To get access to all the sentence-level MT data, disregard the ""split"" column, and only select the task."
nunonmg,2024-01-26 09:34:43+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Hey there,
The added tokens are there for flexibility if you want to fine-tune it for some specific use-case (e.g., MASK, CLS tokens). We only explicitly used at all times during the SFT the <|im_end|> (redefined as the eos token) and the <|im_start|> tokens.
They are learned at finetuning time."
DuarteMRAlves,2024-01-22 18:13:38+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Hi,

Thank you for noticing!
I have fixed the generation config and tested it. It should work as expected.

Can you check on your side if it works now (you may need to redownload it)?"
JaimeLugo,2024-01-23 17:02:20+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Hey Duarte and bpop, thanks for the comments... I am traveling but will share with you my findings in three days. Will download the generation config and test it.  "
setmiddle,2024-01-28 00:31:56+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"trying as is I have ""torch.cuda.OutOfMemoryError: CUDA out of memory""
then deside to quantize in my docker container:

  tgi-towerinstruct-gpu:
    image: ghcr.io/huggingface/text-generation-inference:1.4
    command: --model-id Unbabel/TowerInstruct-7B-v0.1 --quantize eetq --num-shard 1 --max-batch-prefill-tokens 512 --max-input-length 512
    volumes:
      - ./models:/data
    ports:
      - 8102:80
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]


but the quantize process return:

2024-01-28 00:11:40 2024-01-28T00:11:40.004314Z  INFO shard-manager: text_generation_launcher: Waiting for shard to be ready... rank=0
2024-01-28 00:11:49 2024-01-28T00:11:49.839464Z  INFO text_generation_launcher: Server started at unix:///tmp/text-generation-server-0
2024-01-28 00:11:49 
2024-01-28 00:11:49 2024-01-28T00:11:49.925051Z  INFO shard-manager: text_generation_launcher: Shard ready in 811.549244253s rank=0
2024-01-28 00:11:50 2024-01-28T00:11:50.021987Z  INFO text_generation_launcher: Starting Webserver
2024-01-28 00:11:50 2024-01-28T00:11:50.053023Z  INFO text_generation_router: router/src/main.rs:181: Using the Hugging Face API
2024-01-28 00:11:50 2024-01-28T00:11:50.053123Z  INFO hf_hub: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/hf-hub-0.3.2/src/lib.rs:55: Token file not found ""/root/.cache/huggingface/token""    
2024-01-28 00:11:50 2024-01-28T00:11:50.237200Z  WARN tokenizers::tokenizer::serialization: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokenizers-0.14.1/src/tokenizer/serialization.rs:159: Warning: Token '<CLS>' was expected to have ID '32000' but was given ID 'None'    
2024-01-28 00:11:50 2024-01-28T00:11:50.237383Z  WARN tokenizers::tokenizer::serialization: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokenizers-0.14.1/src/tokenizer/serialization.rs:159: Warning: Token '<SEP>' was expected to have ID '32001' but was given ID 'None'    
2024-01-28 00:11:50 2024-01-28T00:11:50.237391Z  WARN tokenizers::tokenizer::serialization: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokenizers-0.14.1/src/tokenizer/serialization.rs:159: Warning: Token '<EOD>' was expected to have ID '32002' but was given ID 'None'    
2024-01-28 00:11:50 2024-01-28T00:11:50.237398Z  WARN tokenizers::tokenizer::serialization: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokenizers-0.14.1/src/tokenizer/serialization.rs:159: Warning: Token '<MASK>' was expected to have ID '32003' but was given ID 'None'    
2024-01-28 00:11:50 2024-01-28T00:11:50.237402Z  WARN tokenizers::tokenizer::serialization: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokenizers-0.14.1/src/tokenizer/serialization.rs:159: Warning: Token '<PAD>' was expected to have ID '32004' but was given ID 'None'    
2024-01-28 00:11:50 2024-01-28T00:11:50.237406Z  WARN tokenizers::tokenizer::serialization: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokenizers-0.14.1/src/tokenizer/serialization.rs:159: Warning: Token '<|im_end|>' was expected to have ID '32005' but was given ID 'None'    
2024-01-28 00:11:50 2024-01-28T00:11:50.237410Z  WARN tokenizers::tokenizer::serialization: /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/tokenizers-0.14.1/src/tokenizer/serialization.rs:159: Warning: Token '<|im_start|>' was expected to have ID '32006' but was given ID 'None'    
2024-01-28 00:11:50 2024-01-28T00:11:50.476490Z  INFO text_generation_router: router/src/main.rs:420: Serving revision 3ce0db8d1e9a39ec8999e57798f28c41218dc077 of model Unbabel/TowerInstruct-7B-v0.1
2024-01-28 00:11:50 2024-01-28T00:11:50.476604Z  INFO text_generation_router: router/src/main.rs:237: Using the Hugging Face API to retrieve tokenizer config
2024-01-28 00:11:50 2024-01-28T00:11:50.529632Z  INFO text_generation_router: router/src/main.rs:280: Warming up model
2024-01-28 00:11:54 2024-01-28T00:11:54.039357Z  INFO text_generation_router: router/src/main.rs:316: Setting max batch total tokens to 2048
2024-01-28 00:11:54 2024-01-28T00:11:54.039449Z  INFO text_generation_router: router/src/main.rs:317: Connected
2024-01-28 00:11:54 2024-01-28T00:11:54.039466Z  WARN text_generation_router: router/src/main.rs:322: Invalid hostname, defaulting to 0.0.0.0"
nunonmg,2024-01-22 11:23:55+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Hi JaimeLugo,
Could you please send an example of ""{chstory}"" so we can try to replicate the error in our end?
It is possible that there might be a slight prompt mismatch, i.e., you may not be using the optimal prompt (we have a list of possible prompts for MT in https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.1 (just filter the task for machine_translation and you'll find all types of prompts there --- there are many to choose from). Our evaluation was conducted using the prompt in the model card."
DuarteMRAlves,2024-01-23 16:01:16+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Hi JaimeLugo,

Adding to the previous answer, we noticed generation_config.json for the model had the incorrect eos_token_id (only the tokenizer had the correct one), causing it to not stop the generation. It should have been fixed, you just need to redownload the model.

Look here for more info: https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1/discussions/4."
JaimeLugo,2024-01-24 11:04:43+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Hey DuarteMRAlves

I confirm the consistency has improved significantly... but still errors happen. For example, it once gave me this response ""20: What? What the actual fuck?!'<|im_end|>"" so it didn't translate at all, and I gave 1500 tokens as input...

Here is my prompt and chinfo as a reference:

      conversation = [  {'role': 'user', 'content': f'Translate the input text into Spanish: \n Input Text: {chstory}'}] 
      prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)

      inputs = tokenizer(prompt, return_tensors=""pt"").to(model.device) 
      outputs = model.generate(**inputs, use_cache=True, pad_token_id=tokenizer.eos_token_id, max_length=4000  , top_k=25, prompt_lookup_num_tokens=10,  do_sample=True)
      output_text = tokenizer.decode(outputs[0]) 
  
      ""ChInfo"": ""0 | Chapter 2: Slave Caravan\n1 | Sunny dreamt of a mountain.\n2 | Jagged and lonesome, it dwarfed other peaks of the mountain chain, cutting the night sky with its sharp edges. A radiant moon bathed its slopes in the ghostly, pale light.\n3 | On one of the slopes, the remnants of an old road stubbornly clung to the rocks. Here and there, weathered paved stones could be seen through the snow. To the right side of the road, a sheer cliff face rose as an impregnable wall. To the left, a silent black sea of nothingness indicated an endless fall. Strong winds crashed into the mountain over and over again, screaming in powerless rage.\n4 | Suddenly, the moon fell over the horizon. The sun rose from the west, streaked across the sky and disappeared in the east. Snowflakes jumped from the ground and returned into the embrace of clouds. Sunny realized that he was seeing the flow of time in reverse.\n5 | In an instant, hundreds of years flew by. The snow retreated, baring the old road. Cold shivers ran down Sunny's back as he noticed human bones littering the ground. A moment later, the bones were gone, and in their place, a slave caravan appeared, moving backwards down the mountain in the clamor of chains.\n6 | Time slowed, stopped, and then resumed its usual pace.\n7 | [Aspirant! Welcome to the Nightmare Spell. Prepare for your First Trialâ€¦]\n8 | Whatâ€¦ what the hell is this?'\n9 | Step. Step. Another step.\n10 | A dull ache was radiating through Sunny's bleeding feet as he was shivering from cold. His threadbare tunic was nearly useless against the biting wind. His wrists were the main source of agony: badly hurt by the iron shackles, they sent a sharp pang of pain every time the freezing metal touched his broken skin.\n11 | What kind of a situation is this?!'\n12 | Sunny looked up and down, noticing a long chain winding up the road, with dozens and dozens of hollow-eyed people â€” slaves just like him â€” shackled to it at small intervals. Ahead of him, a man with broad shoulders and a bloodied back was walking with a measured gait. Behind him, a shifty-looking guy with quick, desperate eyes was quietly cursing under his breath in a language that Sunny did not know, but somehow still understood. From time to time, armed horsemen in ancient-style armor would pass by, giving the slaves menacing looks.\n13 | However you judged it, things were really bad.\n14 | Sunny was more bewildered than panicked. True, these circumstances were not like what the First Nightmares were supposed to be. Usually, freshly chosen aspirants would find themselves in a scenario that presented them with a fair amount of agency: they would become members of privileged or warrior casts, with plenty of access to necessary weapons to at least try to tackle any conflict.\n15 | Starting out as a powerless slave, shackled and already half-dead, was as far from being ideal as one could imagine.\n16 | However, the Spell was as much about challenge as it was about balance. As the old policeman said, it created trials, not executions. So Sunny was pretty sure that, to counter this abysmal start, it would reward him with something good. A powerful Aspect, at least.\n17 | Let's seeâ€¦ how do I do this?'""
  "
nunonmg,2024-01-30 15:43:53+00:00,comment,Unbabel/TowerInstruct-7B-v0.1,"Hi Jaime,
Thanks for raising this issue! We also noted that performance on doc-level translation (i.e. with multiple paragraph or new-line separators) seems to be inconsistent and results vary depending on prompts (some prompt engineering is helpful here). TowerInstruct was really only trained to deal with paragraph-level MT.

We are currently working on a new version of TowerInstruct that will be able to deal better with such documents. Stay tuned!"
jondurbin,2023-08-11 06:17:01+00:00,comment,jondurbin/airoboros-33b-gpt4-2.0,"Indeed.  I made no claim that 2.0 or m2.0 would be better than 1.4; from the model card:
""The 2.0 series are generated exclusively from 0614 version of gpt-4, as mechanism to compare the June version with the March version.""

The 2.0 dataset was made mostly to test whether or not gpt-4 had performance degradation, and it seems perhaps it did.  I posted some evaluation of the output here:
https://www.reddit.com/r/LocalLLaMA/comments/15i53h3/airoboros_20m20_releaseanalysis/"
aetherwu,2023-08-18 09:05:32+00:00,comment,jondurbin/airoboros-33b-gpt4-2.0,"Now they are both suggested by 
Another LLM Roleplay Rankings
https://rentry.co/ALLMRR
airoboros-l2-13b-gpt4-2.0-GPTQ <- for chatting.
airoboros-l2-13b-gpt4-m2.0-GPTQ <- for RP."
leaderboard-pr-bot,2024-03-04 12:11:53+00:00,comment,KnutJaegersberg/falcon-1b-t-sft,"This is an automated PR created with https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr

The purpose of this PR is to add evaluation results from the Open LLM Leaderboard to your model card.

If you encounter any issues, please report them to https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr/discussions"
mlabonne,2024-03-25 10:58:47+00:00,comment,mlabonne/Beyonder-4x7B-v3,"Hi @ancv, thanks! Yes, this should be possible. Fine-tuning should definitely help too, although it might be more cost-efficient to fine-tune your experts instead."
mlabonne,2024-03-25 11:56:27+00:00,comment,mlabonne/Beyonder-4x7B-v3,"Thanks! I don't think it's feasible for me, unfortunately. It would take several days with an A100 and 80 GB of VRAM (see https://github.com/Vahe1994/AQLM?tab=readme-ov-file#quantization-time). I'd be happy if anyone has the compute to do it though. ðŸ‘€"
AiModelsMarket,2024-03-25 12:08:28+00:00,comment,mlabonne/Beyonder-4x7B-v3,"Thank you for your answer . It is ok :( , I understand , I don't have either a supercomputer at my disposal . By the way , just found a few days ago about the AQLM quantization method and now I found one that is supposed to be even better : QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models https://huggingface.co/papers/2310.16795  . Is it possible that you can try this one on the great model of yourself ...(the reason is to be more affordable to run on lower performance hardware ) ? Thank you again ! "
bartowski,2024-03-23 19:51:01+00:00,comment,mlabonne/Beyonder-4x7B-v3,
bartowski,2024-03-23 19:52:15+00:00,comment,mlabonne/Beyonder-4x7B-v3,"Not sure if you'd also want to add my link for GGUF since it contains a few more sizes, but here it is so you can check out and decide:

https://huggingface.co/bartowski/Beyonder-4x7B-v3-GGUF"
mlabonne,2024-03-24 00:16:33+00:00,comment,mlabonne/Beyonder-4x7B-v3,"Thanks @bartowski, adding both!"
Undi95,2023-12-15 18:10:28+00:00,comment,Undi95/Mixtral-8x7B-MoE-RP-Story,"Last update of Ooba, Koboldcpp and Llamacpp support Mixtral model, you actually don't have anything to do beside updating your tools"
Undi95,2023-12-15 03:08:10+00:00,comment,Undi95/Mixtral-8x7B-MoE-RP-Story,"Thanks for the feedback! Yeah it's still experimental stuff, be we done a long way, happy to see you like it tho"
Undi95,2023-12-15 03:07:39+00:00,comment,Undi95/Mixtral-8x7B-MoE-RP-Story,"Thank you for the feedback, appreciate it."
HoangHa,2023-12-15 03:24:01+00:00,comment,Undi95/Mixtral-8x7B-MoE-RP-Story,Thank you for all of your work. If I want to do one how can I do it? 
Undi95,2023-12-15 03:39:13+00:00,comment,Undi95/Mixtral-8x7B-MoE-RP-Story,"Script is not public yet, we mess with it before the author release it.
Will probably go into his github tho : https://github.com/cg123"
HoangHa,2023-12-15 03:47:47+00:00,comment,Undi95/Mixtral-8x7B-MoE-RP-Story,"Wow, thank you. You guys are absolute genius"
robert1968,2024-01-20 03:01:59+00:00,comment,mlabonne/NeuralMarcoro14-7B,This comment has been hidden
mlabonne,2024-01-18 10:57:20+00:00,comment,mlabonne/NeuralMarcoro14-7B,Thanks! This must be due to issues with the tokenizer that is not properly used. Could you send an example to help me debug it?
mlabonne,2024-01-10 10:19:35+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Hi, thanks for your interest! Can you provide an example of a model with a tokenizer_config.json that works with vllm, so I can copy it? "
Giskard,2024-01-10 20:40:03+00:00,comment,mlabonne/NeuralMarcoro14-7B,"You can check your own:

https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B/blob/364dcb71549e6988899ba581cd707d3392e96c1c/tokenizer_config.json#L48

I think it cames from here:
https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B#prompt-format

and then:

https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/91ed666be78da7556f3d79abbb26fff0ee26cb54/tokenizer_config.json#L52"
mlabonne,2024-01-10 21:00:33+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Thanks, done!"
FlorianJc,2024-01-10 23:11:07+00:00,comment,mlabonne/NeuralMarcoro14-7B,"I don't work in my case with vllm.
The model seems to not generate the eos token (<\/s>) even if I manually add it in the prompt template.

As I'm mainly focused on deployment development maybe I'm wrong, but isn't the eos token should be added in the training data right before <|im_end|>  ?

https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs


"
mlabonne,2024-01-10 23:35:32+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Hi, I can look into the issue when I have more time but <|im_end|> is the EOS token. See my code in Phixtral Chat:

* https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L24
* https://huggingface.co/spaces/mlabonne/phixtral-chat/blob/main/app.py#L58"
FlorianJc,2024-01-11 01:01:18+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Ok, so the problem is that vllm use the tokenizer of transformers and the stop condition is when the last generated token match tokenizer.eos_token_id (id=2)

And <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416, 28766, 28767 ]

vLLM support a non OpenAI standard variable in its requests (stop_token_ids) and if I assign the previous list, the model stop generating almost good.

It's not really good because the stop condition is catched when only one token id is matched and not the full list, so all theses tokens become prohibited separately.
If you're curious, you can look at the _check_stop function at vllm/engine/llm_engine.py

Plus the value of ""<"" is not always 523 depending on the previous character.

So in my opinion, the stop token should always be <\/s> (which can be encoded in only one and non ambigous token btw).

P.S: Je suis franÃ§ais et je sais pas si mes explications sont claires, je peux re-expliquer si besoin ;)
Encore une fois je connais mal l'apprentissage mais je connais bien vLLM."
Liangmingxin,2024-01-11 07:24:18+00:00,comment,mlabonne/NeuralMarcoro14-7B,"> Ok, so the problem is that vllm use the tokenizer of transformers and the stop condition is when the last generated token match tokenizer.eos_token_id (id=2)
> 
> And <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416, 28766, 28767 ]
> 
> vLLM support a non OpenAI standard variable in its requests (stop_token_ids) and if I assign the previous list, the model stop generating almost good.
> 
> It's not really good because the stop condition is catched when only one token id is matched and not the full list, so all theses tokens become prohibited separately.
> If you're curious, you can look at the _check_stop function at vllm/engine/llm_engine.py
> 
> Plus the value of ""<"" is not always 523 depending on the previous character.
> 
> So in my opinion, the stop token should always be <\/s> (which can be encoded in only one and non ambigous token btw).
> 
> P.S: Je suis franÃ§ais et je sais pas si mes explications sont claires, je peux re-expliquer si besoin ;)
> Encore une fois je connais mal l'apprentissage mais je connais bien vLLM.

æˆ‘æ˜¯ä¸­å›½äººï¼Œè‹±è¯­ä¹Ÿä¸æ˜¯å¾ˆå¥½ï¼Œå“ˆå“ˆå“ˆ"
Liangmingxin,2024-01-11 07:36:23+00:00,comment,mlabonne/NeuralMarcoro14-7B,"> Ok, so the problem is that vllm use the tokenizer of transformers and the stop condition is when the last generated token match tokenizer.eos_token_id (id=2)
> 
> And <|im_end|> is encoded as [ 523, 28766, 321, 28730, 416, 28766, 28767 ]
> 
> vLLM support a non OpenAI standard variable in its requests (stop_token_ids) and if I assign the previous list, the model stop generating almost good.
> 
> It's not really good because the stop condition is catched when only one token id is matched and not the full list, so all theses tokens become prohibited separately.
> If you're curious, you can look at the _check_stop function at vllm/engine/llm_engine.py
> 
> Plus the value of ""<"" is not always 523 depending on the previous character.
> 
> So in my opinion, the stop token should always be <\/s> (which can be encoded in only one and non ambigous token btw).
> 
> P.S: Je suis franÃ§ais et je sais pas si mes explications sont claires, je peux re-expliquer si besoin ;)
> Encore une fois je connais mal l'apprentissage mais je connais bien vLLM.

When I use vllm to derive this model, even after updating the chat_template, it still often appears that the model doesn't stop outputting (Chinese Q&A) after answering the question reasonably well, resulting in a lot of redundant INSTRUCTIONS, etc., but this problem doesn't occur when using the GGUF page provided by the respondent, so I'm a bit confused. I initially thought it was a chat_template issue, and after switching -- chat_template to use vllm-project/vllm/blob/main/examples/template_alpaca.jinja the problem got a little bit better but the quality of the answers went down. Maybe it's not a problem with chat_template, but the model uses a lot of non-uniform data formats, causing it to have an unstable answer format? What do you think about that? Do you have any further suggestions about vllm deploying this model? Guidance is much appreciated!"
Liangmingxin,2024-01-11 08:54:32+00:00,comment,mlabonne/NeuralMarcoro14-7B,"In order to better debug and resolve this issue, I deployed a login-free page for testing purposes only.
Hardware: RTX 2080ti
Inference framework: vllm v0.2.7
Deployment Instructions:
```
python ./vllm/vllm/entrypoints/openai/api_server.py \
--model './NeuralMarcoro14-7B' \
--tokenizer './NeuralMarcoro14-7B' \
--tokenizer-mode auto \
--dtype float16 \
--enforce-eager \
--tensor-parallel-size 2 \
--trust-remote-code
```

I set some default parameters:
```
""use_beam_search"": true,
  ""temperature"": 0.7
  ""stop_token_ids"": [2],
  ""skip_special_tokens"": true,
  ""add_generation_prompt"": true,
  ""min_p"": 0.05
```

https://fast.connectwithgpt.com/chat/share?shareId=04k2p7on33osfd8jj8of8nm4
This link is valid until 2024-01-13 00:00 CST, please do not enter personal or confidential information."
FlorianJc,2024-01-11 13:52:29+00:00,comment,mlabonne/NeuralMarcoro14-7B,"I noticed something strange.
NeuralHermes training seems to use the same dataset:
https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B

But NeuralHermes perform well with vLLM.
Maybe something changed in the training script @mlabonne ?
"
Liangmingxin,2024-01-11 15:32:10+00:00,comment,mlabonne/NeuralMarcoro14-7B,"> I noticed something strange.
> NeuralHermes training seems to use the same dataset:
> https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B
> 
> But NeuralHermes perform well with vLLM.
> Maybe something changed in the training script @mlabonne ?

![image.png](https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/ZnvVPaOHu9beZAdXUqZQj.png)
Maybe I need to set <|im_end|> exactly instead of ""stop_token_ids"": [2], (aka </s>)? @FlorianJc 
I just found NeuralMarcoro14-7B/tokenizer_config.json
```
""clean_up_tokenization_spaces"": false,
  ""eos_token"":""</s>"",
  ""legacy"": true,
```
And NeuralHermes-2.5-Mistral-7B/tokenizer_config.json is
```
""clean_up_tokenization_spaces"": false,
  ""eos_token"":""<|im_end|>"",
  ""legacy"": true.
```
Hahaha, maybe that's the answer? I'll try!"
Liangmingxin,2024-01-11 15:55:29+00:00,comment,mlabonne/NeuralMarcoro14-7B,"I put NeuralMarcoro14-7B/tokenizer_config.json
```
""clean_up_tokenization_spaces"": false,
  ""eos_token"":""</s>"",
  ""legacy"": true,
```
Changed to be the same as NeuralHermes-2.5-Mistral-7B/tokenizer_config.json
```
""clean_up_tokenization_spaces"": false,
  ""eos_token"":""<|im_end|>"",
  ""legacy"": true.
```
But it doesn't work, and the model doesn't break the output until it reaches the max_tokens... Sad.
![image.png](https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/xEMzZkcozMnpuZ3JJz4Py.png)
"
Liangmingxin,2024-01-11 16:09:24+00:00,comment,mlabonne/NeuralMarcoro14-7B,"I changed ""eos_token"":""<|im_end|>"", back to ""eos_token"":""</s>"", it seems to be a bit better, but still occasionally outputs garbled text, why does the answerer's gguf formatted model doesn't output so much garble, but the vllm deployed one does?"
mlabonne,2024-01-11 21:42:46+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Sorry, I have no idea. I don't know how vllm handles that. I would also try to copy the entire config from NeuralHermes but if that doesn't work either... :("
Liangmingxin,2024-01-11 23:36:11+00:00,comment,mlabonne/NeuralMarcoro14-7B,"> Sorry, I have no idea. I don't know how vllm handles that. I would also try to copy the entire config from NeuralHermes but if that doesn't work either... :(

After a full night of debugging and modifying vllm's deployment parameters, I think it's much better! There are no strange special tokens now, but it still occasionally outputs garbled code when it encounters rare issues, but it's not too much of a problem anymore. I've posted my vllm deployment below for your reference @mlabonne 
```
python ./vllm/vllm/entrypoints/openai/api_server.py \
--model './NeuralMarcoro14-7B' \
--tokenizer './NeuralMarcoro14-7B' \
--tokenizer-mode auto \
--dtype float16 \
--enforce-eager \
--tensor-parallel-size 2 \
--host 0.0.0.0 \
--port xxxx \
--trust-remote-code \
--disable-log-stats \
--disable-log-requests
```
Within vllm I set some default parameters
```
    default_min_p = 0.05
    default_use_beam_search = True
    default_ignore_eos = False
    default_skip_special_tokens = True
```
Then tokenizer_config.json I copied https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B, It's mostly unchanged, but I've changed here ""eos_token"": ""< /s>""
```
{
  ""add_bos_token"": true,
  ""add_eos_token"": false,
  ""added_tokens_decoder"": {
    ""0"": {
      ""content"": ""<unk>"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""1"": {
      ""content"": ""<s>"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""2"": {
      ""content"": ""</s>"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""32000"": {
      ""content"": ""<|im_end|>"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""32001"": {
      ""content"": ""<|im_start|>"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    }
  },
  ""additional_special_tokens"": [],
  ""bos_token"": ""<s>"",
  ""chat_template"": ""{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"",
  ""clean_up_tokenization_spaces"": false,
  ""eos_token"": ""</s>"",
  ""legacy"": true,
  ""model_max_length"": 1000000000000000019884624838656,
  ""pad_token"": null,
  ""sp_model_kwargs"": {},
  ""spaces_between_special_tokens"": false,
  ""tokenizer_class"": ""LlamaTokenizer"",
  ""trust_remote_code"": false,
  ""unk_token"": ""<unk>"",
  ""use_default_system_prompt"": true,
  ""use_fast"": true
}
```
Really like this model of yours! You're invited to try it as I deployed it with vllm (link is still the one above)."
mlabonne,2024-01-12 00:20:48+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Excellent, thank you for providing all these details! It'll be useful as a reference in the future."
Giskard,2024-01-13 17:35:17+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Maybe this is something similar to what was commented in here in the secind parqgraph?

https://huggingface.co/cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser#training

"
mlabonne,2024-01-08 23:14:57+00:00,comment,mlabonne/NeuralMarcoro14-7B,Hahaha thanks @CultriX! :)
CultriX,2024-01-08 23:40:23+00:00,comment,mlabonne/NeuralMarcoro14-7B,"I do wonder though: it seems like yours (whilst performing good overall, let there be no doubts about that) does see the steepest increase in performance in the GSM8K benchmark.

And as somebody rightfully pointed out on my model page: The intel neural chat data includes GSM8k, which is also part of the leaderboard test.

As you know im really new to all of this so I am actually not quite sure how big of a difference this would make and how much it would influence 
- 1. Benchmarking results

(and more importantly:)

- 2. How that would translate to actual model performance versus expected performance based on the benchmarking results.

Could you chime in on that? 
Would it make a substantial difference in either results or in relationship to actual model performance in a real scenario?"
mlabonne,2024-01-08 23:59:04+00:00,comment,mlabonne/NeuralMarcoro14-7B,"Isn't it data from the training split of GSM8k? I don't think that the neural chat data is contaminated (but I might be wrong). If it's really test data, it makes the dataset absolutely useless :(

I don't completely rely on the Open LLM Leaderboard and I use another benchmark suite (with https://github.com/mlabonne/llm-autoeval) for this purpose. It doesn't include GSM8k."
cloudyu,2024-01-24 05:52:01+00:00,comment,cloudyu/Mixtral_7Bx4_MOE_24B,"training steps. 
please use Pluto_24B_DPO_200, and its metrics already there."
IkariDev,2023-10-15 15:52:13+00:00,comment,IkariDev/Athena-v3,pretty good
IkariDev,2023-09-29 04:31:59+00:00,comment,IkariDev/Athena-v3,"Thank you so much for your feedback, i really appriciate it! I'll send you the answers to all that when im home again(should be in around 6 hours).

\- IkariDev :)"
IkariDev,2023-09-29 11:34:47+00:00,comment,IkariDev/Athena-v3,"1. ill put recommended settings in the repo in a couple of hours, try what works best for you.
2. i always suggest using the ""roleplay"" advanced formatting preset, + instruct same, and enabled.
3. Not really, but i am mainly active on the dc servers thebloke(character-roleplay-stories) and sillytavern(local-llm-chat).

I would be happy to see you over there. Thanks again for your feedback!"
elarvee,2023-09-29 13:52:18+00:00,comment,IkariDev/Athena-v3,Thank you for your answers! I'll take a look at those Discord servers. Cheers!
snombler,2023-09-28 22:45:19+00:00,comment,IkariDev/Athena-v3,"I've posted some logs, along with notes, over here if you want to read through them:
https://snombler.neocities.org/logs#athenav3

I check my e-mail more than I check HF messages, so apologies if you say anything here and it takes me a bit to reply.

EDIT: Also, feel free to close this whenever.  Just wanted to congratulate you guys."
Undi95,2023-09-28 22:59:29+00:00,comment,IkariDev/Athena-v3,"Appreciated dude, thank you!"
IkariDev,2023-09-29 04:33:34+00:00,comment,IkariDev/Athena-v3,"hey, thanks to the both of you! I'll talk more later with you guys when im home(should be in like 6 hours)

\- IkariDev :)"
IkariDev,2023-09-29 11:49:15+00:00,comment,IkariDev/Athena-v3,"@snombler hey, thanks for the logs+rating. can i put that on my HF?"
snombler,2023-09-29 12:49:38+00:00,comment,IkariDev/Athena-v3,"@IkariDev 
Yeah, throw it up wherever you want.

I've done a bit more testing with more extreme stuff and there is still a mild positivity/alignment that injects itself into more intense use cases. I won't go into specifics here but it is present. So while the model is compliant, anyone hoping to use it for that sort of content will need to keep an eye on things and Author's Note or context edit. Something to be aware of, it exists in pretty much every model I've ever tested to some extent. It's just hard to get away from the warm fuzzy stuff, it seems. That ""they knew they could work through it together and find common ground"" type stuff as stories roll toward their conclusions, especially. Again, it's in just about everything, but I'm mentioning it for sake of thoroughness in case it's an angle you guys are thinking of tackling.

It does an admirable job when extended to 16k context as well, getting general facts right about something from the beginning of an RP with ~8500 tokens, though it did MINORLY mess up the order of the actions. Not something that would ruin an RP at any rate as even I had to go back and read the specifics to be sure. I haven't tested it thoroughly on that front yet, but I imagine I'll get there. :D"
ktthkim,2023-11-28 01:16:45+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,"@supark 
Could you share your test code and the  transformers version?"
supark,2023-11-29 04:48:46+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,"@ktthkim 

Below are the specific versions for testing configuration:

`Python 3.10`
`Transformers==4.33.2`

The test code itself is lengthy, but it is a standard codebase that you can easily find on the web."
ktthkim,2023-11-29 05:50:25+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,"@supark 
I tested the sample in the readme using transformers==4.33.2, and it worked well for me.
Please make sure to check trust_remote_code=True when using from_pretrained"
supark,2023-11-29 07:21:42+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,"@ktthkim 
There is no issue with loading the model and tokenizer as you mentioned.
However, when trying to fine-tune(QLoRa) using the `Trainer.train()` provided by `Transformer`, the error mentioned above occurs."
ktthkim,2023-11-21 01:41:53+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,LGTM
beomi,2023-11-15 02:07:09+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,"# Test code - works fine!

You can test this PR version using `revision='refs/pr/9'`.

Here's the test code below:

```python
Python 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.17.2 -- An enhanced Interactive Python. Type '?' for help.

In [1]: from transformers import AutoModelForCausalLM
i
In [2]: import torch

In [3]: model = AutoModelForCausalLM.from_pretrained('KT-AI/midm-bitext-S-7B-inst-v1', revision='refs/pr/9', trust_remote_code=True, torch_dtype=torch.bfloat16, device_map={'':0})
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.43s/it]

In [4]: model.save_pretrained('test')

In [5]: 
```"
ktthkim,2023-11-21 01:41:16+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,"Thank you for submitting your PR and conducting the test. We've reviewed your PR internally, and it looks good.
 Additionally, we've updated the SafeSensor format checkpoint."
ktthkim,2023-11-07 08:00:21+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,ì¢‹ì€ ì˜ê²¬ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.  [#7](https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1/discussions/7) [#8](https://huggingface.co/KT-AI/midm-bitext-S-7B-inst-v1/discussions/8)
ktthkim,2023-10-31 11:59:36+00:00,comment,KT-AI/midm-bitext-S-7B-inst-v1,Thank you for your pull request!
leaderboard-pr-bot,2024-03-31 20:53:07+00:00,comment,NeverSleep/Noromaid-7b-v0.2,"This is an automated PR created with https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr

The purpose of this PR is to add evaluation results from the Open LLM Leaderboard to your model card.

If you encounter any issues, please report them to https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr/discussions"
Undi95,2023-12-22 14:22:55+00:00,comment,NeverSleep/Noromaid-7b-v0.2,"Maybe later, we work on a potential 8x7b mixtral one atm. "
Undi95,2023-10-26 15:51:21+00:00,comment,Undi95/MLewd-L2-Chat-13B,"Hello!
There is no dataset, it's a merge of multiple LoRA/Model

- [Undi95/MLewd-L2-13B-Part3](https://huggingface.co/Undi95/MLewd-L2-13B-Part3) (checkpoint of MLewd without LORA)
- [posicube/Llama2-chat-AYT-13B](https://huggingface.co/posicube/Llama2-chat-AYT-13B)
- [zattio770/120-Days-of-LORA-v2-13B](https://huggingface.co/zattio770/120-Days-of-LORA-v2-13B)
- [royallab/Pygmalion-2-13b-SuperCOT](https://huggingface.co/royallab/Pygmalion-2-13b-SuperCOT)
- [Undi95/MMSoul-13b-lora](https://huggingface.co/Undi95/MMSoul-13b-lora)

For a starting point you could check :
- Pygmalion datasets
- 120-Days-of-LORA-v2-13B
- SuperCOT
- All the model/dataset used in https://huggingface.co/The-Face-Of-Goonery/Huginn-13b-FP16

Sorry!

Edit: I gave you access to the private model [Undi95/ReMM-S-Light](https://huggingface.co/Undi95/ReMM-S-Light) and [Undi95/MLewd-L2-13B-Part3](https://huggingface.co/Undi95/MLewd-L2-13B-Part3), the second one was done with an old version of mergekit, all model used to do it are public (outside ReMM-S-Light), here is the recipe:

python main.py ./MLewd-L2-13B-Part1 --cuda --method slerp --base-model Undi95/ReMM-S-Light --merge Undi95/CreativityEngine --weight 0.25
=> MLewd-L2-13B-Part1 (0.75 ReMM / 0.25 CreativeEngine)

python main.py ./MLewd-L2-13B-Part2 --cuda --method slerp --base-model Undi95/MLewd-L2-13B-Part1 --merge Sao10K/Stheno-Inverted-L2-13B --weight 0.20
=> MLewd-L2-13B-Part2 (0.64 ReMM / 0.16 CreateEngine / 0.25 Stheno-Inverted)

python main.py ./MLewd-L2-13B-Part3 --cuda --method slerp --base-model Undi95/MLewd-L2-13B-Part2 --merge The-Face-Of-Goonery/Huginn-v3-13b --weight 0.15
=> MLewd-L2-13B-Part3 (0.544 ReMM / 0.136 CreateEngine / 0.2125 Stheno-Inverted / 0.15 Huginn-v3)"
Sao10K,2023-12-21 04:03:50+00:00,comment,Sao10K/Frostwind-10.7B-v1,"Hi, what exactly do you mean for the learning code?

If it is hyperparameters, this is it:

qLoRA finetune.
2 Epochs -  Batch size of 1 on 4x 4090s.
Learning Rate - 0.00035
Scheduler - Cosine
Optimizer - adamw_bnb_8bit
LoRA rank - 16 | LoRA alpha - 32
lora_target_modules - [  - gate_proj  - down_proj  - up_proj  - q_proj - v_proj  - k_proj  - o_proj]

For the dataset itself, that is private. Number of samples would be based on the model card, so 10% is 5.2K, total 52K. I listed the composition there.
"
Sao10K,2023-12-20 13:09:27+00:00,comment,Sao10K/Frostwind-10.7B-v1,"It should be the default Alpaca format, I trained with that only.

Maybe alpaca-extended format could work, most models are pretty flexible nowadays."
Sao10K,2023-09-13 05:23:29+00:00,comment,Sao10K/Euryale-L2-70B,"Hi, sure. Here they are:

Digitous's SLERP:
https://github.com/Digitous/LLM-SLERP-Merge

Chargoddard's Mergekit (was ties-merge previously):
https://github.com/cg123/mergekit

Gryphe's Gradient Merge (Mythomax Creator):
https://github.com/Gryphe/BlockMerge_Gradient

I did several merges in different orders. Ties-Merge allows for multiple models to be merged at once while the other 2 are strictly 2 models at once.

"
