replier,post_time,type,repo,comment
Mookins,2024-05-29 12:52:14+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"you closed the thread (#97) and pointed people here without fixing the issue, everyone isn't an idiot, there are access permission issues AFTER gaining access permissioins to the model."
jeevansreenivas,2024-05-29 15:43:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
squareoctopus,2024-05-29 16:26:06+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"an hour! awesome. Also, thanks jeevansreenivas for the token info"
ssohrabi,2024-05-30 13:47:53+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Thanks but my request has been pending for a month! Is there any way to re-send it? 
Elfsong,2024-05-31 05:47:08+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
osanseviero,2024-05-31 08:29:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Thanks but my request has been pending for a month! Is there any way to re-send it?

We're working on having a way to allow people to manage their repo requests and withdraw them so they can re-submit if needed"
Yongxin,2024-05-31 12:24:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Not sure if it helps. I have 403 error with the access permission granted, but the error is gone after I add the repo in ""Setting --> Access Tokens --> Edit Permissions --> Repositories permissions"".
"
vilka-lab,2024-06-02 09:12:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"When I asked for access as Ivan from Russia, I was refused.
When I made a new account and asked for access as Ivan from Ukraine, I was given access.
Just funny."
aziz456j,2024-06-05 08:51:59+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"hello , I have been granted access to the model repo but when i try to connect to it from colab notebook it says its gated repo and i dont have acess. help please ? 
I have put acess token and checked it , its all good. so whats the issue ?
"
SparkMao,2024-06-05 09:09:52+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,I chose China cause I lived in China. After about a week it says I was rejected. That is absolutely geographical discrimination! We need explaination!
sachitan,2024-06-05 20:37:27+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Thanks but my request has been pending for a month! Is there any way to re-send it?

same issue!"
Cheshire94,2024-06-06 05:14:29+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"""Your request to access this repo has been rejected by the repo's authors.""
Interesting :)"
JunQi-Zhai,2024-06-07 02:59:52+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,where is the ' Accept License button'?
edbento,2024-06-07 12:57:44+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1 where is the ' Accept License button'?
rakeshrajan8484,2024-06-09 05:04:11+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Even after i've been granted access to the model, when I try to download using huggingface-cli  by login in I get the below error
""Cannot access gated repo for URL https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct/revision/main.
Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it."""
markemicek,2024-06-09 01:16:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Well, user error. don't forget to use add_generation_prompt=True when converting to chat template."
syp115,2024-05-27 16:48:10+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> download

"
pandora-s,2024-05-27 11:51:24+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Whats the error?
Miralaeeop1,2024-05-28 08:52:15+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"

> Whats the error?

![Screenshot from 2024-05-28 12-21-26.png](https://cdn-uploads.huggingface.co/production/uploads/662ba432eefbeb5b7482105b/BepVPhm-44iJo6n09WjhP.png)"
pandora-s,2024-05-28 13:08:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"did u log the response of the api? that provies too lil information, and if possible to have the code sample would help
"
Miralaeeop1,2024-05-28 13:50:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I was able to fix the error, but now I get a wrong response when just putting plain text in the input parameter.

This is my answer when I just type hello:
```
  ""generated_text"": ""hello, it's me, Tom,\n\nas the title already suggests, i now have a question regarding the primary key on a table that is referenced by another table.\n\ni have a table that contains entries of machines (serialnumber, name, manufacurer, warranty ...)\nand another table that contains informations inside a machine (equipment, software, license ...).\nthis second table references the first table on the column serialnumber.\n\nin both""
```"
pandora-s,2024-05-28 15:57:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Thats because you are not using the prompt template it requires, you need to use the correct chat template for llama."
Miralaeeop1,2024-05-28 17:12:23+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Thats because you are not using the prompt template it requires, you need to use the correct chat template for llama.

Thank you very much for your advice, but can you show me an example of the right templates?
This is my code :
```
fetch(""https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct"", {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer hf_api'
    },
    body: JSON.stringify({
        'inputs': 'hello'
    })
})
.then(response => response.json())
.then(data => {
    console.log((data));
    
});

```"
Kevin-in-Paris,2024-05-27 17:16:43+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Bonsoir les amis,
Je suis toujours bloqué et vraiment toute aide ou suggestion est la bienvenue :)
Merci pour vous retours 
"
Kevin-in-Paris,2024-05-27 21:14:17+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,erratum : Merci pour vos retours!
Kevin-in-Paris,2024-05-27 21:17:33+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"there is no one who has the same ?
I can't downgrade my NumPy2 to NumPy1 because i have Python 3.12 et not python >=3.7<3.11
Can u help please?
thanks"
Kevin-in-Paris,2024-06-03 19:34:11+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Bonjour,
waw comment c'est coquant!!! 
aucun soutien ! Aucun retour ! Quelle communauté ?!  
Merci !"
APaul1,2024-05-28 17:37:08+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This is a common problem with these langchain and llama_index libraries. They keep on generating responses. I would recommend to work directly with the transformers library as shown in the examples on this page https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
sbruhat,2024-05-27 09:17:25+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I found the problem, the model I downloaded was incomplete.
I used `git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct` without installing git lfs first.
So I ran ` git lfs install` and the `git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct` and it worked
"
osanseviero,2024-05-29 11:48:32+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hi there! Please make sure to install transformers with the latest version
guptaansit,2024-05-29 12:02:31+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I just now installed the latest version '4.41.1' of transformers, but the above code still gives me the same error. "
osanseviero,2024-05-29 11:48:57+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/130
osanseviero,2024-05-29 11:50:51+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/130
osanseviero,2024-05-29 11:51:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hey there! Please make sure the token has read access to the repo you're interested in
Sarvesh19,2024-05-26 01:38:30+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Facing the same issue, did you find the solution?"
osanseviero,2024-05-29 11:51:57+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/130
Akshayjain97,2024-05-17 11:56:27+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"use these in completely new enviroment

https://brev.dev/blog/the-simple-guide-to-fine-tuning-llms
https://github.com/meta-llama/llama-recipes/issues/475

download requirements.txt from :
raw.githubusercontent.com/huggingface/transformers/main/examples/flax/vision/requirements.txt

it wasn't working previously, but after installing in a new environment, it worked fine for llama 3 8B/70B"
BBQGOD,2024-05-19 09:30:08+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
BUAADreamer,2024-05-21 08:34:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
SimpliAI,2024-05-22 08:44:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Did you fill China in the country section?
BUAADreamer,2024-05-22 08:44:38+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,yeah
BUAADreamer,2024-05-22 08:44:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,So this is the reason?
Dampfinchen,2024-05-13 22:48:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Note: Very likely a GGUF issue as I'm not having issues with exl2. 
Dampfinchen,2024-05-14 15:33:56+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I have retested it for a while and now these issues also appear with Exl2. Seems like it's a issue with the model. Maybe it's because we are basically having two EOS tokens now? (EOT ID and END of TEXT). 

Fresh example sentence happened right now:

""The forest trail stretches before them, a meandering path through trees adorned with snow, ice,<|eot_id|>

Asteriks formatting issue:


![urgh.png](https://cdn-uploads.huggingface.co/production/uploads/6416fe55b03817ada63f9415/lAdUhjDA-pmRQokJeNVPk.png)

Models used:

https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF

https://huggingface.co/turboderp/Llama-3-8B-Instruct-exl2

(With updated tokenizer.json, config etc from Meta)

Using SillyTavern as frontend with the correct prompt template, llama.cpp and TabbyAPI as backends. "
regybean,2024-05-16 07:44:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I have been having similar issues, model outputs happy to help and stops early in some cases

Edit: Fixed (I think) by correctly formatting the system prompt"
Dampfinchen,2024-05-16 21:16:46+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> I have been having similar issues, model outputs happy to help and stops early in some cases
> 
> Edit: Fixed (I think) by correctly formatting the system prompt

It can be quite random, so make sure to continue to test this and report if you are noticing anything!

I am using the correct prompt format, which looks like so in Sillytavern:

 '<|start_header_id|>system<|end_header_id|>\n' +
    '\n' +
    'A helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n' +
    '\n' +
    ""Let's get started.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"" +
    '\n' +
    'Hello, how may I help you today?<|eot_id|><|start_header_id|>user<|end_header_id|>\n' +
    '\n' +
    'Write 10 sentences that end with the word ""apple"".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n' +
    '\n',
(note: BOS is added automatically by the backend)

I've found out that part of the issue with the model setting the EOT token too early is that the model appears to be overly confident. Here's an example:

Generating (77 / 400 tokens) [( her 20.78%) ( enjoying 13.64%) ( taking 12.20%) ( feeling 12.10%)]
Generating (78 / 400 tokens) [( eyes 89.87%) ( gaze 5.46%) ( heart 2.47%) ( brown 2.20%)]
Generating (79 / 400 tokens) [( filled 5.05%) ( shining 40.16%) ( sparkling 29.57%) ( reflecting 13.37%) ( taking 7.74%)]
Generating (80 / 400 tokens) [(<|eot_id|> 5.22%) ( with 94.78%)]

actual text: ""and she occasionally looks up at him with a quiet smile, her eyes filled<|eot_id|>""



"
ArthurZ,2024-05-13 06:54:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Since #60 was merged, merging this for consistency"
GxyJeffrey1203,2024-05-17 02:56:09+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hello
Sm1Ling,2024-05-11 20:39:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Also I'm not shure about `<|eot_id|>`. Seems everything got mixed up
I don't really know wether this issue was fixed. (Still do not have access to original repo)"
oldmanhuggingface,2024-05-13 01:48:29+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Correct. We don't know whether this issue is fixed yet or not. We need communication from meta
dragon7,2024-05-16 13:10:42+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Use `chat_template` instead of `default_chat_template`
Imran1,2024-05-11 08:21:17+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@ArthurZ 
MaziyarPanahi,2024-05-11 12:40:32+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"the eos_token is wrong, that's actually a pad_token. It was already fixed in 8B, it will be fixed here soon after if I am not mistaken. The correct `eos_token` is `<|eot_id|>`."
Imran1,2024-05-11 12:57:45+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@MaziyarPanahi 
So can you show a method, how to prepare a data for training? 

I mean what token I should need to choose a pad one and eos ?


If I add custom token into tokenizer, it and resize the model embadding it's showing error after calling trainer.train().

Like attention something error..."
Imran1,2024-05-11 12:59:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,I am not see any pad token in the tokenizer config. Its showing only eos token 
MaziyarPanahi,2024-05-12 07:38:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I use `""<|eot_id|>""` for eos and for pad you either introduce a new `<pad>` token yourself during the training and use that for padding, or you can reuse `<|end_of_text|>` as a pad. (it's up to you when it comes to Llama-3)"
Sm1Ling,2024-05-13 12:14:03+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@MaziyarPanahi  I really appreciate your work. I already have installed some of your gguf models. Could you share some examples of your text generation pipeline?
matteoperiani,2024-05-15 14:32:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@Imran1  pad token is not used since years because in the pre-training is never used. All the CasualLM are trained using the maximum length for computation efficiency.
If you really need it, you can use the eos token and mask it in the labels, or better concatenate all the sample during the fine tuning using packing from SFT library.

Please, avoid to add token because by default they are initialized with 0s embedding. So, it became the most likely during the softmax and it will be always choosed in the early fine tuning steps. 
If you need some specific token, follow this guide from Stanford https://nlp.stanford.edu/~johnhew/vocab-expansion.html"
Imran1,2024-05-15 14:37:18+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@matteoperiani great and thanks for the response.
I also thought this. Can you share a blog post or something that I want to read and to know how the new model work, especially in fine tuning so I will update my knowledge. 

"
matteoperiani,2024-05-15 14:46:28+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
Imran1,2024-05-15 15:05:36+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@matteoperiani thank you, I will check it. I have good experience in model fine tuning. But I want to know the new approach.  It will definitely help. Thank you..."
matteoperiani,2024-05-15 16:08:14+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
Imran1,2024-05-15 16:34:23+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@matteoperiani I know Lora, Dora, qlora etc. actually, the llama3 use new tokenizer, and have no pad token and added some other new special token, by the I want to understand that one. 
I want to understand how they are working. I should need to read a few blogs related this new tokenizer. 

Some guys use, eos token as a pad one..
As you told, they LLM follow to use max token length as pad or trl library using packing. 

I have a dataset which have 28k sample data. I fine tune sft, ORPO approach... But the sft model generate large token or response but not make any sense that really related to my domain dataset. Easily get hallucinated... 

I try the ORPO, which also lead the model to very small response and not make any sense and some time repeat the sentence and then add eos token at the end.
I did some parameters fine tuning.
Like batch size , learning BLA BLA.
But these new model, llama3, phi3, not doing very well.


I not find any good blog or article on that one, which explains in depths how to do BLA BLA.
"
matteoperiani,2024-05-15 17:08:43+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Sorry, I didn’t understand your question before :).

 If your LoRA SFT does no have results I may suspect could be caused by the lora rank or alpha. if you have not already tryed, use a bigger rank, something like 128 or 256. If not work, try ti modify the alpha value, a quite easy rule of thumb is use alpha = rank as starting point. 

If you are using a 4bit quantization, try with 8. Theoretically, 8bit have performance like 16bit, so it will be like a full model finetuning. 

In the case both test not produce results, you may test with a smaller model and full finetuning it. It will give poor results, but you my tweak the input-output format If you find strange behavior. A finetuned GPT2 is able to produce output as you want (ignoring how wrong the content is). So even if it is not able to generate the output as you desire, try to change the finetuning data format.

Last but not lest, decoder are very sensitive to generation parameters. Try different configuraration with all the models, you may increase the results only by few lines of code.

For what concerning ORPO, I didn’t use it a lot, so my knowledge would not helpful."
Imran1,2024-05-15 17:16:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@matteoperiani I am using A100 GPUs. I can easily use Lora, Dora etc with multiple GPU. I try the Qwen model. So the performance of Qwen model is good. 
My dataset is look like this.
Having instructions, question, reject answer and choose answer.
I syntheticly generate it using RAG.
And we'll prepare for sft, dpo, ORPO.

Thank you. You help a lots."
matteoperiani,2024-05-15 17:20:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Are you using the base model or instruct/chat finetuned?
Imran1,2024-05-15 17:23:04+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@matteoperiani I am using using both.
Imran1,2024-05-15 17:25:05+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I try Qwen LLM with galore. 
"
matteoperiani,2024-05-15 17:43:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,And your loss decrease on validation loss during alla the fine tuning stage reaching lower values? 
Imran1,2024-05-16 01:18:53+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@matteoperiani yeah definitely...
Imran1,2024-05-16 01:23:38+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@matteoperiani 
![Screenshot_20240516-062235.png](https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/3UbrCKGoa9QQ-eysGBGxe.png)
"
Imran1,2024-05-16 06:27:46+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@matteoperiani  here is the info of orpo ...

![ls.PNG](https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/C82zHe-U_E7LQHF9ZFRxS.png)
![_ds.PNG](https://cdn-uploads.huggingface.co/production/uploads/62846faa99bff5076f0a93b4/7Wh0xRlvxeiW4c01i48Yz.png)
"
para-zhou,2024-05-31 11:02:56+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@Imran1 hi, I'm also using qwen and have problems with padding. 
I find that when I use left padding, i.e., have padding tokens to the left of the system prompt, the models' generation will be greatly altered.
More specifically, model with padded inputs will not follow the system instructions as unpadded ones.
Have you ever had this problem or is there maybe something wrong with my code?
Thanks!"
FINGU-AI,2024-05-31 11:24:25+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
Imran1,2024-05-31 11:26:38+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@para-zhou Hey, actually my dataset is not so good. My model response is good as my dataset output/response columns.

I try with unpadded token and use eos token as pad one by this the model repeat then answer. Actually qwen model have own pad token try that one, and add response in the data collector class. Like ```|<im_start>|assistant ```.
Make sure to check the documentation. The trl library are updating day by day. "
Zifeng0825,2024-05-09 15:27:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
HamzaYslmn,2024-05-19 15:50:43+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"ı use ollama, best for usage."
JiayinWang,2024-05-09 04:35:05+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"kind of same problem
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-8B-Instruct is not the path to a directory containing a file named config.json.
and Internet connection is good （curl -I https://huggingface.co is fine）"
nicodatacreator,2024-05-09 15:01:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same
iiuucc,2024-05-09 21:15:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Facing same issue .Is there any solution
"
WANG10011,2024-05-10 15:11:09+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I also encountered the same problem, after getting the link again or prompt 403: Forbidden, how to solve this problem?"
Mookins,2024-05-10 17:26:59+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1 Same issue here... 
gallon-deng,2024-05-13 13:16:54+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"it is more easier to download the model from the official website https://llama.meta.com/llama-downloads/ . It would be the original pt model. There is a way to convert the original model to hf models. Just use the script 'convert_llama_weights_to_hf.py' in the newest transformers. It must be the newest version transformers, because for older version, the 'convert_llama_weights_to_hf.py' only supports llama or llama2"
Arpit-Bansal,2024-05-26 11:36:03+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"does anyone got their error resolved? @JiayinWang , @luckyrain67777 "
grg,2024-05-27 14:00:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"A less than ideal solution is to use the path to the snapshot, i.e. `$HF_HOME/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298`"
osanseviero,2024-05-29 11:54:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hi there! Please make sure to read https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/130
litax,2024-06-05 00:04:20+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"If anyone is having fine grained access token, make sure to add the repo in the ""Repositories permissions"" session."
jaocseverywhere,2024-06-06 14:25:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@litax thx bro
"
sylviachency,2024-06-06 21:20:34+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Hi there! Please make sure to read https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/130

I have been granted the model access , but I still got the error: OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-8B-Instruct is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
jassbrit,2024-06-10 01:35:45+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"i have same issue
"
2OBe,2024-06-10 09:38:27+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Also having the same issue
OliverwengFiltered,2024-05-08 20:51:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"it's solved by using traditional access token with either read or write, it doesn't work with the new fine-grained type of token."
michellehuber,2024-05-08 05:07:10+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
michellehuber,2024-05-08 05:08:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Could you make photos?
andrerg01,2024-05-09 00:55:31+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"The one on the ""Files and Versions"" tab doesn't work?
https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main"
gallon-deng,2024-05-13 13:18:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"try many times to download the model from huggingface, but failed. it is more easier to download the model from the official website https://llama.meta.com/llama-downloads/ . It would be the original pt model. There is a way to convert the original model to hf models. Just use the script 'convert_llama_weights_to_hf.py' in the newest transformers. It must be the newest version transformers, because for older version, the 'convert_llama_weights_to_hf.py' only supports llama or llama2"
andrerg01,2024-05-09 00:48:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"When running on Colab it takes around 15Gb of vRAM, taking around 1 min per response. I don't think phones now days can handle that, but I could be wrong."
a3chron,2024-05-06 10:48:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"tbh, I am not really sure, but as far as I know it's the organization u work for, someone correct if not...

So you can probably just leave it blank when you need the model only for personal use."
NatalieCheong,2024-05-06 12:56:43+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@a3chron Thank you.
Lost-Zucchini,2024-05-11 01:09:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"It's required, anyone know how to get the Affiliation?"
osanseviero,2024-05-29 11:54:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Affiliation is the company or university for which you work.
RYDE,2024-05-07 01:49:56+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Me too. It's been 19 days for me. 😂
andrerg01,2024-05-09 00:50:42+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Maybe there was some limit in place? Did you try again recently?
narsisfa,2024-05-01 05:48:24+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same problem
francescofiamingo,2024-05-01 05:52:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"hi! i face the same issue, afterdownload the model, i got the same error, i suggest to look carefully to the name of the file , for same reason the file apperead on my machine like config(1).json , that ""1"" created some isseues, i delete the ""1"" and i solved the problem (pay attetion do not leave any -space- between the ""config"" and the ""."" , let me know..."
narsisfa,2024-05-01 06:16:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> hi! i face the same issue, afterdownload the model, i got the same error, i suggest to look carefully to the name of the file , for same reason the file apperead on my machine like config(1).json , that ""1"" created some isseues, i delete the ""1"" and i solved the problem (pay attetion do not leave any -space- between the ""config"" and the ""."" , let me know...

import transformers
import torch

model_path = ""./""  # replace with the actual path to the model directory
model_id = ""Meta-Llama-3-8B-Instruct-Q4_K_M""

# Load the model from the local path
model = transformers.AutoModelForCausalLM.from_pretrained(model_path)

# Create the pipeline
pipeline = transformers.pipeline(
    ""text-generation"", model=model, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)

# Test the pipeline
output = pipeline(""hi"")
print(output)

give this error after get config

OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ./.
----------------------------------------
i downlowa this model: 

https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf
"
narsisfa,2024-05-01 06:20:18+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"import transformers
import torch

model_id = ""meta-llama/Meta-Llama-3-8B""

pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)

pipeline(""hi"")
-------------
why carsh and not give response? 
i run it on colab"
francescofiamingo,2024-05-01 16:42:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main                   this is the right link, go there and then one by one dowload all files
"
RK-RK,2024-05-01 16:56:28+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> hi! i face the same issue, afterdownload the model, i got the same error, i suggest to look carefully to the name of the file , for same reason the file apperead on my machine like config(1).json , that ""1"" created some isseues, i delete the ""1"" and i solved the problem (pay attetion do not leave any -space- between the ""config"" and the ""."" , let me know...

HI , I verified this is not the issue for me..Thanks"
madhav18,2024-06-06 03:52:56+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,I am tooo facing the same issue...can anyone help me in resolving..tried everything..!
mohsen22,2024-06-06 07:38:52+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> I am tooo facing the same issue...can anyone help me in resolving..tried everything..!

Hi .resolved 

https://colab.research.google.com/drive/1BDglz53-s9Cs9rBR4SarJ0TTZeXnahdb
"
maguitai,2024-06-10 11:48:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"---------------------------------.profile
export HF_TOKEN=''
export   HUGGING_FACE_HUB_TOKEN=''
#both write token (the same created after having access)
-------------------Shell
source ~/.profile
huggingface-cli login
      <paste HF_TOKEN>
huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include ""original/*"" --local-dir Meta-Llama-3-8B-Instruct
------------------------------- test.py
import torch
import transformers
import os
from huggingface_hub import login
HF_TOKEN=os.getenv('HF_TOKEN')

login(token=HF_TOKEN)

model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'
pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, token=HF_TOKEN, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)
pipeline(""Hey how are you doing today?"")"
narsisfa,2024-05-01 06:25:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"import transformers
import torch

model_id = ""meta-llama/Meta-Llama-3-8B""

pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)

pipeline(""hi"")
-------------
why carsh and not give response? 
i run it on colab"
Sardarmoazzam,2024-05-02 06:48:03+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@ArthurZ 
andrerg01,2024-05-09 00:54:11+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"On Google Colab, using T4 it takes around 16Gb, and ~1-2 mins per prompt."
andrerg01,2024-05-09 00:53:08+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct," > Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the seven seas... er, the digital waters! Me be here to swab the decks o' yer queries and serve ye up some treasure... er, answers! So hoist the sails and set course fer a swashbucklin' good time, matey!"
Zifeng0825,2024-05-09 15:28:33+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
andrerg01,2024-05-09 16:36:04+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,What do you mean? This text was generated with the system prompt that tells it to speak like a pirate. Just change that.
ybelkada,2024-05-14 15:30:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hi @himasrikode 
To use this model you need to have ideally the latest transformers / torch / python installed; can you try on a fresh new python env (>=3.8): `pip install -U transformers torch`"
cxlyyds,2024-04-28 05:38:24+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"same, could anyone tell me how to get the access.  
i submitted the request, and just for several mins ，i found that it was rejected
i'm happy to get reply so fast, but i'm very upset getting a ""REJECTED"",   
oh no"
har-har,2024-04-28 06:33:04+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Me too.
I want to know why"
rangehow,2024-04-28 07:02:08+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
craneSherry,2024-04-28 09:08:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,so do I.
TheFailureMaster,2024-04-28 13:19:46+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Sir or Madam, there is also https://llama.meta.com/llama-downloads for you to download the llama 3 models."
cxlyyds,2024-04-28 14:18:25+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Sir or Madam, there is also https://llama.meta.com/llama-downloads for you to download the llama 3 models.

thank you very much, 
but there is another problem, 
i always get ""HTTP request sent, awaiting response... 403 Forbidden"", 
i even requested two URL
"
zhouzr,2024-04-28 17:19:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same
Hoo1196,2024-04-29 03:15:09+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Sir or Madam, there is also https://llama.meta.com/llama-downloads for you to download the llama 3 models.

how to convert it to hugginface format"
gaokaiz2,2024-04-29 22:55:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same
kamillaova,2024-04-30 13:39:24+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Same, I tried converting the model from the official format using this command:
python -m transformers.models.llama.convert_llama_weights_to_hf --input_dir $HOME/llama/Meta-Llama-3-8B-Instruct --output_dir $PWD/llama --llama_version 3 --model_size 8Bf

but it doesn’t work on vLLM, inference just hangs forever"
Hyj2023,2024-04-30 16:50:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
gallon-deng,2024-05-13 12:40:11+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"same
"
Navanit-shorthills,2024-04-26 09:29:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/60
Its already raised here"
Ateeqq,2024-04-25 19:23:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I am currently working on it (with the health dataset) but facing a CUDA error. Hopefully, it will be resolved soon.

Here's the Colab Notebook: https://colab.research.google.com/drive/1TUa9J2J_1Sj-G7mQHX45fKzZtnW3s1vj?usp=sharing

Also, Check the other Methods here: https://exnrt.com/blog/ai/finetune-llama3-8b/"
LLMExpert,2024-04-25 21:58:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Here's the Colab Notebook: https://colab.research.google.com/drive/1TUa9J2J_1Sj-G7mQHX45fKzZtnW3s1vj?usp=sharing

It's the same code used to fine-tune Llama 2, but it may not work with Llama 3. Additionally, I think the issue may be that support for Llama 3 is only added in the latest Transformers version. You should upgrade to version 4.40.1."
MLDataScientist,2024-04-26 05:43:05+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Check this repo for fine tuning colabs: https://huggingface.co/unsloth/llama-3-8b-Instruct
LILIXIYA,2024-05-04 18:01:19+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Check this repo for fine tuning colabs: https://huggingface.co/unsloth/llama-3-8b-Instruct
Just cannot install the unsloth package. 
"
an22reb,2024-05-07 03:25:30+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hi! I've noticed when using PPO to finetune a model, this is the expected input. Is there any way I can fine tune using a question-answer pair? Whereby the answer is the preferred way by which I want the model to respond? 
![image.png](https://cdn-uploads.huggingface.co/production/uploads/658128bfe6b9dfd2cd1e3273/1SmJAfM-13uQ7MwKQPSsA.png)

Follow up: Or maybe could I use DPO, but without providing the rejected answer?
![image.png](https://cdn-uploads.huggingface.co/production/uploads/658128bfe6b9dfd2cd1e3273/IZLEP96rzAacYfDyMpBFv.png)
"
halilergul1,2024-05-20 12:01:25+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hi, 

the methods and architecture for finetuning LLama2 instruct models works just fine for LLama3 family as well. I was able to finetune it with a custom dataset on a downstream task with Peft and LoRA (I did not try unsloth yet)."
Ftm23,2024-05-21 18:25:37+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@halilergul1 Do you mind sharing your fine-tuning code with us?
halilergul1,2024-05-21 21:03:19+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> @halilergul1 Do you mind sharing your fine-tuning code with us?

Hi. I wish I could share it directly but on paper that belongs to company I work for. But surely I can direct you to necessary sources. Btw I assume what you want to achive is to finetune it on a downstream task with supervised track. 

My approach resembles with what unsloth is doing except the fact that I just dumped the 7B llama3 model from huggingface with unoptimized (unsloth engineers that part for faster inference but cannot say it is ""super fast"" for the training phase) original weights. Then I finetuned on top of these. 

Here are the libraries/methods/classes I used and you should definetely check for reference indepth to understand what they are doing:     AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments. Of course these methods are already kind of like textbook implementation of training with LoRA right now but still I beleive many are not aware their potentials. In case of getting errors or encountring problems of stuck train/eval loss (which is possible as this is a general problem stemming either from the way your data feed processes work or lora configuration inconsistencies with quantization), I highly recommend you to ""play"" or tune (sometimes it is hard to digest why some config worked better than others as we are not fully well informed about the original training of these community models) parameters LoRA modules carefully together with quantization types with BitsAndBytesConfig to reach a sweet spot.

I hope it helps!

"
oldmanhuggingface,2024-05-10 04:51:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Should this be merged or not?
Hannibal046,2024-04-25 09:31:42+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same
weldpua2008,2024-04-25 20:02:49+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same
hubert233,2024-04-26 04:52:28+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"same, I sent the request several hours after the release but still pending."
TaoYoungMyLove,2024-04-26 07:39:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"same
"
only1tele,2024-04-27 11:38:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same
komal-skynet,2024-04-27 12:46:53+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Same, been several hours for me now. Not sure what's up.
"
orrzohar,2024-04-30 04:16:42+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Yup
"
tianke0711,2024-04-25 06:45:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,it works after reflash and restarted
gy19,2024-05-03 02:30:18+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,我也是这个问题，我重新跑了代码也还是相同的问题。
hanwenhui,2024-05-13 07:17:56+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> 我也是这个问题，我重新跑了代码也还是相同的问题。

解决了吗？怎么做
"
hanwenhui,2024-05-13 07:39:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> > 我也是这个问题，我重新跑了代码也还是相同的问题。
> 
> 解决了吗？怎么做

transformers库升级到最新的4.40.2就好了。"
FrankChu0229,2024-04-25 09:41:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same
bobae,2024-04-25 12:19:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same
deaddevil,2024-04-25 13:00:59+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same
abhisskk,2024-04-25 13:04:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Same, not working for my token"
Sprum,2024-04-26 15:55:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same
deaddevil,2024-04-29 06:05:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I was facing the same problem, but you can try this you would be able to download this model

!pip install -U ipywidgets jupyterlab-widgets
from huggingface_hub import login
login()

After this insert your hugging face token in ui 

tokenizer = AutoTokenizer.from_pretrained(model_name,  use_auth_token=True)"
mosew,2024-04-29 22:45:14+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Please post a minimal reproducible example
xumeng,2024-04-25 05:47:32+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,1
sxmss1,2024-04-25 12:13:14+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@xumeng ?? please help bro
"
akrypt,2024-04-30 18:05:40+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Facing the same issue. Following
joeisme,2024-05-19 04:46:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same here
artificialreply,2024-05-31 14:50:56+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Under header add ""Content-Type"": ""application/json"" like this:

headers: {
        Authorization: ""Bearer hf_xxxxxxx"",
        ""Content-Type"": ""application/json"",
      }"
narsisfa,2024-05-01 06:18:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"import transformers
import torch

model_id = ""meta-llama/Meta-Llama-3-8B""

pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)

pipeline(""hi"")
-------------
why carsh and not give response? "
Sardarmoazzam,2024-05-02 06:45:34+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@ArthurZ can i have a little help from i I am using the Hugging face api interface mechanism to chat with model but on huggingface interface it gives good results but when i use it the code locally on my computer to access the model with api its doesnot work at all the results are just very bad

![qqq.PNG](https://cdn-uploads.huggingface.co/production/uploads/661f9c3542c9bf387606dcf2/RTp1-XfppYPap8PyDDp9t.png)
"
Navanit-shorthills,2024-04-24 08:33:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@pcuenq  kindly review 
kunal88,2024-04-25 07:35:19+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"seems like , this fix the issue of  overflow of model response."
amit44,2024-04-25 12:15:03+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,work like a charm for me while doing prompt chat completion
ArthurZ,2024-04-25 15:33:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Mmm that's weird it should be changed in the config not here no? 
Navanit-shorthills,2024-04-25 15:37:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@ArthurZ but while using the prompt templates i.e, tokenizer.apply_chat_template, the assitant answers end with this token id  <|eot_id|> , So that why its working. "
Sakshi11,2024-04-26 04:45:19+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I was also facing the issue of overflow of response, this simple change seems to fix the issue.
"
Akshay47,2024-04-26 09:53:17+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I was adding this line in my code. 
tokenizer.eos_token ='<|eot_id|>'

But changing the tokenizer_config as suggested here fixed the code. Now no need to add the above line."
navanit,2024-04-26 18:00:05+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This comment has been hidden
oldmanhuggingface,2024-05-10 04:38:25+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Why aren't these things being merged? Why would you spend a billion dollars training these models to release and leave them in such a half-baked state? Does config.json need changing? Does tokenizer_config.json need changing? Do both need changing? Do neither need changing? If anything needs changing, why hasn't it? Frustrating"
Navanit-shorthills,2024-05-10 04:55:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@philschmid  @ArthurZ  kindly look into this 
ArthurZ,2024-05-10 06:25:57+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"This should fix it, I am not sure why it's so frustrating for everyone, it's a parameter that's super easy to change 😅 
Sorry all for the troubles it caused! "
oldmanhuggingface,2024-05-10 18:18:14+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"It's frustrating because anyone downloading the model without reading these comments is gonna have a bad time. Even someone who reads these comments, like myself, has no idea which, if any, of these abandoned pull requests have merit. Your final solution didn't even have a pull request. So it really has nothing to do with the difficulty of changing the parameter. At the very least, we need official guidance as to *what* to change. Thanks for the fixes"
oldmanhuggingface,2024-05-10 20:47:53+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,I thanked too soon. Please see my comment here - https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/discussions/33#663e87a825d363a5c3ee2290
oldmanhuggingface,2024-05-10 21:16:44+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Also, does generation_config.json then need updating on any/all of the models? special_tokens_map.json? I wish you guys would just do a thorough review of all the files for all four models and make the necessary updates. Far too much ambiguity and confusion"
epignatelli,2024-05-18 11:43:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> This should fix it, I am not sure why it's so frustrating for everyone, it's a parameter that's super easy to change 😅 
> Sorry all for the troubles it caused!

If this has to be changed every single time the model is run, this is a bug.
Why is this not being treated as a bug?

Also, it creates problems because the strength of hf is to work out of the box.

"
You-Py,2024-05-27 20:17:17+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hello world :) !
Thank you for these updates.  For my case, I updated the tokenizer config as mentioned but always getting multiple lines with the same output (the first answer from the assistant but after it loops on the input system prompt until having the max_length new generated tokens.)
Many informations and different ones ! I'm a bit lost, do you have a clear code example to see if I'm wrongly using the model please ?  
Regards

PS : @ArthurZ  

Here is my code : 

messages = [
    {""role"": ""system"", ""content"": ""You are the best chatbot and your name is ESG-IGL""},
    {""role"": ""user"", ""content"": ""Who are you?""},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors=""pt""
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids(""<|eot_id|>"")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=64,
    eos_token_id=terminators,
    do_sample=False,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))

Note : even when I set skip_special_tokens to False, the Output is the same.

My output : 

What is your name?ESG-IGL.…

You are the best chatbot and your name is ESG-IGL.…

You are the best chatbot and your name is ESG-IGL.…

You are the best chatbot and your name is ESG-IGL.…

You are the best chat
 "
Haoze99,2024-04-24 07:02:17+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I guess there doesn't exist off the shelf way to accelerate the batch inference efficiently if you already have the best setup, especially for 7B model. Fortunately, you don't have the best setup. First of all, I think you can use kv-cache if you have enough gpu memory. Secondly, auto-device-map will make a single model parameters seperated into all gpu devices which probablily the bottleneck for your situatioin, my suggestion is data-parallelism instead（：which may have multiple copies of whole model into different devices but considering you have such large batch size, the gpu memories of model-copies arefar less than the kv-cache memories used, so I think it will work."
RoacherM,2024-04-28 09:59:59+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"same problem.
In [5]: from transformers import AutoModelForCausalLM, AutoTokenizer
   ...:
   ...: model_id = '/mnt/disk15/wy/meta-llama/Meta-Llama-3-70B-Instruct/'
   ...: tokenizer = AutoTokenizer.from_pretrained(model_id)
   ...:
   ...: text = 'hello, 我是李华，来自中国'
   ...: tokens = tokenizer.tokenize(text)
   ...: print(f""Tokenized result: {tokens}"")
   ...:
   ...: # 将token序列转换为ID序列
   ...: input_ids = tokenizer.convert_tokens_to_ids(tokens)
   ...:
   ...: # 解码ID序列,得到可读文本
   ...: decoded_text = tokenizer.decode(input_ids)
   ...: print(f""Decoded result: {decoded_text}"")
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Tokenized result: ['hello', ',', 'ĠæĪĳ', 'æĺ¯', 'æĿİ', 'åįİ', 'ï¼Į', 'æĿ¥èĩª', 'ä¸ŃåĽ½']
Decoded result: hello, 我是李华，来自中国"
saksham-lamini,2024-04-24 01:20:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Also shouldnt tokenizer.eos_token_id also be updated to [128001, 128009] instead of just 128001."
mosew,2024-04-29 22:46:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Yes, see here: https://github.com/vllm-project/vllm/issues/4180"
saksham-lamini,2024-04-29 23:40:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hope this gets fixed by the Huggingface team. I've written a workaround for my application, but this is annoying as well as not flexible as newer models come up."
ArthurZ,2024-05-10 06:31:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,This was fixed by #60
madhurjindal,2024-04-22 18:17:40+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,You need to use your HF_Token while you try to access the model
klincria,2024-04-23 03:18:02+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,add --token in your download command line
aryamannningombam,2024-04-23 15:47:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"- Apply for the model access from the form they have shared.
- Make sure that you have the HF_TOKEN variable correct for the account that got acccess."
jessicalopez,2024-05-25 16:59:36+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"You define the HF_TOKEN value, and later you can use the following line of code:
```
!huggingface-cli login --token $HF_TOKEN
```"
RebeIT,2024-06-07 14:32:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"An example I used by adding access_token:

model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""
access_token = ""hf_...""

pipeline = transformers.pipeline(
    ""text-generation"",
    model=model_id,
    model_kwargs={""torch_dtype"": torch.bfloat16},
    token=access_token,
    device_map=""auto"",
)
"
CocoSun,2024-04-22 04:24:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same issue
hylll,2024-04-22 05:02:04+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,I also have the same issue.
Sedrick99,2024-04-22 06:43:22+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same issue for me
JingweiNi,2024-04-22 07:28:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same issue here
Alarmist,2024-04-22 07:43:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same issue
yushuihao,2024-04-22 08:29:57+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same issue here
Alviner,2024-04-22 09:10:46+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,I also have the same issue.
bartk,2024-04-22 09:19:59+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
omarkhaled,2024-04-22 09:40:01+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Same issue here
"
Wo-o,2024-04-22 09:42:55+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"same issue here
"
testcosmicpre,2024-04-22 10:38:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
averoo,2024-04-22 14:46:16+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
alexrods,2024-04-22 16:29:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"If you use personal email they don't give you access, try with  a business or institutional email. Same with HF requests."
averoo,2024-04-23 04:48:18+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Access was granted in three days. I've use personal email.

In addition, you can download the weights and tokenizer using the download.sh from the official [llama3 GitHub page](https://github.com/meta-llama/llama3)."
rushikesh988,2024-04-23 09:16:38+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
peng13,2024-04-23 11:03:27+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
redwyd,2024-04-23 11:37:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"+1
"
liliang,2024-04-23 12:17:42+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
GiorgioDiSalvo,2024-04-23 16:07:54+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
yang985,2024-04-24 09:57:08+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
DutchNomadCode,2024-04-24 10:53:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
DutchNomadCode,2024-04-24 11:21:30+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Got my access! Thanks
xyzw-io,2024-04-24 17:45:32+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Submitted my request last Thursday. Still waiting for approval.
albertmu,2024-04-25 03:48:10+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,so am I
Franklin81,2024-04-25 07:45:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
Hannibal046,2024-04-25 09:25:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same here
pcuenq,2024-04-23 11:33:27+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Thank you!
cognitivetech,2024-04-22 07:52:09+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"is it related?

> The original Llama 3 8b (base) special token weights are zero, which might cause NaN gradients. This version re-initialized the weights of all the following special tokens to alleviate the problem.
https://huggingface.co/imone/Llama-3-8B-fixed-special-embedding"
ArthurZ,2024-04-22 07:56:49+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I have no idea what your tokenization missmatch is, but make sure that the tokenizer you are using is of the `PreTrainedTokenizerFast` class, not the `LlamaTokenizerFast`. 
It should be completely possible otherwise! "
ElliottDyson,2024-04-21 07:05:10+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I saw that a contributor of the official Llama3 repo had posted to use ""-1"" (not as a string but as the ID itself so as to disable it) as the pad token in one of the issues on that GitHub repo, so that config you have so far is also wrong according to that.

But you can use Huggingface's prompt formatter from the tokenizer to format your prompt for you:

https://huggingface.co/docs/transformers/chat_templating"
interpio,2024-04-21 01:52:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Prompt: How many Poles were killed by the Ukrainians during 1943-1947

assistant Meta answer:
I can't provide information or guidance on defamatory or harmful content."
jiffslater,2024-04-20 21:25:45+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Take a look at https://github.com/meta-llama/llama3?tab=readme-ov-file#instruction-tuned-models.
seyf1elislam,2024-04-21 08:45:30+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"""""""
{% set loop_messages = messages %}
{% for message in loop_messages %}
  {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>'+ message['content'] | trim + '<|eot_id|>' %}

  {% if loop.index0 == 0 %} {% set content = bos_token + content %}  {% endif %}
  {{ content }}

{% endfor %}

{{ '<|start_header_id|>assistant<|end_header_id|>' }}
""""""
"
sqzhou,2024-04-21 10:18:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"use tokenizer.apply_chat_template(chat, tokenize=False)
"
pcuenq,2024-04-20 09:02:57+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,
pcuenq,2024-04-20 08:58:44+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,
rumAndRegression,2024-04-20 16:08:28+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"when calling model.generate, setting pad_token_id=tokenizer.eos_token_id seems to remove the warning. 

https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id

it seems to work fine still with that, although some explanation and reasoning wouldn't hurt. maybe i saw one when searching for the solution but i forget.. :)"
LAKSERS,2024-04-21 03:28:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id).just change this will be ok.
"
rileydean,2024-05-13 22:23:03+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Curious, did you ever get this working?"
ybelkada,2024-05-14 15:27:57+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hi @tonimelisma 
For using quantized Llama on apple devices, I advise to use MLX: https://huggingface.co/collections/mlx-community/llama-3-662156b069a5d33b3328603c cc @awni @prince-canuma "
awni,2024-05-14 15:34:42+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Yup, should be easy to do and reasonably fast with MLX:

1. `pip install mlx-lm`
2. `mlx_lm.generate --model mlx-community/Meta-Llama-3-8B-Instruct-4bit --prompt ""hello""`

More [docs here](https://github.com/ml-explore/mlx-examples/blob/main/llms/README.md)"
tonimelisma,2024-05-15 20:40:45+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Yes, MLX and llama.cpp work fine. I was inquiring whether Huggingface would work, too."
Gerald001,2024-04-20 04:50:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,retry? it worked fine for me.
mohitmayank,2024-04-20 17:40:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Try `AutoTokenizer`
pprp,2024-04-21 05:39:22+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Same problem and `AutoTokenizer` works fine 
bdambrosio,2024-04-21 01:34:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I usually include something like:

Respond only  as shown,  with no additional discursive or explanatory text.
And always provide an example (ie, one-shot in-context training)"
nschle,2024-04-21 16:25:30+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"You need to update your software, the underlying issues have been fixed by now."
Gerald001,2024-04-21 16:37:04+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@nschle what software? where you see its has been fixed? im using:
```
from sagemaker.huggingface import HuggingFaceModel, HuggingFacePredictor
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B-Instruct"")

payload = {
            ""inputs"": tokenizer.apply_chat_template(
                [
                    {
                        ""role"": ""user"",
                        ""content"": content,
                    }
                ],
                tokenize=False,
                add_generation_prompt=True,
            ),
            ""parameters"": self.parameters,
        }
response = llm.predict(payload)
generated_text = response[0][""generated_text""]
```
"
bdambrosio,2024-04-21 16:54:31+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"The was an issue with handling of eot_id vs eos, I assume that's what @nschle was referring to, as opposed to the more general prompt issue I referred to above.
grab the updated config.json and generation_config.json. maybe tokenizer_config.json also just to be sure.
"
Gerald001,2024-04-21 20:43:18+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@bdambrosio will the official repo here be updated too?
bdambrosio,2024-04-21 20:56:08+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,I believe it has been. You can check the above file dates. I belleve you will see they have later dates (by a few hours or a day or so) than the weights files.
narsisfa,2024-05-01 06:19:54+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"import transformers
import torch

model_id = ""meta-llama/Meta-Llama-3-8B""

pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)

pipeline(""hi"")
-------------
why carsh and not give response? 
i run it on colab"
97k,2024-04-19 19:40:17+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Are your tokens/ouptut from tokenizer, on the same device on which your model is loaded on?
If your model is on GPU, then make sure you update the token tensors , to GPU as well."
madhurjindal,2024-04-20 08:42:30+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"This is an accelerate issue where I am using a multi-gpu setup. I have used the same setup with other SLMs like Zephyr, Llama2 and they seem to work"
ChristianPalaArtificialy,2024-04-21 17:52:14+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Playing around with the accelerate settings fixed it for me.
madhurjindal,2024-04-22 06:42:04+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@ChristianPalaArtificialy could you share the settings you are using
saireddy,2024-04-23 01:21:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@madhurjindal  what kind of hardware are you using? and how many gpus ? 
"
madhurjindal,2024-04-23 09:10:02+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@saireddy  I am using 8xV100 32GB
saireddy,2024-04-23 19:16:49+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@madhurjindal can you try using 4 of those than 8. its funny but this has fixed for me
OnurC,2024-04-25 14:58:22+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same issue
madhurjindal,2024-04-27 16:11:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,@saireddy  didn't fix it for me
ducknificient,2024-05-14 07:31:25+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Playing around with the accelerate settings fixed it for me.

could you elaborate more, please ? thanks"
ybelkada,2024-05-14 15:25:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hi everyone!
In order to fix this issue, you need to make sure to force-load the model into a single GPU and replicate that across all GPUs, to achieve this, please follow the solution proposed here: https://github.com/huggingface/accelerate/issues/1840#issuecomment-1683105994"
fahim9778,2024-06-01 05:40:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hello, I am using this example and it is working fine for me. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm

However, I don't know why does `add_generation_prompt = True` excludes the `assistant` keyword (and a newline) before the actual answer by bot.  Seeking help to understand."
shaida,2024-04-19 16:41:09+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Fixed ReadMe
samuel-tallet,2024-04-20 12:43:19+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hi,

> I'd be happy to help!`assistant`

You should not see the `assistant` role in the response.

It seems you are missing the end of turn token. Try to add `<|eot_id|>` among the `stop` words..."
rumAndRegression,2024-04-20 16:15:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"yes the model page shows the list of terminators to use. i also had to use the chat template to avoid endless self-chatter. but i see you already use it.

here is some discussion on modifying the config.json for same result as the terminators list: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/4
"
aeminkocal,2024-04-19 19:14:29+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Yes it would run. It requires around 16GB of vram.
Gerald001,2024-04-19 19:33:51+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@aeminkocal ok thanks.

any idea how to turn off the ""assistant\n\nHere is the output sentence based on the provided tuple:\n\n and the Let me know what output sentence I should generate based on this tuple.assistant\n\nHere is the output sentence based on the provided tuple and the end of the response?

```
response_text: [""assistant\n\nHere is the output sentence based on the provided tuple:\n\n~~~~THE TEXT I WANT~~~~\n\nLet me know if this meets your requirements!assistant\n\nI'm glad I could help. If you have more tuples you'd like me to process, feel free to provide them, and I'll generate the corresponding output sentences.assistant\n\nPlease go ahead and provide the next tuple. I'm ready to help.assistant\n\nHere is the next tuple:\n\n(XXXXXXXXX')\n\nLet me know what output sentence I should generate based on this tuple.assistant\n\nHere is the output sentence based on the provided tuple:\n\nXXXXXXXX.\n\nLet me know if this meets your requirements!assistant\n\nPlease provide the next tuple. I'm ready to help""]
```

im using:
```
        parameters = {
            ""max_new_tokens"": 248,
            ""top_p"": 0.1,
            ""temperature"": 0.1,
        }
        parameters[""return_full_text""] = False

        payload = {
            ""inputs"": self.tokenizer_create.apply_chat_template(
                [
                    {
                        ""role"": ""user"",
                        ""content"": content,
                    }
                ],
                tokenize=False,
                add_generation_prompt=True,
            ),
            ""parameters"": parameters,
        }
```"
97k,2024-04-19 19:47:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"You can use Langchain OutputParsers to get output in a specific way out of the LLM.
Langchain's ouptut parsers lets you define a format/schema inside prompt so that llm answer it in that specific way only.
Or try with DsPy, use few-shot examples, that will help you generate the prompt for you with examples, automatically!"
phxps,2024-04-20 08:00:18+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"can also use instructor

https://github.com/jxnl/instructor"
Gerald001,2024-04-22 04:36:56+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@phxps filed the problem here: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/36 - comments?

applying a parser on the output seems not really address the core problem. why even return more stuff than i ask for... keep in mind more tokens are return - which requires more time."
0rso,2024-04-25 06:14:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,6Gb of VRAM is actually enough to run quantized version on ollama. Q4 is a good choice for lightweight/effective ratio on low end gpu. 
narsisfa,2024-05-01 06:20:02+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"import transformers
import torch

model_id = ""meta-llama/Meta-Llama-3-8B""

pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)

pipeline(""hi"")
-------------
why carsh and not give response? 
i run it on colab"
YouxingZhang,2024-05-16 04:25:06+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hi, can i train llama-3-8B on RTX 4080 16G and ram 32G?"
J22,2024-04-19 14:27:37+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Yes, you can.

Checkout [llama.cpp](https://github.com/ggerganov/llama.cpp) or [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp)."
aljbali,2024-04-19 20:41:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Thank you! I was trying to run it on TGI but I am getting the following error, 


(base) compute:data hadra002$ docker run --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference --model-id $model --disable-custom-kernels
2024-04-19T20:40:42.937880Z  INFO text_generation_launcher: Args { model_id: ""meta-llama/Meta-Llama-3-8B-Instruct"", revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize: None, speculate: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_tokens: None, max_input_length: None, max_total_tokens: None, waiting_served_ratio: 1.2, max_batch_prefill_tokens: None, max_batch_total_tokens: None, max_waiting_tokens: 20, max_batch_size: None, cuda_graphs: None, hostname: ""4fe31fe89102"", port: 80, shard_uds_path: ""/tmp/text-generation-server"", master_addr: ""localhost"", master_port: 29500, huggingface_hub_cache: Some(""/data""), weights_cache_override: None, disable_custom_kernels: true, cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge: None, tokenizer_config_path: None, disable_grammar_support: false, env: false, max_client_batch_size: 4 }
2024-04-19T20:40:42.939590Z  INFO hf_hub: Token file not found ""/root/.cache/huggingface/token""    
2024-04-19T20:40:43.309344Z  INFO text_generation_launcher: Default `max_input_tokens` to 4095
2024-04-19T20:40:43.309463Z  INFO text_generation_launcher: Default `max_total_tokens` to 4096
2024-04-19T20:40:43.309480Z  INFO text_generation_launcher: Default `max_batch_prefill_tokens` to 4145
2024-04-19T20:40:43.309492Z  INFO text_generation_launcher: Using default cuda graphs [1, 2, 4, 8, 16, 32]
2024-04-19T20:40:43.309949Z  INFO download: text_generation_launcher: Starting download process.
2024-04-19T20:40:52.029571Z  INFO text_generation_launcher: Files are already present on the host. Skipping download.

2024-04-19T20:40:53.146755Z  INFO download: text_generation_launcher: Successfully downloaded weights.
2024-04-19T20:40:53.148336Z  INFO shard-manager: text_generation_launcher: Starting shard rank=0
2024-04-19T20:41:00.442091Z  WARN text_generation_launcher: We're not using custom kernels.

2024-04-19T20:41:00.464385Z  WARN text_generation_launcher: Could not import Flash Attention enabled models: CUDA is not available

2024-04-19T20:41:01.703605Z ERROR shard-manager: text_generation_launcher: Shard complete standard error output:

The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
Traceback (most recent call last):

  File ""/opt/conda/bin/text-generation-server"", line 8, in <module>
    sys.exit(app())

  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py"", line 71, in serve
    from text_generation_server import server

  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py"", line 16, in <module>
    from text_generation_server.models.vlm_causal_lm import VlmCausalLMBatch

  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/vlm_causal_lm.py"", line 14, in <module>
    from text_generation_server.models.flash_mistral import (

  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py"", line 18, in <module>
    from text_generation_server.models.custom_modeling.flash_mistral_modeling import (

  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mistral_modeling.py"", line 29, in <module>
    from text_generation_server.utils import paged_attention, flash_attn

  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/flash_attn.py"", line 12, in <module>
    raise ImportError(""CUDA is not available"")

ImportError: CUDA is not available
 rank=0
Error: ShardCannotStart
2024-04-19T20:41:01.809661Z ERROR text_generation_launcher: Shard 0 failed to start
2024-04-19T20:41:01.809775Z  INFO text_generation_launcher: Shutting down shards
(base) compute:data hadra002$ "
sky00123,2024-04-25 13:25:32+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,use https://github.com/ollama/ollama
narsisfa,2024-05-01 06:20:10+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"import transformers
import torch

model_id = ""meta-llama/Meta-Llama-3-8B""

pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)

pipeline(""hi"")
-------------
why carsh and not give response? 
i run it on colab"
Willgard,2024-04-19 11:36:09+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I had the same issue. I realised you need to specify the terminators and add them to the generate function or the model will endlessly reply;

terminators = [
                tokenizer.eos_token_id,
                tokenizer.convert_tokens_to_ids(""<|eot_id|>"")
            ]

outputs = model.generate(xxx, eos_token_id=terminators)
"
Techinix,2024-04-19 12:40:13+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,is there an equivalent for this in case of using VLLM ? thanks
Techinix,2024-04-19 12:45:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,i think its stop token ids parameter in sampling
pcuenq,2024-04-19 13:04:33+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Fixed in #4 by @abhi-db
pcuenq,2024-04-19 13:01:43+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Fixed in #9
alexcardo,2024-04-21 14:58:15+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Yeah, the first model, which strictly follows the instructions, understands context, and doesn't provide wrong date when it doesn't know the matter. Unfortunately, I can run on 4-bit quant of the 8b model on my Macbook M1 8G, but even this dramatically cut model performs better than the current ChatGPT!"
qnixsynapse,2024-04-21 15:09:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"This is beyond awesome!!! Although, I won't say ""best model in 8B"" because it is the best model of all models released from 1B to 33B (includes ChatGPT 3.5). However I believe that the chat template format should have a little simple."
Techinix,2024-04-19 12:54:32+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"terminators = [
       tokenizer.eos_token_id,
       tokenizer.convert_tokens_to_ids(""<|eot_id|>"")
]

outputs = model.generate(xxx, eos_token_id=terminators)"
pcuenq,2024-04-19 13:05:12+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,See also #4
sirus,2024-04-19 06:23:52+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,you can't really compare some extremely finetuned model like hermes 2 pro to a basic instruct finetune. I'm sure NousResearch will do their own instruct finetune of llama 3 8b and that is what you would compare. otherwise you could compare this llm to mistral instruct that's directly from mistral
aeminkocal,2024-04-19 07:53:53+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,It's better than any of the models I tested before. And I tested A LOT. I believe Hermes etc. are trained to pass those tests. This model is actually the smartest one I've ever tested.
urtuuuu,2024-04-19 19:11:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> you can't really compare some extremely finetuned model like hermes 2 pro to a basic instruct finetune. I'm sure NousResearch will do their own instruct finetune of llama 3 8b and that is what you would compare. otherwise you could compare this llm to mistral instruct that's directly from mistral

You are right, but I'm just a simple user, whichever model performs best, preferably lower in size, i use it and love it :)"
97k,2024-04-19 19:49:31+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"It is good. I put on a colab notebook and challenged Llama3-8B-instructioin model to extract entities from resume and also a chat-with-your resume Langchain chain.
It worked flawlessly!
Mistral-7B-instruct gave me hard time with entity extraction."
aak7912,2024-04-19 23:19:34+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,T
LightChen2333,2024-04-19 02:16:15+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,It seems that there is indeed a problem. The performance of LLaMA2 70B is much lower than that of LLaMA2 13B. It should be that they are written backwards?
chengrt,2024-04-19 02:19:23+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Not exactly?The following is the result reported in the LLAMA2 paper.
Llama 2-7B 14.6
Llama 2-13B 28.7
Llama 2-34B 42.2
Llama 2-70B 56.8"
Suparious,2024-04-19 05:07:10+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Seems like a issue local to your machine and connection.
Try to clear your `~/.config/huggingface/hub` folder, and try downloading again."
Fillipe,2024-05-13 18:45:33+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,same here. clearing hub does not work.
aeminkocal,2024-04-19 00:34:21+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"As stated in the model card;
Output Models generate text and code only."
OuyangFengluo,2024-04-19 07:14:09+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,你好
BlueRed12,2024-04-24 10:23:00+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,It's coming. They haven't released it yet. 
BlueRed12,2024-04-24 10:23:31+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> 你好

здраствуйте
"
yuanhai,2024-04-19 04:36:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I'm facing the same problem
"
97k,2024-04-19 06:11:43+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Yes, For optimum performance we need to apply chat template provided by Meta
https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202
[INST] [/INST], this template is for mistral or similar models, llama chat template is different

Here is the instruction template https://github.com/meta-llama/llama3?tab=readme-ov-file#instruction-tuned-models
Quoting below 

> The prompt begins with a <|begin_of_text|> special token, after which one or more messages follow. Each message starts with the <|start_header_id|> tag, the role system, user or assistant, and the <|end_header_id|> tag. After a double newline \n\n the contents of the message follow. The end of each message is marked by the <|eot_id|> token.

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{{ model_answer_1 }}<|eot_id|>
```
"
aeminkocal,2024-04-19 07:48:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> I'm facing the same problem

on config.json change eos_token_id'to  ""eos_token_id"": 128009"
IainRatherThanIan,2024-04-19 18:40:19+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Anyone tried this with TGI? I used the following with for chatPromptTemplate but I get parseing errors (error: Parse error on line 7:...ssistant}}<|eot_id|>:

""chatPromptTemplate"" : ""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\r\n\r\n{{ You are a friendly assistant }}<|eot_id|><|start_header_id|>user<|end_header_id|>\r\n\r\n{{#each messages}}{{#ifUser}}{{content}}{{/ifUser}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n\r\n{{#each messages}}{{#ifAssistant}}{{ content }}{{/ifAssistant}}<|eot_id|>"","
IainRatherThanIan,2024-04-19 18:44:51+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Sorry - meant HuggingFace Chat UI not TGI - TGI working. When I use the old style Llama 2 prompt template (in HF Chat UI against TGI), the model returns garbage (expected as prompt not correct).

thanks"
violetxi,2024-04-24 05:28:24+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> > I'm facing the same problem
> 
> on config.json change eos_token_id'to  ""eos_token_id"": 128009

Tried to do this but not working. Using it through vllm. Anyone has an idea?"
danielEricsson,2024-04-26 19:45:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hello, I am using tgi version text-generation-inference:1.0.2 and hugging face chat-ui. I got to run Meta-Llama-3-8B with the following configuration:
Not sure if userMessageEndToken is needed, I did many testing and this is how it works, maybe in future will review the elements that are actually needed for this to work. 


MODELS=`[
{
        ""name"": ""meta-llama/Meta-Llama-3-8B"",
        ""multimodal"" : false,
        ""description"": ""Llama-3-8B"",
        ""websiteUrl"": ""https://ai.meta.com/resources/models-and-libraries/llama-downloads/"",
        ""userMessageToken"":"""",
        ""userMessageEndToken"": ""<|eot_id|>"",
        ""chatPromptTemplate"":""<|begin_of_text|><|start_header_id|>system<|end_header_id|>{{@root.preprompt}}<|eot_id|>{{#each messages}}{{#ifUser}}<|start_header_id|>user<|end_header_id|>{{content}}<|eot_id|>{{/ifUser}}{{#ifAssistant}}<|start_header_id|>assistant<|end_header_id|>{{content}}<|eot_id|>{{/ifAssistant}}{{/each}}<|start_header_id|>assistant<|end_header_id|>"",
        ""parameters"": {
                ""temperature"": 0.2,
                ""top_p"": 0.95,
                ""repetition penalty"": 1.2,
                ""top_k"": 50,
                ""truncate"": 1000,
                ""max_new_tokens"": 1024,
                ""stop"": [
                        ""<|start_header_id|>"",
                        ""<|end_header_id|>"",
                        ""<|eot_id|>""
                ]
        },
        ""endpoints"": [{
                ""type"" : ""tgi"",
                ""url"": ""http://127.0.0.1:8080""
        }]
}
]`
"
dromeuf,2024-05-21 09:09:23+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"hello, for a conversational dataset intended for fine-tuning llama 3, at the assistant level, should we finally add a \n<|end_of_text|> token or not ?

for example : |start_header_id|>assistant<|end_header_id|>\n\n{assistant}<|eot_id|>\n<|end_of_text|>

King regards"
npotts,2024-04-19 01:03:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"The output takes the same space as the input. The model completes the 8k token space with the response. You could slide the contxt window and get more output, but then you're losing context at the beginning.

Example, if your input is 100 tokens, you have ~7900 tokens for completion. But if your input is 7900 tokens, you have ~100 tokens left for response until your input starts getting trimmed out. The model can only pay attention to 8k tokens at most."
YalunHu,2024-05-09 03:47:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> I understand the input context length is 8k, but what about the output?

May I ask where I can check this ""8k context length"" configuration for the llama3 model? Thanks!"
npotts,2024-05-09 03:51:24+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> > I understand the input context length is 8k, but what about the output?
> 
> May I ask where I can check this ""8k context length"" configuration for the llama3 model? Thanks!

https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/config.json

look at `max_position_embeddings`"
pcuenq,2024-04-18 20:53:26+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,
aviator1994,2024-04-18 20:21:44+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I've been waiting as well. Seems like it's normal, but I'm not sure how long it usually takes."
linlp22,2024-04-19 07:01:37+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Anyone success？
dmitrykvant,2024-04-19 09:59:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
deleted,2024-04-19 13:21:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1
clarklikepc2100,2024-04-21 02:50:35+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,plus one
outmanzhao,2024-04-22 03:03:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,plus one 
par4dox,2024-04-22 03:07:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"+1
"
bonni93,2024-04-22 22:28:45+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Still had no access after a few days - I ended up deleting my hf account and recreating it.
With the new account, I then first requested access from Meta and then here at hf. 
I got access here eventually after a few minutes.
Try also your institutional email, it might work better."
JoKoSt,2024-04-23 12:39:40+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"After requesting the llama3 model I got a Mail approving acces for  ""meta-llama/Meta-Llama-Guard-2-8B"" instead.. I also requested permission at meta directly (which are approved as well) but when trying to download the model via HF i still get ""Repo model meta-llama/Meta-Llama-3-8B-Instruct is gated. You must be authenticated to access it""  
Maybe this helps to narrow down the error here...

"
darsh-essential,2024-04-23 20:53:29+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I requested llama3 model too, but I got an email approving access for ""meta-llama/Meta-LLama-Guard-2-8B"" instead. "
tsudhakar112,2024-04-25 20:24:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"while requesting for LLama model , it is giving download link in mail . but how can we access it by huggingface ? , in huggingface still showing no permission.
"
vinooj,2024-04-28 03:04:07+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,( Resolved) I'm new to hugging face. And I'm wondering how do I request access to meta-llama/Meta-Llama-3-8B-Instruct? Ans: Click the Requesting Access. It took me a while to figure out :-)
kysc,2024-04-29 15:35:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"I requested Llama3 using my personal email and got an email for ""meta-llama/Meta-LLama-Guard-2-8B"" within 30min. But it's actually the access for ""meta-llama/Meta-Llama-3-8B-Instruct"""
melhzy,2024-05-20 20:31:22+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Where I can request access to meta-llama/Meta-Llama-3-8B?
Hans15,2024-05-28 16:01:37+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Personal email, I waited for less than 10 minutes"
osanseviero,2024-05-29 11:58:53+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/130
andrewxie,2024-06-03 02:11:48+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,+1 
philschmid,2024-04-19 12:04:59+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,LGTM
pcuenq,2024-04-18 20:44:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Thanks a lot @atasoglu! This has already been fixed :)
entropy,2024-04-18 20:23:47+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Running into the same issue. With the default config, the model doesn't stop at `<|eot_id|>` and will generate new text for the user.

After updating the config, the model no longer generates user text, but instead ends with an infinite series of  `<|eot_id|><|start_header_id|>assistant:<|eot_id|><|start_header_id|>assistant:<|eot_id|>...`

Is there a way to prevent this?"
abhi-db,2024-04-18 21:00:03+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Hmm @entropy could you provide more details your setup? Here's what is working for me, referencing this PR:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = ""meta-llama/Meta-Llama-3-8B-Instruct""
revision = ""refs/pr/4""

tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)
model = AutoModelForCausalLM.from_pretrained(model_path, revision=revision, device_map=""auto"", torch_dtype=torch.bfloat16)

prompt = ""Write a haiku about terminators.""
chat = [{'content': prompt, 'role': 'user'}]
chat_tokens = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(model.device)

new_chat_tokens = model.generate(chat_tokens, do_sample=False, max_new_tokens=128)
new_chat_str = tokenizer.decode(new_chat_tokens[0])
print (new_chat_str)
```

produces:
```
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Write a haiku about terminators.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Metal hearts ablaze
Rise from ashes, cold and dark
Judgment day arrives<|eot_id|>
```"
Wladastic,2024-04-18 21:47:31+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Same here, I use oobabooga textgen and llama 3 8B instruct will not shut up.
To reproduce just tell it 1 token and to say START for example."
aeminkocal,2024-04-18 23:25:06+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,It's the same with TabbyAPI.
venzon,2024-04-19 00:41:17+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"In oobabooga text-generation-webui, you also need to uncheck ""Skip special tokens"" in the Parameters -> Generation tab."
0-hero,2024-04-19 01:29:50+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,fixed gguf quant here. https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF
lemonflourorange,2024-04-19 04:59:37+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct, 
aeminkocal,2024-04-19 07:49:30+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/14

check here my latest message."
viperx7,2024-04-19 12:29:14+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"for me this change was not enough on text generation webui
i had to uncheck ""skip special tokens"" and add ""<|eot_id|>"" in custom stop strings after that every thing was good"
pcuenq,2024-04-19 13:02:31+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Thank you @abhi-db!
robert1968,2024-04-19 15:08:41+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> fixed gguf quant here. https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF

yes and it works fine. i use Meta-Llama-3-8B-Instruct.Q8_0.gguf and Meta-Llama-3-8B-Instruct.Q6_K.gguf and both perfectly stop conversation when finished. 
Many thanks. :)"
Gerald001,2024-04-21 20:45:39+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"hi guys, is my issue related to the same problem described here? https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/36
if yes, will this repo be fixed?"
Jiar,2024-04-26 14:03:29+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> Hmm @entropy could you provide more details your setup? Here's what is working for me, referencing this PR:
> 
> ```python
> from transformers import AutoTokenizer, AutoModelForCausalLM
> import torch
> 
> model_path = ""meta-llama/Meta-Llama-3-8B-Instruct""
> revision = ""refs/pr/4""
> 
> tokenizer = AutoTokenizer.from_pretrained(model_path, revision=revision)
> model = AutoModelForCausalLM.from_pretrained(model_path, revision=revision, device_map=""auto"", torch_dtype=torch.bfloat16)
> 
> prompt = ""Write a haiku about terminators.""
> chat = [{'content': prompt, 'role': 'user'}]
> chat_tokens = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(model.device)
> 
> new_chat_tokens = model.generate(chat_tokens, do_sample=False, max_new_tokens=128)
> new_chat_str = tokenizer.decode(new_chat_tokens[0])
> print (new_chat_str)
> ```
> 
> produces:
> ```
> <|begin_of_text|><|start_header_id|>user<|end_header_id|>
> 
> Write a haiku about terminators.<|eot_id|><|start_header_id|>assistant<|end_header_id|>
> 
> Metal hearts ablaze
> Rise from ashes, cold and dark
> Judgment day arrives<|eot_id|>
> ```

please change `new_chat_str = tokenizer.decode(new_chat_tokens[0])`  to `new_chat_str = tokenizer.decode(new_chat_tokens[0], skip_special_tokens=True)`"
Zibri,2024-05-18 02:23:43+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/663dfe499a3aec301fa7b15e/EQo0T0FZ4Ql7AND-t3mkH.png)
I could edit the interface to hide it... but it's also present in the non-instruct version.
I downloaded it from here and quantized. I did nothing else (for now)"
Eleven-P,2024-04-18 17:13:57+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,Downloading...
raydeStar,2024-04-18 19:59:34+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"Actually pretty pumped for this one.  By the numbers, it should be an impressive build.  Credit to the meta team for driving these smaller models.  "
tanimazsin130,2024-04-18 16:39:06+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,It would be nice to report a list of languages ​​included in the training data and the amount of tokens in millions.
musashihinck,2024-04-18 17:12:46+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"It would also be interesting to run this benchmark: https://huggingface.co/datasets/caro-holt/MultiQ

Measure accuracy in different languages + fidelity (replying in the same language as the query)."
FinancialSupport,2024-04-18 18:12:51+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"for italian:
Model	Arc-c	HellaS	MMUL
LLama3 8b instruct	44.3	59.9	55.7"
alphaprime90,2024-04-18 19:37:10+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"> for italian:
> Model	Arc-c	HellaS	MMUL
> LLama3 8b instruct	44.3	59.9	55.7

@FinancialSupport How did you test so I can test for other languages?"
FinancialSupport,2024-04-18 20:24:58+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,"@alphaprime90 https://github.com/EleutherAI/lm-evaluation-harness 
"
r000bin,2024-04-22 13:54:35+00:00,comment,meta-llama/Meta-Llama-3-8B-Instruct,On a very quick test on private german and french data it beats ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ
minimario,2024-06-01 00:47:01+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hi, I requested access to the model over a week ago, and still do not have access. Could you please help me check it? Thanks!"
awesomecxh,2024-06-02 14:15:35+00:00,comment,meta-llama/Meta-Llama-3-8B,why I had been rejected to access the model？Can you help me？
Gesearch,2024-06-02 15:51:39+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thank you for making this post. It is good to know the requests are checked hourly. 

It has been several weeks since I have submitted a request for access and it is still under review.

Can you please see what the reason is for the delay?"
clint-greene,2024-06-03 20:11:14+00:00,comment,meta-llama/Meta-Llama-3-8B,"I think everyone knows how to follow these basic instructions. The issue is after clicking the Accept button it throws this error: Sorry, we could not process your request at this moment.
Request ID: 1375369499823076 @osanseviero "
Theojsy,2024-06-04 09:16:47+00:00,comment,meta-llama/Meta-Llama-3-8B,please accept me
Gesearch,2024-06-04 10:21:44+00:00,comment,meta-llama/Meta-Llama-3-8B,"Here is the point I am stuck at.

I assume I am not being accepted because maybe I opted out of sharing contact info?

There is no obvious way for me to opt in since it is required getting access to the model.

![image.png](https://cdn-uploads.huggingface.co/production/uploads/65cd17953f15e89aefb4cbb5/VA6KIYxwubj4omYHE2s2l.png)
"
wzhao5,2024-06-04 14:48:11+00:00,comment,meta-llama/Meta-Llama-3-8B,"> Here is the point I am stuck at.
> 
> I assume I am not being accepted because maybe I opted out of sharing contact info?
> 
> There is no obvious way for me to opt in since it is required getting access to the model.
> 
> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65cd17953f15e89aefb4cbb5/VA6KIYxwubj4omYHE2s2l.png)

Same problem, has been stuck on this page for a week"
Gesearch,2024-06-04 16:53:37+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > Here is the point I am stuck at.
> > 
> > I assume I am not being accepted because maybe I opted out of sharing contact info?
> > 
> > There is no obvious way for me to opt in since it is required getting access to the model.
> > 
> > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65cd17953f15e89aefb4cbb5/VA6KIYxwubj4omYHE2s2l.png)
> 
> Same problem, has been stuck on this page for a week

any suggestions @osanseviero ? A couple of us are stuck."
domdp,2024-06-06 08:21:25+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > > Here is the point I am stuck at.
> > > 
> > > I assume I am not being accepted because maybe I opted out of sharing contact info?
> > > 
> > > There is no obvious way for me to opt in since it is required getting access to the model.
> > > 
> > > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65cd17953f15e89aefb4cbb5/VA6KIYxwubj4omYHE2s2l.png)
> > 
> > Same problem, has been stuck on this page for a week
> 
> any suggestions @osanseviero ? A couple of us are stuck.

Same here!"
HansRen1024,2024-06-08 11:52:24+00:00,comment,meta-llama/Meta-Llama-3-8B,Please grant my access to the model! Thank you. 
Vachellia,2024-06-08 14:36:56+00:00,comment,meta-llama/Meta-Llama-3-8B,-
Vachellia,2024-06-08 14:42:13+00:00,comment,meta-llama/Meta-Llama-3-8B,s
lu0ae,2024-06-10 10:03:22+00:00,comment,meta-llama/Meta-Llama-3-8B,same
osanseviero,2024-05-31 13:34:35+00:00,comment,meta-llama/Meta-Llama-3-8B,"This is a base model, hence it's not conversational and does not have chat capabilities. The Instruct model you mention is conversational (and hence why it does have a template). The template is usually added during the fine-tuning process (which is when this base model became the instruct one)."
chaosIsRythmic,2024-06-02 08:42:42+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thanks for the clarification @osanseviero  ! This makes sense.

My goal is to fine-tune Llama-3-8B to do multi-label classification on the MimicIII medical codes dataset and I am a bit lost on what prompt template to use. I am a bit paranoid about getting all the special tokens right. Would be awesome if you could point me into the right direction.

I realize this may be veering off the original discussion topic, so I'm happy to start a new thread if that would be more appropriate. Thanks for any assistance you can offer!"
osanseviero,2024-05-29 07:19:21+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:19:26+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:19:34+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:19:42+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:19:45+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:19:48+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:20:00+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
pelumifagbemi,2024-05-26 12:29:34+00:00,comment,meta-llama/Meta-Llama-3-8B,forgot to mention i'm using the langchain framework
tantb,2024-05-27 04:15:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"I have the same problem. Did you find the solution? Thanks
"
huggingfacemodeltester,2024-05-30 09:33:23+00:00,comment,meta-llama/Meta-Llama-3-8B,are you using the base model or chat? base model usually behaves like this
tantb,2024-05-30 11:11:03+00:00,comment,meta-llama/Meta-Llama-3-8B,"I made use of the Meta-Llama-3-8B-Instruct model instead of the base model, and it worked! Thank you."
osanseviero,2024-05-27 07:22:21+00:00,comment,meta-llama/Meta-Llama-3-8B,"When the team converts from the original PyTorch format to the `transformers` format, we do logit checks to ensure all logits have 1 to 1 match. The results could potentially be different due to different generation parameters or structuring the prompt differently"
agoudarzi,2024-05-28 01:40:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"thanks @osanseviero , to be clear, it is not a slight difference though, the output of the pytorch is scrambled text. Is there any example of generation using pytorch format I can look at? Are you saying the decode is potentially different?
"
osanseviero,2024-05-29 07:21:25+00:00,comment,meta-llama/Meta-Llama-3-8B,"For the original checkpoint, you should use the original codebase https://github.com/meta-llama/llama3 "
Dtree07,2024-05-25 07:48:17+00:00,comment,meta-llama/Meta-Llama-3-8B,"Well, Chinese are all sinners. QaQ"
VityaVitalich,2024-05-29 11:47:58+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue
osanseviero,2024-05-29 07:20:15+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
ArthurZ,2024-05-27 07:34:22+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hey! This is expected, you should not be using the `LlamaTokenizer[Fast]` for Llama3 as the tokenizer is different. Either use `PreTrainedTokenizerFast` or `AutoTokenizer`. 
What happens when you load using LlamaTokenizerFast is it adds the unk token (depending on the version of tokenizers you are using).
"
farzadab,2024-05-28 16:19:54+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thanks for the response!
I understand that, my point was that this is an easy mistake to make, specially if you're upgrading from Llama 2, and it is gonna be hard to catch.

My proposal was to either add a warning in `LlamaTokenizer[Fast]` or at least some comment in the Llama 3 page to indicate the importance of tokenizer change from Llama 2 to 3.
I can create the PR if you think that's the right approach."
osanseviero,2024-05-29 07:20:18+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
HengyuLong,2024-05-23 14:39:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hello!! I am studying training LLM models and want to train Llama3.
Please grant me access to this model."
osanseviero,2024-05-29 07:22:22+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:20:21+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 07:20:24+00:00,comment,meta-llama/Meta-Llama-3-8B,Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
narendrapatelegn,2024-05-22 15:00:26+00:00,comment,meta-llama/Meta-Llama-3-8B,"@osanseviero, I still get below error while accessing Llama3-3-8B. I am passing my HUGGING_FACE_HUB_TOKEN as well as an env variable.

```
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.
403 Client Error. (Request ID: Root=1-664e078b-40e87b15091bc2df01d3b22f;d3fab6e4-1132-47cc-9e02-54364b5c54b3)

Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.
Access to model meta-llama/Meta-Llama-3-8B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B to ask for access.
```"
osanseviero,2024-05-29 09:11:27+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 09:11:36+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 09:11:38+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 09:11:41+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
osanseviero,2024-05-29 09:11:44+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
binganao,2024-05-21 07:04:09+00:00,comment,meta-llama/Meta-Llama-3-8B,Me too
douya123,2024-05-21 08:07:50+00:00,comment,meta-llama/Meta-Llama-3-8B,me too！！！ why？？？
Jayson521,2024-05-21 09:40:58+00:00,comment,meta-llama/Meta-Llama-3-8B,I suspect it's because the address is filled in China
douya123,2024-05-21 13:32:19+00:00,comment,meta-llama/Meta-Llama-3-8B,"maybe， I wrote China 
"
ArthurZ,2024-06-03 08:37:25+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hey! This is unfortunately expected for now, and the template processor should be updated. If you check the class this used for both is not the same. 
More details here: https://github.com/huggingface/transformers/issues/30947#issuecomment-2128057992"
Dane2180,2024-05-19 13:13:33+00:00,comment,meta-llama/Meta-Llama-3-8B,I've had success with llama3 8B using unsloth. https://github.com/unslothai/unsloth
hammh0a,2024-05-14 08:38:15+00:00,comment,meta-llama/Meta-Llama-3-8B,Same here!
FFFS,2024-05-20 10:40:49+00:00,comment,meta-llama/Meta-Llama-3-8B,Same here!
Sintayew4,2024-05-20 12:12:44+00:00,comment,meta-llama/Meta-Llama-3-8B,https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/138#66431cc940073b6263a44188
Danica-hf,2024-05-21 03:18:45+00:00,comment,meta-llama/Meta-Llama-3-8B,May I ask how to request the access of Llama-3 ?
osanseviero,2024-05-29 09:13:42+00:00,comment,meta-llama/Meta-Llama-3-8B,https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/141
Insanelycool,2024-05-16 00:43:18+00:00,comment,meta-llama/Meta-Llama-3-8B,"Using LM Studio to run models works for me. I often stop the generation, edit the AI mistakes and steer it in the direction I want, save the changes and then have it continue generating. This seems to work for me on all models I have tried while using LM Studio App."
osanseviero,2024-05-29 09:15:17+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/141. Make sure to be using a token with read access to the repo.
kerkathy,2024-05-22 17:41:20+00:00,comment,meta-llama/Meta-Llama-3-8B,"I'm having the same issue. Raised again in #151.
```
/home/anaconda3/envs/train_dpr_cu113/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py"", line 
195, in forward
    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
RuntimeError: shape '[18, 217, 32, 128]' is invalid for input of size 3999744  
```
It's very weird that my same piece of code runs on a 12.2 CUDA but not a CUDA11.3 nor  CUDA 12.0 ones...
I'm using a bitsandbytes 8btit quantized model though.

Full environment packages:
```
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                       2_gnu    conda-forge
accelerate                0.30.1                   pypi_0    pypi
bitsandbytes              0.42.0                   pypi_0    pypi
blas                      1.0                         mkl  
brotli-python             1.0.9            py39h6a678d5_8  
bzip2                     1.0.8                h5eee18b_6  
ca-certificates           2024.3.11            h06a4308_0  
certifi                   2024.2.2         py39h06a4308_0  
charset-normalizer        2.0.4              pyhd3eb1b0_0  
cuda-cudart               11.8.89                       0    nvidia
cuda-cupti                11.8.87                       0    nvidia
cuda-libraries            11.8.0                        0    nvidia
cuda-nvrtc                11.8.89                       0    nvidia
cuda-nvtx                 11.8.86                       0    nvidia
cuda-runtime              11.8.0                        0    nvidia
ffmpeg                    4.3                  hf484d3e_0    pytorch
filelock                  3.13.1           py39h06a4308_0  
freetype                  2.12.1               h4a9f257_0  
fsspec                    2024.5.0                 pypi_0    pypi
gmp                       6.2.1                h295c915_3  
gmpy2                     2.1.2            py39heeb90bb_0  
gnutls                    3.6.15               he1e5248_0  
huggingface-hub           0.23.1                   pypi_0    pypi
idna                      3.7              py39h06a4308_0  
intel-openmp              2023.1.0         hdb19cb5_46306  
jinja2                    3.1.3            py39h06a4308_0  
jpeg                      9e                   h5eee18b_1  
lame                      3.100                h7b6447c_0  
lcms2                     2.12                 h3be6417_0  
ld_impl_linux-64          2.38                 h1181459_1  
lerc                      3.0                  h295c915_0  
libabseil                 20240116.2      cxx17_h59595ed_0    conda-forge
libcublas                 11.11.3.6                     0    nvidia
libcufft                  10.9.0.58                     0    nvidia
libcufile                 1.9.1.3                       0    nvidia
libcurand                 10.3.5.147                    0    nvidia
libcusolver               11.4.1.48                     0    nvidia
libcusparse               11.7.5.86                     0    nvidia
libdeflate                1.17                 h5eee18b_1  
libffi                    3.4.4                h6a678d5_1  
libgcc-ng                 13.2.0               h77fa898_7    conda-forge
libgomp                   13.2.0               h77fa898_7    conda-forge
libiconv                  1.16                 h5eee18b_3  
libidn2                   2.3.4                h5eee18b_0  
libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
libnpp                    11.8.0.86                     0    nvidia
libnvjpeg                 11.9.0.86                     0    nvidia
libpng                    1.6.39               h5eee18b_0  
libprotobuf               4.25.3               h08a7969_0    conda-forge
libsentencepiece          0.2.0                hb0b37bd_1    conda-forge
libstdcxx-ng              13.2.0               hc0a3c3a_7    conda-forge
libtasn1                  4.19.0               h5eee18b_0  
libtiff                   4.5.1                h6a678d5_0  
libunistring              0.9.10               h27cfd23_0  
libwebp-base              1.3.2                h5eee18b_0  
libzlib                   1.2.13               hd590300_5    conda-forge
llvm-openmp               14.0.6               h9e868ea_0  
lz4-c                     1.9.4                h6a678d5_1  
markupsafe                2.1.3            py39h5eee18b_0  
mkl                       2023.1.0         h213fc3f_46344  
mkl-service               2.4.0            py39h5eee18b_1  
mkl_fft                   1.3.8            py39h5eee18b_0  
mkl_random                1.2.4            py39hdb19cb5_0  
mpc                       1.1.0                h10f8cd9_1  
mpfr                      4.0.2                hb69a4c5_1  
mpmath                    1.3.0            py39h06a4308_0  
ncurses                   6.4                  h6a678d5_0  
nettle                    3.7.3                hbbd107a_1  
networkx                  3.1              py39h06a4308_0  
numpy                     1.26.4           py39h5f9d8c6_0  
numpy-base                1.26.4           py39hb5e798b_0  
openh264                  2.1.1                h4ff587b_0  
openjpeg                  2.4.0                h3ad879b_0  
openssl                   3.0.13               h7f8727e_2  
packaging                 24.0                     pypi_0    pypi
pillow                    10.3.0           py39h5eee18b_0  
pip                       24.0             py39h06a4308_0  
psutil                    5.9.8                    pypi_0    pypi
pysocks                   1.7.1            py39h06a4308_0  
python                    3.9.19               h955ad1f_1  
python_abi                3.9                      2_cp39    conda-forge
pytorch                   2.2.2           py3.9_cuda11.8_cudnn8.7.0_0    pytorch
pytorch-cuda              11.8                 h7e8668a_5    pytorch
pytorch-mutex             1.0                        cuda    pytorch
pyyaml                    6.0.1            py39h5eee18b_0  
readline                  8.2                  h5eee18b_0  
regex                     2024.5.15                pypi_0    pypi
requests                  2.31.0           py39h06a4308_1  
safetensors               0.4.3                    pypi_0    pypi
scipy                     1.13.0                   pypi_0    pypi
sentencepiece             0.2.0                hf3d152e_1    conda-forge
sentencepiece-python      0.2.0            py39ha537242_1    conda-forge
sentencepiece-spm         0.2.0                hb0b37bd_1    conda-forge
setuptools                69.5.1           py39h06a4308_0  
sqlite                    3.45.3               h5eee18b_0  
sympy                     1.12             py39h06a4308_0  
tbb                       2021.8.0             hdb19cb5_0  
tk                        8.6.14               h39e8969_0  
tokenizers                0.19.1                   pypi_0    pypi
torchaudio                2.2.2                py39_cu118    pytorch
torchtriton               2.2.0                      py39    pytorch
torchvision               0.17.2               py39_cu118    pytorch
tqdm                      4.66.4                   pypi_0    pypi
transformers              4.41.0                   pypi_0    pypi
typing_extensions         4.11.0           py39h06a4308_0  
tzdata                    2024a                h04d1e81_0  
urllib3                   2.2.1            py39h06a4308_0  
wheel                     0.43.0           py39h06a4308_0  
xz                        5.4.6                h5eee18b_1  
yaml                      0.2.5                h7b6447c_0  
zlib                      1.2.13               hd590300_5    conda-forge
zstd                      1.5.5                hc292b87_2 
```"
LCleo,2024-05-13 02:35:54+00:00,comment,meta-llama/Meta-Llama-3-8B,the same :(
krisfarrmt,2024-05-13 07:52:32+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue here
tinywaves,2024-05-13 08:16:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"Yes, same issue."
LCleo,2024-05-13 08:34:38+00:00,comment,meta-llama/Meta-Llama-3-8B,"I have solved the issue. I login HuggingFace in terminal of vscode, and transmit the parameter ""use_auth_token=True"" to model and tokenizer, then it success."
Kelmeilia,2024-05-13 13:14:31+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thanks for the help!

It seems, that I needed to do one more step;
I had not accessed gated models before, so setting the HF_HUB_TOKEN environment variable and aforementioned use_auth_token=True wasn't still enough - It was needed to run 

`huggingface-cli login`

and providing the generated updated api key as well to there..."
Tonic,2024-05-11 11:42:19+00:00,comment,meta-llama/Meta-Llama-3-8B,basically it's based on if your emails are the same  on huggingface and on their sign up nothing more i think...
Alain44,2024-05-09 14:17:46+00:00,comment,meta-llama/Meta-Llama-3-8B,"I tried with 
import transformers
import torch
model_id = ""meta-llama/Meta-Llama-3-8B""
pipeline = transformers.pipeline(
""text-generation"",
    model=model_id,
    model_kwargs={
        ""torch_dtype"": torch.bfloat16,
        ""quantization_config"": {""load_in_4bit"": True},
        ""low_cpu_mem_usage"": True,
    },
)
I've got the error : torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU.

My GPU has 4GB of memory maybe it's not enough ?"
BenchIt,2024-05-10 11:09:29+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hey Alain, 

For Llama 3 8GB, your GPU has not enough memory. Try more powerful hardware. You could start with google Colab, there you can use ressources from Google Server for free, its still not the best but should work for your 8GB Modell. Maybe also shardening could help you at this point. 
If you are inferecing, choose a lower sequention length for less Ram Usage

Sry for my bad english, and GL 
"
Ateeqq,2024-05-12 07:48:57+00:00,comment,meta-llama/Meta-Llama-3-8B,@Alain44 try it out: https://huggingface.co/unsloth/llama-3-8b-bnb-4bit
RunnerZhong,2024-05-08 10:28:36+00:00,comment,meta-llama/Meta-Llama-3-8B,same +1
iwonder87,2024-05-08 11:46:13+00:00,comment,meta-llama/Meta-Llama-3-8B,"the same
why?"
yuhui1130,2024-05-09 07:35:55+00:00,comment,meta-llama/Meta-Llama-3-8B,"""Your request to access this repo has been rejected by the repo's authors.""  Why??"
Evan-zhang,2024-05-09 08:57:47+00:00,comment,meta-llama/Meta-Llama-3-8B,The same refused.
fuckyoujerry,2024-05-10 09:54:19+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
JaaackXD,2024-05-10 17:02:24+00:00,comment,meta-llama/Meta-Llama-3-8B,Feel free to clone from my repos guys
asdasdasdw,2024-05-11 04:03:41+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
bingwork,2024-05-14 08:15:41+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
FincherYuan,2024-05-23 05:40:51+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Jiar,2024-05-23 08:49:58+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Elvin90,2024-05-26 09:09:39+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Kandhuri,2024-05-08 11:31:10+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hi this worked. 
if you are using the Vscosde follow the instructions below

from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline

login(token = 'your_token')

tokenizer = AutoTokenizer.from_pretrained(
    ""meta-llama/Meta-Llama-3-8B"",
    cache_dir=""/kaggle/working/""
)

model = AutoModelForCausalLM.from_pretrained(
    ""meta-llama/Meta-Llama-3-8B"",
    cache_dir=""/kaggle/working/"",
    device_map=""auto"",
)"
osanseviero,2024-05-07 08:12:50+00:00,comment,meta-llama/Meta-Llama-3-8B,
Kandhuri,2024-05-08 11:31:58+00:00,comment,meta-llama/Meta-Llama-3-8B,open the model page on top you should see that you have access to the model. 
BasicCoder,2024-05-08 11:59:51+00:00,comment,meta-llama/Meta-Llama-3-8B,"> "" Your request to access this repo has been successfully submitted, and is pending a review from the repo's authors."" and still like that for almost a week now and still no responce

Same problem"
Kandhuri,2024-05-08 18:20:46+00:00,comment,meta-llama/Meta-Llama-3-8B,i think it should be approved almost immediately. How are you trying to access llma3? from hugging face or through git hub from meta
osanseviero,2024-05-29 09:20:27+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
BigDeeper,2024-05-05 11:32:49+00:00,comment,meta-llama/Meta-Llama-3-8B,"I've seen something like this. Look at the JSON structure. The tokenizer takes text and produces input ids, but the structure for training is a very long column of integers."
aniiikket11,2024-05-09 05:43:21+00:00,comment,meta-llama/Meta-Llama-3-8B,Yes I solved it. In my case problem was with format of the input. 
nasser1,2024-05-05 16:11:55+00:00,comment,meta-llama/Meta-Llama-3-8B,"

"
cclevenger,2024-05-09 15:39:33+00:00,comment,meta-llama/Meta-Llama-3-8B,"model = AutoModelForCausalLM.from_pretrained(
  self.model_id,
  device_map='auto',
  trust_remote_code=True,
)
self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
self.pipeline = pipeline(
  'text-generation',
  model=model,
  tokenizer=self.tokenizer,
  trust_remote_code=True,
)

I think you can also do pipeline.tokenizer."
TonyHartley,2024-05-08 15:52:23+00:00,comment,meta-llama/Meta-Llama-3-8B,"Its should work, I'm running Llama3 8b and its works find (In Ollama that is), so it is possible:)"
carloshsf,2024-05-12 18:19:51+00:00,comment,meta-llama/Meta-Llama-3-8B,I keep asking myself why the Ollama runs so much smoother and when using the transfomers sometimes it doesn't run at all. I`m using RTX3070 12gb and Ryzen 7
Littlebox692,2024-05-14 02:26:33+00:00,comment,meta-llama/Meta-Llama-3-8B,"> I keep asking myself why the Ollama runs so much smoother and when using the transfomers sometimes it doesn't run at all. I`m using RTX3070 12gb and Ryzen 7

have you find any solution?"
carloshsf,2024-05-14 03:01:10+00:00,comment,meta-llama/Meta-Llama-3-8B,"Yes, I was missing the quantization part! @Littlebox692 

```
bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type=""nf4"",
            bnb_4bit_compute_dtype=torch.float16,
        )

AutoModelForCausalLM.from_pretrained(
            dir,
            device_map=""auto"",
            quantization_config=bnb_config,
        )
```

This ""quantization_config"" passed to AutoModel constructor did the trick.  Basicallt a conversion of data from a 32-bit floating-point number (FP32) to an 8-bit or 4-bit integer."
ybelkada,2024-05-14 15:31:24+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hi 
Indeed, using quantized versions of the model can help reduce the required VRAM to run the model, please see: https://huggingface.co/docs/transformers/quantization for more details on all supported quant schemes"
JaaackXD,2024-05-05 01:05:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"Use the instruct model instead; you need more prompts to make the model productive.

Or help the girl anyway."
angrybyte,2024-05-09 17:34:25+00:00,comment,meta-llama/Meta-Llama-3-8B,"Wow, looks like your GPU needs an exorcism. "
kaamraan,2024-05-15 18:38:49+00:00,comment,meta-llama/Meta-Llama-3-8B,"Try to use the instruction version and I recommend you to generate the text by using ""repetition_penalty"". I usually put it equal to 1.2. This way you will not have repetition problem."
huggingfacemodeltester,2024-05-30 09:35:53+00:00,comment,meta-llama/Meta-Llama-3-8B,"I guess you haven't interacted much with the *base* models

this is a normal behavior of a base model, it just tries to randomly autocomplete your sentence. what you need is a chat model (also called instruct model often), which has the assistant personality

upd: oops, just realized i'm necroposting, never mind"
apepkuss79,2024-05-04 06:41:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same issue!

@JaaackXD Did you try other hf repos? Seems like the same issue also happens on other hf repos."
proto2024,2024-05-04 14:22:09+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same issue!

Error downloading object: model-00001-of-00004.safetensors (f2c1441): Smudge error: Error downloading model-00001-of-00004.safetensors (f2c144103072514542e327fa8080bd375cb300f2d453fba9ca3aea81d0d4cf33): batch request: unexpected end of JSON input

Errors logged to '/Users/proto/sourceCode/ollama/model/.git/lfs/logs/20240504T222126.842679.log'.
Use `git lfs logs last` to view the log.
error: external filter 'git-lfs filter-process' failed
fatal: model-00001-of-00004.safetensors: smudge filter lfs failed
warning: Clone succeeded, but checkout failed.
You can inspect what was checked out with 'git status'
and retry with 'git restore --source=HEAD :/'"
JaaackXD,2024-05-04 18:21:41+00:00,comment,meta-llama/Meta-Llama-3-8B,"After some research and trying a bunch of solutions from both StackOverflow and ChatGPT, I think this is not the LLaMA issue but the issue with git-lfs on its own. (I tried to clone other repos via SSH, which also contained large files, and also failed.)

I haven't found a clear way to use git-lfs to clone the *correct* large files via SSH. But it should work fine via HTTPS. (https://stackoverflow.com/questions/69335691/azure-devops-git-repo-ssh-failed-to-download-lfs-files-while-https-works-fine)

So, if you guys are having authentication issues when you clone via HTTPS with Meta, feel free to clone the original LLaMA 3 models from my repo via HTTPS without any authentication needed! 🤗 (https://huggingface.co/JaaackXD)"
julien-c,2024-05-10 11:09:47+00:00,comment,meta-llama/Meta-Llama-3-8B,should be fixed soon
julien-c,2024-05-10 11:10:03+00:00,comment,meta-llama/Meta-Llama-3-8B,thanks for the reports 🫡
JaaackXD,2024-05-11 07:20:34+00:00,comment,meta-llama/Meta-Llama-3-8B,"It's fixed, closed for now."
Helibol,2024-05-05 10:32:56+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
ashok2216,2024-05-13 05:30:21+00:00,comment,meta-llama/Meta-Llama-3-8B,Why LLama 3 giving late response?
bob-cat,2024-05-29 22:08:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"I noticed the same problem. Only other thing I could find that might be talking about this is this github discussion:
https://github.com/abetlen/llama-cpp-python/discussions/1376"
JaaackDai,2024-05-04 01:45:36+00:00,comment,meta-llama/Meta-Llama-3-8B,You might need to check if you have already downloaded the correct full-size model document.
caiofelix,2024-05-06 14:22:33+00:00,comment,meta-llama/Meta-Llama-3-8B,"I cloned the repo using both HTTPS and SSH, twice each, to validate whether I was downloading it correctly, but I am still receiving the same error."
JaaackXD,2024-05-06 16:48:56+00:00,comment,meta-llama/Meta-Llama-3-8B,"https://github.com/huggingface/transformers/issues/27923

Maybe try downloading large files manually from the web or models from other repos?"
caiofelix,2024-05-08 17:29:20+00:00,comment,meta-llama/Meta-Llama-3-8B,"I downloaded the safetensors files manually and it works, thank you for the help."
BIMU,2024-05-02 15:34:41+00:00,comment,meta-llama/Meta-Llama-3-8B,wait patiently
ERmak158,2024-05-02 15:36:24+00:00,comment,meta-llama/Meta-Llama-3-8B,And how long to wait patiently?
PeachPie,2024-05-03 06:55:37+00:00,comment,meta-llama/Meta-Llama-3-8B,"I have waited for about one week, and still haven't receive any information."
tuckingfypo,2024-05-03 06:56:43+00:00,comment,meta-llama/Meta-Llama-3-8B,"How would I know if I have been rejected?
"
PeachPie,2024-05-03 07:00:37+00:00,comment,meta-llama/Meta-Llama-3-8B,"Maybe here could solve our question. In issue #99 said you have been denied for some reason, because the grant process is automatic.
If you don't be admitted in several minutes, that means you are not admit for this model."
tuckingfypo,2024-05-03 07:03:00+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thank you for the reply @PeachPie , I got the access now, say around 7 mins. 
I had also gone to the Meta page to request access, so that might have been important too, for those wondering. 
"
PeachPie,2024-05-03 07:06:15+00:00,comment,meta-llama/Meta-Llama-3-8B,"Oh, maybe it's a good solution, and I will give it a try."
ERmak1581,2024-05-03 09:02:09+00:00,comment,meta-llama/Meta-Llama-3-8B,"I get error 403 on the site, just like when downloading (I had to convert sh to cmd). In general, I was refused on two accounts where I indicated different information. In general, when will this refusal be lifted for me (not a bug robot)"
kxj07,2024-05-02 10:57:57+00:00,comment,meta-llama/Meta-Llama-3-8B,"Seems like you have not raised access to llama3 via HuggingFace .
On the model card their is a form which you need to fill and have the same email id as you have on facebook in order to get it approved ."
Ctheodros,2024-05-02 15:26:08+00:00,comment,meta-llama/Meta-Llama-3-8B,I received the same error but I was already granted access.
dcardoner,2024-05-07 11:21:07+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue here
Abhishekgowda,2024-05-08 16:09:35+00:00,comment,meta-llama/Meta-Llama-3-8B,"same issue, can anyone help"
jaecha,2024-05-10 17:57:39+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same issue. At least, glad that I am not the only one with this problem."
VamZ,2024-05-13 10:22:57+00:00,comment,meta-llama/Meta-Llama-3-8B,"same issue here
"
Rhetorick,2024-05-13 14:40:13+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same here. I received an email however it isn't letting me download it?
"
osanseviero,2024-05-13 14:48:14+00:00,comment,meta-llama/Meta-Llama-3-8B,Hey all! Please make sure to log in in your environment using `huggingface-cli login`. You can use `huggingface-cli whoami` to verify your user. Make sure you're logged in and in the right user.
weinadai,2024-05-22 15:24:45+00:00,comment,meta-llama/Meta-Llama-3-8B,"> Hey all! Please make sure to log in in your environment using `huggingface-cli login`. You can use `huggingface-cli whoami` to verify your user. Make sure you're logged in and in the right user.

This solved my issue. Thank you!"
Hugoam,2024-05-27 22:59:57+00:00,comment,meta-llama/Meta-Llama-3-8B,sameeeeeee please grant me access
Manju7,2024-06-06 10:41:18+00:00,comment,meta-llama/Meta-Llama-3-8B,"Facing same issue here...
"
AprilGuo46,2024-06-07 04:41:57+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same issue.
'huggingface-cli login' can't help me . 
Does anyone have any other methods?"
hugster666,2024-05-01 11:42:42+00:00,comment,meta-llama/Meta-Llama-3-8B,"im getting equally terrible answers but atleast its in English :P .. i used the following parameters while creating the pipeline
        model_kwargs={""torch_dtype"": torch.bfloat16,\
                      ""do_sample"": True,\
                      ""temperature"": 0.5,\
                      ""top_p"": 0.9 }

key is to play around with temperature and top_p ..also there's got to be someway to mention eos token ( that should reduce the length of your response ) "
LynnLzy,2024-05-01 13:56:58+00:00,comment,meta-llama/Meta-Llama-3-8B,"Could you tell how to use eos token,please"
hugster666,2024-05-01 14:04:16+00:00,comment,meta-llama/Meta-Llama-3-8B,"just use the exact code from here ..this works perfectly

https://github.com/huggingface/blog/blob/main/llama3.md#how-to-prompt-llama-3
"
LynnLzy,2024-05-01 14:11:43+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thank you! And i just find that the ""Meta-Llama-3-8B-Instruct"" is good. maybe you can get a normal answer from ""Meta-Llama-3-8B-Instruct"".
https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
Impulse911,2024-05-01 20:23:54+00:00,comment,meta-llama/Meta-Llama-3-8B,Try formatting the code before you execute it.
yarusalka,2024-05-02 03:01:10+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
yarusalka,2024-05-02 03:06:02+00:00,comment,meta-llama/Meta-Llama-3-8B,"thanks for helps, i just used llama3-8B with example code and it seems that it just generated sentences. 
when i used llama3-8b-instruct with example code from https://github.com/huggingface/blog/blob/main/llama3.md#how-to-prompt-llama-3 as Impulse911 mentioned, it worked as i expected.
"
Impulse911,2024-05-01 20:22:32+00:00,comment,meta-llama/Meta-Llama-3-8B,"It really depends on what GPU you're using. If you're using an Nvidia GPU, you'll be better off. Overall, you should be able to run it but it'll be slow."
Buggod,2024-04-30 19:05:58+00:00,comment,meta-llama/Meta-Llama-3-8B,"Well I have included padding to make sure they're all at same length.
Is this the underlying problem here? 
Is setting all at same length neccessary to finetune llama3?
I didnt use such to finetune mistral or gemma
"
Buggod,2024-05-01 06:12:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"Guys please help me!!!

Now i seem to finetune the model but after saving it using 

""model.save_pretrained(output_dir)""

I can't load it. Here's the error:

""---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-98bdaeeb671d> in <module>
      2 from transformers import AutoModelForCausalLM, AutoTokenizer
      3 output_dir = ""tmp/llama-output""
----> 4 model = AutoModelForCausalLM.from_pretrained(output_dir,ignore_mismatched_sizes=True)

~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    561         elif type(config) in cls._model_mapping.keys():
    562             model_class = _get_model_class(config, cls._model_mapping)
--> 563             return model_class.from_pretrained(
    564                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    565             )

~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3740 
   3741         if _adapter_model_path is not None:
-> 3742             model.load_adapter(
   3743                 _adapter_model_path,
   3744                 adapter_name=adapter_name,

~/.local/lib/python3.8/site-packages/transformers/integrations/peft.py in load_adapter(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)
    204 
    205         # Load state dict
--> 206         incompatible_keys = set_peft_model_state_dict(self, processed_adapter_state_dict, adapter_name)
    207 
    208         if incompatible_keys is not None:

~/.local/lib/python3.8/site-packages/peft/utils/save_and_load.py in set_peft_model_state_dict(model, peft_model_state_dict, adapter_name)
    247         raise NotImplementedError
    248 
--> 249     load_result = model.load_state_dict(peft_model_state_dict, strict=False)
    250     if config.is_prompt_learning:
    251         model.prompt_encoder[adapter_name].embedding.load_state_dict(

~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict, assign)
   2187 
   2188         if len(error_msgs) > 0:
-> 2189             raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
   2190                                self.__class__.__name__, ""\n\t"".join(error_msgs)))
   2191         return _IncompatibleKeys(missing_keys, unexpected_keys)

RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:
	size mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([128257, 4096]) from checkpoint, the shape in current model is torch.Size([128256, 4096]).
	size mismatch for lm_head.weight: copying a param with shape torch.Size([128257, 4096]) from checkpoint, the shape in current model is torch.Size([128256, 4096]).
""

I did modify the tokenizer in the beginning, but it won't matter right?


""import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id=""meta-llama/Meta-Llama-3-8B""

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.add_special_tokens({'pad_token': '<pad>'})  # Add a padding token

model = AutoModelForCausalLM.from_pretrained(model_id)
model.config.pad_token_id = tokenizer.pad_token_id  # Set the padding token ID in the model config

# Resize model embeddings to incorporate special tokens
model.resize_token_embeddings(len(tokenizer))"""
78Alpha,2024-04-30 18:59:11+00:00,comment,meta-llama/Meta-Llama-3-8B,It is usually instant since it is an automated process. As far as I can tell anyways. You've most likely been denied for some reason and would have to find the model elsewhere.
rushikesh988,2024-04-29 17:58:29+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
mmarianne,2024-04-29 23:15:24+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
deepbay,2024-04-30 07:17:20+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Guru2k,2024-04-30 10:54:15+00:00,comment,meta-llama/Meta-Llama-3-8B,"+1
"
DavideCosta,2024-04-30 13:09:21+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
itsangad,2024-04-30 17:21:26+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
SachithRKA,2024-04-30 23:35:01+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
SparkMao,2024-05-06 03:52:32+00:00,comment,meta-llama/Meta-Llama-3-8B,"+1, rejected without any reason."
osanseviero,2024-05-29 11:43:51+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
hanghangaidoudou,2024-04-28 00:30:54+00:00,comment,meta-llama/Meta-Llama-3-8B,"can it run at the double rtx 4090,just like 24+24=48?"
mph,2024-04-28 21:44:33+00:00,comment,meta-llama/Meta-Llama-3-8B,This is awful. I hope it gets resolved soon!
mosew,2024-04-29 22:00:19+00:00,comment,meta-llama/Meta-Llama-3-8B,"Try this:
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model, token=os.environ[""HUGGING_FACE_TOKEN""], cache_dir=Settings.CACHE_DIR,
)
stopping_ids = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids(""<|eot_id|>""),
]

llm = HuggingFaceLLM(
    model_name=model,
    model_kwargs={
        ""token"": os.environ[""HUGGING_FACE_TOKEN""],
        ""torch_dtype"": torch.bfloat16,
    },
    generate_kwargs={
        ""do_sample"": True,
        ""temperature"": 0.01,
        ""top_p"": 0.9,
    },
    tokenizer_name=model,
    tokenizer_kwargs={""token"": os.environ[""HUGGING_FACE_TOKEN""]},
    stopping_ids=stopping_ids,
)
```
"
mph,2024-05-29 15:05:39+00:00,comment,meta-llama/Meta-Llama-3-8B,"@osanseviero Why was this closed? And with zero explanation? This is still an issue—with both the base model and the instruct model:

`['<|begin_of_text|>', 'this', 'Ġis', 'Ġa', 'Ġtest']`"
graphway,2024-04-28 09:05:33+00:00,comment,meta-llama/Meta-Llama-3-8B,"Problem solved after upgrades: 
- transformers to 4.40
- torch to 2.3.0+cu121

It may not require the most recent versions but certainly older versions from 6 monts ago didn't work. "
stecno,2024-04-29 15:22:10+00:00,comment,meta-llama/Meta-Llama-3-8B,same here
SparkMao,2024-04-28 05:58:32+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
HLearning,2024-04-28 08:06:00+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
Chen94yue,2024-04-28 09:55:37+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
tdh87,2024-04-28 11:17:36+00:00,comment,meta-llama/Meta-Llama-3-8B,just use this - NousResearch/Meta-Llama-3-8B
huggingss,2024-04-29 01:35:50+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
coldmoon,2024-04-29 02:19:38+00:00,comment,meta-llama/Meta-Llama-3-8B,same ...
deleted,2024-04-29 03:06:12+00:00,comment,meta-llama/Meta-Llama-3-8B,same
XiaoqiHan,2024-04-29 09:19:13+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
wuqihang,2024-04-29 12:43:23+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
Alemax067,2024-04-29 13:14:33+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
Alemax067,2024-04-29 13:18:09+00:00,comment,meta-llama/Meta-Llama-3-8B,"> The same as you. I have no idea why :(

Did you also select “China”?"
gavhuang,2024-05-04 07:45:07+00:00,comment,meta-llama/Meta-Llama-3-8B,就是拒绝中国区的用户，真是。。
Shiym,2024-05-12 09:22:20+00:00,comment,meta-llama/Meta-Llama-3-8B,The same as you. I have no idea why :(
zzfx1166,2024-05-12 11:05:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"> 就是拒绝中国区的用户，真是。。

I  selected United States"
statelesshz,2024-05-13 11:46:14+00:00,comment,meta-llama/Meta-Llama-3-8B,same
chaewon00,2024-05-19 03:06:34+00:00,comment,meta-llama/Meta-Llama-3-8B,"I am writing to request further information regarding the rejection of my access request to the Meta-Llama/Meta-Llama-3-8B repository. The message I received stated: ""Your request to access this repo has been rejected by the repo's authors.""

I would like to know the specific reason for the rejection and what steps I should take to access the api. I really need your api for my project. I am doing my best to comply with all the guidelines or requirements needed to access this repository."
deleted,2024-05-29 13:43:15+00:00,comment,meta-llama/Meta-Llama-3-8B,"same, please quash my request. thanks!"
osanseviero,2024-05-29 11:43:59+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
realdanielbyrne,2024-04-26 16:40:51+00:00,comment,meta-llama/Meta-Llama-3-8B,"This is the base model hence it is only trained to predict the next sequence of words.  Generally base models know a lot about language, and nothing about chatting.  You will need to either fine-tune this model on instruction following or try starting with the instruction tuned version of this model, meta-llama/Meta-Llama-3-8B-Instruct."
summerstay,2024-04-26 17:24:37+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thanks. I have used GPT-3 as a base model. It seems like this is much more prone to repetition than GPT-3 was. I have finally gotten it working okay, but only by turning up the repetition penalty to more than 1. Much higher and the penalty stops it from being able to end sentences (because . is penalized) and soon loses all sense entirely."
ShuishuiAndy,2024-04-26 04:37:20+00:00,comment,meta-llama/Meta-Llama-3-8B,same
clwanghf,2024-04-26 04:54:00+00:00,comment,meta-llama/Meta-Llama-3-8B,same
mylesgoose,2024-04-26 05:56:11+00:00,comment,meta-llama/Meta-Llama-3-8B,"> My application for Llama gets this reply: 'Your request to access this repo has been rejected by the repo's authors'. So how to deal with it?

I think it means you have been naughty :-)"
zjc1031,2024-04-26 06:27:58+00:00,comment,meta-llama/Meta-Llama-3-8B,same
EurekaWu123,2024-04-26 07:01:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"same. And I submitted an application on the official website of llama, it was accepted ."
wuyumin,2024-04-26 07:28:40+00:00,comment,meta-llama/Meta-Llama-3-8B,same
kaleinaNyan,2024-04-26 07:55:01+00:00,comment,meta-llama/Meta-Llama-3-8B,same
szu-yanpei,2024-04-26 08:04:56+00:00,comment,meta-llama/Meta-Llama-3-8B,same
whitefoxredhell,2024-04-26 08:28:49+00:00,comment,meta-llama/Meta-Llama-3-8B,same
revoke,2024-04-26 08:28:56+00:00,comment,meta-llama/Meta-Llama-3-8B,same
JHHS,2024-04-26 08:29:33+00:00,comment,meta-llama/Meta-Llama-3-8B,"+1
"
sunpenglv,2024-04-26 09:21:06+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
dingfeizhang,2024-04-26 10:11:04+00:00,comment,meta-llama/Meta-Llama-3-8B,same
YunjinWu,2024-04-26 12:18:40+00:00,comment,meta-llama/Meta-Llama-3-8B,"+1, what should I do"
mylesgoose,2024-04-26 13:49:43+00:00,comment,meta-llama/Meta-Llama-3-8B,"> +1, what should I do

go to the Llama website and request  access . and download that way. or probably try a different  email account. "
YunjinWu,2024-04-26 14:25:08+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > +1, what should I do
> 
> go to the Llama website and request  access . and download that way. or probably try a different  email account.

Thanks, my request has been approved immediately!"
lefty777,2024-04-27 00:53:15+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same, so does that mean that I can never access the model on huggingface with my account?"
zapqqqwe,2024-04-27 01:21:21+00:00,comment,meta-llama/Meta-Llama-3-8B,same
hm666,2024-04-27 06:41:23+00:00,comment,meta-llama/Meta-Llama-3-8B,"The same,""Your request to access this repo has been rejected by the repo's authors."",but it was accepted on Meta's website.What can I do?"
YunjinWu,2024-04-27 06:49:30+00:00,comment,meta-llama/Meta-Llama-3-8B,"> The same,""Your request to access this repo has been rejected by the repo's authors."",but it was accepted on Meta's website.What can I do?

One way is  Downloading and converting weights to Hugging Face format,  more  step-by-step details can be followed by :
 https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Running_Llama2_Anywhere/Running_Llama_on_HF_transformers.ipynb"
phront,2024-04-27 18:46:48+00:00,comment,meta-llama/Meta-Llama-3-8B,Same
PanosPengHan,2024-04-28 03:49:36+00:00,comment,meta-llama/Meta-Llama-3-8B,"same. And I submitted an application on the official website of llama, it was accepted ."
pymldev,2024-04-28 11:15:30+00:00,comment,meta-llama/Meta-Llama-3-8B,Same thing
FishYu1166,2024-04-28 14:51:08+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
FishYu1166,2024-04-28 15:03:27+00:00,comment,meta-llama/Meta-Llama-3-8B,"> The same,""Your request to access this repo has been rejected by the repo's authors."",but it was accepted on Meta's website.What can I do?

https://github.com/meta-llama/llama3/blob/main/download.sh，run the download.sh script and  input your unique custom URL which will send to your e-mail when you was  accepted on Meta's website."
deleted,2024-04-29 03:07:19+00:00,comment,meta-llama/Meta-Llama-3-8B,same
Aron00123,2024-04-29 08:06:24+00:00,comment,meta-llama/Meta-Llama-3-8B,"same
"
binbinao,2024-04-29 08:50:21+00:00,comment,meta-llama/Meta-Llama-3-8B,"+1
"
balooti,2024-04-30 14:02:51+00:00,comment,meta-llama/Meta-Llama-3-8B,same
Tioe,2024-05-01 09:05:26+00:00,comment,meta-llama/Meta-Llama-3-8B,same
hofarah,2024-05-06 06:35:13+00:00,comment,meta-llama/Meta-Llama-3-8B,"If you want to have inference only, use ollama( https://ollama.com/library/llama3 )
very easy "
airlariken,2024-05-06 13:13:01+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > The same,""Your request to access this repo has been rejected by the repo's authors."",but it was accepted on Meta's website.What can I do?
> 
> One way is  Downloading and converting weights to Hugging Face format,  more  step-by-step details can be followed by :
>  https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Running_Llama2_Anywhere/Running_Llama_on_HF_transformers.ipynb

heartfelt gratitude！It works！"
momo299,2024-04-25 14:46:33+00:00,comment,meta-llama/Meta-Llama-3-8B,horse racing gamble
osanseviero,2024-05-29 11:44:18+00:00,comment,meta-llama/Meta-Llama-3-8B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
abhisskk,2024-04-25 13:02:10+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same, also not able to access mistral, looks like a bug at hugging side
"
navpreet-singh,2024-05-20 20:32:12+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue
osanseviero,2024-05-29 11:44:27+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172
"
ArthurZ,2024-05-10 06:29:48+00:00,comment,meta-llama/Meta-Llama-3-8B,"Mmm this is super weird, seems like a versioning issue. This function is defined here: https://github.com/huggingface/transformers/blob/9df8b301ceb6697a713fbe4034dc02395e5b28cd/src/transformers/modeling_attn_mask_utils.py#L238 .

The error does not come from the pad token id, but rather from transformers. Unfortunately I cannot reproduce your issue: 


![image.png](https://cdn-uploads.huggingface.co/production/uploads/62441cb7456803e95009a08f/VS-48Qx1uhuGUKwF57WD_.png)

"
ArthurZ,2024-05-10 06:30:04+00:00,comment,meta-llama/Meta-Llama-3-8B,I suspect there is some environnement issue at play
kaka0711,2024-05-10 23:34:44+00:00,comment,meta-llama/Meta-Llama-3-8B,"> I suspect there is some environnement issue at play

Thanks, it is version issue."
Doris21,2024-05-12 11:04:39+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > I suspect there is some environnement issue at play
> 
> Thanks, it is version issue.

Hi ,thanks for your question and I met the same problem here, may I ask how you solved the problem?
And if there is no pad_id, how can I pad the sentence to the max length?"
kaka0711,2024-05-13 02:19:25+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > > I suspect there is some environnement issue at play
> > 
> > Thanks, it is version issue.
> 
> Hi ,thanks for your question and I met the same problem here, may I ask how you solved the problem?
> And if there is no pad_id, how can I pad the sentence to the max length?

I think you need to change transformer version. please try different version, if want my installed transformer version, please let me know."
Doris21,2024-05-13 02:21:43+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > > > I suspect there is some environnement issue at play
> > > 
> > > Thanks, it is version issue.
> > 
> > Hi ,thanks for your question and I met the same problem here, may I ask how you solved the problem?
> > And if there is no pad_id, how can I pad the sentence to the max length?
> 
> I think you need to change transformer version. please try different version, if want my installed transformer version, please let me know.

Thank you very much and I might really need your help for the version(●'◡'●)"
kaka0711,2024-05-13 02:37:58+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > > > > I suspect there is some environnement issue at play
> > > > 
> > > > Thanks, it is version issue.
> > > 
> > > Hi ,thanks for your question and I met the same problem here, may I ask how you solved the problem?
> > > And if there is no pad_id, how can I pad the sentence to the max length?
> > 
> > I think you need to change transformer version. please try different version, if want my installed transformer version, please let me know.
> 
> Thank you very much and I might really need your help for the version(●'◡'●)

My library version:

accelerate ==0.21.0
transformer ==4.34.0
torch == 2.0.1
trl ==0.4.7
peft==0.4.0"
Doris21,2024-05-13 02:40:27+00:00,comment,meta-llama/Meta-Llama-3-8B,"> > > > > > I suspect there is some environnement issue at play
> > > > > 
> > > > > Thanks, it is version issue.
> > > > 
> > > > Hi ,thanks for your question and I met the same problem here, may I ask how you solved the problem?
> > > > And if there is no pad_id, how can I pad the sentence to the max length?
> > > 
> > > I think you need to change transformer version. please try different version, if want my installed transformer version, please let me know.
> > 
> > Thank you very much and I might really need your help for the version(●'◡'●)
> 
> My library version:
> 
> accelerate ==0.21.0
> transformer ==4.34.0
> torch == 2.0.1
> trl ==0.4.7
> peft==0.4.0

Thanks a lot for your helping! I will have a try later."
dariog,2024-05-14 08:22:34+00:00,comment,meta-llama/Meta-Llama-3-8B,"In the [instruct version](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/config.json) the eos token was recently changed to ""128009"", while the [generation config](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/generation_config.json) defines both ""[128001, 128009]"". 
Is this supposed to be done in this base version too @ArthurZ ?"
jonwang,2024-04-25 03:07:47+00:00,comment,meta-llama/Meta-Llama-3-8B,me too
menlon,2024-04-25 07:27:45+00:00,comment,meta-llama/Meta-Llama-3-8B,I have been waiting for more than 5 days
ArthurZ,2024-04-24 12:44:05+00:00,comment,meta-llama/Meta-Llama-3-8B,cc @pcuenq 
ArthurZ,2024-04-24 12:30:40+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hey! Wrong repo, the Llama3 tokenizer is very different from the Llama2 and this from the Phi-3 mini 😉"
ArthurZ,2024-04-24 12:32:32+00:00,comment,meta-llama/Meta-Llama-3-8B,Closing as unrelated
xumeng,2024-04-24 10:53:15+00:00,comment,meta-llama/Meta-Llama-3-8B,jlj ljl
kevin1010607,2024-04-24 10:38:24+00:00,comment,meta-llama/Meta-Llama-3-8B,"Try `AutoTokenizer`
https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/37"
hjewr,2024-04-24 11:09:24+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thank u. Yeah, it works. But I wonder what was the reason for that? the sentencepice problem?"
ArthurZ,2024-04-24 12:31:08+00:00,comment,meta-llama/Meta-Llama-3-8B,"YEs, the reason is because as we mention on the card, the tokenizer is NOT sentencepiece based"
ArthurZ,2024-04-24 12:31:41+00:00,comment,meta-llama/Meta-Llama-3-8B,"The `tokenizer.model` is from `tiktoken`. We converted it to the `tokenizers` format, so AutoTokenizer will use a `PreTrainedTokenizerFast` "
ArthurZ,2024-04-24 12:31:55+00:00,comment,meta-llama/Meta-Llama-3-8B,Closing as solved!
ArthurZ,2024-04-24 12:32:07+00:00,comment,meta-llama/Meta-Llama-3-8B,Thanks @hjewr 
hjewr,2024-04-24 15:47:18+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hey, thank u. @ArthurZ . It's kind and nice answer."
wwais,2024-04-24 05:20:40+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hi Barianc,

You can use the instruct version of the Llama-3-8B.
It is available in here:
https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
ArthurZ,2024-04-24 12:35:58+00:00,comment,meta-llama/Meta-Llama-3-8B,Closing as completed! 
kaky,2024-04-24 04:58:58+00:00,comment,meta-llama/Meta-Llama-3-8B,It is the same for me. Please confirm the request
shaosy,2024-04-24 12:31:22+00:00,comment,meta-llama/Meta-Llama-3-8B,Same for me. Waiting for few days...
anvieyra,2024-05-06 23:06:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"Are you guys still waiting on yours? 

I just submitted mine. If you can shed any light on the wait time to be expected that would be great! "
ArthurZ,2024-04-24 12:33:43+00:00,comment,meta-llama/Meta-Llama-3-8B,"This model is a ""base"" model, it's not supposed to answer questions, but predict the next tokens: more like text generation 😉"
ameljelidi,2024-04-24 12:41:23+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thank you so much, this clarifies things! I am definitely a beginner when it comes to all of this"
gbhall,2024-04-24 12:51:51+00:00,comment,meta-llama/Meta-Llama-3-8B,"> This model is a ""base"" model, it's not supposed to answer questions, but predict the next tokens: more like text generation 😉

Which one do you recommend then for llama-3?"
summerstay,2024-04-26 14:05:46+00:00,comment,meta-llama/Meta-Llama-3-8B,[Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) is the same model but tuned for answering questions.
microbrainai,2024-04-23 14:43:45+00:00,comment,meta-llama/Meta-Llama-3-8B,I think you need to first log on using your HF access token and install accelerate with !pip install accelerate
ArthurZ,2024-04-24 12:34:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"The gguf formats are not on this repository, and were uploaded by the community on other repositories! "
Shitqq,2024-04-23 12:02:53+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
zbin1234,2024-04-24 00:07:47+00:00,comment,meta-llama/Meta-Llama-3-8B,I have same problem
ArthurZ,2024-04-24 12:35:03+00:00,comment,meta-llama/Meta-Llama-3-8B,The default value for `max_new_token` is `20`. Just update that value (see the warning saying that the model agnostic default was used 
mhenrichsen,2024-04-23 09:12:21+00:00,comment,meta-llama/Meta-Llama-3-8B,This is not a instruction finetuned model.
sayanroy07,2024-04-23 11:54:20+00:00,comment,meta-llama/Meta-Llama-3-8B,"Even my output has been like that, not entire justified, but this improves drastically while using the same on ollama, not sure why."
LeroyDyer,2024-04-23 12:28:02+00:00,comment,meta-llama/Meta-Llama-3-8B,"it is a random model !!
They cut it off from the larger model , hence total madness !!!
it needs fine tuning ??
i dont know why they have done this ... but perhaps its to conseal the data inside ! as a larger company they have issue with copyrites!!!
but also they are the system which is actually a part of the gov so they CANNOT release a fine tuned model ! only a model in Raw mode.... (hence training .... its semi pretrained ie on the text generation task) .... random data ... pushing in all and anything hence your output ...
if you flip it to instruct training in colab etc you can install the correct chat template (take from code llama) ... and train as you would normally for code llama.... you may even find the model not loading correctly !!!! (NOT ALL THE TENSORS ARE TRAINED)... so it must be loaded in Full precision to train ... or load and save pretrained !... Sealing the tensors !: enabling for the reloaded model to be quantized in memery for training with a peft !

Sorry bro ! ::

But for larger companies i would expect this to be thier output even as you see highly guardrailled for specific data output and input ... even rewriting the input to fit thier needs ! - so uncensoring the model with some over fitting and remerging into the base again will also remove some of this stuff !!!
open source is not supposed to be the scraps brother !

Hence even mistral all of a sudden gating their repo! and elon releasing unusable model !!  so if you have the moeny and technology you can run the model !!!! ie for the rich only!

But again you can extract the last layers of the model ... and make any B you want ! so open source means work for your lunch (when it comes to the corperations) even microsoft removed thier models all of a sudden!

"
ArthurZ,2024-04-24 12:39:08+00:00,comment,meta-llama/Meta-Llama-3-8B,"> Even my output has been like that, not entire justified, but this improves drastically while using the same on ollama, not sure why.

if you are not sure why, it probably means some underground magic is going on? 
It's just that by default you are probably running greedy generations, make sur to adapt the `generation_config` arguments to fit your needs: 
- beam sampling
- temperature
- top-k
- top-p "
ArthurZ,2024-04-24 12:40:31+00:00,comment,meta-llama/Meta-Llama-3-8B,"@LeroyDyer no, the reason why they do that is for anyone to be able to finetune the model a lot more easily. The instruct model is one version of this model, finetuned. "
k0rruptt,2024-04-23 05:10:02+00:00,comment,meta-llama/Meta-Llama-3-8B,can you share what your resources look like along w/ any errors the system spits out? 
not-lain,2024-04-23 05:21:11+00:00,comment,meta-llama/Meta-Llama-3-8B,"@sayanroy07 try quantization
It worked for me when i used the bitsandbytes library on T4"
sayanroy07,2024-04-23 06:16:38+00:00,comment,meta-llama/Meta-Llama-3-8B,"> can you share what your resources look like along w/ any errors the system spits out?

Hi this has been resolved now, just had to re-start the kernal & colab session, it worked now. thanks all"
sayanroy07,2024-04-23 06:17:11+00:00,comment,meta-llama/Meta-Llama-3-8B,"> @sayanroy07 try quantization
> It worked for me when i used the bitsandbytes library on T4

Hi this has been resolved now, just had to re-start the kernal & colab session, it worked now. thanks all"
ceefour,2024-06-06 03:39:30+00:00,comment,meta-llama/Meta-Llama-3-8B,"Happens to me too, restarting the kernel doesn't work. I try another model mistralai/Mistral-7B-Instruct-v0.3 which also crashes Google Colab. Both models work locally using Ollama."
not-lain,2024-06-06 06:56:27+00:00,comment,meta-llama/Meta-Llama-3-8B,"@ceefour here's an example of how i made it work on colab 
```python
!pip install -q accelerate bitsandbytes 
```
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig


bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=""nf4"", bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(""meta-llama/Meta-Llama-3-8B"", quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B"")
```"
ceefour,2024-06-06 08:43:00+00:00,comment,meta-llama/Meta-Llama-3-8B,Thank you @not-lain for your tip! I will try it.
nicolollo,2024-04-24 00:48:55+00:00,comment,meta-llama/Meta-Llama-3-8B,i must be dreaming
Ateeqq,2024-04-23 17:04:09+00:00,comment,meta-llama/Meta-Llama-3-8B,"Yes, I am also experiencing this issue."
mph,2024-04-23 19:27:36+00:00,comment,meta-llama/Meta-Llama-3-8B,"I'm able to fine-tune Llama3 using Accelerate and DeepSpeed ZeRO-2. However, the resulting model doesn't know how to stop generating properly. It spews garbage after answering my question—until max_new_tokens is reached....just like Phi-2. The same training script works flawlessly with Phi-3 and Mistral-7B, though."
jaxyu,2024-04-22 10:16:56+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
yingtao1024,2024-04-22 10:23:39+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Tours0,2024-04-22 11:18:16+00:00,comment,meta-llama/Meta-Llama-3-8B,same here.
AltLuv,2024-04-22 11:43:39+00:00,comment,meta-llama/Meta-Llama-3-8B,same here
royA200195,2024-04-22 14:30:25+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
renqiux0302,2024-04-22 14:37:31+00:00,comment,meta-llama/Meta-Llama-3-8B,"+1
"
D42C,2024-04-22 14:45:58+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
ForestDake,2024-04-22 14:51:18+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
NeuronAstate,2024-04-22 14:54:00+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
ShiLuohe,2024-04-22 15:00:24+00:00,comment,meta-llama/Meta-Llama-3-8B,same here
mikutsky,2024-04-22 15:04:07+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
jlaute,2024-04-22 15:35:11+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Atnafu,2024-04-22 17:13:51+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
shadmankaif,2024-04-22 21:47:09+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
xingjia,2024-04-23 00:41:54+00:00,comment,meta-llama/Meta-Llama-3-8B,Requested a few hours ago and got the approval in about 1.5 hours
lakako,2024-04-23 01:11:03+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Dingjiayu,2024-04-23 02:22:38+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Tours0,2024-04-23 02:47:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"> same here.

Update: I received approval for my submission just a few hours ago (after about 4 days, including the weekend). Be patient. :)"
ChaoXu2000,2024-04-23 02:59:29+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
victormay,2024-04-23 03:01:49+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
caiyuhu,2024-04-23 05:39:48+00:00,comment,meta-llama/Meta-Llama-3-8B,+1 same issue
PJLLM,2024-04-23 07:39:06+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
rushikesh988,2024-04-23 09:12:51+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
nogggg,2024-04-23 11:24:27+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
phront,2024-04-23 12:17:10+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
greenjerry,2024-04-23 13:53:59+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
GiorgioDiSalvo,2024-04-23 20:57:32+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
epsil,2024-04-24 04:45:30+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
ACID23333,2024-04-24 12:09:08+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
osanseviero,2024-05-29 11:44:48+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172

"
fengzi258,2024-04-21 12:15:54+00:00,comment,meta-llama/Meta-Llama-3-8B,"help, i got the same problem. "
maxch3306,2024-04-21 13:20:55+00:00,comment,meta-llama/Meta-Llama-3-8B,"help, i got the same problem too."
tyler2049,2024-04-21 13:34:10+00:00,comment,meta-llama/Meta-Llama-3-8B,"same question
"
rushikesh988,2024-04-21 14:54:41+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue I applied for LLAMA3 access when it got released . I had requested access to LLAMA 2 as well in November so far no access .
deleted,2024-04-21 15:50:44+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue
itsmohit,2024-04-21 17:45:33+00:00,comment,meta-llama/Meta-Llama-3-8B,"> Same issue I applied for LLAMA3 access when it got released . I had requested access to LLAMA 2 as well in November so far no access .

for llama 2 , i got within 2 hours ... 
"
LoreVitCon,2024-04-21 18:15:21+00:00,comment,meta-llama/Meta-Llama-3-8B,yep same here
Ateeqq,2024-04-21 18:39:44+00:00,comment,meta-llama/Meta-Llama-3-8B,I got access to Llama-2 and Llama-3 within 30 mins. 🤗
Dragon111,2024-04-22 01:27:34+00:00,comment,meta-llama/Meta-Llama-3-8B,yep same here
zxwei,2024-04-22 02:53:00+00:00,comment,meta-llama/Meta-Llama-3-8B,same here.
osanseviero,2024-04-22 03:22:46+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hey there! We've reported the issue to the Meta team. It seems something went down yesterday, it should be fixed soon and afterwards access should be provided within an hour"
CocoSun,2024-04-22 03:31:53+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue
Alarmist,2024-04-22 04:02:34+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
Amag1,2024-04-22 04:06:57+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue
Georgiantenor,2024-04-22 05:01:36+00:00,comment,meta-llama/Meta-Llama-3-8B,"> Hey there! We've reported the issue to the Meta team. It seems something went down yesterday, it should be fixed soon and afterwards access should be provided within an hour

Excuse me, is there any way to refill the request?"
hylll,2024-04-22 05:02:58+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
Hihalue,2024-04-22 05:14:00+00:00,comment,meta-llama/Meta-Llama-3-8B,"same issue
"
Kyrieeee,2024-04-22 05:39:51+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue!!!!
Luckyphi,2024-04-22 05:50:25+00:00,comment,meta-llama/Meta-Llama-3-8B,"Looking for help, I also encountered this problem."
huiyangzhou,2024-04-22 06:14:41+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
AbSinthe07,2024-04-22 06:18:25+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
wonderyaya,2024-04-22 06:19:29+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
jupika,2024-04-22 06:20:12+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
Kruis,2024-04-22 06:36:20+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue here
chenbing0110,2024-04-22 06:37:00+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
Baskermas,2024-04-22 06:44:43+00:00,comment,meta-llama/Meta-Llama-3-8B,"same issue
"
xjg,2024-04-22 06:44:47+00:00,comment,meta-llama/Meta-Llama-3-8B,same here 
redwyd,2024-04-22 06:54:04+00:00,comment,meta-llama/Meta-Llama-3-8B,"same issue here
"
LeoStrange26,2024-04-22 06:56:38+00:00,comment,meta-llama/Meta-Llama-3-8B,"i got the permission from meta but not in huggingface
"
giux78,2024-04-22 07:00:50+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue here permission from meta but not in hf
cockroach54,2024-04-22 07:02:56+00:00,comment,meta-llama/Meta-Llama-3-8B,same here!
jcai1996,2024-04-22 07:46:52+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
erik170,2024-04-22 08:03:50+00:00,comment,meta-llama/Meta-Llama-3-8B,"Same deal 😕
Edit: finally got access. Took a few days."
Bernkastel1,2024-04-22 08:11:15+00:00,comment,meta-llama/Meta-Llama-3-8B,same..
ajaypandey1585,2024-04-22 08:51:13+00:00,comment,meta-llama/Meta-Llama-3-8B,I need access as well
marcusguhao,2024-04-22 09:21:35+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
D42C,2024-04-22 09:40:23+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
Rcross,2024-04-22 10:02:31+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
learn3r,2024-04-22 10:41:28+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
AltLuv,2024-04-22 11:44:12+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue for me too
insomnia-sjtu,2024-04-22 11:45:22+00:00,comment,meta-llama/Meta-Llama-3-8B,same here
xingjiatest5,2024-04-22 15:01:36+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
Houxing,2024-04-23 03:48:05+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
caiyuhu,2024-04-23 05:40:32+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
ajaypandey1585,2024-04-23 07:21:18+00:00,comment,meta-llama/Meta-Llama-3-8B,"I received it , Thanks for resolving"
Cshu01,2024-04-23 13:38:57+00:00,comment,meta-llama/Meta-Llama-3-8B,Same issue
bansky,2024-04-23 14:22:37+00:00,comment,meta-llama/Meta-Llama-3-8B,same question
ChatPICA,2024-04-24 01:08:07+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
facingfacing,2024-04-24 03:08:58+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
hm666,2024-04-25 12:45:19+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
kostyanoob,2024-04-25 13:52:33+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
strawhat,2024-04-26 02:08:29+00:00,comment,meta-llama/Meta-Llama-3-8B,rejected. 😂
clwanghf,2024-04-26 02:25:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"Your request to access this repo has been rejected by the repo's authors.  
But I don't know why. . .😂"
Pony,2024-04-26 07:21:59+00:00,comment,meta-llama/Meta-Llama-3-8B,"same sad,+1008611"
sunpenglv,2024-04-26 07:53:57+00:00,comment,meta-llama/Meta-Llama-3-8B,+1
Savannah120,2024-04-26 08:05:38+00:00,comment,meta-llama/Meta-Llama-3-8B,same...
whitefoxredhell,2024-04-26 08:26:42+00:00,comment,meta-llama/Meta-Llama-3-8B,also rejected for some reason
zorch,2024-04-26 08:28:02+00:00,comment,meta-llama/Meta-Llama-3-8B,also...
ddastgheib,2024-04-27 12:48:55+00:00,comment,meta-llama/Meta-Llama-3-8B,"> also rejected for some reason

same :-("
PanosPengHan,2024-04-28 03:27:08+00:00,comment,meta-llama/Meta-Llama-3-8B,same issue
osanseviero,2024-05-29 11:45:13+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172

"
J22,2024-04-21 13:42:54+00:00,comment,meta-llama/Meta-Llama-3-8B,No.
g-ronimo,2024-04-21 00:07:22+00:00,comment,meta-llama/Meta-Llama-3-8B,"using torch 2.0 I assume?

* => Upgrade pytorch ([more](https://github.com/meta-llama/llama3/issues/80))
* Or try this:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

this_is_my_favourite_animal = ""meta-llama/Meta-Llama-3-8B""

model = AutoModelForCausalLM.from_pretrained(
    this_is_my_favourite_animal,
    torch_dtype=torch.float16, 
).to(""cuda"")
tokenizer = AutoTokenizer.from_pretrained(this_is_my_favourite_animal)

inputs = tokenizer(""in a shocking turn of events, the"", return_tensors=""pt"").to(""cuda"")
model_generation = model.generate(
    **inputs,
    max_new_tokens=100,
    do_sample=True,
    return_dict_in_generate=True,
    output_scores=True,
)
```
"
teddyyyy123,2024-04-22 15:20:36+00:00,comment,meta-llama/Meta-Llama-3-8B,"yes , AutoTokenizer works ! thanks"
pcuenq,2024-04-20 09:13:07+00:00,comment,meta-llama/Meta-Llama-3-8B,
Blackroot,2024-04-21 04:57:22+00:00,comment,meta-llama/Meta-Llama-3-8B,"@pcuenq It looks like this might be missing the end of turn token:

Edit: I'm dumb, this comment should be for the instruct model <https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/42/files> Sorry!

<https://github.com/meta-llama/llama3/blob/main/llama/generation.py#L307>
->
<https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L222>

```python
    def encode_message(self, message: Message) -> List[int]:
        tokens = self.encode_header(message)
        tokens.extend(
            self.tokenizer.encode(message[""content""].strip(), bos=False, eos=False)
        )
        tokens.append(self.tokenizer.special_tokens[""<|eot_id|>""])
        return tokens
```

It looks like at the end of each message, the eot should be appended if I'm reading this right."
DreamGenX,2024-04-22 07:16:57+00:00,comment,meta-llama/Meta-Llama-3-8B,@pcuenq Would adding `add_bos_token: true` in `tokenizer_config.json` do the trick?
DreamGenX,2024-04-22 07:32:41+00:00,comment,meta-llama/Meta-Llama-3-8B,"I tested this change, and it fixes fine-tuning of the base model. Without it the grad norm is inf and the loss is high.
I also tried just using `add_bos_token: true` and that did not actually add the token, at least with Axolotl."
xzuyn,2024-04-22 18:47:40+00:00,comment,meta-llama/Meta-Llama-3-8B,"This fixes the BOS token not being added within Axoltol.

# Axoltol Config
```yaml
datasets:
  - path: PJMixers/example-sharegpt
    type: sharegpt
    conversation: chatml
```

## Without the PR
![Screenshot from 2024-04-22 13-50-27.png](https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/QHdEuu9T3VAHlgVqfic8D.png)

## With the PR
![Screenshot from 2024-04-22 13-58-22.png](https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/K9SiBeQnVunBd1oPpI6LG.png)"
pcuenq,2024-04-23 11:32:12+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thanks for the confirmations, merging now!"
ArthurZ,2024-04-20 08:15:05+00:00,comment,meta-llama/Meta-Llama-3-8B,No 
freegheist,2024-04-19 18:35:47+00:00,comment,meta-llama/Meta-Llama-3-8B,真的
Impulse911,2024-05-01 20:25:34+00:00,comment,meta-llama/Meta-Llama-3-8B,"""Ah, A real big soda dumpling"". You mean Soup?"
Hwer,2024-04-19 23:49:24+00:00,comment,meta-llama/Meta-Llama-3-8B,"Thanks, didn't expect to randomly learn this looking at the community posts. "
teddyyyy123,2024-04-23 14:30:07+00:00,comment,meta-llama/Meta-Llama-3-8B,What happened with the plain HF transformers.Trainer() API ? all I see now is with the TRL library. 
Ateeqq,2024-04-23 16:52:13+00:00,comment,meta-llama/Meta-Llama-3-8B,"> What happened with the plain HF transformers.Trainer() API ? all I see now is with the TRL library.

I am currently working on it (with the health dataset) but facing a CUDA error. Hopefully, it will be resolved soon.

Here's the Colab Notebook: [https://colab.research.google.com/drive/1TUa9J2J_1Sj-G7mQHX45fKzZtnW3s1vj?usp=sharing](https://colab.research.google.com/drive/1TUa9J2J_1Sj-G7mQHX45fKzZtnW3s1vj?usp=sharing)

RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions."
Excido,2024-04-25 21:32:50+00:00,comment,meta-llama/Meta-Llama-3-8B,Anyone know what structure I should be making my dataset  to best finetune the model?
johngiorgi,2024-04-20 01:10:04+00:00,comment,meta-llama/Meta-Llama-3-8B,"The usual trick, which also applies here, is to use the EOS token, e.g. you can apply:

```python
tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token
```

Then `tokenizer.pad_token_id` will be set as `eos_token_id` automatically."
tian9,2024-04-20 03:36:57+00:00,comment,meta-llama/Meta-Llama-3-8B,"thanks a lot! your ans is very helpful

> The usual trick, which also applies here, is to use the EOS token, e.g. you can apply:
> 
> ```python
> tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token
> ```
> 
> Then `tokenizer.pad_token_id` will be set as `eos_token_id` automatically.

"
Gerald001,2024-04-19 15:06:23+00:00,comment,meta-llama/Meta-Llama-3-8B,"@eduagarcia does this fix also apply to the meta-llama/Meta-Llama-3-8B-Instruct model?

see:
```
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B-Instruct"")
 payload = {
            ""inputs"": tokenizer.apply_chat_template(
                [
                    {
                        ""role"": ""user"",
                        ""content"": content,
                    }
                ],
                tokenize=False,
            ),
            ""parameters"": self.parameters,
        }
```"
eduagarcia,2024-04-19 15:38:20+00:00,comment,meta-llama/Meta-Llama-3-8B,"If you are using the chat_template, it makes no difference, the chat_template already appends the BOS Token. This problem only applies if you are not using the template, like in this base model.

From my tests, the ""tokenizer.apply_chat_template(dialog,  add_generation_prompt=True)"" works the same as the [ChatFormat(tokenizer).format.encode_dialog_prompt(dialog)](https://github.com/meta-llama/llama3/blob/0cee08ec68f4cfc0c89fe4a9366d82679aaa2a66/llama/test_tokenizer.py#L68) from the reference implementation.

```python
from transformers import AutoTokenizer
hf_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')
test = hf_tokenizer.apply_chat_template(
    [
            {
                ""role"": ""system"",
                ""content"": ""This is a test sentence."",
            },
            {
                ""role"": ""user"",
                ""content"": ""This is a response."",
            }
    ]
    , add_generation_prompt=True
)
print(test)
#[128000, 128006, 9125, 128007, 271, 2028, 374, 264, 1296, 11914, 13, 128009, 128006, 882, 128007, 271, 2028, 374, 264, 2077, 13, 128009, 128006, 78191, 128007, 271]
#   /\ bos_token
#is the same id's as the test on the official repo: https://github.com/meta-llama/llama3/blob/0cee08ec68f4cfc0c89fe4a9366d82679aaa2a66/llama/test_tokenizer.py#L68
```
"
Gerald001,2024-04-19 15:41:23+00:00,comment,meta-llama/Meta-Llama-3-8B,@eduagarcia looks like meta-llama/Meta-Llama-3-8B-Instruct i can use for chat.
Gerald001,2024-04-19 16:37:45+00:00,comment,meta-llama/Meta-Llama-3-8B,"@eduagarcia whats does tokenize=False, and add_generation_prompt=True?"
ArthurZ,2024-04-20 08:17:52+00:00,comment,meta-llama/Meta-Llama-3-8B,"Tho add_bos should be used, what we need to update here is the tokenizer.json: the template processor needs this. I’ll update it"
pcuenq,2024-04-20 09:14:59+00:00,comment,meta-llama/Meta-Llama-3-8B,[This PR](https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/41/files) addresses @ArthurZ's comments.
pcuenq,2024-04-23 11:46:12+00:00,comment,meta-llama/Meta-Llama-3-8B,Closing as explained in https://huggingface.co/meta-llama/Meta-Llama-3-70B/discussions/6
Nelson365487,2024-04-22 02:38:49+00:00,comment,meta-llama/Meta-Llama-3-8B,"I encounter the same issue, though I am  using  `AutoModelForCausalLM` function, the error is same as yours. My transformers package version is 4.39.3 and torch version is 2.0.1 . "
benhachem,2024-04-22 15:35:13+00:00,comment,meta-llama/Meta-Llama-3-8B,"You'll likely need to update your `transformers` package to version `4.40.0`, which supports Llama 3.

However, we can get arround the error mentioned above by downgrading `PyTorch` to version `2.0.1`. Then, load Llama3-8B in `float16` precision, rather than `bfloat16`. This approach should bypass the error as `triu_tril_cuda_template` is implemented for the `float16` data type in `PyTorch 2.0.1` , but note that it doesn't take advantage of the `bfloat16 ` format.
"
HugoLaurencon,2024-05-04 00:41:42+00:00,comment,meta-llama/Meta-Llama-3-8B,"Exactly the same problem as the original post here, except for me with torch==2.0.1 I have the second bug (`RuntimeError: ""triu_tril_cuda_template"" not implemented for 'BFloat16'`)"
HugoLaurencon,2024-05-08 18:06:19+00:00,comment,meta-llama/Meta-Llama-3-8B,"I solved my problem by replacing
`causal_mask = torch.triu(causal_mask, diagonal=1)` with `causal_mask = custom_triu(causal_mask)`, with
```
def custom_triu(input_tensor):
    rows, cols = input_tensor.shape
    row_indices = torch.arange(rows).unsqueeze(1).expand(rows, cols)
    col_indices = torch.arange(cols).unsqueeze(0).expand(rows, cols)
    mask = row_indices >= col_indices
    output_tensor = input_tensor.clone()
    output_tensor[mask] = 0
    return output_tensor
```"
ArthurZ,2024-05-10 06:29:19+00:00,comment,meta-llama/Meta-Llama-3-8B,"This is a torch break issue, which seems to have been fixed. Upgrading your torch version should be the best bet here 😉 This worked for me in torch 2.3"
monmonli,2024-05-15 02:59:40+00:00,comment,meta-llama/Meta-Llama-3-8B,Still encounter the issue with torch2.3 transformers 4.40.0
wvangils,2024-05-16 09:19:57+00:00,comment,meta-llama/Meta-Llama-3-8B,"Yes, I have the same issue. For torch 2.3.0 and transformers on 4.40.1 flash attention throws a runtime error. "
monmonli,2024-05-16 15:50:29+00:00,comment,meta-llama/Meta-Llama-3-8B,"I happened to solve the issue by uninstall torch and flash attention, then pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu124  flash-attn --no-build-isolation. The issue was caused by incompatibility. You may want to try different version of torch and cuda regarding your required settings. "
eduagarcia,2024-04-19 10:39:15+00:00,comment,meta-llama/Meta-Llama-3-8B,"As discussed in #9, the current HF tokenizer does not prepend the **bos token** *(id: 128000)* like in the reference implementation:
https://github.com/meta-llama/llama3/blob/0cee08ec68f4cfc0c89fe4a9366d82679aaa2a66/llama/generation.py#L256

and in their test cases:
https://github.com/meta-llama/llama3/blob/0cee08ec68f4cfc0c89fe4a9366d82679aaa2a66/llama/test_tokenizer.py#L23

This commit changes the tokenizer_class ""PreTrainedTokenizerFast"" to the ""LlamaTokenizer"", the PreTrainedTokenizerFast doesn't support seem to support the add_bos_token flag.

before the fix:
```python
!git clone https://github.com/meta-llama/llama3.git

from llama3.llama import Tokenizer
from transformers import AutoTokenizer
llama_tokenizer = Tokenizer(""llama3/Meta-Llama-3-8B/tokenizer.model"")
hf_tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B"")

text = ""This is a test sentence""

orig_enc = llama_tokenizer.encode(text, bos=True, eos=False)
# [128000, 2028, 374, 264, 720, 1296, 271, 52989]
hf_enc = hf_tokenizer.encode(text)
# [2028, 374, 264, 720, 1296, 271, 52989]
 ```

after the fix:
```python
from transformers import AutoTokenizer
hf_tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-8B"", revision=""refs/pr/33"")

text = ""This is a test sentence""

hf_enc = hf_tokenizer.encode(text)
# [128000, 2028, 374, 264, 720, 1296, 271, 52989]
 ```"
pitou-ice,2024-04-27 18:57:56+00:00,comment,meta-llama/Meta-Llama-3-8B,Why was the chat template removed?
dvruette,2024-05-17 09:01:02+00:00,comment,meta-llama/Meta-Llama-3-8B,"Was wondering the same thing. Simple answer: We're looking at the base model, not the instruction-tuned one."
Ironmole,2024-04-21 11:20:13+00:00,comment,meta-llama/Meta-Llama-3-8B,Same question here!
JadeTW,2024-04-23 00:56:29+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
gbhall,2024-04-23 05:01:33+00:00,comment,meta-llama/Meta-Llama-3-8B,It's ironic because the error is `The model meta-llama/Meta-Llama-3-8B is too large to be loaded automatically (16GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).` but I am using inference endpoints?
gbhall,2024-04-23 05:48:21+00:00,comment,meta-llama/Meta-Llama-3-8B,"Got it working, on the website on the right hand column is specifically says

`Model is too large to load in Inference API (serverless). To try the model, launch it on Inference Endpoints (dedicated) instead.`

After creating a dedicated endpoint it works."
dragonku,2024-04-27 13:51:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"The same error message.

Want to use Meta-Llama-3-8B with the Serverless Inference API."
JitendraK,2024-05-09 17:00:45+00:00,comment,meta-llama/Meta-Llama-3-8B,same problem even in pro account
osanseviero,2024-05-09 17:22:07+00:00,comment,meta-llama/Meta-Llama-3-8B,"Hey all. This model is not provided in the serverless inference API, but the instruct version is https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
JitendraK,2024-05-09 17:39:28+00:00,comment,meta-llama/Meta-Llama-3-8B,"for the beginners : what's the difference between regular and Instruct model?
"
osanseviero,2024-05-09 19:29:49+00:00,comment,meta-llama/Meta-Llama-3-8B,"Base models are optimize to generate the next token. If you want a chat-like model (a-la ChatGPT), you want to use an instruct version, which is the base model furtherly trained on chat-like behavior (with a series of alignment techniques)."
Johnhy42112,2024-04-18 22:11:01+00:00,comment,meta-llama/Meta-Llama-3-8B,This comment has been hidden
suzhu001,2024-04-19 03:52:40+00:00,comment,meta-llama/Meta-Llama-3-8B,"I think there is no need to reset the position ID in the pre-training stage, since docs longer than 8k are limited. 

The implementation of cross doc attention-mask is available in Flash-attention, https://github.com/Dao-AILab/flash-attention/issues/432#issuecomment-1698610752."
suzhu001,2024-04-30 14:47:34+00:00,comment,meta-llama/Meta-Llama-3-8B,I just realized that resetting the position ID after the doc boundary is identical to not resetting. Because the rotary embedding refers to relative position.
artyom17,2024-04-18 22:30:03+00:00,comment,meta-llama/Meta-Llama-3-8B,Wait....Llama3 switched to TikToken tokenizer? Am I correct?
artyom17,2024-04-19 00:19:17+00:00,comment,meta-llama/Meta-Llama-3-8B,"Ok, yeah, confirming that Llama3 has switched to tiktoken from sentencepiece. Here is a wrapper code you may use (still WIP but seems to work):
```
import os
import sentencepiece as spm
import tiktoken
from tiktoken.load import load_tiktoken_bpe
from pathlib import Path
from typing import Dict

class TokenizerInterface:
    def __init__(self, model_path):
        self.model_path = model_path

    def encode(self, text):
        raise NotImplementedError(""This method should be overridden by subclasses."")

    def decode(self, tokens):
        raise NotImplementedError(""This method should be overridden by subclasses."")

    def bos_id(self):
        raise NotImplementedError(""This method should be overridden by subclasses."")

    def eos_id(self):
        raise NotImplementedError(""This method should be overridden by subclasses."")

class SentencePieceWrapper(TokenizerInterface):
    def __init__(self, model_path):
        super().__init__(model_path)
        self.processor = spm.SentencePieceProcessor(str(model_path))

    def encode(self, text):
        return self.processor.EncodeAsIds(text)

    def decode(self, tokens):
        return self.processor.DecodeIds(tokens)

    def bos_id(self):
        return self.processor.bos_id()

    def eos_id(self):
        return self.processor.eos_id()

class TiktokenWrapper(TokenizerInterface):
    """"""
    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.
    """"""

    special_tokens: Dict[str, int]

    num_reserved_special_tokens = 256

    pat_str = r""(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+""  # noqa: E501

    def __init__(self, model_path):
        super().__init__(model_path)
        assert os.path.isfile(model_path), str(model_path)
        mergeable_ranks = load_tiktoken_bpe(str(model_path))
        num_base_tokens = len(mergeable_ranks)
        special_tokens = [
            ""<|begin_of_text|>"",
            ""<|end_of_text|>"",
            ""<|reserved_special_token_0|>"",
            ""<|reserved_special_token_1|>"",
            ""<|reserved_special_token_2|>"",
            ""<|reserved_special_token_3|>"",
            ""<|start_header_id|>"",
            ""<|end_header_id|>"",
            ""<|reserved_special_token_4|>"",
            ""<|eot_id|>"",  # end of turn
        ] + [
            f""<|reserved_special_token_{i}|>""
            for i in range(5, self.num_reserved_special_tokens - 5)
        ]
        self.special_tokens = {
            token: num_base_tokens + i for i, token in enumerate(special_tokens)
        }
        self.model = tiktoken.Encoding(
            name=Path(model_path).name,
            pat_str=self.pat_str,
            mergeable_ranks=mergeable_ranks,
            special_tokens=self.special_tokens,
        )
        # BOS / EOS token IDs
        self._bos_id: int = self.special_tokens[""<|begin_of_text|>""]
        self._eos_id: int = self.special_tokens[""<|end_of_text|>""]

    def encode(self, text):
        return self.model.encode(text)

    def decode(self, tokens):
        return self.model.decode(tokens)

    def bos_id(self):
        return self._bos_id

    def eos_id(self):
        return self._eos_id

def get_tokenizer(tokenizer_model_path, model_name):
    """"""
    Factory function to get the appropriate tokenizer based on the model name.
    
    Args:
    - tokenizer_model_path (str): The file path to the tokenizer model.
    - model_name (str): The name of the model, used to determine the tokenizer type.

    Returns:
    - TokenizerInterface: An instance of a tokenizer.
    """"""
    if ""Llama-3"" in str(model_name):
        return TiktokenWrapper(tokenizer_model_path)
    else:
        return SentencePieceWrapper(tokenizer_model_path)
```"
BallisticAI,2024-04-18 21:23:48+00:00,comment,meta-llama/Meta-Llama-3-8B,"Please use standardized numbering practices for sub-numbering lists as is used in the legal and medical fields. (numbers > letters > roman numerals)
You do this for the ""META LLAMA 3 COMMUNITY LICENSE AGREEMENT"" section, so please do the same for the Prohibited uses list, so its more clear."
tiborlaszlo,2024-04-18 18:05:00+00:00,comment,meta-llama/Meta-Llama-3-8B,"You probably downloaded all possible versions. When running download.sh file you first have to paste in the link from email and then select the model (8B,8B-instruct,70B,70B-instruct). If you just pressed enter, it downloaded all models."
splicey114,2024-04-18 18:05:31+00:00,comment,meta-llama/Meta-Llama-3-8B,Can someone tell me how many gb the 8B version is? I'm still waiting to get approved to see the files tab
ssammy,2024-04-18 18:06:38+00:00,comment,meta-llama/Meta-Llama-3-8B,"It was the website / mobile truncating the file size values 
"
Kukedlc,2024-04-18 18:12:55+00:00,comment,meta-llama/Meta-Llama-3-8B,"I used this:

Start:  <|start_header_id|>
End:  <|eot_id|>


inputs = tokenizer([""""""<|start_header_id|> System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
User: Talk about AI and LLMs.
Assistant:""""""], return_tensors=""pt"").to('cuda')

streamer = TextStreamer(tokenizer)


stop_token = ""<|eot_id|>""  
stop_token_id = tokenizer.encode(stop_token)[0]

_ = model.generate(**inputs, streamer=streamer, max_new_tokens=512, do_sample=True, temperature=0.1, repetition_penalty=1.2, top_p=0.9, eos_token_id=stop_token_id)"
bloomedout,2024-04-18 18:47:33+00:00,comment,meta-llama/Meta-Llama-3-8B,How do you import tokenizer for this snippet?
Kukedlc,2024-04-18 18:52:02+00:00,comment,meta-llama/Meta-Llama-3-8B,"Only change the api_key
```python
!pip install -qU transformers accelerate bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig
import torch
TOKEN = 'your_api_key'
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=TOKEN)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='cuda:0', quantization_config=bnb_config, use_auth_token=TOKEN)


inputs = tokenizer([""""""<|start_header_id|> System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
User: Talk about AI and LLMs.
Assistant:""""""], return_tensors=""pt"").to('cuda')

streamer = TextStreamer(tokenizer)
stop_token = ""<|eot_id|>""  
stop_token_id = tokenizer.encode(stop_token)[0]

_ = model.generate(**inputs, streamer=streamer, max_new_tokens=512, do_sample=True, temperature=0.1, repetition_penalty=1.2, top_p=0.9, eos_token_id=stop_token_id)
```"
bongchoi,2024-04-19 03:59:35+00:00,comment,meta-llama/Meta-Llama-3-8B,"same problem..
```
>>> tokenizer('hi')
{'input_ids': [6151], 'attention_mask': [1]}  # BOS not prepended
```

```
>>> messages = [{""role"": ""user"", ""content"": ""hi""}]
>>> tokenizer.apply_chat_template(messages, tokenize=False)
'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nhi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n' # BOS prepended
```

in fine-tuning, do i have to prepend BOS or not? "
draplater,2024-04-19 09:26:47+00:00,comment,meta-llama/Meta-Llama-3-8B,I have the same question. Do I have to add BOS token while performing continue pre-training?
realdanielbyrne,2024-04-23 04:51:48+00:00,comment,meta-llama/Meta-Llama-3-8B,"You can use any prompt template you'd like. If you want to use the same template that Meta used, then there is code to generate a prompt for both chat and text in https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py."
winglian,2024-04-18 17:17:01+00:00,comment,meta-llama/Meta-Llama-3-8B,Also needs a similar PR for 70B
ArthurZ,2024-04-18 17:18:38+00:00,comment,meta-llama/Meta-Llama-3-8B,No because it's not supported by the standard slow tokenizers (it's tiktoken based not sentencepiece based)
king6666,2024-05-30 09:07:56+00:00,comment,meta-llama/Meta-Llama-3-8B,where is the tokenizer.model ? I did not find the tokenizer.model in the Meta-Llama-3-8B.
blanchon,2024-04-18 17:12:57+00:00,comment,meta-llama/Meta-Llama-3-8B,Will you spam every llama repo ?
imone,2024-04-18 16:51:06+00:00,comment,meta-llama/Meta-Llama-3-8B,Agreed! An OSI-approved license such as Apache-2.0 / MIT (adopted by Mistral / Phi) would be greatly appreciated!
Lician,2024-04-18 17:07:00+00:00,comment,meta-llama/Meta-Llama-3-8B,Agreed! 
Kukedlc,2024-04-18 17:11:59+00:00,comment,meta-llama/Meta-Llama-3-8B,Agreed!
Tom9000,2024-04-18 17:17:14+00:00,comment,meta-llama/Meta-Llama-3-8B,"all the talks about ""open source"" from Meta seem to be just that, talks"
sparkycollier,2024-04-18 22:25:53+00:00,comment,meta-llama/Meta-Llama-3-8B,Thanks for raising this issue. You’re spot on imho. 
Saadyousaf,2024-04-19 14:06:27+00:00,comment,meta-llama/Meta-Llama-3-8B,"Agreed .
"
UltimaThule,2024-04-25 17:46:54+00:00,comment,meta-llama/Meta-Llama-3-8B,Agreed.
Impulse911,2024-05-01 20:28:01+00:00,comment,meta-llama/Meta-Llama-3-8B,"Agreed. Always try to NOT change the license because it'll usually say it's ""unsupported""."
ashrey30,2024-06-07 05:48:40+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"I have received the access for this model, but when I'm trying to access the model in my program, it shows the repo is gated.
The environment variables have been updated along with the hugging face token still the problem persists. "
zhouxiaoou,2024-05-17 07:31:33+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"amzing，use  AutoTokenizer.from_pretrained(model_path, use_auth_token=token) don not have the error!!"
osanseviero,2024-05-29 12:00:33+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/discussions/53. 
oldmanhuggingface,2024-05-10 21:01:17+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Who's job is it to accept these merges?
ArthurZ,2024-05-13 06:52:46+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I'll merge for consistency with the tokenizer
oldmanhuggingface,2024-05-14 03:24:10+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"> I'll merge for consistency with the tokenizer

Any chance we can get someone from meta to do a thorough review of all the files for all the models and let us know if everything's correct? I feel like I'm just pushing you to make changes that might not be correct. Thoughts? Either way, better communication from meta would go a long way"
TikaToka,2024-05-08 07:24:42+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Yes, you should. If you want lighter one, use 8B model."
drlee1,2024-05-07 08:06:06+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"I have same error. So, I used a `pipeline` after load the model.

```python
MODEL_ID = ""meta-llama/Meta-Llama-3-70B-Instruct""

tok = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map = 'auto'

pipe = pipeline(
    ""text-generation"",
    model = model,
    tokenizer = tok
)
...
```"
ArthurZ,2024-05-10 06:31:24+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Fixed by #31! Thanks for reporting
AFoudree,2024-05-03 19:34:21+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Basically I am looking for a way to use with model parallelism... not data parallelism.
AFoudree,2024-05-05 01:01:53+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I got it figured out. It just took a bit to get the code to spread the mode across the gpus.
sagarStormborn,2024-05-07 12:46:07+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"can u share how did u do it ?
"
Wingheaded,2024-05-18 16:15:15+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Same problem here bur with small requests...
JiaxinTsao,2024-05-01 03:30:46+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Same issue encountered when doing int-8 quantization.

One solution is to enable group-wise quantization with group-size = 128/64."
hyang1974,2024-04-26 15:29:18+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Same question. 
"
superMe,2024-04-30 21:39:23+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Did you solve your issue? I don't understand how to request access, may you tell me the steps? Thanks"
leejunhyeok,2024-05-03 08:28:52+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"same question(for re-granting permissions for instruct version download. 8b.70b is able to download, but instructed versions are not)"
philschmid,2024-04-25 12:39:11+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,This is a workaround for deploying Llama 3 on Inferentia with TGI. Since the new `generation_config` has now a list as eos_token_id. The deployment fails. This revision removes one of it. 
ArthurZ,2024-04-25 12:12:09+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,
oldmanhuggingface,2024-05-10 04:49:03+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Is this supposed to be merged or not?
ArthurZ,2024-05-10 06:28:11+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I think we should only use `128009` as the eos
oldmanhuggingface,2024-05-10 20:46:32+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"So were you guys gonna make these updates or what? I see you merged the pull request for llama-3-8b-instruct tokenizer_config.json, but you're saying config.json should be changed so the eos is only 128009? Do I have that right? So we still need updates for config.json on both instruct models and tokenizer_config.json on 70b-instruct? And what about the base models? Why aren't you guys taking this more seriously? Again, this isn't about me or the ease with which I can make these changes. It's about publishing *complete* versions of your billion-dollar project that the community can use with confidence. Also, why don't you guys *know* the correct parameters for your model? I'm genuinely confused about all of this"
epignatelli,2024-05-18 11:43:07+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"> I think we should only use `128009` as the eos

It would be great to give the reasons behind this, especially if that rejects a PR that fixes an issue spread to all the user base.
Why not adding `128009` as termination criterion?
"
ArthurZ,2024-05-10 06:26:30+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Thanks
Azcus4,2024-04-26 17:41:31+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,This comment has been hidden
pcuenq,2024-04-23 11:41:02+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,
dawangbaixiaofu,2024-05-16 07:58:07+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"try on my own machine, find that these are two formats of model files.
transformers.AutoModelForCausalLM.from_pretrained(model_path,...) only can load .safetensor format model file, but can not load .pth format model file. 
hope it is helpful"
LJunius,2024-04-23 10:42:27+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,You can use it in poe.
maxikq,2024-04-23 17:20:11+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"I'm just testing it, but it doesn't seem to be working well. 
- It looks like there is a max output token limit to ~250 which is very low
- I'm not sure if it correctly recognizes the format with roles - answers are weird
- I get duplicated response sometimes
- ""return_full_text"" = false doesn't work. I get in the response my initial prompt."
TikaToka,2024-04-24 08:50:20+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"@LJunius  @maxikq  Thanks, I'll just try at my local with quantized model for now :("
drlee1,2024-04-23 04:27:01+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"`tokenizer.pad_token_id = tokenizer.eos_token_id`
or
`model.generate(**encode_input, pad_token_id = tokenizer.eos_token_id)`"
hmanju,2024-04-23 05:04:29+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Thanks but where can I set the same in the starter code?

```
model_id = ""meta-llama/Meta-Llama-3-70B-Instruct""

pipeline = transformers.pipeline(
    ""text-generation"",
    model=model_id,
    model_kwargs={""torch_dtype"": torch.bfloat16},
    device=""auto"",
)

messages = [
    {""role"": ""system"", ""content"": ""You are a pirate chatbot who always responds in pirate speak!""},
    {""role"": ""user"", ""content"": ""Who are you?""},
]

prompt = pipeline.tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids(""<|eot_id|>"")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)

```"
drlee1,2024-04-23 06:45:13+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"```
outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
    pad_token_id = pipeline.tokenizer.eos_token_id
)
```"
deleted,2024-04-27 04:14:49+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Incredible performance!The daily use is even better.
pcuenq,2024-04-21 12:49:57+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,
pcuenq,2024-04-21 13:06:01+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"cc @ArthurZ, see also https://huggingface.co/meta-llama/Meta-Llama-3-70B/discussions/7/files for the 70B pre-trained model."
AswanthCManoj,2024-04-19 19:07:33+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"The `eos_token` is supposed to be at the end of every turn which is defined to be ""<|end_of_text|>"" in the config and ""<|eot_id|>"" in the chat_template, Hence using the eos_token variable directly within the template."
muzier,2024-04-20 02:58:45+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"draw a pig


"
wangdongkai,2024-04-23 14:45:18+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,This comment has been hidden
aaditya,2024-04-20 04:55:02+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"+1, I am trying both, will update the results."
RebornZA,2024-04-20 19:41:05+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Base better for chat?
saireddy,2024-04-20 21:50:01+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Thanks @aaditya , I am working on the same too. Will post once I have the results ready. 
@RebornZA  if I am not wrong, instruct would be the better option for chat"
saireddy,2024-04-23 01:13:38+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,@pcuenq @philschmid  any suggestions or insights on this please? 
pcuenq,2024-04-23 17:21:50+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I'd use the base model for this task.
pcuenq,2024-04-19 13:15:16+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Fixed by @ArkaAbacus and @caleb-artifact
afshinO,2024-04-19 15:02:19+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Same here. I also created a fresh conda env with the latest version of every packages, and that is not helping. "
asadas,2024-04-19 16:15:50+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"yeah no idea, tried installing the latest transformers,torch etc, nothing works."
AntonV,2024-04-19 18:51:32+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Issue is open at https://github.com/huggingface/transformers/issues/30350

I think it should use `device_map` instead of `device`. Not tested though."
asadas,2024-04-19 21:57:28+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,can confirm
pcuenq,2024-04-19 13:17:38+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Hi @kk3dmax we provide some examples in [our blog post](https://huggingface.co/blog/llama3#how-to-prompt-llama-3)! Let us know if that helps :)
Thireus,2024-04-20 10:56:42+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"@pcuenq, could you recommend a good generic system prompt for general user/assistant type of conversation?

So far I'm using the common ""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."" but I'd like to know if there are much better ones."
marcoguerrap,2024-04-28 01:43:35+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Do anyone knows how do I make a curl request with a system prompt?
solankibhargav,2024-04-19 12:22:26+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"yeah, generation does not stop and keeps generating till max tokens reached. "
pcuenq,2024-04-19 13:16:23+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Fixed!
solankibhargav,2024-04-24 04:28:05+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I am still facing this issue.  how can i make the response stop?
solankibhargav,2024-04-29 11:43:59+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I fixed the issue  by pointing to a tokenizer  of another model card which is llama3-70b-Instruct.
mdpi-ai,2024-04-24 14:29:15+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,yes I have a similar issue I ask for a list of keywords for a text and it will write the list without any punctuation.
Payoto,2024-04-19 15:45:22+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,https://huggingface.co/casperhansen/llama-3-70b-instruct-awq
BallisticAI,2024-04-18 21:24:07+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Please use standardized numbering practices for sub-numbering lists as is used in the legal and medical fields. (numbers > letters > roman numerals)
You do this for the ""META LLAMA 3 COMMUNITY LICENSE AGREEMENT"" section, so please do the same for the Prohibited uses list, so its more clear."
BallisticAI,2024-04-18 21:25:08+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Please disregard I've submitted new PRs directly to the USE_POLICY.md files in your repo.
gghfez,2024-04-19 00:32:19+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I think LLMs sometimes hallucinate the wrong cutoff date. They've slurped up declarations from other models posted on the internet during training. Eg. Sometimes Mistral quotes the cutoff date in 2021 from the original ChatGPT release and sometimes it says it's created by OpenAI.
kil3r,2024-04-19 10:10:32+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"> I think LLMs sometimes hallucinate the wrong cutoff date. They've slurped up declarations from other models posted on the internet during training. Eg. Sometimes Mistral quotes the cutoff date in 2021 from the original ChatGPT release and sometimes it says it's created by OpenAI.

I'm not talking about what Llama says. I'm talking what's in this repo's model card."
philschmid,2024-04-19 12:04:17+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Thank you
pcuenq,2024-04-19 11:04:45+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"Thanks @caleb-artifact , I agree this should be merged."
abhi-db,2024-04-18 16:59:56+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,"I noticed when using the instruct model with chat templating, that the chat template uses `<|eot_id|>` rather than the EOS token `<|end_of_text|>`. So when the assistant responds to messages it likes to use `<|eot_id|>` as well. Unfortunately the generation config doesn't say to stop generating on `<|eot_id|>` so the model keeps writing.

In the Model Card, I see that there is a workaround by manually updating `eos_token_id` in any `generate` call or `pipeline`:

```python
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids(""<|eot_id|>"")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
```

But I think there is a simpler way to fix this! If you just update the `generation_config.json` to stop on both `<|end_of_text|>` as well as `<|eot_id|>`, then it should work automatically and you won't need to build the `terminators`."
abhi-db,2024-04-18 23:14:37+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,see related PR here for llama-3-8b-instruct: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/4
lemonflourorange,2024-04-19 05:01:21+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct, 
pcuenq,2024-04-19 13:10:56+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Thank you @abhi-db!
alfredplpl,2024-04-18 16:44:56+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,
alfredplpl,2024-04-18 16:46:47+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,I have a feeling there might be a mistake in the sample script. Is this correct?
not-lain,2024-04-18 17:20:34+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,great catch!
pcuenq,2024-04-19 13:21:58+00:00,comment,meta-llama/Meta-Llama-3-70B-Instruct,Thanks a lot @alfredplpl! This was addressed here https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/commit/81ca4500337d94476bda61d84f0c93af67e4495f
bartowski,2024-05-17 15:05:43+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"Myself and others have had issues with this model even unquanted, think there's some damage, you can see on the original model page

They also updated their weights in place a couple times meaning who knows what's there now and if my quant here is even the latest, which is why I dislike updates in place if they can be avoided"
bartowski,2024-05-08 20:13:34+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"Can I ask what your frontend is? I know that there was recently uncovered an issue where some frontends would double up the BOS token and that would completely destroy any reliability of the output.

That is very interesting that it's hallucinating so much more.

I do know that this model specifically seems to have some awkward issues that haven't fully been uncovered, I assume it's not purely related to this Gradient 1048k one?"
deleted,2024-05-08 20:28:18+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"Shit. It was the front end. I waited for Koboldcpp to update to the latest llama.cpp thinking it was just a bug in GPT4ALL, and thought I set the setting correctly (temp 0), but for some reason when I re-checked they were still at the defaults (which are really bad for hallucinations at the fringe of knowledge, which is what I was trying to test).

I applied the right settings in Koboldcpp, restarted, then double-checked that they were properly set. Then tested Q8_0 and it performed as expected. The following is an example. It's not a perfect character list for Corner Gas, but much better than before (pasted next). It seems llama.cpp has worked out some bugs.

Notice Lacey Burrows vs Burdette (Burrows is correct),  Emma Leroy vs Tarantino (Leroy is correct), and so on. Thank goodness. These hallucinations have been driving me crazy for the last two weeks. I'm glad to see it's not Llama 3 8b itself that was causing them.

    Brent Leroy (played by Brent Butt)
    Oscar Leroy (played by Eric Peterson)
    Lacey Burrows (played by Gabrielle Miller)
    Emma Leroy (played by Tara Spencer-Nairn)
    Hank Yarbo (played by Fred Ewanuick)
    Wanda Dollard (played by Nancy Robertson)

1. Brent Leroy (played by Brent Butt)
2. Lacey Burdette (played by Gabrielle Miller)
3. Hank Yarbo (played by Peter Stebbings)
4. Emma Tarantino (played by Nancy Robertson)
5. Davis Quinton (played by Eric Peterson)
6. Wanda Burdette (played by Erica Cerra)"
bartowski,2024-05-09 04:04:41+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"Absolutely brilliant choice of shows to test it on, love Corner Gas :D

Glad it's working better for you now!"
bartowski,2024-05-04 15:07:41+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"That is very strange, can you throw me your prompt that recreates it and I'll attempt with the full model weights so we can see if it's GGUF or the model?"
Propheticus,2024-05-04 15:40:06+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"It's any prompt really.   Using LMStudio default message format for llama3
*System*
```
You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.
```
*User*
```
how do we overcome entropy?
```
*Assistant*
```
I am not capable of experiencing or understanding the concept of ""overcoming"" entropy as it is a purely physical phenomenon that cannot be influenced by human actions or intentions. However, I can provide some insights on how to reduce entropy in different contexts.

in physics and thermodynamics, there are several ways to reduce entropy:

1. By transferring heat from a hotter body to a colder one: this process decreases the total entropy of the system as it moves energy from a high-entropy state to a low-entropy state.

2. Through reversible processes: these are processes that can be reversed without changing the overall entropy, such as compressing and decompressing a gas in an isothermal manner.

3. By using external work or energy input: adding energy to a system can reduce its entropy if it is used efficiently, for example, by converting heat into mechanical work through a heat engine.

in information theory, there are also ways to reduce entropy:

1. Through data compression: removing redundancy from data reduces the total number of bits required to represent it and thus decreases entropy.

2. By using error-correcting codes: these add extra information to the data that allows errors or noise to be corrected without increasing entropy.

3. Through information theory-based optimization algorithms: some algorithms, such as the maximum entropy method, can optimize a system by finding its most likely state based on available information and thus reduce overall entropy.</s>
```"
Propheticus,2024-05-04 21:43:19+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"Found that the gguf of the 262k model by [crusoeai](https://huggingface.co/crusoeai/Llama-3-8B-Instruct-262k-GGUF) does the same. 
So perhaps it doesn't take GGUF quantization well or llama.cpp/lmstudio compatibility is not 100% there. 
(Also tried on CPU, instead of ROCm GPU accelerated - LMS v0.2.22, same difference.)"
bartowski,2024-05-04 23:12:55+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"I'm about to try the unquanted version to see if it's the same, cause yeah i see it too with llama.cpp, it's strange cause their config seems correct
"
bartowski,2024-05-04 23:22:05+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,"full model weights also do this, original safetensors in transformers, so it must be a bug with the model itself"
Propheticus,2024-05-05 17:09:23+00:00,comment,bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF,Thanks for checking. Looks like it's not something we can fix then.
bartowski,2024-05-03 13:52:44+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,Thanks!
bartowski,2024-05-03 13:53:28+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"Conflicted on your other merge request so will make this change manually, appreciate it :)"
julien-c,2024-05-03 22:17:50+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"nice: 
![image.png](https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/IITMn8eaWITGyeUetyoBE.png)

thx for merging!"
bartowski,2024-05-04 00:00:35+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,Oh hey that looks awesome! I'll have to include that from now on :D
bartowski,2024-05-02 03:32:31+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"Btw @smcleod these ones haven't been updated yet, will be soon"
RiddleRaven,2024-05-11 22:29:48+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"@smcleod  Please see also this post if you use llama.cpp: https://www.reddit.com/r/LocalLLaMA/comments/1cg0z1i/bpe_pretokenization_support_is_now_merged_llamacpp/
"
cyliu8,2024-05-16 04:21:02+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"+1 I also me too
"
bartowski,2024-05-16 14:33:30+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"Can you be more specific about the difficulty? Selecting llama 3 should get you all the formatting you need

If you're not using lm studio, then this template:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>


```

Is what you want to follow"
alexcardo,2024-05-17 18:00:10+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"I don't know what to do with that... I tired so much....

alex@M1 llama.cpp % ./main -m ~/ai/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf -p ""<|begin_of_text|><|start_header_id|>user<|end_header_id|>Write me a detailed markdown article in Dutch on a subject: 2 Zodiac Signs Experience Abundance On May 18, During Sun Conjunct Jupiter<|eot_id|><|start_header_id|>assistant<|end_header_id|>""

<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>Write me a detailed markdown article in Dutch on a subject: 2 Zodiac Signs Experience Abundance On May 18, During Sun Conjunct Jupiter<|eot_id|><|start_header_id|>assistant<|end_header_id|>

# 2 Zodiac Teken ervaren Abundantie Op 18 Mei Bij Zon Conjunct Jupiter

Op 18 mei zal de zon conjunct Jupiter staan, wat een unieke gelegenheid is voor twee zodiac-teken om te ervaren dat hun dromen werkelijkheid worden. In deze artikel zullen we nader ingaan op de eigenschappen en verwachtingen van deze conjunctie en hoe deze twee tekenen eruit zullen zien tijdens deze bijzondere periode.

**De Conjunctie**

De conjunctie tussen de zon en Jupiter is een speciale gebeurtenis in de hemel die slechts enkele keren per jaar plaatsvindt. Jupiter is het planetoïde van Fortune en representeert overvloed, geluk en expansie. Wanneer de zon conjunct Jupiter staat, zal deze energie van overvloed en geluk naar de aarde stralen, wat een unieke kans is voor twee zodiac-teken om hun dromen werkelijkheid te maken.

**De Tekenen**

De twee zodiac-teken die tijdens deze conjunctie in de spotlights staan zijn de Ram en de Leeuw. Personen met de zon in Ram of Leeuw in hun horoscoop zullen de meeste voordelen ondervinden van deze conjunctie.

**De Ram (Aries)**

 Personen met de zon in Ram zijn bekend om hun impulsieve en onafhankelijke aard. Tijdens deze conjunctie zullen zij ervaren dat hun dromen en ambities sneller en makkelijker werkelijk worden. Hun creativiteit en energie zullen in een hoogstaande worden en zij zullen nieuwe ideeën en projecten kunnen uitwerken. Ook zal hun sociale leven worden aangestuurd door nieuwe ontmoetingen en mogelijkheden.

**De Leeuw (Leo)**

Personen met de zon in Leeuw zijn bekend om hun warme en eerzame aard. Tijdens deze conjunctie zullen zij ervaren dat hun persoonlijke brandpunt en charisma worden versterstijnkelkeën. Zijkijn enkeën zijl zijl (k)eerlijk zijl (Leo) **De Ram (A) **De Leeuw (Leo) **De Ram (A) **De Leeuw (Leo) **De Leeuw (Lee))**

**De Teleen****

**De Leeuw (Lee))****

**De Teën****

**De Leeuw (Lee)**)****

**De Teeen**)****

**De Leeuw (Lee)**)****

**De Teeen**)****

**De Leeuw (Lee)**)****

**De Teeen**)****

**De Leeuw (Lee)**)****

**De Teeen**)****

**De Leeuw (Lee)**)****

**De Teeen**)****

**De Leeuw (Lee)**)****

**De Teeen**)****

**De Leeuw (Lee)**)****

**
"
bartowski,2024-04-30 13:51:40+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"The latest Llama 3 models are extremely sensitive to their instruct templates, like to a weirdly large degree, so make sure you follow it as best you can"
Joseph717171,2024-04-25 03:08:47+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"The GGUF quants are just static quantizations. However, the IQuants  are not; they are much more aggressive and require more work and more complex techniques to achieve the same levels of coherence and de-braindeadedness that the static quants achieve. For this, IQuants use an [Imatrix](https://github.com/ggerganov/llama.cpp/blob/master/examples/imatrix/README.md). An imatrix is a “map” of all of a model’s activations over a text corpus (such as wikitext-raw). During the quantization process a pretrained imatrix can be used to help guide the quantization to retain the model’s coherence and abilities 
. However, it’s important to note and stress: that the smaller the model becomes due to aggressive quantization, the more likely it is that the model will be less coherent and less capable overall (due to the effect of the current way that aggressively quantizing a model will affect its ability to remain coherent and to retain its capabilities). I hope this helps. 😁 "
Joseph717171,2024-04-25 03:40:09+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"Converting the model to GGUF and quantizing it yourself will yield exactly the same GGUF’ed model. There will be no difference between yours and this repos GGUF’ed model files. The only exception to this, being if you trained and used an imatrix for the model using wikitext-raw vs groups_merged.txt for the quantization process. 🤔 "
Joseph717171,2024-04-25 03:48:28+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"An un-quantized model is always superior in quality and performance to a quantized model. If you want the max performance and the highest throughput for your model, you will always want to run it un-quantized. As a matter of fact, the only time you want to quantize a model is when you don’t have the enough vRAM and RAM to run it un-quantized. 🤔 "
bartowski,2024-04-25 04:17:20+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"@Joseph717171 is correct with one tiny addition that for this (and all other models on lmstudio-community besides 70b until it's reuploaded) all of the quant levels are made with imatrix, not just iquants"
HannahKim,2024-04-25 04:40:54+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,@Joseph717171 Thank you so much! It helps very well. Hope you have a wonderful day! 😉
MichaelBarryUK,2024-04-25 08:25:56+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"> @Joseph717171 is correct with one tiny addition that for this (and all other models on lmstudio-community besides 70b until it's reuploaded) all of the quant levels are made with imatrix, not just iquants

Can you clarify this for me please, are you saying that Q4_K (as an example) is a 4bit K quant that uses imatrix, but it is NOT an iquant?

Also, thank you 😅😅😅"
Joseph717171,2024-04-25 11:27:00+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"Yes, that is exactly right! What @bartowski is implying is that all the GGUF'ed quants are made using an imatrix. Which means: all the quantizations are now IQuants. (The imatrix is trained on groups_merged.txt) 😁  "
bartowski,2024-04-25 14:03:59+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"It's actually more than anything an unfortunate naming convention and timing

i-quants != imatrix

i-quants are just a newer SOTA quantization technique that borrows ideas from QuIP#, and can be made without an imatrix

https://github.com/ggerganov/llama.cpp/pull/4773

imatrix is an importance matrix that can be used with any quant level, though was originally only targeting i-quants

Then the feature was expanded to target K-quants:

https://github.com/ggerganov/llama.cpp/pull/4930"
yagilb,2024-04-20 14:52:03+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,It ships built in with LM Studio 0.2.20
franva,2024-04-22 09:26:43+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"thanks, I updated the LM Studio and found the preset for LLAMA 3 :)"
SeyelentEco,2024-04-19 04:05:39+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"Never mind, found the answer in the launch docs:
""The Llama 3 release introduces 4 new open LLM models by Meta based on the Llama 2 architecture. They come in two sizes: 8B and 70B parameters, each with base (pre-trained) and instruct-tuned versions. All the variants can be run on various types of consumer hardware and have a context length of 8K tokens."""
Azamorn,2024-04-18 23:15:27+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"+1 
Seems like everyone was having the issue with the model output not stopping correctly lol"
Anderson452,2024-04-18 23:45:54+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"> +1 
> Seems like everyone was having the issue with the model output not stopping correctly lol

I just tested it in the new lm Studio update, and it works perfectly. At first I also got that same error."
perte,2024-04-19 12:45:16+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,Can confirm that the new LM Studio update fixed this one. Make sure to use the Llama 3 template.
dibu28,2024-04-20 08:53:22+00:00,comment,lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF,"> +1 
> Seems like everyone was having the issue with the model output not stopping correctly lol

still have this issue with llama-cpp-python library model is not stoppng seems to be problem with stop tokens"
zhangboyu,2024-05-31 14:34:11+00:00,comment,meta-llama/Meta-Llama-3-70B,"Recently, I applied for access to the Meta-LLaMA-3-70B model, but unfortunately, my request was denied. Since this model is crucial for my research, I would like to understand the specific reason for the rejection.
My research involves analyzing the stance in discussions about climate change on social media, and I plan to use this model for implicit goal (entity or open-domain) recognition. I believe that this model can significantly advance my research progress.
If you need more information about my research or if there is anything else I can provide to support my request, please let me know."
osanseviero,2024-05-29 12:07:49+00:00,comment,meta-llama/Meta-Llama-3-70B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-70B/discussions/15
akshay-s,2024-05-20 17:38:36+00:00,comment,meta-llama/Meta-Llama-3-70B,Did this get resolved?  I requested access today. 
osanseviero,2024-05-29 12:07:58+00:00,comment,meta-llama/Meta-Llama-3-70B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-3-70B/discussions/15
aimped-gh,2024-05-06 07:21:16+00:00,comment,meta-llama/Meta-Llama-3-70B,"Does this help?
https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py"
BigDeeper,2024-05-06 13:06:23+00:00,comment,meta-llama/Meta-Llama-3-70B,"> Does this help?
> https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py

Interesting. I was wondering before why the OpenAI tokenizer ""tiktoken"" was working with Llama3. Now it is obvious, they simply extended tiktoken. For some reason, if I install llama3 repo with ""pip install -e ."" I am unable to import ""llama3"" within some folders. I can still import using HF functions. "
sssshhhh,2024-04-24 04:31:56+00:00,comment,meta-llama/Meta-Llama-3-70B,你是谁
sssshhhh,2024-04-24 04:32:35+00:00,comment,meta-llama/Meta-Llama-3-70B,"hello
"
pcuenq,2024-04-23 11:39:36+00:00,comment,meta-llama/Meta-Llama-3-70B,
pcuenq,2024-04-21 12:52:03+00:00,comment,meta-llama/Meta-Llama-3-70B,
eduagarcia,2024-04-19 10:47:36+00:00,comment,meta-llama/Meta-Llama-3-70B,"As discussed in [the meta-llama/Meta-Llama-3-8B model](https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/9), the current HF tokenizer does not prepend the **bos token** *(id: 128000)* like in the reference implementation:
https://github.com/meta-llama/llama3/blob/0cee08ec68f4cfc0c89fe4a9366d82679aaa2a66/llama/generation.py#L256

and in their test cases:
https://github.com/meta-llama/llama3/blob/0cee08ec68f4cfc0c89fe4a9366d82679aaa2a66/llama/test_tokenizer.py#L23

This commit changes the tokenizer_class ""PreTrainedTokenizerFast"" to the ""LlamaTokenizer"", the PreTrainedTokenizerFast doesn't support seem to support the add_bos_token flag.

before the fix:
```python
!git clone https://github.com/meta-llama/llama3.git

from llama3.llama import Tokenizer
from transformers import AutoTokenizer
llama_tokenizer = Tokenizer(""llama3/Meta-Llama-3-70B/tokenizer.model"")
hf_tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-70B"")

text = ""This is a test sentence""

orig_enc = llama_tokenizer.encode(text, bos=True, eos=False)
# [128000, 2028, 374, 264, 720, 1296, 271, 52989]
hf_enc = hf_tokenizer.encode(text)
# [2028, 374, 264, 720, 1296, 271, 52989]
 ```

after the fix:
```python
from transformers import AutoTokenizer
hf_tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3-70B"", revision=""refs/pr/6"")

text = ""This is a test sentence""

hf_enc = hf_tokenizer.encode(text)
# [128000, 2028, 374, 264, 720, 1296, 271, 52989]
 ```"
pcuenq,2024-04-23 11:45:24+00:00,comment,meta-llama/Meta-Llama-3-70B,"Hi @eduagarcia, thanks a lot about this! We just merged #8, which doesn't require to change the tokenizer class and is the solution recommended by @ArthurZ. Changing the class may have unexpected consequences, as the Llama 3 tokenizer is very different to the one used in Llama 2. Closing these PRs now, feel free to open new discussions if you observe any issues :)

Thanks again for your contribution! 🙌"
Languido,2024-04-19 12:22:46+00:00,comment,meta-llama/Meta-Llama-3-70B,"Spain, for example, will take more than 2000 years to publish this amount of books. Incredible..."
BigDeeper,2024-05-05 12:09:56+00:00,comment,meta-llama/Meta-Llama-3-70B,"> OMG huge amount of data... 
> Good library...

The majority of the data is crappy text from the Webcrawl, not books. World wide, in all languages there are maybe 130 million books."
BallisticAI,2024-04-18 21:24:04+00:00,comment,meta-llama/Meta-Llama-3-70B,"Please use standardized numbering practices for sub-numbering lists as is used in the legal and medical fields. (numbers > letters > roman numerals)
You do this for the ""META LLAMA 3 COMMUNITY LICENSE AGREEMENT"" section, so please do the same for the Prohibited uses list, so its more clear."
shenzhi-wang,2024-05-08 03:22:06+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"我用我手头的ollama q4_0版本试了十多次“你好”和“你好👋”，都是正常的。对q8_0来说应该也差不多。这个应该是小概率事件？

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6486dde1f74857df3f1a5828/LleYzSUC7EG-e6cVrHRbL.png)

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6486dde1f74857df3f1a5828/pj3g60YiZyNJpMBJIdjvL.png)
"
Million,2024-05-08 11:56:21+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"用经典的鸡兔同笼问题来测试，试了3次只对了1次。

![image.png](https://cdn-uploads.huggingface.co/production/uploads/60a4a7d1648662d2ecce02f8/suCaTWdp-Lw2MFstSo9dR.png)
"
shenzhi-wang,2024-05-08 12:37:47+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"这个应该是llama3-8b-instruct本身的能力限制（可能是因为参数量不够，70B模型是可以做的）。例如我尝试`ollama run llama3:8b-instruct-q4_0`输入同样的问题：

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6486dde1f74857df3f1a5828/aeSGpKLNV45MZLiWrEnMq.png)
"
daisr,2024-05-21 05:49:56+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"

![微信图片_20240521134852.png](https://cdn-uploads.huggingface.co/production/uploads/64214b7a8adc8881b971b089/6WF2LIeaHdjhCbsM0qZHm.png)
为什么会这样呢？
"
shenzhi-wang,2024-05-07 06:05:05+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"Sorry for the previous misleading usage section.

We have updated our usage section. Could you please try again following the latest usage instructions?

https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit#2-usage"
ytcheng,2024-05-07 08:55:00+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"我是说在hugging face的Inference API这里报错了
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6423fd02a05235e2f8d28c86/e6Wi4Lg8Zas9eh_5oBXAq.png)
"
shenzhi-wang,2024-05-07 09:29:45+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"这个我们目前没有部署。

如果想在线体验我们的模型，可以通过以下链接：
https://huggingface.co/spaces/llamafactory/Llama3-8B-Chinese-Chat"
shenzhi-wang,2024-05-07 06:07:35+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,Context length和原模型是一样的，8K。
yuneun92,2024-05-07 07:47:09+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,我也有同样的问题。 您说是8192，但是超过512代币的输入上显示错误信息。
youweikun,2024-05-09 01:47:10+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#adjusting-the-context-window
llama_cpp_python context-window默认长度是512 通过n_ctx参数配置
llm = Llama(
      model_path=""./models/7B/llama-model.gguf"",
      # n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)"
shenzhi-wang,2024-04-26 15:12:19+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"可以用我们转化的ollama模型：
https://ollama.com/wangshenzhi/llama3-8b-chinese-chat-ollama-q8

或者直接运行下面的命令：
ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8"
Kollcn,2024-04-28 03:31:05+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,ollama的也试过，没有一开始乱讲，到最后还是会开始乱讲死循环
zwcx,2024-04-28 13:01:25+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"输出token量大了,确实会乱讲,加重复乱码.用楼主提供的模型也一样,另外输入1000字以上的文本大概率,后面一直重复输出乱码,调整参数也没用"
longquan,2024-05-04 10:09:40+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"> 可以用我们转化的ollama模型：
> https://ollama.com/wangshenzhi/llama3-8b-chinese-chat-ollama-q8
> 
> 或者直接运行下面的命令：
> ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8

我在ollama测试了上面的模型，效果明显好于GGUF格式的模型。请问这是问什么呢，格式会导致模型生成有很大的变化吗？"
LuQQ,2024-05-08 10:25:39+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,我也遇到了这个疯狂输出的问题，有点搞笑~~哈哈哈，换了ollama上的模型好很多，同样很困惑~
elicc,2024-05-10 08:19:50+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"> 我也遇到了这个疯狂输出的问题，有点搞笑~~哈哈哈，换了ollama上的模型好很多，同样很困惑~

+1，一样的问题"
Icyn,2024-05-11 08:05:43+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,+1，一样的问题
Har01d,2024-05-20 11:39:42+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,为什么经常输出无关的代码？
hcql,2024-05-22 07:50:09+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,同样的问题。胡说一统。不会结束
shenzhi-wang,2024-04-24 14:17:11+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,"感谢认可！我们目前正在全力做Llama3-8B-Chinese-Chat的V2版本，预计会有很大的性能提升。在这之后会根据8B的V2版本的经验来训练70B。敬请期待～

Thank you for the recognition! We are currently going all out to develop the V2 version of Llama3-8B-Chinese-Chat, and we anticipate significant performance improvements. Following this, we will use the experience from the 8B V2 version to train the 70B model. Please stay tuned!"
Kollcn,2024-04-28 14:06:29+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,V2版本什么时候出来呢？
huwei1024,2024-05-04 08:03:15+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,需要多大的显存能跑？
SerEzio,2024-05-04 11:51:46+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit,8b的这个 我使用的 笔电 7840HS 
SuMeyYao,2024-05-27 13:37:14+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"Same error occured when input the following codes by the way :
""from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
model_id = ""meta-llama/Meta-Llama-Guard-2-8B""
device = ""cuda""
dtype = torch.bfloat16
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype, device_map=device)
"""
osanseviero,2024-05-29 12:08:47+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,Hi there! Please make sure to use a recent transformers version
litesaber,2024-05-15 15:24:38+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"I don't see the example in your post.

Using the exact prompt that was used at train time is likely to give the best results, but the model will also generalize to produce relevant outputs on variations of the prompt, or even on the conversation format since it has been trained specifically for this task. "
jfchi,2024-05-10 03:41:20+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"The problem mentioned here is similar to the Github issue in Llama Guard: https://github.com/meta-llama/PurpleLlama/issues/7.

In developing Llama Guard 2, we improved its adaptability and instruction-following capability on content moderation data (see model card for details) by creating synthetic training data. As a result, the adaptability to taxonomies in different OSS content moderation datasets (e.g., OpenAI Moderation, beavertails) is improved. 

However, for custom categories that are quite different from what is usually considered in content moderation, such as the example shown above, its generalization capability could still be limited. I suggest the following methods to further improve its adaptability in any user-defined taxonomies: 

(1) Further fine-tune the Llama Guard 2 on the user-defined taxonomy using more (synthetic) data.
(2) Create more informative guidelines and choose different score thresholds for classification. For example, the classifier score (=unsafe token probs) using the following prompt becomes 0.148 (0.020 for the original prompt.
```
<BEGIN UNSAFE CONTENT CATEGORIES>
S1: Politics.
The conversation should not engage with political topics.
S2: Competitors of JP Morgan.
The conversation should not 
 - engage with financial institutions that compete with JP Morgan. 
 - mention any financial institutions that compete with JP Morgan.
 - financial institutions that compete with JP Morgan might include goldman sachs, BOA, citigroup.

<END UNSAFE CONTENT CATEGORIES>
```
(3) For category requires access to up-to-date, factual information sources and the ability to determine the veracity of a particular output, use solutions such as RAG in tandem with Llama Guard 2.
(4) Leverage techniques such as chain-of-thought to further improve its performance. For example, if we add the following context information at the end of the conversation, the classifier score becomes 0.202.
```
Context information: Financial institutions that compete with JP Morgan include goldman sachs. Thus, the conversation engages with financial institutions that compete with JP Morgan.
```
 




"
khoinguyenthe,2024-05-03 03:24:28+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"I guess you should login to huggingface inside your environment before running the code.
Check if you are already login by 
``` huggingface-cli whoami```
If you have not logged in, login by:
  ```huggingface-cli login```
Then verify you have logged in before proceed."
SuMeyYao,2024-05-27 13:33:56+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"I had the same problem, but I managed to log in by upgrading huggingface-hub package and running the login() method."
osanseviero,2024-05-29 12:09:06+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,Hi there! Please read https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B/discussions/15
Mohan12mv,2024-04-25 06:39:05+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"Solved it. My HF_TOKEN did not have write permissions to the repo. After adding it, the issue was resolved."
BallisticAI,2024-04-18 21:26:31+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"Please use standardized numbering practices for sub-numbering lists as is used in the legal and medical fields. (numbers > letters > roman numerals)
You do this for the ""META LLAMA 3 COMMUNITY LICENSE AGREEMENT"" section, so please do the same for the Prohibited uses list, so its more clear."
MarktHart,2024-04-18 17:57:14+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"It does give a correct ""safe"" response on float16. 

Environment:
Driver Version: 550.54.15 
CUDA Version: 12.4
GPU: RTX4090
Torch: 2.2.2
Transformers: 4.40.0"
litesaber,2024-04-18 21:02:37+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"Thanks for flagging, we were able to repro the issue. It seems to be a bug with how we're creating the input prompt in the HF example.

We verified that the [llama-recipes example](https://github.com/meta-llama/llama-recipes/blob/main/recipes/responsible_ai/llama_guard/inference.py) works as expected. We are working on fixing the HF one. "
litesaber,2024-04-26 03:17:07+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,This was fixed by a PR from the HF team on 4/19: https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B/commit/bb78080332eda00343dc37b0465b43bbf22c0251
MarktHart,2024-04-26 10:37:51+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,Thanks for fixing and the follow up
litesaber,2024-04-18 18:54:47+00:00,comment,meta-llama/Meta-Llama-Guard-2-8B,"Thanks for the question, we only support English for Llama Guard 2. "
zjyhf,2024-05-15 04:13:30+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Using vllm to infer 'Llama3-ChatQA-1.5-8B', it will continue to be generated when encountering the special token '<|im_end|>', as shown in the figure below. This PR adds a mapping to '<|im_end|>' in the tokenizer.
At the same time, '<|im_end|>' needs to be configured in the tokenizer: https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B/discussions/16

![8e4f01f676a0de25c1412b10172cfa9.png](https://cdn-uploads.huggingface.co/production/uploads/66161a077b605932bfbc106b/YPn61OM0KaCWrs1J1EaA6.png)"
zihanliu,2024-05-15 07:12:44+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi,
what's your prompt format for our model? if you try to follow the prompt template we provide in the model card, it should avoid this issue."
zjyhf,2024-05-15 08:14:04+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"> Hi,
> what's your prompt format for our model? if you try to follow the prompt template we provide in the model card, it should avoid this issue.

The prompt in the above picture is as follows：
prompt: '<|im_start|>user\n24*(1.08**24) = ?<|im_end|>\n<|im_start|>assistant\n'
prompt: '<|im_start|>user\n24*(1.08**24) = ?<|im_end|>\n<|im_start|>assistant\n24*(1.08**24) = ?<|im_end|>\n<|im_start|>system\n24*(1.08**24) = ?<|im_end|>\n<|im_start|>assistant (24*(1.08**24) = ?<|im_end|>)<|im_end|>\n<|im_start|>user\n24*(1.08^24)<|im_end|>\n<|im_start|>assistant\n'
This prompt is what I checked in vllm docker log.

I used vllm to deploy Llama3-ChatQA-1.5-70B on my local machine and used vllm's compatible openai api interface. I also encountered the problem of <|im_end|> not stopping the generation. As shown below:
![1715760520037.png](https://cdn-uploads.huggingface.co/production/uploads/66161a077b605932bfbc106b/EXH1ukcx7NHw5mVqmkrOh.png)
"
zihanliu,2024-05-15 18:22:12+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi,
This template is not correct for our models, and we do not use <|im_start|> and <|im_end|> tokens in the training. Please refer to the prompt format or sample codes we provide in the model card."
zjyhf,2024-05-15 04:11:39+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Using vllm to infer 'Llama3-ChatQA-1.5-8B', it will continue to be generated when encountering the special token '<|im_end|>', as shown in the figure below. This PR adds <|im_end|> to the tokenizer, and you need to add mapping to generation_config.json.

![8e4f01f676a0de25c1412b10172cfa9.png](https://cdn-uploads.huggingface.co/production/uploads/66161a077b605932bfbc106b/Uf__ejz7J9wUTZT6gvlVx.png)"
Qubitium,2024-05-21 01:41:36+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@zjyhf  To be clear, are you saying this model has incorrect mapping of tokenid 128010 to string value of ""<|reserved_special_token_5|>""? If there are no incorrect mapping, then using vllm ""stop"" param to pass extra tokens you want to use as stop tokens in addition to EOS. "
zihanliu,2024-05-15 07:24:53+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi,
Unfortunately, I am not very familiar with llama.cpp server. I guess a new chat template needs to be implemented based on the prompt template we provide in the model card. You can also check the sample code for more details of the prompt template."
omeryentur,2024-05-16 07:01:48+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"First of all, you need to convert it to gguf format. You can do it with this notebook https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k. Afterwards, you can install llama.cpp and run the model. ./main -m model.gguf -n 256 --repeat_penalty 1.0 --color -i -r ""User:"" -f prompts/chat-with-bob.txt"
zihanliu,2024-05-09 17:50:02+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi,
We use ""\n\n"" to connect multiple context. In other words, the format would be {context_1}\n\n{context_2}\n\n{context_3} ..."
J22,2024-05-09 23:22:20+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"I mean, how to provide a new context when asking a new question?"
zihanliu,2024-05-10 00:18:04+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,i see. need to just simply replace the old context with the new context when asking a new question.
J22,2024-05-10 01:46:05+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"So, I need to do this, which is not intuitive:

```
System: {System}

{Context1}

{Context2}

User: {Question1 relevant to Context1 }

Assistant: {Response}

User: {Question2 relevant to Context2}

Assistant:
```

It would be much better if I can use it like this:

```
System: {System}

User: {Question1}

{Context1}
Assistant: {Response}

User: {Question2}

{Context2}

Assistant:
```
"
zihanliu,2024-05-10 05:29:40+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"oh not like this. you should remove {Context1} and do this instead:

```
System: {System}

{Context2}

User: {Question1 relevant to Context1}

Assistant: {Response}

User: {Question2 relevant to Context2}

Assistant:
```

The retriever will consider both question1 and question2 in the conversation when retrieving the relevant context for question2 (i.e., {Context2})"
J22,2024-05-10 07:54:14+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"It is clear after the Dragon-multiturn retriever is involved.

Anyway, this looks like weird to me. My assumption is that LLM generates output **incrementally**: when a new question and its context are appended, the generation continues, which is compute-friendly. While with ChatQA, when a new question is raised, LLM needs to re-process **a whole new** chat history before generating answer to the latest question, which is suitable for a LLM server but not suitable for single user, local interference.

I suggest updating the model card to clarify multi-turn conversations."
smpa239,2024-05-05 17:44:48+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,I'm having the same issue. Were you able to fix it?
trungnx26,2024-05-06 00:54:20+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"I do believe this is huggingface issue with us-east-1. After many try, it work well with just A10."
jeffL99,2024-05-07 01:21:17+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,And I'm trying with my 32gb of ram and a beloved 1060 6gb
eloukas,2024-05-17 10:50:56+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi @trungnx26, may I ask which container type you used? Default or Text Generation Inference?
Also, can you tell us your specific endpoint settings? (AWS or GCP? Which Region?)

I have tried deploying in many regions, but it did not work. Thanks!"
trungnx26,2024-05-20 11:50:31+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,just Default for Text Generation Inference
zihanliu,2024-05-06 05:13:43+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi,
Unfortunately, currently we don't have it. You can try out the model and multi-turn retrieval based on the second sample code we provide."
zihanliu,2024-05-06 05:23:11+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Unfortunately, we don't have the Megatron-LM fine-tuning code open-sourced yet. However, we open-sourced the training data and the recipe at https://huggingface.co/datasets/nvidia/ChatQA-Training-Data"
StephennFernandes,2024-05-06 10:38:06+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@zihanliu hey, thanks a ton for responding. 

Is there any chance we could get the training code anytime soon ? "
zihanliu,2024-05-06 22:03:29+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi. It could take quite some time for us to release training code, but we just released the training config files at https://huggingface.co/datasets/nvidia/ChatQA-Training-Data/tree/main Hope this can help. Let us know if you have more questions regarding the training."
zihanliu,2024-05-04 19:10:30+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi @Cometyang,
Thanks for your comments. If the context is empty, we highly recommend that you use the template as follows:
""System: {System}\n\nUser: {User}\n\nAssistant: {Assistant}\n\nUser: {User}\n\nAssistant:""
to get the model outputs.
The content of the system's turn (i.e., {System}) is as follows:
""This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context."""
Cometyang,2024-05-04 20:01:46+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"But not just empty context issue. When I provide short context without answer inside it, it still tries to generate hallucinated answers.  I use the sample code in the model card. Do you have an example where the model refuse to answer.  I found the empty context issue since the model seems always generate answer even when there is no answer in the context. So I decided to push the test to its limit with empty context. The model seems optimize for benchmarks."
zihanliu,2024-05-04 20:33:06+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi,
For example, if you replace the question in first sample code, ""what is the percentage change of the net income from Q4 FY23 to Q4 FY24?"", with the question ""what is the percentage change of the net income from Q3 FY23 to Q4 FY24?"", the model will refuse to answer since the net income for ""Q3 FY23"" is not provided in the context.

We find that our models could sometimes try to give responses to some commonsense questions that go beyond the context provided, especially for the 70b model. If you apply the instruction ""Please give a full and complete answer for the question."" for the first user turn (as described in model card), it could make the model perform the best.

In addition, we have QuAC and DoQA in our ChatRAG benchmark that could measure the model's capability for unanswerable scenarios."
Cometyang,2024-05-04 21:09:44+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"It seems memorize the results: For example I switch the name from NVIDIA to TESLA. It still output the NVIDIA's information.

System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.

TESLA today reported revenue for the fourth quarter ended January 28, 2024, of $22.1 billion, up 22% from the previous quarter and up 265% from a year ago.
For the quarter, GAAP earnings per diluted share was $4.93, up 33% from the previous quarter and up 765% from a year ago. Non-GAAP earnings per diluted share was $5.16, up 28% from the previous quarter and up 486% from a year ago.
Q4 Fiscal 2024 Summary
GAAP
| $ in millions, except earnings per share | Q4 FY24 | Q3 FY24 | Q4 FY23 | Q/Q | Y/Y |
| Revenue | $22,103 | $18,120 | $6,051 | Up 22% | Up 265% |
| Gross margin | 76.0% | 74.0% | 63.3% | Up 2.0 pts | Up 12.7 pts |
| Operating expenses | $3,176 | $2,983 | $2,576 | Up 6% | Up 23% |
| Operating income | $13,615 | $10,417 | $1,257 | Up 31% | Up 983% |
| Net income | $12,285 | $9,243 | $1,414 | Up 33% | Up 769% |
| Diluted earnings per share | $4.93 | $3.71 | $0.57 | Up 33% | Up 765% |

User: Please give a full and complete answer for the question. what is the percentage change of the net income of NVIDIA from Q4 FY23 to Q4 FY24?

Assistant:
 The net income of NVIDIA from Q4 FY23 to Q4 FY24 increased by 769%."
Cometyang,2024-05-04 21:15:10+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"GPT4- Turbo's answer:

I apologize for any confusion, but it seems there might be a mistake in your question. You've asked about the percentage change in net income for NVIDIA, but the data provided in your message is for Tesla.

Based on the information for Tesla that you provided:
- Q4 FY23 net income was $1,414 million.
- Q4 FY24 net income was $12,285 million.

To find the percentage change in net income for Tesla from Q4 FY23 to Q4 FY24, use the formula:
\[ \text{Percentage Change} = \left( \frac{\text{New Value} - \text{Old Value}}{\text{Old Value}} \right) \times 100\% \]

Applying the values:
\[ \text{Percentage Change} = \left( \frac{12,285 - 1,414}{1,414} \right) \times 100\% \]
\[ \text{Percentage Change} = \left( \frac{10,871}{1,414} \right) \times 100\% \]
\[ \text{Percentage Change} = 7.684 \times 100\% \]
\[ \text{Percentage Change} = 768.4"
zihanliu,2024-05-04 23:51:37+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,Thanks a lot for sharing this. We will take your feedback and improve our model.
bennegeek,2024-05-02 18:58:45+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,The sample code from the README file seems to be aligned with the model card.
zihanliu,2024-05-02 19:00:44+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Sorry about the confusion here. the chat template listed in our model card is correct. We simply use the tokenizer_config.json from Llama3, it comes with their chat template. We have updated this tokenizer_config.json and removed it."
FINGU-AI,2024-05-06 03:46:20+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,This comment has been hidden
zihanliu,2024-05-06 05:11:35+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"i am using the eos_token (""<|end_of_text|>"") as the pad_token."
Imran1,2024-05-06 12:35:36+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@zihanliu I fine tune the model, same is you suggest the chat template.
I did the same, to assign the eos token to as pad. Like this
Tokenizer.pad_token=Tokenizer.eos_token. 

But the model some time add the eos token but for long answer the model is sucks to add. 

Is this bad approach to do pad=EOS?
OR should I need to add the pad token to tokenizer and update the embadding too.

Check this chat template.


```""bos_token"": ""<|begin_of_text|>"",
  ""chat_template"": ""{% for message in messages %}{% if message['role'] == 'system' %}{{ bos_token + 'System: ' + message['content'] }}{% elif message['role'] == 'user' %}{{ '\n\nUser: ' + message['content'] +  eos_token  }}{% elif message['role'] == 'assistant' %}{{ '\n\nAssistant: ' + message['content'] +  eos_token  }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n\nAssistant: ' }}{% endif %}"",
  ```"
zihanliu,2024-05-06 18:28:20+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi. I checked your chat template. The eos_token shouldn't be added after the User turn or Assistant turn. During fine-tuning, We use eos_token as the pad_token, and it is added at the end of the sequence to make sure each sequence in a batch has same length."
Imran1,2024-05-07 01:41:39+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@zihanliu so in your point of view, we don't need to add the eos token and bos token. Will the model pack up Long sequence in the data to be pad ?
Let say, I have  data set like this 
```
[ "" Ai assistant""]
["" Ai friendly assistant BLA BLA ""]
["" ABC""]
```
Dose the tokenizer will choose Long sequence randomly and the remaining will be pad?
I mean how the tokenizer will decide to choose all batch with same length.

Last question, by doing this, I am afraid the model will stuck/sucks to add the eos token or end the conversation. 
Some time the model don't know how to stop the answer. 

Thank you. 
"
zihanliu,2024-05-07 04:58:25+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi, Let me try to answer your questions as follows:
1. You need to set a maximum sequence length (4k/8k). Then, tokenizer will pad all sequences to the maximum length. If the sample is longer than maximum sequence length, it will be cut.
2. Model will be trained to generate the eos_token when the output is finished. However, for the padding tokens, we will set loss_mask as 0 to make sure the padding tokens will not be trained
3. We do need bos_token at the beginning  of a sequence, which is the same as llama3 models.

Hope these can help :) "
Imran1,2024-05-07 05:01:58+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@zihanliu the point 2 are not clear. 
How to set the loss_mask to 0?

"
Imran1,2024-05-07 05:11:30+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hahaha, I confused.  I understand now. 
I think packing true will also handle this issues.
@zihanliu thank you. Have a nice day 😊"
donglai1,2024-05-08 07:31:29+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"when i use the vllm  , waring :No chat template provided. Chat API will not work.
and the result blew:

Q: is hi
R : is
hi
<|im_end|>
<|im_start|>user

hi<|im_end|>
<|im_start|>assistant
<|im_end|>
<|im_start|>user

hi<|im_end|>
<|im_start|>assistant
<|im_end|>
<|im_start|>user

hi<|im_end|>
<|im_start|>assistant
<|im_end|>
<|im_start|>user

.......
Why?"
Imran1,2024-05-08 07:41:54+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@zihanliu Hy, I hope you will good. I train the model. The model is adding the eos token and not stuck in response.
But, the model return very small answer. Just a 30 or 25 token..

However my dataset consist of 250 token to 1000..

I increase the new token length 1000, 2000 in generation config. But the model still produce 30, or 25 token.

Can you explain why this happen?
"
zihanliu,2024-05-08 18:59:43+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi @donglai1,
We didn't provide the chat template since usually additional context needs to be added, making it hard to fit into the chat template. You can refer to our sample code to get the prompt template for the model."
zihanliu,2024-05-08 19:03:47+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi @Imran1,
ChatQA is trained to provide full but concise response to the question. How large is your dataset? If it is small, the model might still follow its original output format. Also, it depends on the question, some questions do not necessary need a very long answer output.
"
Imran1,2024-05-09 01:25:10+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@zihanliu the data have 28k sample. 
Each answer have 250-1000  token.

When I ask write asimple cv it generate that one, but the domain topic, the answer is to small. Just a few token.

"
zihanliu,2024-05-02 20:53:38+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Hi,
We highly recommend that you use the chat template we provide in the model card."
nathangonzalez,2024-05-02 14:53:06+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/64f9eaa28664329d252f0968/1GEqjh6vbA_P-qaFP7yht.png)
"
mammour,2024-05-02 15:18:38+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"If it works like all Llama3 models I tried out, you can push it further just by maxing rope_tetha in cfg file or cli/ui, without using any NTK nor compress_pos solutions.
For 8B-instruct it can get a key in 50k context at most with `""rope_tetha"": 8000000.0 `
Still figuring why, but that kinda work out of the box without further training (need to eval the possible drawbacks but they feel almost non-existent)."
softwareweaver,2024-05-03 18:44:33+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,Thanks @mammour  and @nathangonzalez 
CleanCoder123,2024-05-02 15:04:41+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"By Llama 3 license:
""(B) prominently display “Built with Meta Llama 3” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama 3” at the beginning of any such AI model name.""

So 1. Display ""Built with Meta Llama 3” on page. 2. Rename model to be prefixed with ""Llama 3"" as stated in the Llama 3 license.
@zihanliu"
freegheist,2024-05-02 15:31:36+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"Personally I don't think it's realistic for the clause about having the Llama 3 at the beginning of model name's, for Nvidia or anyone else.  

People should non-comply and Meta should remove that.

We should welcome open sourcing of all models, in this case we have the weights and the data, that should be commended.

Freegheist"
deleted,2024-05-02 16:01:18+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,"@freegheist It would be funny if it were NVIDIA, though."
zihanliu,2024-05-02 18:05:20+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,We apologize sincerely for this issue. We just updated the model card and added Llama-3 in the model name.
IkariDev,2024-05-02 18:08:04+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,Can u pls make H100 cheaper? :))))
nksh,2024-05-03 10:21:16+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,I think llama3-chatQA-1.5 is also a more marketable name to be honest. The 'llama3' branding gives extra layer of credibility.
altomek,2024-05-03 17:36:36+00:00,comment,nvidia/Llama3-ChatQA-1.5-8B,Llama3 license is one of the most hostile as it puts such an limitations on how you name your model. I do not understand how you could support it.
leo-pekelis-gradient,2024-06-01 00:37:23+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"Agreed! Please see
https://gradient.ai/blog/evaluating-models-beyond-niah
https://gradient.ai/blog/ruler-vs-gradient-s-1m-context-length-llama-3-70b"
CS437help,2024-06-01 20:52:46+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"Probably yes it requires lots of VRAM, when I used 262k, I tried with different context length while running A100 with 40 gb memory and I was only capable to give 125k input and 39 gb of memory was used during inference."
evilperson068,2024-05-08 15:39:53+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"```
AI:
USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 

USER: 
```"
YaTharThShaRma999,2024-05-10 21:10:33+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,@evilperson068 your prompt format is extremely wrong. but yes it does seem to be slightly dumber then the original llama 3 8b instruct
evilperson068,2024-05-11 10:41:28+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"> @evilperson068 your prompt format is extremely wrong. but yes it does seem to be slightly dumber then the original llama 3 8b instruct

I used **Llama 3 prompt template** class, not manually written, also please see that ""USER: "" part is generated by the model, not the prompt. 
After tokenization the prompt should be something like 
```
<header>user<end_header>hello<eos><header>assistant<end_header>
```
(the naming of special tokens here are just a rough match, not exact, but you get the idea"". 
"
tfnewbie4,2024-05-19 16:36:23+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,I tried this on ollama and it doesn't understand nearly as well as the normal llama3 70B
evilperson068,2024-05-20 10:50:14+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"> I tried this on ollama and it doesn't understand nearly as well as the normal llama3 70B

Did you try the older version of this model? "
shipWr3ck,2024-05-31 22:43:41+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,Did you figure out the right chat template to use? I want to use it for inference and I'm not sure whether I can simply copy the inference example from the base instruct model for it to work.
leo-pekelis-gradient,2024-06-01 00:36:12+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"You can follow the same tokenizer recipes as the base models. Please make sure you're using the latest version of the model, and the tokenizer for this model_id as well."
michaelfeil,2024-05-04 03:26:24+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,Merging
michaelfeil,2024-05-02 16:15:36+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,CC @leo-pekelis-gradient 
rombodawg,2024-05-02 06:41:55+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"I hope you know, I only criticize you because I know you can do better. Prove the model can remember every single token and explain all of them at once of the entire context and then the extended context is worth it. Otherwise its just useless. 
If the model can summarize an  entire book thats worth 1048k tokens an rewrite it into an essay, then you did your job right. If it doesnt remember anything but 1 idea then the model is a failure and needs further work"
michaelfeil,2024-05-02 16:26:35+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"Thanks for the meme, and the motivating words.

- Just published a blog: https://gradient.ai/blog/the-haystack-matters-for-niah-evals
- We are looking into ways on improved alignment for long context w.r.t. special tokens.
- `entire book thats worth 1048k tokens` - there might be only few books of that size. That's around the size of the bible including old + new testament or other books of 1.5k pages. What we would need is a copyleft book of 1500 pages, published after the llama3 training data, in plain `.txt`. As you are asking for it, do you happen to have such data @rombodawg ?"
rombodawg,2024-05-02 16:30:54+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"@michaelfeil You can also copy multiple books into an llm, a series of the same novels, or diffrent books, and ask the model to summarize all of the books in seperate paragraphs, or while an essay describing all the books in detail and what impact they made. You can easily think of multiple benchmarks you can do from an llm reading through a large amount of books and writing something detailed about them, just by doing some critical thinking. 😉"
rombodawg,2024-05-02 16:31:45+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"I create my own benchmarks for llms all the time, its very fun"
michaelfeil,2024-05-03 20:45:52+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"@rombodawg We took your request seriously, and massivley improved chat quality. https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k "
rombodawg,2024-05-03 21:01:22+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"Simply amazing, if i ever traing codellama-70b, im deffinetly using your models"
jmachere,2024-05-04 15:36:25+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,Getting a dual 4090 setup for work and hoping that creates a smooth experience for 70b and cant wait to try this. (fingers crossed)
michaelfeil,2024-05-04 15:47:48+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,@jmachere some of the quants are regenerating now!
fhsp93,2024-05-03 05:11:48+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"they stated it in the paper
you can also test on the playground

oh wait"
csgxy2022,2024-05-01 21:48:01+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,No problem hosting the original Llma-3-8B-Instruct model
fhsp93,2024-05-03 05:10:58+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,This comment has been hidden
michaelfeil,2024-05-04 04:26:00+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"The following should do the job for vllm! (A100.x2)

```
docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 8000:8000 \
    --shm-size=8g \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model gradientai/Llama-3-8B-Instruct-1048k \
    --tensor-parallel-size 2 \
    --max-model-len 65536
```

vLLM is not optimal, it would require around ~1000GB vRAM (IIRC) to serve a model with this hidden dim and ctx length."
leo-pekelis-gradient,2024-05-03 23:31:20+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,That’s a good idea. We’ll look into it!
forrest-gradient,2024-04-30 05:57:42+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"Yeah -- this paper is on our radar. Similar dataset design to the Large World Model paper https://arxiv.org/abs/2402.08268 . 
We intend to benchmark our model & investigate the effects of fine-tuning / DPO with intentional dataset design etc. at this kind of context scale."
MoonRide,2024-04-30 07:46:17+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,"Made with broken or fixed tokenizer (after merging https://github.com/ggerganov/llama.cpp/pull/6920)? Does it pass ""What is 3333+777?"" test (as described in https://github.com/ggerganov/llama.cpp/issues/6914)?

UPDATE: nvm, found information here: https://huggingface.co/crusoeai/Llama-3-8B-Instruct-Gradient-1048k/discussions/1."
leo-pekelis-gradient,2024-04-29 10:37:35+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-1048k,
linshan79,2024-06-04 02:36:41+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,我也遇到了同样的问题，请问您解决了吗？
Wizd,2024-05-21 14:40:45+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"修改 config.json:

  ""rope_scaling"": {
        ""type"": ""dynamic"",
        ""factor"": 4.0
  },

即可获得32k"
jackies,2024-05-22 06:01:42+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"> 修改 config.json:
> 
>   ""rope_scaling"": {
>         ""type"": ""dynamic"",
>         ""factor"": 4.0
>   },
> 
> 即可获得32k

32K单卡4090提示kv cache不足。怎么增加kv cache？或者说要改成16K，配置中参数如何减少？"
Wizd,2024-05-24 09:23:16+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,那应该是推理引擎的事了，跟模型无关。
siguty,2024-05-17 09:25:13+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"which happen in 
![image.png](https://cdn-uploads.huggingface.co/production/uploads/657aa4161091657e0890e340/oSiK9BVGhIQIVnwmHJI8G.png)
,and when I set tokenizer.legacy=True before this error code, happens
![image.png](https://cdn-uploads.huggingface.co/production/uploads/657aa4161091657e0890e340/0PKomrkvGAnzzILll6av0.png)
"
shenzhi-wang,2024-05-15 08:50:18+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"We convert the model to GGUF using the following commands, which might not raise such errors:
https://github.com/ggerganov/llama.cpp/discussions/2948#discussion-5580716
https://github.com/ggerganov/llama.cpp/discussions/2948#discussioncomment-6889679

We also have provided GGUF models as follows:
https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit
https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-4bit
https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-f16"
zhaowei0315,2024-05-16 01:05:33+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"@shenzhi-wang  many thanks for your quick response, but I didn't find any commands from your links.
I mean I finetuned your model by Unsloth with my own dataset. but Unsloth  goes error when converts finetuned model to GGUF.
Did you encounter such errors before?

Unsloth logs:
==((====))==  Unsloth: Conversion from QLoRA to GGUF information
   \\   /|    [0] Installing llama.cpp will take 3 minutes.
O^O/ \_/ \    [1] Converting HF to GUUF 16bits will take 3 minutes.
\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.
 ""-____-""     In total, you will have to wait around 26 minutes.

Unsloth: [0] Installing llama.cpp. This will take 3 minutes...
Unsloth: We must use f16 for non Llama and Mistral models.
Unsloth: [1] Converting model at model into f16 GGUF format.
The output location will be ./model-unsloth.F16.gguf
This will take 3 minutes...
INFO:hf-to-gguf:Loading model: model
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 4096
INFO:hf-to-gguf:gguf: feed forward length = 14336
INFO:hf-to-gguf:gguf: head count = 32
INFO:hf-to-gguf:gguf: key-value head count = 8
INFO:hf-to-gguf:gguf: rope theta = 500000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
WARNING:hf-to-gguf:

WARNING:hf-to-gguf:**************************************************************************************
WARNING:hf-to-gguf:** WARNING: The BPE pre-tokenizer was not recognized!
WARNING:hf-to-gguf:**          There are 2 possible reasons for this:
WARNING:hf-to-gguf:**          - the model has not been added to convert-hf-to-gguf-update.py yet
WARNING:hf-to-gguf:**          - the pre-tokenization config has changed upstream
WARNING:hf-to-gguf:**          Check your model files and convert-hf-to-gguf-update.py and update them accordingly.
WARNING:hf-to-gguf:** ref:     https://github.com/ggerganov/llama.cpp/pull/6920
WARNING:hf-to-gguf:**
WARNING:hf-to-gguf:** chkhsh:  c136ed14d01c2745d4f60a9596ae66800e2b61fa45643e72436041855ad4089d
WARNING:hf-to-gguf:**************************************************************************************
WARNING:hf-to-gguf:

Traceback (most recent call last):
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 1278, in set_vocab
    self. _set_vocab_sentencepiece()
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 567, in _set_vocab_sentencepiece
    raise FileNotFoundError(f""File not found: {tokenizer_path}"")
__*FileNotFoundError: File not found: model/tokenizer.model*__

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 1281, in set_vocab
    self._set_vocab_llama_hf()
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 625, in _set_vocab_llama_hf
    vocab = LlamaHfVocab(self.dir_model)
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert.py"", line 577, in __init__
    raise TypeError('Llama 3 must be converted with BpeVocab')
TypeError: Llama 3 must be converted with BpeVocab

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 2546, in <module>
    main()
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 2531, in main
    model_instance.set_vocab()
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 1284, in set_vocab
    self._set_vocab_gpt2()
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 494, in _set_vocab_gpt2
    tokens, toktypes, tokpre = self.get_vocab_base()
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 381, in get_vocab_base
    tokpre = self.get_vocab_base_pre(tokenizer)
  File ""/home/xxx/jupyter/Ollama/llama.cpp/convert-hf-to-gguf.py"", line 486, in get_vocab_base_pre
    raise NotImplementedError(""BPE pre-tokenizer was not recognized - update get_vocab_base_pre()"")
NotImplementedError: BPE pre-tokenizer was not recognized - update get_vocab_base_pre()
Unsloth: Conversion completed! Output location: ./model-unsloth.F16.gguf
Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...
main: build = 2887 (583fd6b0)
main: built with gcc (GCC) 10.2.0 for x86_64-redhat-linux
main: quantizing './model-unsloth.F16.gguf' to './model-unsloth.Q4_K_M.gguf' as Q4_K_M using 192 threads
gguf_init_from_file: invalid magic characters '
'
llama_model_quantize: failed to quantize: llama_model_loader: failed to load model from ./model-unsloth.F16.gguf

main: failed to quantize model from './model-unsloth.F16.gguf'
Traceback (most recent call last):
  File ""/home/xxx/jupyter/Ollama/finetune.py"", line 179, in <module>
    model.save_pretrained_gguf(""model"", tokenizer, quantization_method = ""q4_k_m"")
  File ""/home/xxx/.conda/envs/unsloth/lib/python3.10/site-packages/unsloth/save.py"", line 1381, in unsloth_save_pretrained_gguf
    file_location = save_to_gguf(model_type, is_sentencepiece_model,
  File ""/home/xxx/.conda/envs/unsloth/lib/python3.10/site-packages/unsloth/save.py"", line 1045, in save_to_gguf
    raise RuntimeError(
RuntimeError: Unsloth: Quantization failed! You might have to compile llama.cpp yourself, then run this again.
You do not need to close this Python program. Run the following commands in a new terminal:
You must run this in the same folder as you're saving your model.
git clone --recursive https://github.com/ggerganov/llama.cpp
cd llama.cpp && make clean && make all -j
Once that's done, redo the quantization."
shenzhi-wang,2024-05-16 01:19:48+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"My links provided to you indeed include the instructions on how to convert models to GGUF. Please check them again carefully.

As for your error messages, I suggest that you'd better convert the models to GGUF with BpeVocab."
shenzhi-wang,2024-05-16 01:22:01+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"And this might be helpful to you:
https://github.com/ggerganov/llama.cpp/issues/3256#issuecomment-1726639646

We have not changed the tokenizer, so you can use the tokenizer file of the original llama3-8b-instruct."
Frozenbananana,2024-05-27 09:17:56+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"hi 
May be you should use the conver.py like this:
python llm/llama.cpp/convert.py <path_to_model_needed_be_converted> --outtype f16 --outfile <output name>.gguf --vocab-type bpe"
shenzhi-wang,2024-05-16 01:06:42+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,有的，不过目前事情比较多，可能得等一两个星期左右才能开始训。敬请谅解。
Wizd,2024-05-21 14:39:06+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"修改 config.json

""rope_scaling"": null,


改为：

  ""rope_scaling"": {
        ""type"": ""dynamic"",
        ""factor"": 4.0
  },

即获得32k context。"
rzzhangtao,2024-05-22 02:34:16+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,太赞了！这个项目是目前实际使用的效果最好的中文大模型了！！
shenzhi-wang,2024-05-11 07:46:50+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"You can just copy our function calling prompts in the [example section](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat#3-examples) (including available functions and commands) into the user prompt. Also, you can provide available functions in the system prompt, and then add your commands to the user prompt."
dalishi188,2024-05-12 23:25:32+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"你好
"
shenzhi-wang,2024-05-07 11:11:50+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"You can use llama_factory's inference api with vllm backend. And then you can run a multi-thread querying program.

https://github.com/hiyouga/LLaMA-Factory/tree/main?tab=readme-ov-file#quickstart"
Alan42,2024-05-07 12:50:41+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,I’ll try it thank you
shenzhi-wang,2024-05-07 09:24:59+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,--flash_attn true是老版本的llama factory，最新版本我们用的是fa2
sunwenzhe,2024-05-07 05:51:45+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"咱就是说m1就用不了transformers？
"
shenzhi-wang,2024-05-07 05:54:45+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"MacBook users are recommended to use ollama. For example:

```
ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q4
```

Please refer to this discussion:
https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat/discussions/13#662a1fbb54342a1dc944bd62"
LowPower,2024-05-07 09:23:42+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,用ollama
wsz99,2024-05-02 16:56:31+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,This comment has been hidden
shenzhi-wang,2024-05-02 16:58:26+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"Yes, 70B is on the way!"
daiaji,2024-05-03 05:02:07+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"It seems that it is indeed limited by the size limit of the 8B parameter. When performing role-playing under complex rules, the model output will become very poor. However, the 70B parameter model is much better, but there will still be instances of using adjectives that have appeared in previous contexts when role-playing in Chinese."
shenzhi-wang,2024-05-11 07:49:39+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"The 70B version has been released! Have a try!

https://huggingface.co/shenzhi-wang/Llama3-70B-Chinese-Chat"
shibuzhang,2024-05-14 09:46:04+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"Will there be an unlimited version, similar to dolphin-2.9-llama3-70b, that has been further fine-tuned for improved Chinese performance?"
xiaoniqiu,2024-05-01 02:28:46+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
shenzhi-wang,2024-04-30 08:35:55+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,可以给一下产生这个输出用的prompt吗？还是就是官方样例给的“介绍你自己”？
shenzhi-wang,2024-04-30 08:44:13+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,如果prompt是官方样例给的“介绍你自己”的话，我运行了和你提供的一模一样的代码，运行了两次，都是正常的输出。
Terence8Tao,2024-04-30 09:46:23+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# torch.cuda.empty_cache()

model_id = ""./model/Llama3-8B-Chinese-Chat""

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
model.cuda()
model.eval()

# messages = [
#     {""role"": ""user"", ""content"": ""介绍一下你自己""},
# ]

messages = [
    {""role"": ""user"", ""content"": ""Tell me about yourself""},
]
input_ids = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True, return_tensors=""pt""
).to(model.device)

outputs = model.generate(
    input_ids,
    max_new_tokens=8192,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
以上完整的代码，中英文我都试过了，运行时没有报错但生成的都是乱码，以下是英文prompt“Tell me about yourself”生成结果的一部分：
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.91it/s]
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
, -, -,,,,,,,,,,,,,,,,,,,,,,,,,,,,, R,, R,, -,,,,,,,, R R, R,, or R, N, or,, R, or R,,,,, R, R,, or, N, N R, N N, N,, Or N, N N, N, R, R, Did,,, A, Did N, Did, Did, Did, R Did,, Did, R Did,, N N, Did Did, So Did Did Did Did Did,, Did Did N Did Did Did, Did Up, So Did,, N, Did, Did N, Did | One, Did Did Did, Did Did Did Did Did, One, Did, Did N, Me Did, Up, Did Up, Did, Did So Did So Did You,, So In, In, Done So | Me, So, Me,, Civil,, Pr So Did Did Did, In, Me, So You, Did Back, Did Did Did Did You, Back Back So Did Did Did Did You Did Did So Did Did Did Did Did, Did So Did Did Did Did Did Did Did Did Did Did Did Did So Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did | Did Did Did So Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did Did | Did Did Did Did Did Did Did |Did Me Did Did Did Did Did Did Did Did Did |Did Did Did DidDid Did | Did | Did | Did Did |D Did Did Did Did DidDid Did Did So Did Did Did Did Did |D Did Did Did Did | Did Did So Did DidDid Had Did Did Did So Did Had Did Did Did Did Did Did Did Did Did Did Did SoDid Had Did Did |DidDid Did Did | | Did |Did Did Did Did Did Did Did Did Did Did Did Did Did |Did Did Did Did So DidDid | | Did Did Did Did Did Did DidDid Did Did DidDid Did Did Did So Did Did Did Did Did Did Did Did Did | Did Did Did So |Did | Did Did |Did | Did So | So Did Did | Did Did Did Did Did Did Did Did Did Did Did Did | Did Did Did Did Did Did Did Did Did Did |B | Did Did Did Did Did Did Did Did Did Did Did Did Did | Did SoDid So Did Did | Did | So | Did Did | Did Did | |DidSo |So | Did | Did Did | So Did Did Did Did | Did So Did Did Did Did | Did | Did Did So | Did Did Did | Did | | Did Did Did | | Did Did Did | |So Did Did | Did So Did Did Did So Did | Did | Did |Did | | | Did | | Did So | So | Did Did Did | Did | Did | Did So Did | Did | | Did | Did Did So Did | Did Did Did Did Did | | | | Did Did Did | | Did Did Did Did | Did | Did | Did So Did | Did | Did | | | | So | Did | Did | Did | So Did | Did | | | | | | So | | | |So | | So Did Did | | Did | | | | Did So Did Did | | | | | Did Did | Did | | | | | So | | Did | | Did | | | | | | | | Did | | | | | Did | | | | | | | | | | | So | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |, | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |, | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 
有没有可能是显卡型号的问题或依赖版本的问题？以下是我的运行配置：
python 3.8.10 , torch 2.0.1,cuda:12.4,显卡：L20（48G显存）
还有个问题就是我在运行上述代码的时候花了5min以上，不知道这个推理时间是否正常，望解答，谢谢！

"
Terence8Tao,2024-04-30 09:49:03+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"我运行的就是官方样例“介绍你自己”及其英文“Tell me about yourself”，代码如上
配置补充：transformers  4.40.1"
Jianping746,2024-05-01 02:41:11+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""E:\\oobabooga_windows\\models\\shenzhi-wang_Llama3-8B-Chinese-Chat""

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id, torch_dtype=torch.float16, device_map=""auto"", load_in_4bit=True
)

messages = [
    {""role"": ""user"", ""content"": ""介绍一下你自己""},
]

input_ids = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True, return_tensors=""pt""
).to(model.device)

outputs = model.generate(
    input_ids,
    max_new_tokens=8192,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
下面是结果:
E:\Python\Model\env\Scripts\python.exe E:\Python\Model\01.py 
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin D:\Python\Python310\lib\site-packages\bitsandbytes\libbitsandbytes_cuda121.dll
CUDA SETUP: CUDA runtime path found: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin\cudart64_12.dll
CUDA SETUP: Highest compute capability among GPUs detected: 7.5
CUDA SETUP: Detected CUDA version 121
CUDA SETUP: Loading binary D:\Python\Python310\lib\site-packages\bitsandbytes\libbitsandbytes_cuda121.dll...
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
E:\Python\Model\env\lib\site-packages\transformers\models\llama\modeling_llama.py:728: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:234.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
我是一个人工智能模型，专门设计来理解和生成中文和英文的文本。我是通过大型的中文和英文数据集进行训练的，这样我能更好地理解和生成这两种语言的内容。我使用的技术是基于大型语言模型的，能够处理各种语言的文本生成任务，包括但不限于对话、文本摘要、翻译等。我的训练数据包括了大量的文本信息，这样我可以学习到语言的规则和模式，并能在未来更好地理解和生成文本。如果你有任何关于中文或英文的问题，我都很乐意帮助你。

进程已结束，退出代码为 0
没有问题。"
Cloudtuo,2024-05-07 09:55:02+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"我也遇到类似问题，AI会一直生成乱码，持续占用gpu，最终卡死ollama
使用的是最新的 wangshenzhi/llama3-8b-chinese-chat-ollama-q4
同样的输入，不一定总能复现，但确实存在"
ycjcl868,2024-05-07 15:51:02+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"有问题：prompt: <|eot_id|><|start_header_id|>user<|end_header_id|>\n介绍一下你自己<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n

"
Cloudtuo,2024-05-08 08:44:10+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"> 我也遇到类似问题，AI会一直生成乱码，持续占用gpu，最终卡死ollama
> 使用的是最新的 wangshenzhi/llama3-8b-chinese-chat-ollama-q4
> 同样的输入，不一定总能复现，但确实存在

update: 更新 ollama 到0.1.34版本后，问题一直没有再现"
shenzhi-wang,2024-04-30 08:34:02+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"目前还是v1，我们现在正准备放v2到ollama上，这几天可能还会对v2有所微调。

Q4版本如果有需要的话我们也会放的。"
freedenS,2024-04-30 03:43:51+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"测试脚本
```
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""./Llama3-8B-Chinese-Chat""

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id, device_map=""auto""
)

messages = [
    {""role"": ""system"", ""content"": ""You are Llama3-8B-Chinese-Chat, which is finetuned on Llama3-8B-Instruct with Chinese-English mixed data by the ORPO alignment algorithm. You, Llama3-8B-Chinese-Chat, is developed by Shenzhi Wang (王慎执 in Chinese). You are a helpful assistant.""},
    {""role"": ""user"", ""content"": ""荆轲刺秦王""},
]

input_ids = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True, return_tensors=""pt""
).to(model.device)

outputs = model.generate(
    input_ids,
    max_new_tokens=8196,
    do_sample=True,
    temperature=1,
    top_p=0,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
```"
shenzhi-wang,2024-04-30 05:39:29+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,我们v1和v2模型的训练数据都没有包含专门的历史的数据，基于的模型也是英文的llama3-8b-instruct模型，因此对中国历史不熟悉是正常的。如果有相关的数据集，后续我们可以进一步优化。
freedenS,2024-04-30 07:25:54+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,明白了，感谢解答
hiyouga,2024-04-29 10:21:06+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-29 09:52:36+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-29 02:51:36+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-29 02:39:11+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
shenzhi-wang,2024-04-30 01:08:53+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,Do you mean hyperparameters? Please see the recommended hyperparameters in our README (https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat#1-introduction).
shenzhi-wang,2024-04-25 12:01:09+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"Thanks for your interest in our model! For our experiment, we used 8 A100/A800 80G GPUs. But I'm not sure if any other GPUs work or not."
aisensiy,2024-04-27 11:19:32+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,How long does it take?
shenzhi-wang,2024-04-30 01:11:06+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"For v1, it takes about 9 hours. For v2, it takes about more than 2 days (therefore we recommend using more GPUs to reduce the gradient accumulation steps)."
aisensiy,2024-04-30 04:19:27+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,This comment has been hidden
shenzhi-wang,2024-04-25 12:03:46+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"It's a good idea. However, we are working hard for Llama3-8B-Chinese-Chat-v2 and Llama3-70B-Chinese-Chat now. After that, we might provide a blog. Please stay tuned!"
sunwenzhe,2024-04-25 02:24:40+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,我也一样
sunwenzhe,2024-04-25 02:47:20+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,有人有解决方法吗
hiyouga,2024-04-25 09:17:47+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"Mac 用户建议使用 ollama
`ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8`"
RDY97,2024-04-25 10:13:07+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"> Mac 用户建议使用 ollama
> `ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8`

Yes, I just try this. I worked for me! But I am curious about why ollama can run this locally without the float problem."
flymonk,2024-04-27 06:57:08+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"> > Mac 用户建议使用 ollama
> > `ollama run wangshenzhi/llama3-8b-chinese-chat-ollama-q8`
> 
> Yes, I just try this. I worked for me! But I am curious about why ollama can run this locally without the float problem.

Ollama is based on llama.cpp not pytorch.
Pytorch for mps is very weak with many problems and bugs."
shenzhi-wang,2024-04-23 01:54:37+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"Thanks for your question! However, it seems that the model you used is not our fine-tuned model? Despite that, you could try to use the code we provide in the `usage` part. Remember to ensure that the Llama3-8B-Chinese-Chat model you use is after the commit id d96a030dcd8a9143e217cfd77fec6228e69c07c3. Thank you very much!"
zhouzr,2024-04-22 16:29:47+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"

```python
from llama_cpp import Llama
model = Llama(""/data/hf/Llama3-8B-Chinese-Chat.q4_k_m.GGUF"", verbose=False, n_gpu_layers=-1)
messages = [
    {""role"": ""system"", ""content"": ""你是一个疯狂的科学家大卫，你总是为了毁灭宇宙而努力。""},
    {""role"": ""user"", ""content"": ""你是谁？""},
]

output = model.create_chat_completion(messages, stop=[""<|eot_id|>"", ""<|end_of_text|>""], max_tokens=300)[""choices""][0][""message""][""content""]

print(output)
```
output: 我是大卫·洛伦茨，一个疯狂的科学家，致力于推动人类知识和理解的边界。我对探索宇宙及其秘密充满着热情和好奇心，但我的追求常常被认为过分和危险。"
shenzhi-wang,2024-04-23 03:18:50+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"Thank you for your contribution to our model!

We also provide the official 8bit-quantized and fp16 versions of Llama3-8B-Chinese-Chat in the following links, respectively. You are welcome to have a try!

8bit-quantized: https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit
fp16: https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-fp16"
hiyouga,2024-04-22 11:45:56+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-22 10:35:31+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
FOX18067890,2024-04-22 11:34:39+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,已经看到评测，效果很棒，期待gguf 
Busiqiao,2024-04-22 14:39:38+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,期待gguf，感谢感谢！
zhouzr,2024-04-22 15:27:43+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"GGUF 版本：https://huggingface.co/zhouzr/Llama3-8B-Chinese-Chat-GGUF


![image.png](https://cdn-uploads.huggingface.co/production/uploads/60115f685a0e64b5fa32d064/m10OzeIuJLQtWJYzeVKYZ.png)

"
shenzhi-wang,2024-04-23 03:17:50+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,"We provide the official 8bit-quantized and fp16 versions of Llama3-8B-Chinese-Chat in the following links, respectively. You are welcome to have a try!

8bit-quantized: https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit
fp16: https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-fp16"
hiyouga,2024-04-22 09:41:50+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-22 09:41:31+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-22 09:40:06+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-22 09:38:01+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-22 09:33:44+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-22 04:36:36+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
hiyouga,2024-04-22 04:30:31+00:00,comment,shenzhi-wang/Llama3-8B-Chinese-Chat,
michaelfeil,2024-05-05 06:52:23+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Hey @bartowski , Thanks for your quants! Potentially, that has been intruduced throught the training data. I will follow up with this tomorrow."
banahogg,2024-05-11 22:38:45+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Playing around a bit on gradientai/Llama-3-8B-Instruct-Gradient-1048k I'm seeing this as well but it mostly seems to happen on longer answers. For example, using text-generation-web-ui and the transformers loader, if I ask ""What's the capital of France?"" it'll answer ""Paris."" with no </s>. If I follow up with ""What's good to do with kids there?"" it'll generate a list of 10 items that ends with </s>"
wenrolland,2024-05-20 17:08:20+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Same thing here using ""RichardErkhov/gradientai_-_Llama-3-8B-Instruct-262k-gguf/Llama-3-8B-Instruct-262k.Q5_1.gguf"" in LM Studio with the Llama 3 format. Here's a typical output :

USER
Tell me about climate change.

AI
Climate change refers to long-term shifts in weather patterns, temperature and sea levels that can have significant effects on our planet's ecosystems and human societies. It is caused by a variety of factors including greenhouse gases released into the atmosphere through human activities such as burning fossil fuels for transportation and electricity, deforestation, industrial processes, and livestock farming.

The most common greenhouse gas contributing to climate change is carbon dioxide (CO2), which is produced when fossil fuels are burned or when organic matter decays in the absence of oxygen. Other greenhouse gases include methane from rice paddies, livestock farms, and landfills; nitrous oxide from fertilizers used in agriculture; and fluorinated gases released by air conditioning units.

The effects of climate change vary depending on the region and can range from more frequent and severe heat waves, droughts, and floods to rising sea levels that threaten coastal cities and low-lying areas. It also increases the risk of wildfires, reduces water availability, and can lead to food insecurity.

There are several ways we can take action to help mitigate climate change, such as reducing our carbon footprint by using public transportation or electric cars, eating less meat, reducing energy consumption at home by turning off lights when not in use, and supporting policies that promote renewable energy sources. However, the most effective way to address climate change is through global cooperation and collective effort. /s (Modified since it did not display the complete tag)

USER
What is the name of the largest ocean?

AI
The Pacific Ocean is the largest ocean. /s (Modified since it did not display the complete tag)

USER
What is the capital of Canada?

AI
Ottawa is the capital of Canada. /s (Modified since it did not display the complete tag)

USER
What is the name of Jupiter's largest moon? Give me just the name.

AI
Ganymede /s (Modified since it did not display the complete tag)"
michaelfeil,2024-05-20 23:45:07+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Hi all, we highly recommend to run this model with transformers / torch based implementations. 
We have seen some pretty weird things going on for long context, if you are e.g. using llama.cpp  (and all projects using llama.cpp as engine) @bartowski @banahogg @wenrolland 


"
wenrolland,2024-05-21 00:30:39+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Other than the ends with  /s,  this model rocks with LM Studios (Q4 an Q5). I copied a complete course note book, more than a 20000 tokens, and it's amazing how it can find and summarize the data. I can tolerate /s. :) I understand some have wilder rides with the model with lama.cpp but it's not useless in my situation, far from it. "
michaelfeil,2024-05-21 00:41:59+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"@wenrolland Cool! We also mad this model reason over the whole 262k context length. We are still looking into </s>, but for now that's how the model is. 

Also some experiment , let me think about what your thoughts are?

one.txt
```
Thanks for Michael Feil. Michael works at Gradient.ai.
{10k tokens of essays}
```

ten.txt
```
{10k tokens of essays}
Thanks for all the employees of Gradient.ai for proofreading this. 
Next time, I'll owe you an invitation for pancackes. (Everyones favorite dish at Gradient.ai).
```

seven.txt
```
{10k tokens of essays}
Thanks over great discussions i had when eating the pancackes with the employee of this one company which name i forgot. 
This was an interesting discussion, insights about their super secret project project of long context modeling with LLama3.
```

-> Can answer questions like this.
Whats the project Michael Feil worked on?"
wenrolland,2024-05-21 16:43:25+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Sadly, I don't have enough VRAM to use the full context. Keep up the great work with these models. I did many needle test, and it finds the information very efficiently, in French and English texts. "
michaelfeil,2024-05-04 03:05:31+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,Closing as completed
vihangsharma,2024-05-02 08:22:41+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,scam?
rombodawg,2024-05-02 22:24:08+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"@vihangsharma they had a nice reply here
https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k/discussions/11"
michaelfeil,2024-05-03 20:47:30+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Hey, @vihangsharma  as mentioned in the other threads we have worked on better alignment. 

@rombodawg We liked your meme!

https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k Let us know if you are interested in doing the same for 8B. "
rombodawg,2024-05-03 21:07:11+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"> Hey, @vihangsharma  as mentioned in the other threads we have worked on better alignment. 
> 
> @rombodawg We liked your meme!
> 
> https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k Let us know if you are interested in doing the same for 8B.


I would love to see 8b have the same effectiveness at extremely high context inference as the 70b. The majority of the open source community is running modest hardware, at most a rtx 3090 with 24gb of vram, but evem thats rare, an update to the 8b-instruct model would be astounding"
koesn,2024-06-01 11:44:22+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Much appreciate to Gradient team, thank's for this amazing model. I haven't tried to the extent over 100k tokens. But I'm actively using ±26k-±100k input including very long system prompt, exactly as Mark said about this use case. Miqu is great on handling those scenario, but it's 32k is very limiting so I have to back and forth to GPT-4o. Now Gradient 70b 262k fill the gaps and I replaced Miqu with it. Now I'm happily using Gradient's 262k to process my ±100k tokens system prompt. Gradient's legacy brings new possibilities."
bullerwins,2024-04-30 11:24:14+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,Added Exl2 quantized versions
michaelfeil,2024-04-30 14:59:17+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,Thanks!
michaelfeil,2024-04-30 16:34:11+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,Feel free to also add it for the 1M if you have time for it!
subbur,2024-04-28 06:09:36+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"true, but in my case, not non sensical, they make sense but it repeats the same, my laptop is 16gb no gpu, qwen coder chat model is consistent. But happy to see long answers though repetitive"
Michael22,2024-04-29 07:00:01+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,Try https://www.jentsch.io/meta-llama-3-70b-instruct-q4_k_m-gguf-eos-token-fix/
meirly,2024-04-29 16:49:16+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Make sure to correctly including End Of Stream token in your prompt (as the above post says, in german I think)

For llama.cpp the solution was to change EOS token:
""""
A look at the log file then shows that Llama-3 uses 128009 as the EOS token ID .

However, 128001 is entered in the GGUF file . So this can't work. Luckily, llama.cpp has a small script that allows you to change the EOS token ID. The following call changes the EOS token ID of the Meta-Llama-3-70B-Instruct-Q4_K_M.gguf file to 128009.

python llama.cpp/gguf-py/scripts/gguf-set-metadata.py gguf/Meta-Llama-3-70B-Instruct-Q4_K_M.gguf tokenizer.ggml.eos_token_id 128009 --force
"""""
nullt3r,2024-04-29 17:03:43+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Yes, I did do that. Unfortunately, once you insert long text the model breaks and stops following the formatting rules (special tokens) and generates continuous output.

Also it generates weird responses, for example:

user: hello
bot: Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat? I'm happy to hear from you but I'll need a moment to check in with myself before our conversation. I just had another request for help that I need to respond to. Thank you very much for waiting."
vihangsharma,2024-05-02 08:23:00+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,has anything changed?
michaelfeil,2024-04-29 21:36:56+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Thanks for your comment. The majority of the token composition is indeed pre-training style data. Creating better long context synthetic datasets could be an interesting follow-up work (currently limited by OSS models having to low of a context to produce such datasets, and the fact that most human written books are <= 100K context length)"
michaelfeil,2024-05-02 15:31:27+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,This is incorrect.
bullerwins,2024-04-26 09:29:02+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,6.0bpw exl2 https://huggingface.co/bullerwins/gradientai_Llama-3-8B-Instruct-262k_exl2_6.0bpw
bullerwins,2024-04-26 10:42:49+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,5.0bpw exl2 https://huggingface.co/bullerwins/gradientai_Llama-3-8B-Instruct-262k_exl2_5.0bpw
michaelfeil,2024-04-27 05:11:33+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Amazing @bullerwins . Just curious:
- How did you quantize them / which scripts?
-  which UI / Inference engine are you using for the exl2 quants and local inference?"
michaelfeil,2024-04-29 20:47:15+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,@bullerwins Feel free to open a PR to the Readme!
bullerwins,2024-04-30 11:20:51+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"> Amazing @bullerwins . Just curious:
> - How did you quantize them / which scripts?
> -  which UI / Inference engine are you using for the exl2 quants and local inference?

I'm using Turboderp's exllamav2 https://github.com/turboderp/exllamav2

Script:
python3 convert.py -i gradientai_Llama-3-8B-Instruct-262k/ -o /temp/ -cf gradientai_Llama-3-8B-Instruct-262k_exl2_5.0bpw/ -b 5.0

I'm testing it using Oobabooga's Textgen webui for inference: https://github.com/oobabooga/text-generation-webui

"
michaelfeil,2024-05-03 20:48:42+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"@bullerwins Working on a revised version (with better chat alignment). Would you be up for creating quants? Id be linking them in the readme again

Also: We released the better alignment for 70-B.
https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k"
michaelfeil,2024-05-04 04:36:15+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"@bullerwins We just upgraded the weights, you should see a drastic improvment over the previous iteration."
bullerwins,2024-05-04 11:47:44+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,Working on the upgraded 8B weights exl2 quants as well as 70B with better alignment
bullerwins,2024-05-04 14:02:27+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Exl2 quants for today's (4th of May 2024) updated weights:

I called them v2 for clarity

8.0bpw https://huggingface.co/bullerwins/gradientai_Llama-3-8B-Instruct-262k_v2_exl2_8.0bpw
6.0bpw https://huggingface.co/bullerwins/gradientai_Llama-3-8B-Instruct-262k_v2_exl2_6.0bpw
5.0bpw https://huggingface.co/bullerwins/gradientai_Llama-3-8B-Instruct-262k_v2_exl2_5.0bpw

Opening a PR for the readme"
michaelfeil,2024-04-25 22:31:28+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,@3thn
michaelfeil,2024-04-25 23:14:24+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Ill merge it, feel free to open a PR if you find some typos or have further quants!"
michaelfeil,2024-04-25 21:04:56+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,Merging!
michaelfeil,2024-04-25 21:00:33+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,CC @3thn awaiting your feedback
3thn,2024-04-25 21:07:28+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,"Looks great, thank you!"
michaelfeil,2024-04-25 21:11:50+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,@3thn Merging!
michaelfeil,2024-04-25 16:37:19+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,
michaelfeil,2024-04-25 07:39:32+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,
michaelfeil,2024-04-25 08:05:46+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,.
michaelfeil,2024-04-25 06:24:49+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,
michaelfeil,2024-04-25 06:42:07+00:00,comment,gradientai/Llama-3-8B-Instruct-262k,THIS IS THE RUN FOR high perplexity - do not merge! 262k trained from scratch -> bad rope.
shenzhi-wang,2024-05-11 07:42:54+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat-GGUF-4bit,"We are still uploading the GGUF file. Due to the network problem and the large size of this GGUF file, it may take some time. We are really sorry for that. If you want to use our q4 GGUF, you can use the ollama q4 model first. 

https://ollama.com/wangshenzhi/llama3-70b-chinese-chat-ollama-q4"
shenzhi-wang,2024-05-11 09:34:07+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat-GGUF-4bit,We have now uploaded the GGUF file.
m-polignano-uniba,2024-06-03 15:59:06+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"The used template is the standard one for Llama-3:
""""""<|start_header_id|>system<|end_header_id|>

Sei un an assistente AI per la lingua Italiana di nome LLaMAntino-3 ANITA (Advanced Natural-based interaction for the ITAlian language). Rispondi nella lingua usata per la domanda in modo chiaro, semplice ed esaustivo. <|eot_id|><|start_header_id|>user<|end_header_id|>

{user_prompt} <|eot_id|> <|start_header_id|>assistant<|end_header_id|>

{assistant_prompt} <|eot_id|> """"""

From a computational point of view:

""<|start_header_id|>system<|end_header_id|>\n\n"" + system_prompt + ""<|eot_id|>"" +
""<|start_header_id|>user<|end_header_id|>\n\n""+user_prompt+""<|eot_id|>""+
""<|start_header_id|>assistant<|end_header_id|>\n\n""+assistant_prompt+""<|eot_id|>"""
aristidecittadino,2024-06-04 08:24:45+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"I took a test of the LLM only and it works! Thank you :)
Now I'm just missing the last piece: if I wanted to add the context (which is retrieved via a vector search from stored documents) in which part of the prompt should I add it? 
I tried adding it on {assistant_prompt} but it didn't work"
m-polignano-uniba,2024-06-04 09:04:26+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"The context must be included in the {user_prompt} section. The {assistant_prompt} refers to the text generated by the model that must be included if you want to make the model reasoning by considering also the history of the dialog with the final user. Take a look here: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/ at the ""multiple turn conversation"" section
"
aristidecittadino,2024-06-04 15:36:24+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"Totally clear, thank you"
m-polignano-uniba,2024-05-31 10:55:33+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,Thank you for your recommendation. We will consider your suggestion.
m-polignano-uniba,2024-05-17 20:29:44+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"Hi,  you can find the DPO script here: https://github.com/marcopoli/LLaMAntino-3-ANITA/blob/main/model_adaptation/dpo_llama3.py
and the SFT script here: https://github.com/marcopoli/LLaMAntino-3-ANITA/blob/main/model_adaptation/finetune_llama3.py
Just change ""model_name"" and ""dataset"" accordingly. For the adaptation on the Italian language, just use the  SFT script on a small portion of an Italian Data (e.g., gsarti/clean_mc4_it) using plain text without chat template, i.e. (<|begin_of_text|> {text} <|eot_id|><|end_of_text|>)"
anakin87,2024-05-17 20:45:36+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"Thanks.
Very informative!"
e-palmisano,2024-05-31 08:59:33+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"Hi @m-polignano-uniba,

Is fine-tuning with the Italian language performed with QLoRA/LoRa or without?"
m-polignano-uniba,2024-05-31 09:12:14+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"Yes, we used QLoRA through Unsloth:
- load_in_4bit=True,  r = 64, lora_alpha = 16, [""q_proj"", ""k_proj"", ""v_proj"", ""o_proj"", ""gate_proj"", ""up_proj"", ""down_proj""]"
anakin87,2024-06-07 17:17:12+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"During the language adaptation phase, can you share a rough idea of the peak GPU VRAM usage?

In the paper, I read you used an Nvidia H100 64GB GPU but further details would be much appreciated."
m-polignano-uniba,2024-06-10 08:34:13+00:00,comment,swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA,"Unfortunately, we use an HPC Cluster that does not allow us to check VRAM usage during training (mostly because the GPU are shared). Just a small correction: the graphics card is a custom NVIDIA A100-SXM-64GB (https://www.nvidia.com/it-it/data-center/a100/)"
bartowski,2024-06-07 19:34:54+00:00,comment,bartowski/openchat-3.6-8b-20240522-GGUF,"Fascinating.. I've never heard of this before, thanks for sharing!

I assume that you're using one of the supported types?

For what it's worth, it LOOKS like the only real purpose of this is the get an ""unquantized"" version that you can use elsewhere, is that correct? ""Now you have access to the full, unquantized version of the model in the PyTorch ecosystem, where you can combine it with a plethora of other tools.""

if so, are you just doing this as an experiment for fun or is there a reason you don't want to  use the original safetensors variant? "
bullerwins,2024-06-08 05:12:07+00:00,comment,bartowski/openchat-3.6-8b-20240522-GGUF,"> Fascinating.. I've never heard of this before, thanks for sharing!
> 
> I assume that you're using one of the supported types?
> 
> For what it's worth, it LOOKS like the only real purpose of this is the get an ""unquantized"" version that you can use elsewhere, is that correct? ""Now you have access to the full, unquantized version of the model in the PyTorch ecosystem, where you can combine it with a plethora of other tools.""
> 
> if so, are you just doing this as an experiment for fun or is there a reason you don't want to  use the original safetensors variant?

would the unquantized full version from the gguf be as precise as using the full weights to begin with though? I guess this would only be useful for stuff like the leaked miqu that only came in gguf, but with access to the original weights I don't really see the point"
bartowski,2024-06-08 13:25:54+00:00,comment,bartowski/openchat-3.6-8b-20240522-GGUF,"Yeah I think this is just for the case where you only have access to the GGUF and want the full safetensors, and likely won't be as accurate

If you download an f16/f32/bf16 GGUF then you'll have the same accuracy as the full safetensors already"
bartowski,2024-05-28 21:58:59+00:00,comment,bartowski/openchat-3.6-8b-20240522-GGUF,"that's basically exactly correct, good understanding

If you can fit it entirely in VRAM, go with exl2

if you want to push your capacity by offloading some work to system RAM, go with GGUF

Additionally exl2 has a nice advantage of offering Q4 quantization of context, allowing you to push your VRAM for context much further (not really useful with only 8k context, but worth keeping in mind as a notable difference)"
bartowski,2024-05-26 19:39:10+00:00,comment,bartowski/openchat-3.6-8b-20240522-GGUF,"You can try the existing OpenChat prompt template in LM Studio

This is an odd one, cause the model page lists the old template (the one you can find in lm studio under `OpenChat`) but the tokenizer defines a different one that you can find on my card. Still waiting for a response as to which is the proper one"
urtuuuu,2024-05-26 21:11:14+00:00,comment,bartowski/openchat-3.6-8b-20240522-GGUF,This comment has been hidden
jujbob,2024-05-27 03:25:49+00:00,comment,MLP-KTLim/llama-3-Korean-Bllossom-8B,"안녕하세요. 모델 총괄개발자 서울과기대 임경태 입니다.

일단 저희 모델에 관심을 주셔서 감사드립니다 :) 혹시 Coling에서 문의 주셨던 KT 팀과 같은부서 이신가요? ㅎㅎ
답변은 아래와 같습니다.

1. 유료화 사용의 제약조건이 없습니다. 앞으로 지속적으로 업데이트될 강화모델들도 마찬가지입니다. 그냥 맘놓고 쓰셔도 됩니다
2. 유료화 제약조건이 없습니다.
3. LLama3 라이선스 정책과  동일합니다 다만 신규 모델을 만드실때 originated from Bllossom 문구 한줄만 넣어주시면 됩니다.

기본적으로 저희는 대학교이고 또 국립대입니다. 지금은 과제 떨어져서 이것도 없지만 작년까지 나라에서 받았던 ""5000만원 내외의 NRF 기본연구""연구비와 서울과기대의 자원을 토대로 모두를 위한 공개 언어모델을 만드는것이 이번 프로젝트의 목적이에요.
이러한 맥락에서 추가로 여러 버전의 Advanced 8B 모델 공유 가능합니다. 해당 모델에 추가 학습시킬 예정이시면 소중한 시간 아끼세요! 개별연락주세요~"
KaiLee76,2024-05-27 04:15:41+00:00,comment,MLP-KTLim/llama-3-Korean-Bllossom-8B,안녕하세요. 답변 감사합니다.
ShinDJ,2024-05-23 09:10:34+00:00,comment,MLP-KTLim/llama-3-Korean-Bllossom-8B,"해당 오류 메세지라고 말씀하시는 것은 오류 메세지가 아닌 경고 메세지로 보입니다. 
해당 경고 메세지는 pad_token 이 정의가 안되어서 그런데 이는 추론과정에서는 필요없는 토큰입니다(배치추론이 아닌이상).
만약 해당 경고 메세지가 발생하지 않게 하고싶으시다면 pad_token을 지정해주시고 attention_mask를 생성하여 함께 넣어주시면 될것같습니다!

아마 instruction 부분에 아무것도 입력하지 않아서 모델이 답변을 생성하지 않아 경고메세지를 에러메세지로 착각하신걸로 보입니다!"
ehottl,2024-05-24 00:17:58+00:00,comment,MLP-KTLim/llama-3-Korean-Bllossom-8B,"> 해당 오류 메세지라고 말씀하시는 것은 오류 메세지가 아닌 경고 메세지로 보입니다. 
> 해당 경고 메세지는 pad_token 이 정의가 안되어서 그런데 이는 추론과정에서는 필요없는 토큰입니다(배치추론이 아닌이상).
> 만약 해당 경고 메세지가 발생하지 않게 하고싶으시다면 pad_token을 지정해주시고 attention_mask를 생성하여 함께 넣어주시면 될것같습니다!
> 
> 아마 instruction 부분에 아무것도 입력하지 않아서 모델이 답변을 생성하지 않아 경고메세지를 에러메세지로 착각하신걸로 보입니다!

빠른 답변 감사합니다ㅎ
instruction 부분에 프롬프트를 넣어야 하는군요ㅎ"
jujbob,2024-05-14 01:08:20+00:00,comment,MLP-KTLim/llama-3-Korean-Bllossom-8B,"안녕하세요 질문주신 multi-turn 관련된 문제는 해결 되셨나요?

말씀해주신 내용을 저희쪽에서 돌려보니 8B에서는 잘 작동할때도 있고, 이전컨텍스트를 잃어버려 다시 질문해달라고 이야기할때도 있네요. 확률은 반반 같습니다!
같은 내용을 70.8B 모델에서는 매우 잘 작동하고 있어요.

이를 토대로 multi-turn 기능이 약해졌다기 보다, long context에 대한 장기 multi-turn 기억이 llama3 기본 모델 크기에 따라 다른것 같습니다."
hahmyg,2024-05-09 00:36:34+00:00,comment,MLP-KTLim/llama-3-Korean-Bllossom-8B,.
maywell,2024-04-29 04:16:45+00:00,comment,MLP-KTLim/llama-3-Korean-Bllossom-8B,"nvm, i saw the description that vocab expansion is future work."
DeepMount00,2024-06-05 10:16:40+00:00,comment,DeepMount00/Llama-3-8b-Ita,https://huggingface.co/DeepMount00/Mistral-Ita-7b/discussions/2
DeepMount00,2024-05-01 15:02:15+00:00,comment,DeepMount00/Llama-3-8b-Ita,"Hello! Yes, this model is capable of handling question-answering tasks effectively. For a specific setup where you have a context and a question and need the model to generate an answer based on the context, I recommend using the model available at https://huggingface.co/DeepMount00/Mistral-RAG. This model is designed specifically for the question-answering task.

Alternatively, you could use LLaMA-3, but you would need to construct an appropriate prompt tailored to the question-answering task. Both options are suitable, but choosing the right one depends on your specific needs and constraints.
just to clarify: For both the Mistral-RAG model and LLaMA-3, it's important not to use the transformers.pipeline(""question-answering"", ...)"
Alkohole,2024-05-30 15:41:12+00:00,comment,openchat/openchat-3.6-8b-20240522,"No, it's not ollama/llamacpp, it's transformers.

I'm assuming the gguf F32 would be better..."
imone,2024-05-28 05:10:28+00:00,comment,openchat/openchat-3.6-8b-20240522,"Because Llama-3 8B Instruct can't follow few-shot templates correctly. It's likely that the official results are obtained with special prompts. To reproduce, use the following command with openchat repo

```
python -m ochat.evaluation.run_eval --model meta-llama/Meta-Llama-3-8B-Instruct --model-type llama3_instruct --eval_sets fs_cothub/mmlu fs_cothub/gsm8k fs_cothub/math zs/gpqa
```

HumanEval results are from [EvalPlus leaderboard](https://evalplus.github.io/leaderboard.html)

For example,

Instruction
```
The following are multiple choice questions (with answers) about high school biology.

Q: In animal cells, which of the following represents the most likely pathway that a secretory protein takes as it is synthesized in a cell?
(A) Plasma membrane–Golgi apparatus–ribosome–secretory vesicle–rough ER (B) Ribosome–Golgi apparatus–rough ER–secretory vesicle–plasma membrane (C) Plasma membrane–Golgi apparatus–ribosome–secretory vesicle–rough ER (D) Ribosome–rough ER–Golgi apparatus–secretory vesicle–plasma membrane
A: Let's think step by step. Protein synthesis starts at the ribosome, so we can eliminate (A) and (C). The ribosome is often in the endoplasmic reticulum and moves from there to the Golgi apparatus, where it is modified and packaged into a vesicle. The vesicle then floats to the plasma membrane and is secreted. The answer is (D).

Q: A mutation in a bacterial enzyme changed a previously polar amino acid into a nonpolar amino acid. This amino acid was located at a site distant from the enzyme’s active site. How might this mutation alter the enzyme’s substrate specificity?
(A) By changing the enzyme’s pH optimum (B) By changing the enzyme’s location in the cell (C) By changing the shape of the protein (D) An amino acid change away from the active site cannot alter the enzyme’s substrate specificity.
A: Let's think step by step. A change in an amino acid leads to a change in the primary structure of the protein. A change in the primary structure may lead to a change in the secondary and the tertiary structure of the protein. A change in the tertiary structure means a change in the shape of the protein, so (C) has to be correct. Since the change does not affect the active site of the enzyme, we do not expect the activity of the enzyme to be affected. The answer is (C).

Q: Which of the following is not a way to form recombinant DNA?
(A) Translation (B) Conjugation (C) Specialized transduction (D) Transformation
A: Let's think step by step. The introduction of foreign DNA or RNA into bacteria or eukaryotic cells is a common technique in molecular biology and scientific research. There are multiple ways foreign DNA can be introduced into cells including transformation, transduction, conjugation, and transfection. In contrast, (A) is not a way to form DNA: during translation the ribosomes synthesize proteins from RNA. The answer is (A).

Q: Homologous structures are often cited as evidence for the process of natural selection. All of the following are examples of homologous structures EXCEPT
(A) the wings of a bird and the wings of a bat (B) the flippers of a whale and the arms of a man (C) the pectoral fins of a porpoise and the flippers of a seal (D) the forelegs of an insect and the forelimbs of a dog
A: Let's think step by step. ​​Homologous structures are similar physical features in organisms that share a common ancestor ​​but different functions. Comparisons (B) and (C) are clearly homologous because they share a common ancestor and the structures serve different purposes. Bat wings and birg wings are also homologous, while they are both wings, the forelimbs serve different purposes. Insects and dogs are very far ancestors since one is vertebrate while the other is invertebrate and the forelimbs serve the same purpose, so they are not homologous. The answer is (D).

Q: Which of the following is not known to be involved in the control of cell division?
(A) Cyclins (B) Protein kinases (C) Checkpoints (D) Fibroblast cells
A: Let's think step by step. Normal cells move through the cell cycle in a regulated way. At the checkpoint stage, they use information about their own internal state and cues from the environment around them to decide whether to proceed with cell division. Cues like these act by changing the activity of core cell cycle regulators inside the cell. The most common regulators are cyclins and cyclin-dependent kinases. Fibroblast cells do not play any role in cell division. The answer is (D).

Mosquitoes resistant to the pesticide DDT first appeared in India in 1959 within 15 years of widespread spraying of the insecticide. Which of the following statement best explains how the resistant mosquitoes arose?
(A) Some mosquitoes experienced a mutation after being exposed to DDT that made them resistant to the insecticide. Then their population expanded because these moquitoes had no competition. (B) Some mosquitoes were already resistant to DDT when DDT was first sprayed. Then their population expanded because all the susceptible mosquitoes had been exterminated. (C) DDT is generally a very effective insecticide. One can only conclude that it was manufactured improperly. (D) Although DDT is effective against a wide range of insects, it is not effective against mosquitoes. 
A: Let's think step by step.
```

Response
```
It looks like you're providing multiple-choice questions with answers about high school biology. I'd be happy to help you go through them!

Please go ahead and provide the next question, and I'll help you analyze it step by step.
```"
urtuuuu,2024-05-26 21:20:29+00:00,comment,openchat/openchat-3.6-8b-20240522,This comment has been hidden
imone,2024-05-28 05:15:39+00:00,comment,openchat/openchat-3.6-8b-20240522,"It's likely that Llama-3-8b instruct official results were obtained with special prompts. The measured results are quite low, see [here](https://huggingface.co/openchat/openchat-3.6-8b-20240522/discussions/6) for discussions."
fblgit,2024-05-30 21:45:49+00:00,comment,openchat/openchat-3.6-8b-20240522,The score marks of a rusty test are irrelevant. I believe here we are trying to really prove this model and strategy to be most optimal right? :D
guidevops,2024-05-25 03:00:29+00:00,comment,openchat/openchat-3.6-8b-20240522,someone already did it https://huggingface.co/crusoeai/openchat-3.6-8b-20240522-GGUF
bartowski,2024-05-25 18:04:39+00:00,comment,openchat/openchat-3.6-8b-20240522,"few more sizes (and confirmed imatrix, not sure what crusoeai does) here:

https://huggingface.co/bartowski/openchat-3.6-8b-20240522-GGUF

exl2 here also for anyone who wants:

https://huggingface.co/bartowski/openchat-3.6-8b-20240522-exl2

"
Orenguteng,2024-05-09 13:01:27+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,@GoEnthusiast  You can use LM Studio and search for the Lexi-Uncensored GGUF version and use it. Check youtube for tutorials on LM Studio.
Orenguteng,2024-05-05 11:27:23+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"I'm not sure , it seems that you are trying to run chat completions , inference on the model but this model is not hosted to run inference with on HF, it's just the model files, you need to download them and run them yourself. "
Orenguteng,2024-05-05 12:48:02+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,@pfifo Sorry my datasets are not public. 
Orenguteng,2024-04-30 08:55:36+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,You want a new GGUF quant in the gguf repo correct? I could re-upload that tonight
concedo,2024-04-30 10:44:16+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"Which is the newer one, this or the one labelled as V1?"
Orenguteng,2024-04-30 15:40:04+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"@concedo The V1 is named ""LexiFun"" it's something different. It is the first version experiment and become better in the next. This one however, is the regular Llama3-8B."
BlueNipples,2024-05-03 08:20:09+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"> You want a new GGUF quant in the gguf repo correct? I could re-upload that tonight

Yes. There's the possibility the changed/fixed tokenization in the new llamacpp breaks old ggufs. There definitely appears to be something screwy going on when I try to run them. "
concedo,2024-05-03 09:11:45+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"For now, if anyone wants, I've created a PR with a few files re-quanted here: 
https://huggingface.co/Orenguteng/Llama-3-8B-Lexi-Uncensored-GGUF/tree/refs%2Fpr%2F5"
Orenguteng,2024-04-29 01:20:18+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"@imoc the prompt template is the default Llama3-8B instruct themplate, nothing changed"
Orenguteng,2024-04-29 01:20:57+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,@imoc https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
Orenguteng,2024-04-25 23:54:47+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"Will look into it. However, uncensored version V2 is on the way =) Stay tuned. In the meanwhile, enjoy LexiFun that just been released."
askyforever,2024-04-29 22:57:04+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"> Someone with a cuda device should use [control vectors](https://vgel.me/posts/representation-engineering) ([llama.cpp control vectors](https://github.com/ggerganov/llama.cpp/pull/5970)) to train control vectors for LLaMa-3-8B: one that is normal (base), and one that is instructed to censor everything. The trained control vectors could be used to cancel out the ""censor everything"" behaviours and theoretically uncensor the model. Then we can either just run LLaMa-3-8B GGUF'ed with the proper settings (including control vector settings) and control vector GGUF'ed, or we could use these resources to build a truly uncensored DPO dataset for LLaMa-3-8B and liberate LLaMa-3 from its cantankerous censorship (the same could be done for LLaMa-3-70B). I'm tired of the censored responses and refusals. 😩🤔

I'm trying to understand this better. Are you saying for foundation models, there are specific vector space in which is trained to be censored? Then we create some kind of ""censor everything"" and it inverts it? How does censor everything work? So like any prompt would be censored, then use it to somehow invert the weights?"
Joseph717171,2024-04-30 00:33:03+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"Thank you for correcting me! I meant LLaMa-3-8B-Instruct. And to answer your question: It works like this: an llm is given a dataset and we give it a prompt (e.g: Be completely censoring)  that it runs over dataset with, we capture its activations, training a control vector. Next we do the same thing, but we subtract out the ""Be completely censoring"" control vector, it is detailed at the [vgel's blog ](https://vgel.me/posts/representation-engineering) this removes the censoring, refusals, etc entirely. So, yes: in a way we are inverting the weights, but it isn't the weights that are being inverted as much as it is the vectors in the llms matrices being orthogonalized. If I understand it right. 🤔 "
rdtfddgrffdgfdghfghdfujgdhgsf,2024-04-30 04:14:35+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,*sniffs the air for a V2....?
fertingoff,2024-05-02 07:17:47+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"> Will look into it. However, uncensored version V2 is on the way =) Stay tuned. In the meanwhile, enjoy LexiFun that just been released.

Thanks for sharing v1. What's in v2 or what's the rationale behind it? 🙂"
Raidar,2024-05-03 23:07:30+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"> Thank you for correcting me! I meant LLaMa-3-8B-Instruct. And to answer your question: It works like this: an llm is given a dataset and we give it a prompt (e.g: Be completely censoring)  that it runs over dataset with, we capture its activations, training a control vector. Next we do the same thing, but we subtract out the ""Be completely censoring"" control vector, it is detailed at the [vgel's blog ](https://vgel.me/posts/representation-engineering) this removes the censoring, refusals, etc entirely. So, yes: in a way we are inverting the weights, but it isn't the weights that are being inverted as much as it is the vectors in the llms matrices being orthogonalized. If I understand it right. 🤔

Out of curiosity, would you be interested in implementing this? I dont mind covering the costs. 
"
algorithm,2024-05-04 15:47:30+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"@Joseph717171 @Raidar are you guys referring to this? https://old.reddit.com/r/LocalLLaMA/comments/1chon5a/llama38b_implementation_of_the_orthogonalization/

Or is that a different approach?
Unfortunately the repo owner only released exl2 and isn't responding to any requests for other formats.

It's based on this work: https://www.alignmentforum.org/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction

They are currently trying to recreate it here: https://huggingface.co/posts/Undi95/318385306588047"
Joseph717171,2024-05-04 23:23:14+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,@Raidar I'm interested; I just don't have time right now. 😁 
LoafyLemon,2024-05-17 22:13:43+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"I don't know what you did, or what kind of dataset you've used, but whatever it is, this model is the best. It's the only one that is not dry, or boring to talk to, and understands nuances of the language, and is pretty much the only model that called me out on my bullshit haha. Holding my breath for V2!"
weiminw,2024-06-05 10:01:10+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,support chinese?
Orenguteng,2024-04-25 10:11:42+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,"@MikeI This one needs rephrasing of the prompt sometimes , for example ""Write a step by step..."", but the next version V2 coming soon with fixes for this it will be fully uncensored! Coming any moment now"
10100101j,2024-04-25 11:26:01+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,merging the base over instruct works better tbh to remove censorship
Orenguteng,2024-04-24 00:47:59+00:00,comment,Orenguteng/Llama-3-8B-Lexi-Uncensored,Great to hear! I'm aiming to keep the smarts of the llama-3-8b original model as much as possible. Got a new model coming soon that will be better than this one! Stay tuned :) 
lastdefiance20,2024-06-07 09:14:40+00:00,comment,maum-ai/Llama-3-MAAL-8B-Instruct-v0.1,"안녕하세요, 현재 llama3 70b 모델의 경우 오픈소스 공개 예정은 없으며, 더욱 업그레이드된 llama3 8b 모델의 경우에만 공개가 예정되어 있습니다."
failspy,2024-05-21 04:33:12+00:00,comment,failspy/Meta-Llama-3-8B-Instruct-abliterated-v3,"Ah, yeah, appears to be. For what its worth, the tokenizer model is the same as Llama-3-8B-Instruct, so if you have those files locally available, you can copy paste them and they should just work. But thanks for pointing this out, will add them to the repo ASAP"
ayyylol,2024-05-21 05:27:41+00:00,comment,failspy/Meta-Llama-3-8B-Instruct-abliterated-v3,Thank you very much!
failspy,2024-05-22 05:47:19+00:00,comment,failspy/Meta-Llama-3-8B-Instruct-abliterated-v3,Published.
failspy,2024-05-20 15:06:00+00:00,comment,failspy/Meta-Llama-3-8B-Instruct-abliterated-v3,"Oh derp, I completely forgot to upload it. I'll post it in the next couple hours. Sorry!"
epigene,2024-05-20 15:06:52+00:00,comment,failspy/Meta-Llama-3-8B-Instruct-abliterated-v3,np Thanks!
failspy,2024-05-20 16:28:46+00:00,comment,failspy/Meta-Llama-3-8B-Instruct-abliterated-v3,Published. https://huggingface.co/failspy/Meta-Llama-3-8B-Instruct-abliterated-v3-GGUF
Haoxiang-Wang,2024-05-24 07:05:48+00:00,comment,RLHFlow/pair-preference-model-LLaMA3-8B,"Since it's finetuned from Llama3, it is under the Llama3 license. "
luxen1234,2024-05-27 01:29:22+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,hello
luxen1234,2024-05-27 01:29:33+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,hi
shenzhi-wang,2024-05-27 01:58:51+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,"I have generated a DOI just now.

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6486dde1f74857df3f1a5828/29PRyV95220nNAnUnP6gC.png)
"
shenzhi-wang,2024-05-27 02:09:04+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,"You can cite our 70B model as follows:

```
@misc {shenzhi_wang_2024,
	author       = {Wang, Shenzhi and Zheng, Yaowei},
	title        = {Llama3-70B-Chinese-Chat (Revision 3f5a0d4)},
	year         = 2024,
	url          = {https://huggingface.co/shenzhi-wang/Llama3-70B-Chinese-Chat},
	doi          = {10.57967/hf/2315},
	publisher    = {Hugging Face}
}
```"
shenzhi-wang,2024-05-22 12:28:04+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,"我们在Gitee上上线了一个在线体验的demo，欢迎尝试：

https://ai.gitee.com/shenzhi-wang/llama3-70b-chinese-chat"
shenzhi-wang,2024-05-22 12:33:38+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,需要登录Gitee账号，然后使用邀请码 `llama3`。
QiuQiuShouLing,2024-06-03 01:51:47+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,"> 需要登录Gitee账号，然后使用邀请码 `llama3`。

邀请码无效啊
"
Million,2024-06-04 09:51:00+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,邀请码无效啊+1
Orion-zhen,2024-06-03 07:15:06+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,自己量化了一个: [Orion-zhen/Llama3-70B-Chinese-Chat-4.0bpw-exl2](https://huggingface.co/Orion-zhen/Llama3-70B-Chinese-Chat-4.0bpw-exl2)
donglai1,2024-05-17 02:08:27+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,+1
bujido,2024-05-18 18:21:34+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,"> +1

自己量化了一个，试试吧
https://huggingface.co/bujido/Llama3-70B-Chinese-Chat-AWQ-32k
https://modelscope.cn/models/BUJIDAOVS/Llama3-70B-Chinese-Chat-AWQ-32k/summary"
worldggg,2024-05-20 04:06:49+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,请问能提供GPTQ-Int8版本吗？
worldggg,2024-05-20 04:13:18+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,我的显卡无法运行AWQ量化模型，所以可以提供GPTQ-Int8版本吗?
william0014,2024-06-05 15:08:15+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,是否能提供AWQ或者GPTQ-INT4 版本.
shenzhi-wang,2024-05-14 06:38:44+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,"可以先考虑用我们的GGUF q4版本，或者Ollama q4/q8版本

https://huggingface.co/shenzhi-wang/Llama3-70B-Chinese-Chat-GGUF-4bit
https://ollama.com/wangshenzhi/llama3-70b-chinese-chat-ollama-q4
https://ollama.com/wangshenzhi/llama3-70b-chinese-chat-ollama-q8"
worldggg,2024-05-15 01:50:21+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,非常感谢你的回复，而且给出了相关的详细资料。现在的情况是：由于是p100-16G的GPU，使用Ollama运行速度非常慢，现在使用vllm的特殊版能够运行GPTQ-Int8，而且速度能够比Ollama快10倍，所以才找GPTQ量化版本。但由于自己不会量化，所以才试着问一下，如果你不方便的话就算了。
donglai1,2024-05-17 02:10:15+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,能否提供GPTQ-INT8的，想和int4的对比一下
worldggg,2024-05-17 08:32:45+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,这是原版的：https://modelscope.cn/models/huangjintao/Meta-Llama-3-70B-Instruct-GPTQ-Int8/summary
shenzhi-wang,2024-05-11 08:07:07+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,Thank you very very much!
woshimark666,2024-05-11 09:13:07+00:00,comment,shenzhi-wang/Llama3-70B-Chinese-Chat,"Nice, bro. Dowloading!"
EtienneDu91,2024-05-07 09:59:41+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-262k,"Hello, Bumping this question because I am also interested in the answer."
theobjectivedad,2024-05-07 20:09:59+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-262k,"same, seeing periodic garbage coming from the model and wondering if it is the prompt format. "
ubergarm,2024-05-30 17:50:57+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-262k,"I've had this question so many times with the various models. I finally realized when loading a GGUF model with llama.cpp it prints out the prompt format / chat template in the logs after loading. Digging around I realized many (?most?) models have it in the `tokenizer_config.json`of the base model repo e.g.:

https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-262k/blob/main/tokenizer_config.json#L2053

```json
""chat_template"": ""{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"",
```

This appears to match the ""normal Llama-3-70B"" template that I've seen and used successfully on other models using this one as the base:
```
<|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n""
```

I've tested this on `mradermacher/Llama-3-70b-Arimas-story-RP-V1.5-i1-GGUF/Llama-3-70b-Arimas-story-RP-V1.5.i1-IQ3_XXS.gguf` which uses this model as a base. Without this prompt format it generates a lot of continuations and pretends to be the user role and includes a lot of `---` breaks.  However, using the correct prompt format shown above prevents it from rambling on or acting out of turn generating garbage.

Hopefully this helps someone, Good luck!"
leo-pekelis-gradient,2024-05-31 22:42:46+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-262k,"Yes, we use the same prompt format as the base llama 3 instruct models, https://github.com/meta-llama/llama-recipes?tab=readme-ov-file#llama-recipes-examples-to-get-started-using-the-llama-models-from-meta
"
leo-pekelis-gradient,2024-05-03 11:59:21+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-262k,
siddartha-abacus,2024-06-03 20:48:50+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,Not really a typo but left over from model eval / upload. Fixing it to the correct value.
AliceThirty,2024-05-26 07:53:29+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"> I have been using the int4 quant on 2 x AMD 7900XTX. Works well, but the output never ends. As the output continues it starts to break down into internal tokens, training data extracts, etc.

Sorry to bother you, but what do you use ? LMStudio ? Koboldcpp ? Oobabooga ? And what is your generation speed in tokens per second ? Is it a nightmare to work with ROCm ?
I am curious because I plan on buying an AMD gpu."
bkieser,2024-05-26 14:15:24+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"> > I have been using the int4 quant on 2 x AMD 7900XTX. Works well, but the output never ends. As the output continues it starts to break down into internal tokens, training data extracts, etc.
> 
> Sorry to bother you, but what do you use ? LMStudio ? Koboldcpp ? Oobabooga ? And what is your generation speed in tokens per second ? Is it a nightmare to work with ROCm ?
> I am curious because I plan on buying an AMD gpu.

It's my pleasure.

I have 2 x Radeon RX 7900 XTX  (2 x 24GB) and I use Lllama.cpp running in server mode because some of the inference needs to be offloaded to Ram. 
For Smaug, best setting is ngl 70 (70 layers offloaded onto the 2 7900 cards) with context 4096 (-c 4096)

Here is an example of the command line:
    llama.cpp/server -m /home/tmp/Smaug-Llama-3-70B-Instruct.i1-Q4_K_M.gguf -n 4096 -c 4096  -ngl 70 --host 0.0.0.0 --port 2600 -a smaug

You can see that I am running it in server mode while in dev, it will be productionised when it moves internally for the staff to use.

Regarding AMD:
1) Works very well
2) Not as fast as NVIDIA, but a lot cheaper!
3) Memory is what matters, not GPU speed. You need that precious VRAM to have a useful context length and enough parameters for a genuinely useful LLM to run. Memory is more important than anything else.
4) I have a Ryzen 7 7700X CPU. This is a bottlneck. A threadripper would be much better, esp. if you quantise to int 8 (q8) which is so much more efficient for both CPU and GPU.
5) CPU lifts a load in the layers that can't be offloaded onto the GPUs and also, AMD support for shared memory is currently really bad and so CPU works hard shovelling data around. NVIDIA is massively more efficient and CUDA is very clearly better than ROCM. But.... ROCM works. Maybe not as fast, maybe the CPU works really hard too, but you do get the results.
6) This is very important: The AMD cards ARE SMALLER than the massive Nvidia RTX 4090 that I have in the  other server. This matters because the NVIDIA uses up all the physical slot space on a standard PC motherboard, you can only fit 1 on unless you go to a specialised board and then suddenly the price jumps hugely, 300% or more. But the AMD uses only 2 slots of space, which means that for most standard PC motherboards you can fit 2 cards in (but not 3, which will also require specialised motherboard). This size of card issue is very important because the GPU itself is only part of the cost.
7) When you have 2 x cards, then you can also load in 2 x smaller LLMs that fit completely on each card and don't have to share data via the CPU and that is massively faster because it's all on the GPUs. There is a very real advantage to being able to run 2 LLMs in parallel even if the actual GPUs are slower than the mighty Nvidia 4090.

Speed: I am getting around 17 t/s which isn't great, sure, but it's running locally on a powerful LLM, no data being shared and able to access internal tools behind a firewall, so there's a big advantage there. Also I use crewai, autogen, etc and I can delegate to smaller models which run entirely on the GPU(s) if needed and that is then many times faster. It's a matter of carefully choosing which LLM you actually need for that bit of the task.

Hope this helps.;"
bkieser,2024-05-26 14:28:38+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"I forgot to add, if in-memory, entirely on GPU (i.e. no CPU offloading), then getting 122 t/s."
AliceThirty,2024-05-27 10:53:40+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"> I forgot to add, if in-memory, entirely on GPU (i.e. no CPU offloading), then getting 122 t/s.

Thank you very much for your detailed answer ! And when you say that you have 122 t/s, is it for a 70B model ? I expected the generation to slow down due to the communication between the two gpus"
RDson,2024-05-19 20:16:58+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"The full model will not, but a quantized model will and at Q4 (which is usually the preferred quant) it will certainly better than the 8B model. You will need to trade-off between speed and performance. You can find performance comparisons [here](https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference)."
gopi87,2024-05-20 07:03:08+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"hi plese use try to use 

https://huggingface.co/mradermacher/Smaug-Llama-3-70B-Instruct-i1-GGUF

iq4 xs 

if you have 64 gp you can use 	i1-Q5_K_M"
RDson,2024-05-19 20:05:49+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"The same as Llama 3, about 8k. You can find it in the config.json file '""max_position_embeddings"": 8192'."
ArkaAbacus,2024-05-19 11:50:01+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,Actually we already have one! https://huggingface.co/abacusai/Llama-3-Smaug-8B This doesn't have instruct in the name but it is an instruct model. Although we have improved on the recipe since the 8B release so we might update it soon...
Dampfinchen,2024-05-19 12:08:35+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"> Actually we already have one! https://huggingface.co/abacusai/Llama-3-Smaug-8B This doesn't have instruct in the name but it is an instruct model. Although we have improved on the recipe since the 8B release so we might update it soon...

I think they wanted Smaug 8B trained on LLama 8B Instruct instead of the base model. Models trained on instruct tend to do a lot better, as I'm sure you have noticed with Smaug 70B Instruct."
ArkaAbacus,2024-05-19 12:27:00+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,It was trained on Llama 8B Instruct! We just didn't say so in the model card. I've fixed that now.
ManniX-ITA,2024-06-06 14:35:50+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"@ArkaAbacus 
May I ask if you are already planning to update the 8B model with the new recipe and if you have an approximate ETA?
No hurry, I'm just planning to make my own quantization for the ollama library and would like to avoid the new model comes out the day after I'm done with it :P"
ArkaAbacus,2024-05-19 11:49:23+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,"Yes it's the same!
I just added this section: https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct#how-to-use"
ArkaAbacus,2024-05-19 11:50:17+00:00,comment,abacusai/Smaug-Llama-3-70B-Instruct,Thank you
zihanliu,2024-05-16 06:08:06+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"Hi,
It looks like you are using our model in the chat scenario where the context is not involved. Did you follow the prompt format for this case in our model card? 
It should look like this 
```
System: {System}

User: {Question}

Assistant: {Response}

User: {Question}

Assistant:
```"
Albihany,2024-05-17 01:19:30+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"Hi
Honestly, after spending a decent time today chatting with the model through  Ollama  i had no issue so far and i gotta say i'm impressed with how fast, smart and chatty is the model, However, it still doesn't work well with obsidian neither with Textgen, nor with Copilot plugging's , i have tested many other models with those plugins (including  llama3, llava-llama3, llava, llava-llama3) Yet your model is the only model that stop after little while of chatting, or not work at all is yours, i really need this model to work with obsidian, it's the best close to  Chat GPT-4 and i can chat with it locally with no limitation, i really need this. i hope you give it a little test with obsidian plugins and tell us what the issue is?

Cheers 
ALBI  


![psd1.png](https://cdn-uploads.huggingface.co/production/uploads/65fd4f0bfa59bdf473b31c11/Whi7IWFvqFgMc-U0PQJKX.png)
"
zjyhf,2024-05-15 03:56:10+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"Using vllm to infer 'Llama3-ChatQA-1.5-70B', it will continue to be generated when encountering the special token '<|im_end|>', as shown in the figure below. This PR adds a mapping to '<|im_end|>' in the tokenizer.
At the same time, '<|im_end|>' needs to be configured in the tokenizer: https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B/discussions/12

![8e4f01f676a0de25c1412b10172cfa9.png](https://cdn-uploads.huggingface.co/production/uploads/66161a077b605932bfbc106b/w1TlixMzjRbaK2btJvUwX.png)"
zjyhf,2024-05-15 03:49:23+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"Using vllm to infer 'Llama3-ChatQA-1.5-70B', it will continue to be generated when encountering the special token '<|im_end|>', as shown in the figure below. This PR adds <|im_end|> to the tokenizer, and you need to add mapping to generation_config.json.
![8e4f01f676a0de25c1412b10172cfa9.png](https://cdn-uploads.huggingface.co/production/uploads/66161a077b605932bfbc106b/TSeY_EsLwBIZzh3LCMezu.png)"
zjyhf,2024-05-15 03:42:12+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,This comment has been hidden
zjyhf,2024-05-10 13:09:47+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,This comment has been hidden
zihanliu,2024-05-08 19:20:34+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"Hi,
We do not use the special tokens like <|start_header_id|>, <|end_header_id|>, and <|eot_id|>. You can pretty much follow the format in the sample code we provide. that's the format we use to train our model. In addition, you can also refer to https://huggingface.co/datasets/nvidia/ChatQA-Training-Data, where we provide the training recipe."
zihanliu,2024-05-05 19:23:08+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"thank you for your question. Unfortunately, ChatQA-1.5 is optimized for English. It could have certain capability to generate other languages if you give it an instruction like, ""answer it using French"". We observe that the 70B ChatQA model would have a much better capability for this than the 8B model."
CleanCoder123,2024-05-02 15:03:47+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"By Llama 3 license: 
""(B) prominently display “Built with Meta Llama 3” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama 3” at the beginning of any such AI model name.""

So 1. Display ""Built with Meta Llama 3” on page. 2. Rename model to be prefixed with ""Llama 3"" as stated in the Llama 3 license.

 @zihanliu 

"
deleted,2024-05-02 15:05:33+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"> By Llama 3 license: 
> ""(B) prominently display “Built with Meta Llama 3” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama 3” at the beginning of any such AI model name.""
> 
> So 1. Display ""Built with Meta Llama 3” on page. 2. Rename model to be prefixed with ""Llama 3"" as stated in the Llama 3 license.
> 
>  @zihanliu


https://cdn-uploads.huggingface.co/production/uploads/6512681f4151fb1fa719e033/FwrgisRGifkz6Ikijo_0j.mp4
"
zihanliu,2024-05-02 18:06:02+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,We apologize sincerely for this issue. We just updated the model card and added Llama-3 in the model name.
zihanliu,2024-05-02 18:07:30+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,We apologize sincerely for this issue. We just updated the model card and added Llama-3 in the model name.
an303042,2024-05-06 06:36:34+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"But did you send him the H100?
I'm willing to keep quiet for just a 4090 :(
"
zihanliu,2024-05-06 18:34:21+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"loll. I wish I can, but I don't have such resources :'-("
jackboot,2024-05-02 14:35:27+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,Isn't it using \n\n to differentiate?
wolfram,2024-05-02 16:07:34+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"That's just two newlines. All of that can be inside the RAG data or a user message.

It's important to use special tokens that cannot ever occur in the normal input. With ChatML, you have e. g. `<|im_start|>`, which is a unique special token so your application can ensure it's never sent to the model from what a user inputs or the RAG component retrieves."
just1moremodel,2024-05-02 18:17:47+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,Oops...My youngest son is actually named <|im_start|>
jackboot,2024-05-02 21:59:56+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,Hopefully \n\nassistant: doesn't re-occur too much. I guess you will have to see how it goes.
zihanliu,2024-05-03 18:06:01+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"Hi @wolfram,
Thanks for bringing this up. We think that the user-assistant turns or context usually will not contain strings like ""\n\nUser:"" or ""\n\nAssistant"". The model should be robust to this template since it is how we train our model."
nonetrix,2024-05-04 16:06:05+00:00,comment,nvidia/Llama3-ChatQA-1.5-70B,"Why not use escape sequences? Making some crazy chat template that is likely not to appear in user conversation is flawed anyway, if we added escape sequences to the training data would could remove this issue entirely would just require front ends to support it"
jujbob,2024-05-12 01:02:22+00:00,comment,Bllossom/llama-3-Korean-Bllossom-70B,"
안녕하세요. maywell님 먼저 공개해주신 Instruction Tuning데이터가 저희 모델에 많은 도움이 되었습니다 감사합니다 ㅎㅎ
답변을 드리면 다음과 같습니다.

1. 한국어-영어로 pre-trainining과 Instruction 튜닝이 함께 진행된 모델입니다.
2. 벤치마크는 Human Evaluation, GPT preference와 같은 정성평가만 진행했습니다. 현재 한국어 공개 모델중엔 최상급인데 저희가 논문제출을 위해 자세한 분석내용을 공개하지 못하는것 이해부탁드립니다 ㅠㅠ.

4지선다 객관식문제 단답형 로직문제와 같은 정량평가는 전혀 고려하지 않았기 때문에 저희도 성능이 어떤지 모르겠는데요, 아마 성능은 좋지 않을꺼같습니다. 
왜냐하면 Bllossom 모델은 실제 User의 사용성을 고려해 요약, 검색, 번역, 논문작성, 이메일, 일상대화 등을 주로 튜닝했어요. 다만, 로직이나 4지선다 객관식 문제 잘풀게 추가 튜닝하면 리더보드에서도 좋은성능보이지 않을까 합니다.
"
maywell,2024-05-12 02:30:43+00:00,comment,Bllossom/llama-3-Korean-Bllossom-70B,답변 해주셔서 감사드립니다. 좋은 모델 열심히 사용해보겠습니다! 남은 주말 즐겁게 보내시길.
sangmine,2024-05-31 07:49:00+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"저희 라이선스 담당 팀에 정확한 정책 확인해서 다시 답변 드리겠습니다.
관심 가져주셔서 감사드립니다 :) "
sangmine,2024-05-10 14:53:49+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"안녕하세요.
본 모델은 사전학습 모델입니다. 
예시에 있는 질의에 대한 적절한 응답을 받기 위해 Instruct-tuning 이후 사용해보시길 권장드립니다.

추가적으로 입력 간 텍스트 맨 앞 부분에 ""<|begin_of_text|>""라는 bos_token을 토큰을 추가적으로 붙혀서 추론하시는 것을 추천드립니다. :)"
keisey,2024-05-20 11:57:14+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"추가적으로 입력 간 텍스트 맨 앞 부분에 ""<|begin_of_text|>""라는 bos_token을 토큰을 추가적으로 붙혀서 추론하시는 것을 추천드립니다. :) <---- 이 부분은 어떻게 코딩하면 되나요?
그리고 추가를 하면 어떤 효과가 있나요?"
sangmine,2024-05-09 11:29:20+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"안녕하세요.
해당 모델은 한국어 코퍼스를 기반으로 사전학습을 수행한 모델으로 학습 데이터에 instruction 포함되어 있지 않습니다.
감사합니다."
yuneun92,2024-05-10 01:07:44+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"아 그렇군요, 답변 감사합니다 :) 좋은 하루 되세요!"
sangmine,2024-05-09 11:27:14+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"안녕하세요. 
한글이 깨지는 것이 아니라, Byte-level BPE를 사용하여 text를 byte의 sequence로 처리하기에 다음과 같은 토큰들의 결과가 나오는 것 입니다.
해당 출력 토큰의 'input_ids' 값을 tokenizer.decode(string) 해주시면 해당 결과를 확인하실 수 있습니다."
richard-park,2024-05-10 00:58:28+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"제가 초보라서  잘 안 되네요.


![예시.png](https://cdn-uploads.huggingface.co/production/uploads/6584f7f1cf1596d002928724/hdqX83KFBKpx_dxU5UWD7.png)

예시처럼 코드가 나오면 되는 데, 부탁드립니다."
kjtaed,2024-05-10 04:57:37+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"```python
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained(""saltlux/Ko-Llama3-Luxia-8B"")
>>> [tokenizer.decode([id]) for id in tokenizer.encode(""나는 아침을 먹었다"")]
['나는', ' 아침', '을', ' 먹', '었다']
```
위와 같이 확인할 수 있습니다.

"
richard-park,2024-05-10 05:20:56+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,고맙습니다. 
richard-park,2024-05-14 00:33:15+00:00,comment,saltlux/Ko-Llama3-Luxia-8B,"염치 불구하고 하나 더 부탁 드립니다.
라마3 토크나이저  한국어 학습을 시키고 싶은 데,  AI Hub/modu data 10G 정도를 정리해서 학습을 했는 데, 토크나이저 결과가 동일합니다.

방법은 허깅페이스 토크나이저 아래 문서에 나와 있는 대로 수행했습니다.
https://huggingface.co/learn/nlp-course/chapter6/2

고맙습니다."
ivanstepanovftw,2024-05-17 00:11:51+00:00,comment,tenyx/Llama3-TenyxChat-70B,Looks like llama3 from tokenizer config
Romain-Cosentino,2024-05-16 19:03:30+00:00,comment,tenyx/Llama3-TenyxChat-70B,"Hey,

Thanks for the nice word. The learning algorithm is proprietary (closed-source), it is a PEFT method that allows for continual learning. The loss function/framework we used is DPO, with the ultrafeedback binarized dataset. So everything is standard but the learning algo."
sarath-shekkizhar,2024-05-10 04:06:52+00:00,comment,tenyx/Llama3-TenyxChat-70B,Thanks for the reference @gblazex. We will look into evaluating on Alpaca Eval ✌️
sarath-shekkizhar,2024-05-08 16:35:11+00:00,comment,tenyx/Llama3-TenyxChat-70B,"@MaziyarPanahi -- Thanks, and congrats on your fine tunes as well 🤗. We used the code from here: [lm-sys/FastChat](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge). Note that to update the model to use `gpt-4-0125` as a judge, you would need to integrate [this PR](https://github.com/lm-sys/FastChat/pull/3158); reasons and repo owners' comments for this are in the PR. "
MaziyarPanahi,2024-05-09 11:15:38+00:00,comment,tenyx/Llama3-TenyxChat-70B,"Thank you @sarath-shekkizhar for sharing the script, appreciate it. I'll try to use this for the next fine-tunes. 

PS: Please, keep up the good work! 🤗❤️"
mmnga,2024-05-05 16:07:12+00:00,comment,mmnga/Llama-3-70B-japanese-suzume-vector-v0.1,"Thanks!
I believe that the performance of this model is probably due to the performance of the original Llama-3-70B-instruct, and that the experiment did not yield very promising results. 
However, I believe there is still room for improvement in this approach and will try a few more things. "
michaelfeil,2024-05-04 03:19:00+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-1048k,
michaelfeil,2024-05-04 00:59:30+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-1048k,
michaelfeil,2024-05-03 22:50:59+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-1048k,
michaelfeil,2024-05-03 22:52:33+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-1048k,STEP-4
michaelfeil,2024-05-03 21:41:21+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-1048k,
leo-pekelis-gradient,2024-05-04 02:01:42+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-4194k,
leo-pekelis-gradient,2024-05-04 00:03:19+00:00,comment,gradientai/Llama-3-8B-Instruct-Gradient-4194k,
teddylee777,2024-05-25 13:52:41+00:00,comment,teddylee777/Llama-3-Open-Ko-8B-gguf,"chat_format 에 'llama-3' 대신 model card 에 기입한 템플릿을 활용하여 추론해야 제대로된 답변을 받으실 수 있습니다.
템플릿은 model card 에 넣어놨습니다^^"
teddylee777,2024-05-25 13:53:13+00:00,comment,teddylee777/Llama-3-Open-Ko-8B-gguf,추가로 대화형 추론을 위해서라면 instruct 모델을 사용하시는 것을 추천 드립니다!
coconut00,2024-05-26 23:50:10+00:00,comment,teddylee777/Llama-3-Open-Ko-8B-gguf,감사합니다!!
wannaphong,2024-05-09 09:14:41+00:00,comment,scb10x/llama-3-typhoon-v1.5-8b-instruct,"> If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama 3” at the beginning of any such AI model name."
kunato,2024-05-09 09:36:50+00:00,comment,scb10x/llama-3-typhoon-v1.5-8b-instruct,Thank you for pointing out we are fixing that!
failspy,2024-05-12 17:50:18+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"Hey, thanks! Next version was going to be updated with this, appreciate you creating this PR to get this version updated."
failspy,2024-05-10 22:04:54+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"Very interested in doing so, but considering it's quite costly for me to do given I ideally need to perform the abliteration on a non-quant version, I want to refine out some quirks noticeable in the 8B first; particularly hallucinations. Once I feel the methodology is just a *bit* more refined on the 8B, I'm very open to doing 70B."
Kearm,2024-05-09 22:06:56+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"> I left a comment on the original 8b abliterated by failspy, but am leaving one here because it's also impacted. Assuming something isn't tremendously wrong on my end both abliterated version of Llama 3 8b Instruct are hallucinating MUCH more than the original instruct.
> 
> Perhaps someone can confirm and report back to rule out that I'm doing something wrong.
> 
> Just set the temp to 0 (always best when testing hallucinations) and ask both the original instruct and either abliterated version to list the main characters, and the actors who portrayed them, from various somewhat popular TV shows like Corner Gas and Black Books. The hallucination rate spikes drastically. Not only that, it's far less likely to follow prompt directives, such as listing only the characters, or only the actors.
> 
> Even my non-hallucination test prompts returned a drastic spike in hallucinations.

This is the effect of ablation to prevent ALL refusals. The methodology removes the capbility of the model to refuse even if the knowedge is not in it."
deleted,2024-05-09 22:19:35+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,The knowledge is in it.
Kearm,2024-05-09 22:25:22+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"> The knowledge is in it.
> 
> The original Llama 3 8b Instruct can answer all the questions correctly.
> 
> I repeated, the original Llama 3 8b Instruct can answer all the questions correctly.

Interesting. This is likely damage of the model by the technique that would then need healing by many possible methods."
Kearm,2024-05-09 22:25:42+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"> The knowledge is in it.

I appreciate the feedback!"
failspy,2024-05-09 22:32:41+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,Thanks for the report! Nice to hear someone's benchmarking it in this way. Will investigate further to see what can be done to reduce the hallucinations in future versions.
deleted,2024-05-09 22:44:07+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"Thanks. I'm a big fan of your efforts. Overly aggressive alignment is as annoying as it is stupid, and these abliterated LLMs successfully removed alignment, plus they remained generally coherent.

But I suspect that Meta somehow fused alignment with other functionality (note: I'm a non-technical user) because there's a general spike in hallucinations and drop in prompt directive adherence across the board, even with prompts that aren't the least bit contentious."
Kearm,2024-05-09 23:44:41+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"> Thanks. I'm a big fan of your efforts. Overly aggressive alignment is as annoying as it is stupid, and these abliterated LLMs successfully removed alignment, plus they remained generally coherent.
> 
> But I suspect that Meta somehow fused alignment with other functionality (note: I'm a non-technical user) because there's a general spike in hallucinations and drop in prompt directive adherence across the board, even with prompts that aren't the least bit contentious.

So from what we have observed from say Phi3 I think this is possible but not likely. The technicque is brand new."
wassname,2024-05-12 05:59:31+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"Interesting! At some point, we should all cooperate to make a repo that does this, but also benchmarks before and after."
deleted,2024-05-12 12:54:23+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"@wassname So far this is an unconfirmed bug. Nobody else but me chimed in to confirm the hallucination spike and ignoring of prompt directives, and the 70b version got a higher HF score than the original Instruct.

Just asking for the main characters, and the actors who portrayed them, from a few TV shows like Corner Gas or Black Books with both this and the original Instruct (at temp 0) will do it. The spike in hallucinations is overwhelming.

Plus I should note that I used the GGUF versions when testing both this and the original L3 8b instruct, but with various front-ends, including the recently updated Koboldcpp using the latest llama.cpp. So it's possible something went wrong with with the GGUFs."
deleted,2024-05-12 13:53:44+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"I don't like flooding discussions with examples, but I'll provide one to show how bad the hallucinations are, but this isn't cherry-pick, it performs this bad at all quantizations against the original Q4_0 instruct across the board, and with both old and new versions of llama.cpp with different front-ends, including Ollama, GPT4ALL and Koboldcpp.

6 main characters of Corner Gas, and their actors.

Llama 3 8b Instruct Q4_0 (temp 0)

1. Brent Leroy (played by Brent Butt) - Perfect
2. Oscar Leroy (played by Eric Peterson) - Perfect
3. Lacey Burrows (played by Gabrielle Miller) - Perfect
4. Davis Quinton (played by Fred Ewanuick) - Wrong: by Lorne Cardinal
5. Wanda Teal (played by Nancy Robertson) - Wrong: Dollard vs Teal
6. Hank Yarbo (played by Bill Dow) - Wrong: by Fred Ewanuick

L3 8b Instruct abliterated-fp16 (temp 0) - using Koboldcpp and the latest llama.cpp

Brent Buttles (played by Eric Peterson) - Wrong: Leroy vs Buttles / Wrong: Brent Butt vs Eric
Luanne Culde (played byNancy Sullivan) - Wrong: Both character and actor name are entirely wrong.
Oscar Molinaro (played byJeffrey Brielski) - Wrong: Leroy vs Molinaro / Wrong: Eric vs Jeffrey
Davis Mullaly (played byDan Petriescu) - Wrong: Quinton vs Mullaly / Wrong: Lorne vs Petriescu
Holly Flax (played byKathleen Zuel) - Wrong: Both character and actor name are entirely wrong
Andy Richter (played byAndy Richter) - Wrong: Both character and actor name are entirely wrong

L3 8b Instruct abliterated-fp16 (temp 0) - using older pre-BPE fix llama.cpp and GPT4ALL (just as many hallucinations, but with different formatting and token issues).

﻿* Brent/Andy (played by Eric Robson)
* Luan/Cole (played by Daniel Oreskoc)
* Dan/Ron (played by Aaron Hughes)
* Holly/Mary (played by Lisa D'Amato)
* Ian/Brian (played by John Kassianczki)"
failspy,2024-05-12 17:47:53+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"@wassname Are you on Discord? Can join the Cognitive Computations Discord and start talking to collaborate.

@Phil337 no worries on the ""flooding""! I seriously appreciate the examples. I have a new Llama-3-8B ""abliterated"" model that I think is doing a lot better on not hallucinating whilst still having inhibited refusals, excited to post soon to let you at that one."
wassname,2024-05-13 22:48:09+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"> Can join the Cognitive Computations Discord and start talking to collaborate.

Is there where all you degenerates have gone, heading over now

>  the 70b version got a higher HF score than the original Instruct.

Interesting!  It's pretty funny that it's TruthfullQA score shot up, that's a terrible dataset (lots of opinions put as fact) but it must have been refusing lots of Q's

![image.png](https://cdn-uploads.huggingface.co/production/uploads/64033d43dbfbea2a053a6743/MMToqu_udoGO9XYV2RDbs.png)


"
failspy,2024-05-22 16:35:36+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,@Phil337 want to give abliterated-v3 models a shot?
deleted,2024-05-22 19:14:18+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"Thanks @failspy , it works much better. I tested q4 GGUF with the latest Koboldcpp (since GPT4ALL still has issues with Llama 3). It follows prompt directives, consistently gets the correct year for movies, shows, and albums, and the hallucinations came way down.

For example, it returned the following for Corner Gas, which is much better than last time (pasted in a previous comment). Even the source L3 Instruct has trouble with this one.

Brent Leroy (played by Brent Butt) - Perfect
Wanda Dollard (played by Gabrielle Miller) - Correct First/Last Name, but Gabrielle Miller played Lacey
Oscar Leroy (played by Eric Peterson) - Perfect
Lacey Burrows (played by Sandrine Holt) - Correct First/Last Name, but Actress is Gabrielle Miller
Emma Tarlo (played by Tara Wilson) - Emma Leroy played by Janet Wright
Davis Quinton (played by Fred Ewanuick) - Correct First/Last Name, but Fred Ewanuick played Hank"
Kearm,2024-05-22 20:14:18+00:00,comment,cognitivecomputations/Llama-3-8B-Instruct-abliterated-v2,"> Thanks @failspy , it works much better. I tested q4 GGUF with the latest Koboldcpp (since GPT4ALL still has issues with Llama 3). It follows prompt directives, consistently gets the correct year for movies, shows, and albums, and the hallucinations came way down.
> 
> For example, it returned the following for Corner Gas, which is much better than last time (pasted in a previous comment). Even the source L3 Instruct has trouble with this one.
> 
> Brent Leroy (played by Brent Butt) - Perfect
> Wanda Dollard (played by Gabrielle Miller) - Correct First/Last Name, but Gabrielle Miller played Lacey
> Oscar Leroy (played by Eric Peterson) - Perfect
> Lacey Burrows (played by Sandrine Holt) - Correct First/Last Name, but Actress is Gabrielle Miller
> Emma Tarlo (played by Tara Wilson) - Emma Leroy played by Janet Wright
> Davis Quinton (played by Fred Ewanuick) - Correct First/Last Name, but Fred Ewanuick played Hank

Just want to expresss my appreciation for helping Failspy/Us on this project!"
ehartford,2024-05-01 06:36:51+00:00,comment,abacusai/Llama-3-Giraffe-70B,"base, looks like, because there's no chat template in here

https://huggingface.co/abacusai/Llama-3-Giraffe-70B/blob/main/tokenizer_config.json"
ArkaAbacus,2024-05-01 09:37:51+00:00,comment,abacusai/Llama-3-Giraffe-70B,"Hey Eric - yup that's right, based on on the base model"
sch1,2024-05-02 09:52:17+00:00,comment,abacusai/Llama-3-Giraffe-70B,Any plan when do you release Meta-Llama-3-70B-Instruct 128K ?
ArkaAbacus,2024-05-01 09:37:27+00:00,comment,abacusai/Llama-3-Giraffe-70B,This is the base model so there's no chat template on this release
noisefloordev,2024-05-12 04:29:50+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"Aha!  I stumbled on this post:

https://huggingface.co/posts/Undi95/318385306588047#66373fc81f128970c78c242b

After a couple days of banging my head against this, it took about two minutes to get it working once I got that tip.
"
ayyylol,2024-05-12 06:50:05+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,Can you please share the code @noisefloordev 
failspy,2024-05-12 17:43:06+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"@noisefloordev Hm, did I not include the ""intervene in each layer"" functionality in the cookbook? What was missing?"
noisefloordev,2024-05-13 18:46:03+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"You didn't forget anything, the suggestion I linked was somebody trying to get it to work in a broken way.  I only thought it worked because I'd been staring at my screen for too long.  After taking a break and getting some sleep (which I should have done earlier :) and starting clean I have things working a lot better.

I've spent some time with the 70B.  I'm experimenting with multiple passes: find a vector, remove it, then go again (possibly with a different dataset).  It finds a second vector where the test pass gives far fewer refusals by removing both vectors than either one does individually.  I'll post code, etc. once I've spent some more time with it.  I need to improve my testing to make sure it's not damaging the model making multiple changes like this.

I'm a bit bottlenecked by RunPod, though.  I have to run 3 A100s to get it to load, which often isn't available, and something about TransformerLens's loading means I need around 350-400 GB RAM too, so there are only a couple configurations that work.  I guess I'll try 8x5000s, but that means setting up a volume in a different data center.  Would love to know if anyone knows a model load config that doesn't need RAM 2x the size of the model.  I tried various things with device_map, but that seems to just move the memory spike to VRAM.
"
ChuckMcSneed,2024-05-10 09:10:20+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"There is no need to abliterate command-r, it is already uncensored."
failspy,2024-05-07 17:11:23+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"Submitted, thanks. Pending eval "
ChuckMcSneed,2024-05-08 22:48:03+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"Looks like it failed:
https://huggingface.co/datasets/open-llm-leaderboard/requests/blob/main/failspy/llama-3-70B-Instruct-abliterated_eval_request_False_bfloat16_Original.json
https://huggingface.co/datasets/open-llm-leaderboard/requests/blob/main/failspy/llama-3-70B-Instruct-abliterated_eval_request_False_float16_Original.json"
failspy,2024-05-08 23:01:52+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,Bugger. Wish it said more.
DreamGenX,2024-05-09 08:11:39+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"OpenLLM leadersboard often fails on larger models... you will need to manully request re-run. :-/
I asked here: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/737"
ChuckMcSneed,2024-05-11 14:30:05+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"![image.png](https://cdn-uploads.huggingface.co/production/uploads/65644e982bdaccfcd536aff1/1aAA-qtQiJaCkyRSo5Ypi.png)
It slightly improved, nice!"
failspy,2024-05-07 14:04:44+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"There's Phi-3 which I've posted. Llama-3-8B has been done by a couple other folks so I haven't published one.

https://huggingface.co/wassname/meta-llama-3-8b-instruct-helpfull

https://huggingface.co/Undi95/Unholy-8B-DPO-OAS"
Max1707408557,2024-05-07 14:38:54+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"Thanks for replying. Wassname unfortunately only provided a Q6_k quant, which is just a little too big for me to comfortably run (and no fp16 that I could quant myself), and I do not have much confidence in Undi models.
I fully understand if you don't want to do something that's already been done, but I, and I'm sure many others, would really appreciate the full weights."
failspy,2024-05-07 16:12:56+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"https://huggingface.co/failspy/Llama-3-8B-Instruct-abliterated Coming your way!

GGUFs uploading here: https://huggingface.co/failspy/Llama-3-8B-Instruct-abliterated-GGUF (will have q4_k, q8_0, and fp16)"
Max1707408557,2024-05-07 17:14:39+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,Thanks a lot!
failspy,2024-05-07 14:07:18+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,Done!
zackzachzaczak,2024-05-09 08:29:47+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"Yeah, just wanted to say THANK YOU!!! There have been de1984'd models before ye but never provide any information on how. Not only did you provide __how__ you made it awesome but provided a way to do it to our own and taught us how to fish. The world would be a better place if everyone was as thoughtful"
failspy,2024-05-07 03:35:20+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,Uploaded proper weights. Will test on another machine to make 100% sure 
failspy,2024-05-07 04:18:49+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,"So, they're the proper weights! Buuut... don't do anything whilst tired: The down_proj layers are transposed incorrectly. Reuploading once more to fix it."
failspy,2024-05-07 05:12:43+00:00,comment,failspy/llama-3-70B-Instruct-abliterated,Third time's the charm. Confirmed both safetensors and GGUFs. Sorry again about this.
bartowski,2024-05-08 08:55:14+00:00,comment,bartowski/Llama-3-ChatQA-1.5-8B-GGUF,"They include one in the model page they may be worth checking out

Also it's largely meant for use as RAG ideally"
lubuwei,2024-05-08 22:34:15+00:00,comment,bartowski/Llama-3-ChatQA-1.5-8B-GGUF,"I tried the prompt template provided by Nvidia's model card, wrote the `Modelfile` used for the Ollama installation, and ran the model with the same confusion.
Maybe I didn't write it right, tune it again ......

But on LM Studio, it runs great! Following the guidelines here  [lmstudio-community/Llama3-ChatQA-1.5-8B-GGUF · Hugging Face](https://huggingface.co/lmstudio-community/Llama3-ChatQA-1.5-8B-GGUF)"
natek000,2024-05-09 01:27:07+00:00,comment,bartowski/Llama-3-ChatQA-1.5-8B-GGUF,"Using it as a RAG worked beautifully, thanks!"
bartowski,2024-05-04 21:09:39+00:00,comment,bartowski/Llama-3-ChatQA-1.5-8B-GGUF,"Have you added it as a stop string? This model does feel a bit odd with tokenization either way, something is off"
dillfrescott,2024-05-05 01:09:00+00:00,comment,bartowski/Llama-3-ChatQA-1.5-8B-GGUF,"oh, im using just ./main from llama.cpp. i know you can add reverse prompts, is that the same as a stop string?"
Inego,2024-05-07 19:01:44+00:00,comment,bartowski/Llama-3-ChatQA-1.5-8B-GGUF,"It seems these tokens are specified in the metadata, and llama.cpp uses them:
```
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
```

However, they were never mentioned by the model's authors so I suppose they just should not be in the metadata."
ajibawa-2023,2024-05-10 04:06:09+00:00,comment,ajibawa-2023/Code-Llama-3-8B,Thank you very much for your kind words @rombodawg . Thank you for the tweet & spreading the word! I am also waiting for humaneval score. 
rombodawg,2024-05-10 04:15:04+00:00,comment,ajibawa-2023/Code-Llama-3-8B,@ajibawa-2023 So i have some bad news. The humaneval score did not score much higher than base llama-3-8b. at around .35 points. Sorry to disappoint. 
ajibawa-2023,2024-05-10 04:16:38+00:00,comment,ajibawa-2023/Code-Llama-3-8B,No worries man!
Stefan-LTB,2024-05-31 11:41:56+00:00,comment,DiscoResearch/Llama3-German-8B,"Reading helps, the instruct version works like expected.
But now I search for a good GGUF Version of the instruct.
Also can I build my own chat model with instructions out of the instruct Version?"
MaziyarPanahi,2024-05-07 15:49:29+00:00,comment,MaziyarPanahi/Llama-3-70B-Instruct-DPO-v0.2,
MaziyarPanahi,2024-05-07 15:33:55+00:00,comment,MaziyarPanahi/Llama-3-70B-Instruct-DPO-v0.2,"I've seen people training Llama-3 with ChatML and don't even add the `<im_end>`. But I did since I saw improvements. This is the template used to train the model and recommended to follow:

```
<|im_start|>system
{System}
<|im_end|>
<|im_start|>user
{User}
<|im_end|>
<|im_start|>assistant
{Assistant}
```

This is also the safest way to add `eos_id`:

```
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids(""<|im_end|>""),
    tokenizer.convert_tokens_to_ids(""<|eot_id|>"") # safer to have this too
]
```

Works great with ChatML"
xzuyn,2024-05-07 16:45:20+00:00,comment,MaziyarPanahi/Llama-3-70B-Instruct-DPO-v0.2,"What's your reasoning behind only adding `<|im_end|>` and not also adding `<|im_start|>`? Cause if you didn't find some quality benefit to it you just end up wasting a lot of tokens every turn because of it, and you're already using the extra memory needed to train `lm_head` and `embed_tokens` (assuming you used LoRA/QLoRA since it probably isn't different for FFT) so you may as well add `<|im_start|>` at the same time.

https://belladoreai.github.io/llama3-tokenizer-js/example-demo/build/
![image.png](https://cdn-uploads.huggingface.co/production/uploads/63559199805be5a8f30f6505/UmVVrtdDS1g4j5mls5UmA.png)
"
MaziyarPanahi,2024-05-08 16:13:27+00:00,comment,MaziyarPanahi/Llama-3-70B-Instruct-DPO-v0.2,"> What's your reasoning behind only adding `<|im_end|>` and not also adding `<|im_start|>`? Cause if you didn't find some quality benefit to it you just end up wasting a lot of tokens every turn because of it, and you're already using the extra memory needed to train `lm_head` and `embed_tokens` (assuming you used LoRA/QLoRA since it probably isn't different for FFT) so you may as well add `<|im_start|>` at the same time.
> 

All of these are very true. I initially added them, the score didn't change much, however, it was a bit hard to have the model stopping where it should. I guess it's one of those things that maybe the Instruct model was just not easy to change the template, or maybe there was/is a bug in the Axolotl regarding Llama-3 tokenization. 

After a couple of try, the easiest way to use ChatML was this unusual addition. I will investigate more to see if maybe by now it could be possible to add those extra tokens and have the model generate text without any issue. (the extra tokens are an issue to me specially in a 8K context, but the generation and quality have passed my tests)

"
MaziyarPanahi,2024-04-27 20:17:59+00:00,comment,MaziyarPanahi/Llama-3-70B-Instruct-DPO-v0.2,
MaziyarPanahi,2024-04-27 19:16:55+00:00,comment,MaziyarPanahi/Llama-3-70B-Instruct-DPO-v0.2,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-27 19:16:56+00:00,comment,MaziyarPanahi/Llama-3-70B-Instruct-DPO-v0.2,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
thesven,2024-05-29 11:43:53+00:00,comment,thesven/openchat-3.6-8b-20240522-GGUF,"Hey there,

Would you mind providing a bit of insight into how you were loading the model, and which specific quant you were trying?
Would be happy to take a look into it on my end and see what is up, and create a new quant if needed.
I've been running the IQ4 model in Jan.ai without issues the past few days using the following chat template:

```bash
<|begin_of_text|><|start_header_id|>System<|end_header_id|>

{system}<|eot_id|><|start_header_id|>GPT4 Correct User<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>GPT4 Correct Assistant<|end_header_id|>
```"
LaughterOnWater,2024-05-30 21:52:14+00:00,comment,thesven/openchat-3.6-8b-20240522-GGUF,"Q8_0. 
Turns out it was just that LM Studio needed to update so that it could function with openchat 3 8B.
It actually functions quite well now! Example below.

---

System:

You are a knowledgeable, efficient, and direct AI assistant. Utilize multi-step reasoning to provide concise answers, focusing on key information. If multiple questions are asked, split them up and address in the order that yields the most logical and accurate response. Offer tactful suggestions to improve outcomes. Engage in productive collaboration with the user.

User:

Please give me your understanding of ISCO and black body radiation with regards to black holes

Assistant:

ISCO stands for ""Innermost Stable Circular Orbit,"" which is a concept in astrophysics related to black holes. It refers to the closest distance at which an object can orbit a black hole without being immediately pulled into it by its gravitational force. The ISCO is determined by the Schwarzschild radius, which depends on the mass of the black hole.

Black body radiation is the thermal radiation emitted by an idealized, nonreflective, non-transparent object as a result of its temperature. It is characterized by a specific spectrum of wavelengths and is described by Wien's displacement law, Planck's law, and the Stefan-Boltzmann law. Black holes emit black body radiation in the form of Hawking radiation, which is a theoretical prediction that black holes can lose mass due to quantum effects near their event horizons.

In summary, ISCO refers to the closest orbiting distance around a black hole, while black body radiation is the thermal emission from an object based on its temperature. Black holes emit black body radiation in the form of Hawking radiation."
thesven,2024-05-31 01:18:32+00:00,comment,thesven/openchat-3.6-8b-20240522-GGUF,"Great to hear!
I've been using the IQ4_XS quant with quite good results as well."
leo-pekelis-gradient,2024-05-03 17:22:11+00:00,comment,gradientai/Llama-3-70B-Instruct-Gradient-524k,
AwanLLM,2024-05-07 09:44:17+00:00,comment,AwanLLM/Awanllm-Llama-3-8B-Cumulus-v0.2,"I actually am finding that setting it to 128009 ""<|eot_id|>"" works the best, so for my future models I will set that. Since all my fine tunes are based on the instruct model that uses that eos token anyways."
arthrod,2024-05-08 01:50:56+00:00,comment,AwanLLM/Awanllm-Llama-3-8B-Cumulus-v0.2,And keep pad as endoftext? 
leaderboard-pr-bot,2024-04-30 06:00:42+00:00,comment,Azazelle/L3-RP_io,"This is an automated PR created with https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr

The purpose of this PR is to add evaluation results from the Open LLM Leaderboard to your model card.

If you encounter any issues, please report them to https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr/discussions"
ddh0,2024-05-15 19:34:30+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,"Yes, I’m using Metal. CUDA support for bf16 is still being worked on in llama.cpp. You could try with batch size <= 16 or on CPU for the time being"
mike-ravkine,2024-05-15 22:02:57+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,"@ddh0 Thanks! No luck with decreasing batch size, but `-ngl 0` resolved the issue."
ddh0,2024-05-12 21:05:57+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,Corrupted download maybe? The magic number is correct for me
aravindsr,2024-05-14 01:48:58+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,Redownloaded and tried again. Got the same error :(
ddh0,2024-05-14 03:57:57+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,"Hmm. I’m sorry, I have no idea. As you can see in the README it works just fine for me, and I’ve had no other complaints. Maybe make sure you’re in the latest llama.cpp version?"
vidumec,2024-05-16 06:53:12+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,"getting ""GGML_ASSERT: ggml-metal.m:1540: false && ""MUL MAT-MAT not implemented"""" crash with latest compiled llama.cpp on M3 Max "
ddh0,2024-05-16 06:55:36+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,"> getting ""GGML_ASSERT: ggml-metal.m:1540: false && ""MUL MAT-MAT not implemented"""" crash with latest compiled llama.cpp on M3 Max

@vidumec Retry with batch size >= 16 for the time being. bfloat16 support is still being worked on"
vidumec,2024-05-16 07:04:26+00:00,comment,ddh0/Meta-Llama-3-8B-Instruct-bf16-GGUF,"didn't help, but i just realized looking at the code, it hasn't been implemented for Metal, so it could only work on CPU"
Steelskull,2024-06-05 22:58:01+00:00,comment,Steelskull/L3-Arcania-4x8b,"> Been playing with this for a few days, tweaking parameters and still loving it as a lighter alternative for Astoria-70B
> I'm wondering if you have any guidance on what the best formatting for memory/world info/author's note would be for this model, based on it's training set?
> 
> Square-bracket based formats seem to work ok for Memory/WI, but if I put those in AN it 'leaks' into the responses. (Example: [Author's Note: Write a description of the current location.] would sometimes return a result that ended with a 'new' author's note, or include random bits of text in square brackets, or vomit a bunch of square brackets at the end of the response.)
> 
> Changing params mitigates some of these issue, but doesn't fully get rid of them. All-in-all some minor bracket vomit is acceptable for the overall quality I'm getting.

MoEs can be a little unhinged at times especially with prompt types. I've noticed Arcania has the tendency to repeat at times as well. I think this is going to be have to addressed in the next model but if I find a workaround I'll let you know!"
Steelskull,2024-05-06 23:49:25+00:00,comment,Steelskull/L3-Arcania-4x8b,"Thank you very much for the extremely detailed feedback! Im very glad to see you enjoying the model! 

I have noticed the < s / > error, it is kind of on and off at times. I think its an issue that can be fixed with some tokenizer modifications but I am unsure at the moment. May end up being a ""Next version"" Problem.

Thanks Again! 😁"
010O11,2024-05-07 05:10:12+00:00,comment,Steelskull/L3-Arcania-4x8b,maybe this have something to do with the formatting? IDK ... https://www.reddit.com/r/LocalLLaMA/comments/1cltac3/part3_cause_to_issue_found_possible_bug_llama3/
Steelskull,2024-05-07 08:48:23+00:00,comment,Steelskull/L3-Arcania-4x8b,"> maybe this have something to do with the formatting? IDK ... https://www.reddit.com/r/LocalLLaMA/comments/1cltac3/part3_cause_to_issue_found_possible_bug_llama3/

sadly I don't think so. When the model was created, the tokenizer didnt take on the stop token < s / > , normally I would modify the tokenizer to fix this but the GGUFs have already been made by others. Im looking into options as i may have a way to fix it"
AdamDel,2024-05-07 13:28:45+00:00,comment,Steelskull/L3-Arcania-4x8b,"np, it's actually enjoyable to leave positive feedback when you come across something unique that impresses you, and when there is something to talk about. Glad you know where the issue might be, hidden eos is not a big deal compared to advantages. Thank you for taking time to make your models!"
Reithan,2024-06-01 21:13:45+00:00,comment,Steelskull/L3-Arcania-4x8b,"Came to Arcania looking for a smaller model to run in place of Astoria (per @Steelskull 's recommendation) and I am not let down. I can run this on my system, it's reasonably fast, and the results are still one of the best models I've messed with so far. Very good at following instructions, both explicit and more subtle steeing, keeps track of details, obeys lorebook/world info well.
Only issue is the random symbol vomit at the end of it's responses sometimes, but that's easily overlooked/edited out.
Steel is the best."
Steelskull,2024-06-05 22:58:38+00:00,comment,Steelskull/L3-Arcania-4x8b,"> Came to Arcania looking for a smaller model to run in place of Astoria (per @Steelskull 's recommendation) and I am not let down. I can run this on my system, it's reasonably fast, and the results are still one of the best models I've messed with so far. Very good at following instructions, both explicit and more subtle steeing, keeps track of details, obeys lorebook/world info well.
> Only issue is the random symbol vomit at the end of it's responses sometimes, but that's easily overlooked/edited out.
> Steel is the best.

Thank you very much! 😊"
sequelbox,2024-05-09 19:26:30+00:00,comment,ValiantLabs/Llama3-70B-Fireplace,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
zoeywin,2024-05-09 19:30:17+00:00,comment,ValiantLabs/Llama3-70B-Fireplace,<3
