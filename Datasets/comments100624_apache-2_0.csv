replier,post_time,type,repo,comment
julien-c,2023-04-27 16:40:36+00:00,comment,distilbert/distilgpt2,"lgtm:)

if this is ready to merge, feel free to (ping if more perms are needed)"
lysandre,2022-11-14 21:36:03+00:00,comment,distilbert/distilgpt2,Thank you!
sgugger,2022-07-22 08:12:57+00:00,comment,distilbert/distilgpt2,Thanks!
julien-c,2022-06-20 12:38:59+00:00,comment,distilbert/distilgpt2,maybe for @pierric
pierric,2022-08-09 22:14:19+00:00,comment,distilbert/distilgpt2,"Hi @GayCodeGal, sorry for the late reply!

Your code seems indeed to be the same as https://github.com/huggingface/tflite-android-transformers/blob/master/models_generation/gpt2.py, which is the script that was used to generate the TFLite version for both gpt2 and distilgpt2. My guess here is that it has to do with the versions of TFLite you're using - if you don't have any error during the generation, the generated model is probably correct, but it's possible that it's incompatible with the android app because of a different TFLite version used in the app (it seems to be TFLite 2.0, see https://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/build.gradle#L56)."
Marissa,2022-05-28 01:10:34+00:00,comment,distilbert/distilgpt2,"@VictorSanh @julien-c @victor @nazneen

I removed the use of collapsible sections (except for 2 subsections in the ""how to use"" section). Also corrected emissions estimate."
julien-c,2022-05-30 12:09:18+00:00,comment,distilbert/distilgpt2,"looks good to me! 

only nit is, I think `co2_eq_emissions` is supposed to be in grams, no @sasha?"
Marissa,2022-05-30 18:05:36+00:00,comment,distilbert/distilgpt2,thanks for catching that! the docs do say grams as well https://huggingface.co/docs/hub/model-repos#carbon-footprint-metadata
julien-c,2022-05-31 07:41:30+00:00,comment,distilbert/distilgpt2,ok then lets update to grams and merge this PR?
Marissa,2022-06-01 23:40:55+00:00,comment,distilbert/distilgpt2,"Updated to grams! Also, @Es-O and I got some small additional feedback from @Meg about the model card, so we made updates based on that feedback."
Marissa,2022-06-01 23:43:13+00:00,comment,distilbert/distilgpt2,@julien-c could you merge if you think its ok? thanks!
nazneen,2022-05-26 02:37:35+00:00,comment,distilbert/distilgpt2,👍
patrickvonplaten,2022-05-26 02:50:39+00:00,comment,distilbert/distilgpt2,cc @VictorSanh
julien-c,2022-05-27 08:16:13+00:00,comment,distilbert/distilgpt2,"that's in grams, correct?"
Marissa,2022-05-28 00:05:26+00:00,comment,distilbert/distilgpt2,"estimate was inaccurate, closing this PR and opening a new one with other changes and the correct estimate"
patrickvonplaten,2022-05-31 07:46:15+00:00,comment,distilbert/distilgpt2,Sounds good - thanks @Marissa!
patrickvonplaten,2022-05-26 00:55:20+00:00,comment,distilbert/distilgpt2,"That looks great to me! 
Thanks a lot @Marissa

What does the author think @VictorSanh - good for merge?"
nazneen,2022-05-26 00:56:33+00:00,comment,distilbert/distilgpt2,LGTM too!
patrickvonplaten,2022-05-26 01:10:00+00:00,comment,distilbert/distilgpt2,"Alright, let's merge it - hope that's fine for you @VictorSanh ! Otherwise we can still revert afterwards (#feature request haha @julien-c) :-) "
VictorSanh,2022-05-26 08:08:42+00:00,comment,distilbert/distilgpt2,"lgtm!

only thing i would note: `**Hours used:** 8` -> it's actually in the order of magnitude of 1 week on a node with 8 16GB v100

would you like to change that @Marissa? :)"
julien-c,2022-05-27 08:15:19+00:00,comment,distilbert/distilgpt2,"can we wait for a little bit more feedback from the team before merging, next time?

In particular, not a huge fan of the collapsible sections UX-wise. What do others think?"
victor,2022-05-27 08:24:19+00:00,comment,distilbert/distilgpt2,I don't like collapsable sections either: they can be used for certain very long sections but not everywhere like this and the main content should not be collapsed.
patrickvonplaten,2022-05-27 15:06:10+00:00,comment,distilbert/distilgpt2,"Sounds good, so general review policy might help here @julien-c"
Marissa,2022-05-27 21:55:55+00:00,comment,distilbert/distilgpt2,thanks for the feedback -- will create a new PR with those changes!
patrickvonplaten,2022-05-31 07:46:31+00:00,comment,distilbert/distilgpt2,Sounds good!
Essacheez,2024-05-28 10:41:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
jayoon16,2024-05-23 08:44:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Same... HELP
hitoruna,2024-05-23 10:53:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"1. Get a token here
2. in your environment set export HUGGINGFACE_TOKEN=your token
3. export HUGGINGFACE_API_KEY=your token
4. Run  huggingface-cli login --token your token
5. After that you have access to the resource

The question is why do I need this for... I dont know...
Hope it helps"
pandora-s,2024-05-15 17:40:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi there !! Check here: https://docs.mistral.ai/getting-started/open_weight_models/ 

As it is now, 16gb of VRAM minimum for full precision- However !! This does not take into account quantization, you can run quants with less hardware requirements, you might want to check it out since most people do not run the full precision model, will depend on ur use case too !"
aahcmh,2024-05-14 15:40:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi David!
1st: Go to the Mistral model (The version you want to use) on Hugging Face then click on ""Agree and accept repository"".

2nd: Execute these two lines: 
!pip install --upgrade huggingface_hub
!huggingface-cli login --token $HUGGING_FACE_TOKEN
"
Davidfer066,2024-05-14 15:57:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hello Ahmed.
Thanks for your response.
I have just did that, and it keeps telling me the same error.
When I put huggingface-cli login --token $HUGGING_FACE_TOKEN it tells me this: 
 huggingface-cli login --token $HUGGING_FACE_TOKEN
usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN] [--add-to-git-credential]
huggingface-cli <command> [<args>] login: error: argument --token: expected one argument

I don´t know what to do.
Thanks again for responding my message.
Regards."
aahcmh,2024-05-14 16:11:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Make sure there's no spaces between the letters in your access token and no more than one space between the ""--token"" argument and the HF_TOKEN."
hitoruna,2024-05-20 03:07:43+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"When running huggingface-cli login --token $HUGGING_FACE_TOKEN
huggingface-cli <command> [<args>] login: error: argument --token: expected one argument

This is assuming that  $HUGGING_FACE_TOKEN has some value, but now it is empty. From where do we get  $HUGGING_FACE_TOKEN?"
SavantofIllusions,2024-05-12 20:52:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format

"
pixelpandacreative,2024-05-13 03:05:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format
😁 
"
julien-c,2024-05-03 22:30:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,i think this issue is more for the langchain repo
AliAmmar270,2024-05-01 10:18:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi, did you figure it out? I also suddenly started getting this error."
Subhayan1,2024-05-01 14:57:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Not yet, still getting the same error."
social6,2024-05-02 00:54:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I am also getting this error. My code worked a few days ago. 
nitishr06,2024-05-02 05:02:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"My code was working earlier but getting this error now 
"
joangonzaleezzz,2024-05-02 09:24:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I have the same issue
kaurjasp,2024-05-02 22:28:17+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Suddenly starts getting this error without any code changes. It was working fine 2 days back.

yes"
a7mmdll,2024-05-03 06:33:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
bassem14,2024-05-04 11:36:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I have the same issue  , anyone solve it ?
"
a7mmdll,2024-05-04 14:06:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Just updated langchain library to the latest
"
nitishr06,2024-05-08 07:22:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Did any one solve the issue?
"
vaidehirao,2024-05-09 18:22:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"i solved by changing huggingface_hub.interface_api  file on 152 line i replaced api_url as  as self.api_url = f""{INFERENCE_ENDPOINT}/models/{repo_id}"""
Adilmar,2024-05-10 21:22:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Same problem was working and just stopped
nij4t,2024-05-14 11:20:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"You can alternatively override the client's api url.
E.g.
```python
llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)
llm.client.api_url = 'https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct'
llm.invoke('foo bar')
```"
vaidehirao,2024-05-15 20:31:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Great that also works @nij4t 
Adilmar,2024-05-16 22:47:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I can solve it by updating the libraries:langchain and huggingface-hub
vaidehirao,2024-05-17 16:33:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Can you share the command? Which version u updated?
Adilmar,2024-05-27 11:24:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@vaidehirao  uninstall as previous verses and  

pip install langchain and 
pip install huggingface-hub "
vamsivallepu,2024-04-30 06:36:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Working on something similar. No matter what the prompt is, it's providing answers from out of context. Have you found solution?"
acerutti,2024-04-24 21:31:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I got the same error. It worked for two API requests and then the same error appeared.
w8jie,2024-04-24 09:45:17+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,+1
drmasad,2024-04-27 06:44:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,+
jonnywesten,2024-05-01 13:17:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,+1
GustavoBMG,2024-04-23 16:54:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hey ... try passing the token also into the `AutoTokenizer`, not only on `AutoModelForCausalLM`:

`AutoTokenizer.from_pretrained(model_id, token = '<your token>')`

`AutoModelForCausalLM.from_pretrained(model_id, token = '<your token>')`"
GustavoBMG,2024-04-23 16:55:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Please also accepts the terms and conditions in the 'Homepage' of the model.

In addition, try passing the token also into the `AutoTokenizer`, not only on `AutoModelForCausalLM`:

`AutoTokenizer.from_pretrained(model_id, token = '<your token>')`

`AutoModelForCausalLM.from_pretrained(model_id, token = '<your token>')`"
GustavoBMG,2024-04-23 16:55:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hey ... try passing the token also into the `AutoTokenizer`, not only on `AutoModelForCausalLM`:

`AutoTokenizer.from_pretrained(model_id, token = '<your token>')`

`AutoModelForCausalLM.from_pretrained(model_id, token = '<your token>')`"
timing418,2024-04-24 05:21:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"In bash 
$ huggingface-cli login
then the screen will ask you to Enter your token (input will not be visible):
paste your token from settings => access tokens 

this worked for me"
CTJP,2024-04-24 09:12:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hey, Thanks for the advice, sadly neither of the above worked for me. 
huggingface-cli whoami
Is returning the correct user name so the token is definitely working and I am able to load the tokenizer but not the model or config.
I have replicated this in several environments one using vllm in Vertex AI, a local notebook and a Colab enterprise notebook."
GustavoBMG,2024-04-24 09:20:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"ok, weird ... can you post the error message?"
mav1814,2024-04-24 11:22:32+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@GustavoBMG  where do i pass?

AutoTokenizer.from_pretrained(model_id, token = '<your token>')

AutoModelForCausalLM.from_pretrained(model_id, token = '<your token>')

Im trying to do it on the privategpt repository via docker.."
GustavoBMG,2024-04-24 14:57:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"After the token param:

AutoTokenizer.from_pretrained(model_id, token = 'bfksdhbfksdbgkdsbgfhd')

AutoModelForCausalLM.from_pretrained(model_id, token = 'fdsbfndksjfdsnfkds')


something like this"
mav1814,2024-04-24 15:14:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"sorry my i wrote wrong.
I know here to put the token, i just don't know where i put the autoTokenizer.from and the CausalILM. When i say where, i'm talking about the file."
GustavoBMG,2024-04-25 08:07:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@mav1814 ... hey I'm not following what you are saying ...

What I mean is: copy the code snippet example from the model card and put the parameters there."
palanikumar,2024-04-23 11:14:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I am also getting this same error today(4/23/2024)
Robin-singh,2024-04-23 12:56:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"OSError: You are trying to access a gated repo.
401 Client Error
i have access also created new api still facing issue
"
GustavoBMG,2024-04-23 16:57:17+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"If you already have accepted the terms and conditions at the 'Homepage',  try passing the token also into the `AutoTokenizer`, not only on `AutoModelForCausalLM`:

`AutoTokenizer.from_pretrained(model_id, token = '<your token>')`

`AutoModelForCausalLM.from_pretrained(model_id, token = '<your token>')`"
Robin-singh,2024-04-25 08:04:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Now it working fine, i am using notebook_login

from huggingface_hub import notebook_login
notebook_login()"
4Phlaime,2024-04-30 08:26:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi, Same for me. I try tro use the model withe the pipeline, but it doesn’t work. It write me : ""Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must be authenticated to access it.""
But i already have accept the term, and when i go on mistral hugging face page, it's write, ""Gated model
You have been granted access to this model""
Is someone have a solution ? "
varnit19,2024-05-06 02:51:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I had the same error. I just regenerated the API key with ""write"" permissions, and it worked."
royfiz,2024-05-07 14:24:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"same error OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like mistralai/Mistral-7B-Instruct-v0.2 is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
hitoruna,2024-05-20 03:13:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,What token? Where do we get this token everybody is talking about?
GustavoBMG,2024-05-20 10:48:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Follow this @hitoruna 

https://huggingface.co/docs/transformers.js/en/guides/private"
carlotamdeluna,2024-04-23 07:20:25+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Me too
WaleedJubeh,2024-04-23 07:46:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"it's down, please solve it"
GustavoBMG,2024-04-23 16:58:23+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"It is now gated ... you need to create a token, accept the terms and conditions, and passing the token also into the `AutoTokenizer`, not only on `AutoModelForCausalLM`:

`AutoTokenizer.from_pretrained(model_id, token = '<your token>')`

`AutoModelForCausalLM.from_pretrained(model_id, token = '<your token>')`"
Shubhang720,2024-04-21 03:57:13+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I am also facing issues while loading the model have been stuck since night approx 14 hrs i guess !! the model is down !!
"
barunsaha,2024-04-21 07:08:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Same here. One of my Spaces using Mistral stopped working. Then I found that it is not working in the model page as well.
wasd123y,2024-04-21 07:27:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"cc from duscussion below https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/discussions/98

Just look at ""files"" page - https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main
MistralAI added agreement which you need to accept. IDK if it possible via download api."
hvkkvh,2024-04-21 12:42:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I do not really understand what you mean here, as the page you linked is the repo of mistral on hugging face, which has the latest commit approx 1 month back, and api(not the mode stored locally) stopped working just day before yesterday.
Can you be exact about the agreement you are talking about?
"
MaziyarPanahi,2024-04-21 13:05:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"For everyone else who doesn't want to deal with this BS, here is a collection of all the models with their latest commits: 
https://huggingface.co/collections/MaziyarPanahi/mistralai-6624cd82c322e4c148ca813a

![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/Jm-9lbDBC3KqXg7vUD7Jo.png)

They will never be removed, nor gated! Enjoy them in your production and don't let any troll tells you otherwise! You should never be grateful for unprofessional behavior, free or not!
"
tlholden489,2024-04-22 22:26:25+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I have accepted the agreement and still cannot get my old space or even a  new space to do anything except timeout
m4rialoureiro,2024-04-20 07:55:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I'm getting the exact same error on my side: HTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2

The above exception was the direct cause of the following exception:

HfHubHTTPError                            Traceback (most recent call last)
HfHubHTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: 3cc2NFKMmOESTRjl-4VSi)

Model mistralai/Mistral-7B-Instruct-v0.2 is currently loading

The above exception was the direct cause of the following exception:

InferenceTimeoutError                     Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py in post(self, json, data, model, task, stream)
    248                     # If Model is unavailable, either raise a TimeoutError...
    249                     if timeout is not None and time.time() - t0 > timeout:
--> 250                         raise InferenceTimeoutError(
    251                             f""Model not loaded on the server: {url}. Please retry with a higher timeout (current:""
    252                             f"" {self.timeout})."",

InferenceTimeoutError: Model not loaded on the server: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2. Please retry with a higher timeout (current: 400)."
Issafre,2024-04-20 08:05:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"the model didn't update since 27 day so i think the problem will solved in the close
"
m4rialoureiro,2024-04-20 08:13:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Do you know any other free model that I can use?
I'm doing a technical project in an interview process and it's the first time I'm working with LLMs and I could make this one work from a tutorial...
I'm using this code to load it:
llm = HuggingFaceEndpoint(
    timeout=400,
    repo_id=""mistralai/Mistral-7B-Instruct-v0.2"",
    max_new_tokens=240,
    # top_k=10,
    # top_p=0.95,
    # typical_p=0.95,
    temperature=0,
    # repetition_penalty=1.03,
    huggingfacehub_api_token='X'
)
But now I don't know any other model that should work with this code and I have until tomorrow night to finish the assignment. Can anyone help by suggesting a free alternative? Thank you!"
Issafre,2024-04-20 08:38:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I don't think you can use other models with the same code because the parameters of each model are different You can browse the models and read their documentation 
m4rialoureiro,2024-04-20 09:27:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Thanks, hopefully it is back up soon enough... I'm currently trying out the 'tiiuae/falcon-7b-instruct' but the quality of the answers is not comparable"
hvkkvh,2024-04-20 10:15:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Iguess we should just download it next time and use it locally, my whole project stopped because of this. I am using this with python.
yesterday's error
{'error': 'Model mistralai/Mistral-7B-Instruct-v0.2 is currently loading', 'estimated_time': 579.3385620117188}
today's error
{'error': 'Service Unavailable'}

they did it even without notifying lol"
Issafre,2024-04-20 10:50:43+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Yes that 's right i think we must run it localy i hope it is work soon
hvkkvh,2024-04-20 11:19:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Okay I inspected the issue further, I guess the development team might be updating the model on hugging face right now, but It can definitely be the case that some updates are not working with hugging face at the moment, because I have found a solution to use it for now. Do you want the intermediate solution?"
m4rialoureiro,2024-04-20 11:23:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Yes, if you can share, that would be great!"
m4rialoureiro,2024-04-20 11:24:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I am using google colab because my personal computer doesn't have much power, I don't know if this complicates things."
hvkkvh,2024-04-20 11:40:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I am using the APIs, so that I do not have to download the model myself, but planning to download as soon as it is updated on the hugging face card, and host it myself privately, to develop my app better, I do not know if the way I approach it with API would be similar to yours. Do you still want it?"
m4rialoureiro,2024-04-20 11:53:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"the way i'm currently doing it is through the lang chain HuggingFaceEndpoint, do you think that would work? My knowledge on this framework is somewhat limited, I'm currently on the learning process so I'm sorry if I have some stupid question"
hvkkvh,2024-04-20 12:19:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I guess you are using APIs.
You used Mistral-7B-Instruct-v0.2 before langchain right?
if yes, then I can help you!
just mail me on: izcilicio@gmail.com
as I do not want everyone to flood that solution as well.
For me, it is working properly in my old project after adjusting."
Issafre,2024-04-20 13:40:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Can you share the solution with us please 
hvkkvh,2024-04-20 14:03:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"yes sure can you send me an email, or leave your email here, I really dont want this flooded, as I am really cautious if that also stops working because of the number of requests, so until hugging face api for mistral starts working again."
Issafre,2024-04-20 14:50:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Ok done
hvkkvh,2024-04-20 15:09:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Sent mail to both of you!
bencause,2024-04-20 15:21:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,save me TAT hvkkvh
wasd123y,2024-04-21 07:25:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Just look at ""files"" page - https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main
MistralAI added agreement which you need to accept. IDK if it possible via download api."
MaziyarPanahi,2024-04-21 13:07:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"For everyone else who doesn't want to deal with this BS, here is a collection of all the models with their latest commits: 
https://huggingface.co/collections/MaziyarPanahi/mistralai-6624cd82c322e4c148ca813a

![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/Jm-9lbDBC3KqXg7vUD7Jo.png)

They will never be removed, nor gated! Enjoy them in your production and don't let any troll tells you otherwise! You should never be grateful for unprofessional behavior, free or not!
"
deleted,2024-04-21 16:06:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Just look at ""files"" page - https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main
> MistralAI added agreement which you need to accept. IDK if it possible via download api.

Right, the people who spent time and resources to create them are simply asking for people to agree to some simple terms. And people have been adding their HF token to use API so it cant be that hard.

And those trying to circumvent, should be ashamed of yourself."
MaziyarPanahi,2024-04-21 16:50:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
deleted,2024-04-21 17:06:23+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"
> PS: People like you should contribute more, openly, freely, for years, before coming here and insult or shame others without knowing what they do! 

I have, far more than you could ever comprehend. And i stopped that decades ago, to due  unappreciative dickheads like you. One list time. Fuck off, self-entitled scumbag."
MaziyarPanahi,2024-04-21 17:46:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
deleted,2024-04-21 17:52:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"My actual name is none of your business.  ( hint, my nick is a acronym, which clearly you dont understand. not as smart as you think you are.  ha )  And for the record: No, your generation didnt 'can' me.  Scumbag children like you are owned by people like me.  You are just too stupid to realize it.

Next week I may want to call the HF founder ( and mistral ) about you violating terms of service around here.   Ill decide later if its worth my time.  "
MaziyarPanahi,2024-04-21 18:04:23+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
MaziyarPanahi,2024-04-21 18:07:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
hvkkvh,2024-04-21 18:43:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Both of you, please stop acting like kids, this a forum for help, not for petty fights.
Users who are following this thread are getting updates to your fight messages. So please refrain from doing it.
This particular thread is not because of MISTRAL's agreement and similar, it is api related, i.e. users who have already accepted the terms and conditions. So in conclusion, stop your cat fight."
MaziyarPanahi,2024-04-21 18:46:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Both of you, please stop acting like kids, this a forum for help, not for petty fights.
> Users who are following this thread are getting updates to your fight messages. So please refrain from doing it.
> This particular thread is not because of MISTRAL's agreement and similar, it is api related, i.e. users who have already accepted the terms and conditions. So in conclusion, stop your cat fight.

you are correct and apologies to everyone. "
Mahfujul,2024-04-23 03:57:58+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,even after accepting the aggrement it says service unavailable
WaleedJubeh,2024-04-23 08:23:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,service unavailable
dordonezc,2024-04-24 00:32:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Service unavailable keeps appearing, it suggested to try a higher timeout. Does anyone know how to do that?
"
hvkkvh,2024-04-24 09:37:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"You can email me at: izcilicio@gmail.com
"
WaleedJubeh,2024-04-25 08:39:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,it fixed for me
fyp-llm,2024-04-23 05:12:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,The rate limit is 300 API calls per hour per API token. You can generate another HF token to call API again
XIX181,2024-04-19 22:34:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,The 
bencause,2024-04-20 04:00:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2, + 1
bencause,2024-04-20 04:21:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"getting same payload. 
2024-04-19 23:10:54,875 - DEBUG - Starting new HTTPS connection (1): api-inference.huggingface.co:443
2024-04-19 23:10:56,065 - DEBUG - https://api-inference.huggingface.co:443 ""POST /models/mistralai/Mistral-7B-Instruct-v0.2 HTTP/1.1"" 503 108
2024-04-19 23:10:56.070 Uncaught app exception
Traceback (most recent call last):
"
sytossml,2024-04-18 17:41:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Same
sytossml,2024-04-18 17:43:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I passed hugging face  token to constructor and it worked
CESIOtheone,2024-04-18 21:16:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"How do I do that?
"
DBL0422,2024-04-18 23:22:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,why？
linhnk,2024-04-18 23:29:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,same
xvzan,2024-04-19 01:26:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,istrli
liketheflower,2024-04-19 01:26:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I have another machine with cached file. By using that cached file, it works but needs to find a formal solution.
The cache file can be found in ~/.cache/huggingface"
Jiten987,2024-04-19 04:07:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Same problem I can face today so what is the solution of this problem or how to handle it?
ravindul97,2024-04-19 04:26:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Make a request to access the repo and then pass a access token by passing `token=<your_token>`.

from huggingface_hub import login
login(token=""your_access_token"")

An access_token_can be get by your hugging face account settings"
splendidbird,2024-04-19 06:56:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"This worked for me:
I went to model card and clicked ""Agree and access repository"".  Then I was able to start downloading.
![image.png](https://cdn-uploads.huggingface.co/production/uploads/64be1b64b567ae97c34ae97e/hnAwVwMhoierxPpvhjCZS.png)
"
alex8642,2024-04-19 15:57:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> This worked for me:
> I went to model card and clicked ""Agree and access repository"".  Then I was able to start downloading.
> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64be1b64b567ae97c34ae97e/hnAwVwMhoierxPpvhjCZS.png)

sadly this did not work for me. I also tried putting my huggingface token in my privategpt settings.yaml file without success. Not sure what others have done here. The information is too vague (I do not understand it well enough) for me to follow."
Mant1,2024-04-19 23:10:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Same issue... I am trying to use this with VLLM. How to pass token with vllm
alexakkol,2024-04-20 01:52:00+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Same
"
nunomlucio,2024-04-20 09:36:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"pip install --upgrade huggingface_hub

huggingface-cli login

YOUR_ACCESS_TOKEN


Done"
alex8642,2024-04-20 11:24:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> pip install --upgrade huggingface_hub
> 
> huggingface-cli login
> 
> YOUR_ACCESS_TOKEN
> 
> 
> Done

THANK YOU."
MaziyarPanahi,2024-04-20 20:12:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"why? without any heads up, this just broke so many pipelines in production. "
deleted,2024-04-20 20:27:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> why? without any heads up, this just broke so many pipelines in production.

if you rely on a free service, and dont plan ahead for it to vanish totally tomorow. you get burnt.  What happens when HF start charging for any access?  Or they vanish, due technical problems, throw in the towel, or are shut down forcefully..  ""whaaaa i should have got the files downloaded locally to keep""  ( or even a project. )

"
muradkhan,2024-04-21 07:52:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"it worked for me after ""Agree and access repository"" but now it's not working again. What could be the possible solution."
MaziyarPanahi,2024-04-21 08:15:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
deleted,2024-04-21 11:17:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> > > bla bla nonsense bla bla whining

ignore

"
MaziyarPanahi,2024-04-21 12:56:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
deleted,2024-04-21 14:03:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Well i did say id ignore but one last comment before i disable updates:  You are an entitled whiny ass gen Z bitch.  If you dont pay, you have ZERO right to complain. YOU are part of the problem. 

"
MaziyarPanahi,2024-04-21 14:14:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
bennison,2024-04-22 18:41:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> I passed hugging face  token to constructor and it worked

Why do we need to use this token to run this model on local environment?"
bennison,2024-04-22 18:42:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> > This worked for me:
> > I went to model card and clicked ""Agree and access repository"".  Then I was able to start downloading.
> > ![image.png](https://cdn-uploads.huggingface.co/production/uploads/64be1b64b567ae97c34ae97e/hnAwVwMhoierxPpvhjCZS.png)
> 
> sadly this did not work for me. I also tried putting my huggingface token in my privategpt settings.yaml file without success. Not sure what others have done here. The information is too vague (I do not understand it well enough) for me to follow.

What if I need to handle this on server, How to do this in server?"
jcamsler,2024-05-07 08:00:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"It works for me in google collab in doing this after cliking on the model ""Agree and access repository""

<code>
!pip install --upgrade huggingface_hub
</code>
<br>
<code>
!huggingface-cli login --token  $REPLACE_WITH_YOUR_TOKEN_ON_HUGGING_FACE
</code>
<br>
<br>
Result :
<code>
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /root/.cache/huggingface/token
Login successful
</code>"
robin3,2024-05-10 13:01:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"When you want to do through your terminal where you have your active environment . 
Just run two command.
1. pip install --upgrade huggingface_hub
2. huggingface-cli login --token ""Your access token""
Then you will see something like this
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/.cache/huggingface/token
Login successful
All set!!!!!!!!!!!!!!1
"
amrohendawi,2024-05-19 20:00:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I had to go to the model's page on huggingface and click ""I accept the terms"" in order for the download to work."
hitoruna,2024-05-20 03:15:13+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Everybody talks about the TOKEN but where is this token gotten from? How do we get this token? 
hitoruna,2024-05-20 03:15:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> I passed hugging face  token to constructor and it worked

What token?"
amrohendawi,2024-05-20 06:00:32+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> > I passed hugging face  token to constructor and it worked
> 
> What token?

Obviously and like any API service you use, you should go to your user settings and generate a token for using the API.
Just google ""how to generate token for huggingface hub""."
thomashjorth,2024-06-02 07:58:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> > I passed hugging face  token to constructor and it worked
> 
> What token?

https://huggingface.co/docs/hub/security-tokens
..also check https://huggingface.co/docs/hub/spaces-sdks-docker#secrets-and-variables-management"
ayu8393,2024-04-26 10:45:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"they do not support mistral models with the old inference engine. so you should try to use the latest inference engine DeepSpeed-MII.  Here's an example for running a mistral model:

import mii
pipe = mii.pipeline(""mistralai/Mistral-7B-v0.1"")
response = pipe([""DeepSpeed is""], max_new_tokens=128)
print(response)

"
jiangtaozh,2024-05-01 18:31:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"in the backend of mii, the inference is still backed by deepspeed inference engine. I tried, it is the same. no memory footprint reduction along the tensor parallism with the following code.

import argparse
import mii

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--model"", type=str, default=""mistralai/Mistral-7B-v0.1"")
    parser.add_argument(""--tensor-parallel"", type=int, default=1)
    args = parser.parse_args()
    mii.serve(args.model, tensor_parallel=args.tensor_parallel)
    print(f""Serving model {args.model} on {args.tensor_parallel} GPU(s)."")
    print(f""Run `python client.py --model {args.model}` to connect."")
    print(f""Run `python terminate.py --model {args.model}` to terminate."")=
main()
"
jiangtaozh,2024-05-01 18:33:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,https://github.com/microsoft/DeepSpeed-MII/issues/329
NahuelEscujuri,2024-04-18 14:10:25+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"me too :"")
"
NahuelEscujuri,2024-04-18 14:35:00+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"oh, I found a way to do it:

 const hf = new HfInference(""yourToken"")

  async function Generate(){
    for await (const output of hf.textGenerationStream({
      model: ""mistralai/Mistral-7B-Instruct-v0.2"",
      inputs: 'how to feel you?',
      parameters: { max_new_tokens: 250 }
    })) {
      console.log(`// ${output.token.text} //`, output.generated_text);
    }
  }
  
  return (
    <div>
       <button onClick={Generate}>Generar</button>
    </div>
  );

the final result returns the full text

(sorry i don't know how to write code in comments)"
alexrods,2024-04-12 23:52:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I got better results with the next code:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

device = ""cuda""
model_name = ""mistralai/Mistral-7B-Instruct-v0.2""
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

input_text = ""ask to mistral""
input_ids = tokenizer.encode(input_text, return_tensors='pt')

model.to(device)
model_inputs = input_ids.to(device)

output_ids = model.generate(
        model_inputs
        )
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

```

"
dmsovetov,2024-04-18 14:59:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Stumbled upon your question while googling because of the same problem. As a workaround I used additional [INST] tags and seems that it works in my case.
So the final prompt is: 

`[INST] <I put a context here and instructions> [/INST][INST]Answer the following question: <The question goes here>[/INST]`

Does anyone knows about other options?"
ImranzamanML,2024-04-12 14:44:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,....
ImranzamanML,2024-04-12 14:45:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
singhay,2024-04-14 01:32:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,It uses RoPE scaling
Andriy,2024-04-14 04:18:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@singhay This doesn't solve the quadratic space and time complexity of attention in transformers.
singhay,2024-04-15 20:55:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"You're right that no it does not, it's a hack to improve how the positions are embedded of a sequence. Instead of 1,2,3... it's more advance like 1, 1.25, 1.5, 1.75, 2 -> 4x more positions that can be incorporate."
MCH-C,2024-05-21 11:42:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Any idea why Sliding Window Attention has been abandoned
Yemeral,2024-04-03 10:47:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi! I have actually been working on that these days. I had thought about writing a post on Medium describing the process but since I have never published anything before, it might take me some time.

In the meantime, I can give you some tips if you want, although you might have already figured it out."
deeplearner123,2024-04-03 12:41:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"My goal is creating a local chatbot with mistral 7b which connects my text data and mistral 7b and this agent must read german language very well. If you could provide something, I will be happy."
Yemeral,2024-04-03 13:05:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"If you want to chat with your local text data, wouldn't it be better to implement a RAG pipeline instead of using agents?"
deeplearner123,2024-04-03 13:44:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I am actually not an expert on this topic and I am trying to learn how to connect a big llm and a data source but llm must work without any API connection. It must be downloaded on my own device. Yeah maybe it would be better on that way but I don't have really any idea..
Yemeral,2024-04-03 14:30:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Ok. Check this: https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146
It is a good starting point.

"
deeplearner123,2024-04-03 21:02:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"That is great thanks for your support :D 
"
DavidMolano,2024-04-03 23:45:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I'm having an error runing simple tools on agent.
The error is: LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools...

Do you recognize this error?"
Yemeral,2024-04-04 05:35:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> I'm having an error runing simple tools on agent.
> The error is: LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools...
> 
> Do you recognize this error?

I'm almost certain it happens because you're not managing the 'stop words'. In each iteration, langchain sends the LLM a list of words that, if found in the output, must be removed along with all the text following them.

In the _call method of the LLM class, there's an optional 'stop' parameter. That's the list of words the model will receive.

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:

By the way, I think I will be able to publish the tutorial this weekend."
DavidMolano,2024-04-05 20:09:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> > I'm having an error runing simple tools on agent.
> > The error is: LLM output produced both a final answer and a parse-able action:: Answer the following questions as best you can. You have access to the following tools...
> > 
> > Do you recognize this error?
> 
> I'm almost certain it happens because you're not managing the 'stop words'. In each iteration, langchain sends the LLM a list of words that, if found in the output, must be removed along with all the text following them.
> 
> In the _call method of the LLM class, there's an optional 'stop' parameter. That's the list of words the model will receive.
> 
>     def _call(
>         self,
>         prompt: str,
>         stop: Optional[List[str]] = None,
>         run_manager: Optional[CallbackManagerForLLMRun] = None,
>         **kwargs: Any,
>     ) -> str:
> 
> By the way, I think I will be able to publish the tutorial this weekend.

Actually I set the stop by this way:

agent = create_react_agent(hf, tools, prompt=prompt_agent, stop_sequence=['\nFinal Answer'] )

agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    verbose=True,
    tools=tools,
    max_iterations=2,
    return_intermediate_steps=True,
    #handle_parsing_errors=True,
    memory=memory
)

while True:
    response = agent_executor.invoke({""input"":input("">>>"")})
    print(response[""output""])"
Yemeral,2024-04-07 07:07:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Well, stop words don't work like that. By the way, here is the tutorial link: https://medium.com/@jorgepardoserrano/building-a-langchain-agent-with-a-self-hosted-mistral-7b-a-step-by-step-guide-85eda2fbf6c2
It includes a link to the related colab. I hope it helps."
Yemeral,2024-04-09 09:36:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Just to comment that I have reviewed and edited the notebook. I have significantly improved the prompting, and now it is much more reliable."
DavidMolano,2024-04-12 14:14:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Thank you @Yemeral I was able to solve my problem using your tutorial.

"
Qubitium,2024-03-28 02:11:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,The original config is correct. 
Kearm,2024-03-28 12:03:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> The original config is correct.

Then why can't I use all 32k context? I have SFT'ed the Base 7b 0.2 and it will summarize 29k Mistral tokens of text without issue. This one wil not."
Qubitium,2024-03-30 21:23:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@Kearm  I think the model loader is wrongly thinking all Mistral uses sliding-window when v0.2 explicitly says it doe not support slidding window attention. Check with the model loader framework. I could be wrong on this.
Kearm,2024-04-01 04:35:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@Qubitium I was using raw most recent transformers over 6 GPU's 2 3090 Ti and 4 3090's. My attempt to replicate again fails, but on my SFT of  it works perfectly with the new base v0.2 model."
FusionCow,2024-03-25 21:12:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Ai flux made a video about it
xzuyn,2024-03-25 21:18:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,They probably just copied the README from the previous version and didn't properly update it until now.
MaziyarPanahi,2024-03-25 21:26:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> They probably just copied the README from the previous version and didn't properly update it until now.

They only have like 5 models, I mean it’s not that hard to double check after months after the release. 
I believe they didn’t want people to know there is a base v0.2 either for strategic reasons or just avoid people keep bugging them about using an instruct without having the pretrained model. (I am just happy we have a new open-source model, so thank you 🙏)
"
xzuyn,2024-03-25 21:37:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> > They probably just copied the README from the previous version and didn't properly update it until now.
> 
> They only have like 5 models, I mean it’s not that hard to double check after months after the release. 
> I believe they didn’t want people to know there is a base v0.2 either for strategic reasons or just avoid people keep bugging them about using an instruct without having the pretrained model. (I am just happy we have a new open-source model, so thank you 🙏)

Maybe, but since it came out all I've seen is people assuming it's a different base (and being sad that Mistral didn't release it's base alongside the Instruct version as they did with the v0.1 version) since it's config isn't using the same SWA window as `mistralai/Mistral-7B-v0.1` or `mistralai/Mistral-7B-Instruct-v0.1`. Only recently with people posting about this README have I seen anything thinking otherwise."
vamshigvk,2024-03-25 15:50:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"can someone please help fix this error: invalid hostname defaulting to 0.0.0.0
"
vamshigvk,2024-04-29 08:44:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,this issue occurred due to wrong security-group usage. issue fixed.
ribhu,2024-03-22 16:54:58+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I don't work at Mistral but according to the LMSys Chat Arena, its knowledge cutoff is December 2023"
OPPEYRADY,2024-04-03 21:32:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Similar issues with a finetuned model as well, it will sometimes work but othertimes generate massive pitfalls of information till it hits max tokens

UPDATE: Ran a test, Instruct v1 does not seem to have this issue."
Goldenblood56,2024-04-04 04:12:25+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Right I've ran into this it feels like with Ooba it got worse 2-3 months ago. But maybe that's just me? I think a certain update made this issue worse. 
GuMMYY,2024-04-12 02:05:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Sliding window attention matters
> Similar issues with a finetuned model as well, it will sometimes work but othertimes generate massive pitfalls of information till it hits max tokens
> 
> UPDATE: Ran a test, Instruct v1 does not seem to have this issue.

"
Goldenblood56,2024-05-01 03:03:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Sliding window attention matters
> > Similar issues with a finetuned model as well, it will sometimes work but othertimes generate massive pitfalls of information till it hits max tokens
> > 
> > UPDATE: Ran a test, Instruct v1 does not seem to have this issue.

What do you mean? Do I want it on or off? How do I tell? How do I change it? It is baked into the Quant or is it a setting? Thanks. "
dustyatx,2024-03-20 13:46:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,..
nyoshida,2024-03-27 01:58:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,This comment has been hidden
Mkaggie23,2024-03-30 21:31:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hey @Shivkumar27 

Check this response here: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/125#65c481d27fe765199ae3add2

the structure of your query should be something like this (from the discussion mentioned above)
query({
    ""inputs"": ""[INST] You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. Do not give the context again in your response\nGenerate a concise and informative answer in less than 100 words for the given question\nSEARCH RESULT 1: The title is Rush, year is 2013, budget is 500000, earning is 300000, genere is action.\nQUESTION: What is the release date of rush?\n[/INST]"",
    ""parameters"": {
        ""return_full_text"": False
    }
})

Edit: The discussion above is for Mixtral 8x7B, but I tested it with this model and it works fine "
nepto,2024-03-13 16:01:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Yeah i faced it when seeing this(thats why i came here),but solved(partially) it after reading ur question ; )

It seems that the inference interface provided in hugging face website uses similar backend approach for this model.

This model works more like completion model to me here.
when is ask ""what is 1+1"" it just asks that back to me! no change in phrase and stuff.
but then i asked ""USER: what is 1+1 ASSISTANT:"", and then it completed the answer! i need to make some scripting to just get out the assistant part out of text,separated from user ; )

seems this is implemented so that the same output can be joined with user's input to easily make a chat interface."
capnchat,2024-03-07 22:12:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,And I just tested using the fireworks.ai instance mentioned in the tutorial and it works perfectly well irrespeoctive of the generations ettings so it has to be the BitsAndBytes config that is the culprit... will be able to verify once I get access to a better VM again!
jilijeanlouis,2024-03-24 18:49:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Try outlines:
https://outlines-dev.github.io/outlines/reference/vllm/
"
jsnmls2,2024-04-19 21:54:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,same problem
SzymonSt2808,2024-04-20 10:13:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,mistral in v0.1 the same
PccNLP,2024-03-02 03:42:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I'm not a professional, but the solution I encountered for the same problem before is:
---
In huggingface transformer

device = ""cuda""
model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"")
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"")
model.to(device).half()
---

Regarding vLLM, I think you can inquire on their GitHub.
https://github.com/vllm-project/vllm/issues"
itod,2024-03-20 08:02:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Solved, just for the record if other face the same issue: Had to vonvert the model to GGUF, int8 or fp16 and then it worked."
GluttonousCat,2024-03-06 05:16:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I have the same question about how to fine tuning by instruction prompt. I try fine-tuning by using the First Prompt, but the effort is not good.
@devendrachaplot @arthurmensch "
recallfun,2024-04-04 02:28:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"3rd prompt works for me, you can also use alpaca format which works great for fine-tuning the base model"
narai,2024-02-29 20:49:43+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Solved with:

ckpt = ""mistralai/Mistral-7B-Instruct-v0.2""

config = AutoConfig.from_pretrained(ckpt)
config.update({'sliding_window': 4096})

model = AutoModelForCausalLM.from_pretrained(ckpt, device_map=""auto"", config=config)
tokenizer = AutoTokenizer.from_pretrained(ckpt, config=config)"
ybelkada,2024-03-04 00:51:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi @narai 
You can also solve the issue by updating transformers - `pip install -U transformers`"
GTimothee,2024-03-22 16:59:00+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"you can also just add the option in from_pretrained (instead of using AutoConfig just for that aim): AutoModelForCausalLM.from_pretrained(ckpt, device_map=""auto"", sliding_window=4096)"
deleted,2024-04-18 03:33:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> you can also just add the option in from_pretrained (instead of using AutoConfig just for that aim): AutoModelForCausalLM.from_pretrained(ckpt, device_map=""auto"", sliding_window=4096)

However it says that v0.2 doesn't use sliding-window-attention, should we set sliding_window=4096, or just set sliding_window=32k"
manaschauhan,2024-02-29 20:31:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Traceback (most recent call last):
  File ""/opt/conda/bin/text-generation-server"", line 8, in <module>
    sys.exit(app())
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py"", line 83, in serve
    server.serve(
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py"", line 207, in serve
    asyncio.run(
  File ""/opt/conda/lib/python3.9/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/opt/conda/lib/python3.9/asyncio/base_events.py"", line 647, in run_until_complete
    return future.result()
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py"", line 159, in serve_inner
    model = get_model(
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py"", line 252, in get_model
    return FlashMistral(
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py"", line 312, in __init__
    SLIDING_WINDOW_BLOCKS = math.ceil(config.sliding_window / BLOCK_SIZE)

By passing the Sliding window value we can update this 
![Screenshot 2024-03-01 at 2.00.08 AM.png](https://cdn-uploads.huggingface.co/production/uploads/64f5a23dc20615897cbd8c4c/7AC263as7NZLIWJNz2_dp.png)"
manaschauhan,2024-03-01 05:24:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I have pasted the error logs above for the model when sliding_window:""null"". 


But after adding the value for the sliding window we can resolve the error. I have successfully deployed the model now.
![Screenshot 2024-03-01 at 3.53.02 AM.png](https://cdn-uploads.huggingface.co/production/uploads/64f5a23dc20615897cbd8c4c/e5mwGfyL4sERDZDQPPXQb.png)
"
AShipperley,2024-02-22 17:45:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"It's in [their paper](https://arxiv.org/pdf/2310.06825.pdf), just ctrl+f and search ""system prompt"". It's under section 5.1.

""Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,
unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity."""
narai,2024-02-29 20:55:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"When I try to add a system prompt it gives an error.

messages = [
    {""role"": ""system"", ""content"": ""Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.""},
    {""role"": ""user"", ""content"": """"""What is the capital of Germany?""""""},
]

TemplateError: Conversation roles must alternate user/assistant/user/assistant/...


So, perhaps that changed after the paper was written or was changed in v0.2.
"
bingw5,2024-03-07 02:08:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"The chat template is defined in `Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json`, by default, it doesn't accept system role. You may need to composite a new chat template to support system role."
AShipperley,2024-04-05 02:46:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> When I try to add a system prompt it gives an error.
> 
> messages = [
>     {""role"": ""system"", ""content"": ""Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.""},
>     {""role"": ""user"", ""content"": """"""What is the capital of Germany?""""""},
> ]
> 
> TemplateError: Conversation roles must alternate user/assistant/user/assistant/...
> 
> 
> So, perhaps that changed after the paper was written or was changed in v0.2.

If you send a full code snippet I could try it on my machine, but otherwise you can just use the Llama-2 prompt structure:

```
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST]
```"
pandora-s,2024-02-18 17:54:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi, Could you please give us more context and a sample of the code? We cannot really help that much with just this."
ybelkada,2024-02-16 07:28:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi @yotam433 ! 
Can you share the full snippet? 
You should try a snippet that we expose on model card such as: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1#run-the-model"
chuangxinlezhi,2024-04-02 02:56:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,1
chuangxinlezhi,2024-04-02 02:56:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,very cool
ybelkada,2024-02-14 06:15:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi @Aloukik21 ! 
I think this model should be supported in vllm - feel free to open an issue directly on their repository: https://github.com/vllm-project/vllm"
Navanit-shorthills,2024-03-05 10:20:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@Aloukik21  are you able to resolve it. 
I am also stuck in this"
EricMMD,2024-03-06 09:34:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@Navanit-shorthills 
If thats helps for me it works with this in the ""offline_inference.py"":
llm = LLM(model=""mistralai/Mistral-7B-v0.1"", max_model_len=20000, gpu_memory_utilization=0.9 ), depending what GPU ure using you can set the len."
Navanit-shorthills,2024-03-06 09:42:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@EricMMD Thank you 
"
silvacarl,2024-04-04 18:43:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,is  max_model_len=20000 arbitrary or just simple the max number of tokens i cen expect to inference?
dfrank,2024-04-14 07:10:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I checked the documentation on this parameter and it says: ``model context length. If unspecified will be automatically derived from the model``
junli8848,2024-04-30 19:04:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I have the same issue when using the vllm docker container to run the model. 
Is there a way to specify the argument `gpu_memory_utilization=0.9` in the vllm's docker command? When I execute the docker command:

```
docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env ""HUGGING_FACE_HUB_TOKEN=<secret>"" \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model mistralai/Mistral-7B-Instruct-v0.2  --gpu_memory_utilization 0.9
```

Then I got the errors as below:
```
api_server.py: error: unrecognized arguments: --gpu_memory_utilization 0.9
```
"
Aloukik21,2024-04-30 19:15:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,this is the correct argument for cli: --gpu-memory-utilization
junli8848,2024-04-30 21:24:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@Aloukik21  Thanks!
Berketarak,2024-02-10 14:03:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hello, 

For clarification, did you finetune the model?"
expiderman,2024-02-12 17:12:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Hello, 
> 
> For clarification, did you finetune the model?

No. I'm using it as is, via huggingface.
I have tried with other temperature values, but the result is the same"
pandora-s,2024-02-12 21:32:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"This is usually not a temperature problem, if you can, increase ""max_new_tokens"", it's usually this parameter that allows you to increase the number of tokens to output."
expiderman,2024-02-13 15:08:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> This is usually not a temperature problem, if you can, increase ""max_new_tokens"", it's usually this parameter that allows you to increase the number of tokens to output.

I'm doing tests and I think it has solved the problem. I have set max_new_tokens to 2000. Do you know what is the maximum that can be used? or what is a suitable number?
Thank you very much for your answer!"
pandora-s,2024-02-13 17:13:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Well...in theory there is no upper limit, as you can complete a text as much as you want... BUT, there is a limit factor, and that's the context window/length.

To make it simple, how much it will remember before start to forget the beginning of the text.

I believe Mistral as a context length of 4k*2, it can go up to 8k in theory with the slide window method that they use, but 4k would be the most reliable max length.

To sum up: 4k or 8k depending on what you want or need, more than 8k and it will start forgetting what it was supposed to do."
JohananYTL,2024-02-22 05:48:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I'm having the same issue, however changing the max_new_token doesn't seem to change it. 
For example, this is my current code:
```python
llm = HuggingFaceHub(
         repo_id=""mistralai/Mistral-7B-Instruct-v0.2"", 
         model_kwargs={""temperature"": 0.7, ""max_new_token"" : 8000},
         huggingfacehub_api_token=API_TOKEN
         )
      print(llm.invoke(""[INST]Explain to me how to analyze IT ticket data.[\INST]""))
```
And the output I get is:
```[INST]Explain to me how to analyze IT ticket data.[\INST]Analyzing IT ticket data involves several steps that can help you identify trends, resolve issues, and improve your IT support process. Here's a general approach you can follow:

1. Data Collection: The first step is to collect all the relevant IT ticket data. This may include information such as ticket creation date, category, subcategory, priority, status, resolution time, requester, assignee, and any additional notes or comments. You can use IT service management (ITSM
```

Ive tried playing around with the max_new_token parameter, but nothing seems to be changing the output."
pandora-s,2024-02-22 07:01:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Could you try `max_new_tokens` instead of max new token ?
bumel,2024-03-07 13:00:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"This looks as the template problem to me. Cleanup the template a bit. Remove the ""###"" before the [INST] and the last most ""#"" after the [/INST]. 
So the template would be: 
```
 template=""""""[INST] Instruccion: Responde en español a las preguntas del usuario según el contexto.
Si no encunetras una respuesta adecuada en el contexto, responde que no tienes información suficiente.

{context}

### question:
{question} (responde en castellano) [/INST]
""""""
```
"
expiderman,2024-03-07 21:48:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> This looks as the template problem to me. Cleanup the template a bit. Remove the ""###"" before the [INST] and the last most ""#"" after the [/INST]. 
> So the template would be: 
> ```
>  template=""""""[INST] Instruccion: Responde en español a las preguntas del usuario según el contexto.
> Si no encunetras una respuesta adecuada en el contexto, responde que no tienes información suficiente.
> 
> {context}
> 
> ### question:
> {question} (responde en castellano) [/INST]
> """"""
> ```

Thanks! This also helped me. Some models are very sensitive to templates."
ybelkada,2024-02-13 05:48:11+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi @Pradeep1995 
How do you verify that `</s>` is not generated? Can you make sure you decode all tokens with `skip_special_tokens=False` ?
Also, it might be possible that the model do not attend to these tokens during training, could you inspect the attention mask of your training protocol and make sure the token `</s>` is correctly attended?"
Pradeep1995,2024-02-13 06:25:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@ybelkada 
Before training, I initiated the tokenizer as follows
```
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"", trust_remote_code=True,use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = ""right""
```
I haven't put anything like``` skip_special_tokens=False/True``` in the tokenizer before training.

Also after training, during decoding time after inference i tried the both ways with the same tokenizer used above
```
tokenizer.decode(output_tokens,skip_special_tokens=True)
and
tokenizer.decode(output_tokens,skip_special_tokens=False)
```
But the model not generating  and so not decoding the special tokens.

Is my method correct?
"
ArthurZ,2024-02-13 06:46:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,It's more in the training did you add the eos and bos to every prompt? Also for training eos = pad seems wrong you will always ignore the eos but you need to pay attention to it when you train
Pradeep1995,2024-02-13 07:21:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"```
sentence1</s>sentence2
sentence3</s>sentence4
.....
.....
```
This is the format of my training data. I didn't explicitly mention anything like eos and bos in the training data, rather than the ```</s>``` In the middle of each data sample.
what I want is for the model should generate the special token(```</s>```) during the inference in the middle of the sentence rather than at the end. 
so how can I modify the code for that? please share the snipped if possible. 
@ybelkada  

"
ybelkada,2024-02-13 07:29:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I see, I think by default the `DataCollatorForLanguageModeling` masks out the EOS token during training, can you share your training snippet?"
Pradeep1995,2024-02-13 08:37:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"```
import torch
from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from trl import SFTTrainer
model_name = ""mistralai/Mistral-7B-Instruct-v0.2""
#For model
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map=""auto"",
)

#For tokenizer
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = ""right""

peft_model = .....
peft_config=  .....
training_arguments = ....
#Dataset format  sentence1</s>sentence2, sentence3</s>sentence4,....etc
data    = dataset

#SFTTrainer
trainer = SFTTrainer(
    model=peft_model,
    train_dataset=data,
    peft_config=peft_config,
    dataset_text_field=""prompt"",
    max_seq_length=3000,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)
trainer.train()
```
@ybelkada   please check"
ybelkada,2024-02-14 06:20:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@Pradeep1995 
Thanks ! 
Is your dataset already formatted as `sentence1</s>sentence2, sentence3</s>sentence4,....etc` ? If that's the case you need to set `packing=False`. The other solution is to set the token for separating each `sentence` differently than `</s>` as that token is already used as the EOS token.
Does also this issue helps : https://github.com/huggingface/trl/issues/1283 ?
Let me know how it goes!"
ybelkada,2024-02-02 01:59:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi @Pradeep1995 
Thanks for the issue, probably yes, have you tried it yet?"
ashbushster,2024-01-29 18:06:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Update the version from ""1.1.0"" to ""1.3.3"""
ki11b451c,2024-01-30 00:12:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hey,  does it seem to be working thus far? im tyring to figure out a way to run it without sacrificing to much w a quant version. my comp is an macbook 8gig... what would you suggest? 


"
adhiltortil,2024-01-31 14:48:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I had been using the model from Automodal using the code:

model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"", torch_dtype=torch.float16, attn_implementation=""flash_attention_2"").

I want to deploy the model on sage maker. Is this the right way to load the model with flash attention?

# Hub Model configuration. https://huggingface.co/models

hub = {
'HF_MODEL_ID':'mistralai/Mistral-7B-Instruct-v0.2',
'SM_NUM_GPUS': json.dumps(1),
'HF_TASK':'text-generation',
'attn_implementation':""flash_attention_2"",
'torch_dtype':'torch.float16'
}"
mmarianne,2024-01-29 22:48:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,same question here.
guidevops,2024-01-30 02:29:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,this version of mistral-7b is 32k tokens of context.
cognitivetech,2024-02-04 11:14:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"in fact, according to [the paper](https://arxiv.org/pdf/2310.06825.pdf) it is 8192
| Parameter | Value |
| --- | --- |
| dim | 4096 |
| n_layers | 32 |
| head_dim | 128 |
| hidden_dim | 14336 |
| n_heads | 32 |
| n_kv_heads | 8 |
| window_size | 4096 |
| context_len | 8192 |
| vocab_size | 32000 |"
hanshupe,2024-02-04 11:19:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Thats what I found confusing as well, thought its a very basic question, but didnt find a clear statement."
cognitivetech,2024-02-04 14:23:25+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"among the earliest conversations in this repo, someone read the config file and saw `""max_position_embeddings"": 32768` misinterpreting that as meaning context length. 

However, I have seen updated to 8k on some model leaderboards, and really this is important for self-extend, which is why I wanted to know.

Mistral 0.1 also was trained with the same 8192 context"
hanshupe,2024-02-04 14:59:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"okay, so 8k is the max. content length then? what is the meaning of the config file in this case?"
cognitivetech,2024-02-05 20:49:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"The terms ""training context"" and ""max_position_embeddings"" are both related to the handling of input sequences in transformer-based neural network models, particularly in the context of natural language processing (NLP). However, they refer to different aspects of sequence processing:

1. **Training Context:**
   - The ""training context"" typically refers to the length of input sequences used during the training phase of the model. It represents the number of tokens or words considered as context by the model during the training process. The training context determines the size of the input sequences that the model is exposed to during training.
   - For example, if a model has a training context of 512 tokens, it means that during training, input sequences of up to 512 tokens in length are used to train the model's parameters (weights and biases).

2. **max_position_embeddings:**
   - The ""max_position_embeddings"" parameter specifies the maximum length of input sequences that the model can process during both training and inference. It represents the maximum number of tokens or words that the model's architecture and implementation can handle.
   - This parameter determines the size of the positional embedding matrix, which encodes positional information for tokens in a sequence, and limits the length of sequences that the model can effectively process.
   - For example, if a model has a max_position_embeddings value of 1024, it means that it can handle input sequences of up to 1024 tokens in length during both training and inference.

In summary, while both the training context and max_position_embeddings parameters are related to handling input sequences in transformer-based models, the training context specifically refers to the length of input sequences used during training, while max_position_embeddings defines the maximum length of input sequences that the model can handle during both training and inference."
hanshupe,2024-02-05 20:54:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Thanks for the clarification. Then from the user perspective, the relevant one are the 32k, meaning that i can input 32k tokens into my model and  can expect that those tokens are considered for generating the output."
cognitivetech,2024-02-05 22:32:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"if that's your interpretation, you are welcome to go ahead with that. my experience is that performance is stretched at the 8k limit. 

The way I read above (GPT3.5 output)  is that its architecture supports fine-tuning up to 32k, but that it will work best out-of-box using 8k max"
hanshupe,2024-02-06 06:14:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Ok got it!
fahadh4ilyas,2024-04-06 22:18:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> if that's your interpretation, you are welcome to go ahead with that. my experience is that performance is stretched at the 8k limit. 
> 
> The way I read above (GPT3.5 output)  is that its architecture supports fine-tuning up to 32k, but that it will work best out-of-box using 8k max

Does this means I do not need to do interpolation to position embedding if I want to fine tune mistral with a longer context?"
pandora-s,2024-04-07 01:48:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Okay so there is a LOT of confusion here.

You are essentially confused between v1 and v2, the one with 8k context length with sliding window and a max of 32k is v1, v2 has a raw context size of 32k without sliding window.

The paper @cognitivetech talked about is the OG paper from the base model v1.

The v2 is based on a different base model with a raw 32k context size.

SO, to sum up, Instruct v1 has 32k with 8k sliding window, and v2 has a pure raw 32k context size.

The answer is 32k.

I hope this answered everyones concerns :> love u"
Andriy,2024-04-11 21:21:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@pandora-s  How does v2 manage to have a raw 32k context size without sliding window? Full attention has quadratic space complexity. 32k x 32k would require too much space no GPU would support. Even with FlashAttention 2, 8k seems to be the limit. How does it work?"
pandora-s,2024-04-12 08:41:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Pretty sure it uses GQA (Grouped Query Attention), but for you to ask this, I guess you missed a lot of things, I mean, mixtral has been using 32k for a while, command r doesnt even use GQA (tho command r plus does).
Sliding Windows has been slowly been forgotten and avoided to be fair. 🤔"
Andriy,2024-04-13 06:46:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@pandora-s Thanks! How does 32k context size (or 128k context size like in command r) work without GQA? Full attention for such long contexts would require too much memory.
pandora-s,2024-04-13 17:37:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> @pandora-s Thanks! How does 32k context size (or 128k context size like in command r) work without GQA? Full attention for such long contexts would require too much memory.

It does ! It takes a lot of memory, thats most likely the reason they used GQA on the PLUS version, I'm sadly not aware if they did use something else, but I do know that command r consumes a lot of memory in regards of context size."
sudarsa,2024-02-05 12:30:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Use torch_dtype=torch.bfloat16 while loading the model; it will make your output faster.
sabber,2024-03-08 06:35:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I also used SFT with custom chat dataset. I got the similar problem. The output of the model are all over the places. Sometimes fine tuned model generates the same input. Did you able to fix your problem? I guess tokenization have a big role. 
0G3NS3C,2024-03-16 04:55:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Same story here ...
pandora-s,2024-01-15 14:50:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Could u be more clear? How different are they?
aledane,2024-01-15 14:54:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Yes exactly. I want to understand if, with respect to the standard model we find in this page, for the HuggingChat they are using some set of parameters or if everything is set as default, or if there is an intermediate step (like a fine tuning). Thanks."
pandora-s,2024-01-15 15:06:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Sorry, I might not have been clear, I meant to ask what exactly differs from the answers the model in huggingchat provided compared to this one? Cause it shouldnt be a fine tuned version or anything, they should be the same I believe."
aledane,2024-01-15 16:39:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Ok, I try to explain my use case at high level.

I am trying to use this model as binary classifier. So, I am using a prompt in such a way that the LLM takes a text and classifies it in a class or the other one.
I used the model with the original setting of parameters (by using the HuggingFace pipeline) within the ""text-generation"" task. 
However, I get different answers by the HuggingChat with respect to my model deployed on AWS Sagemaker (I tried on some texts and the classification is often different).
For this reason, I would like to understand if the model deployed in the HuggingChat has some particular configration or other that differs from that one you obtain by using the ""pipeline"" mode of HuggingFace.

I hope to have been a bit more clear."
pandora-s,2024-01-15 17:12:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I see, it should be the same for as far as I know it. If you want deterministic results however (the same result every time you give it the same input) you must set the temperature to 1, and just to be sure, are you using the prompt format provided? It would be really helpful if I could have some code to see what's going on."
pandora-s,2024-01-15 17:21:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"For information, you can access the source code of certain spaces by yourself on hugging face. For example: https://huggingface.co/spaces/huggingchat/chat-ui-template has a lot of information about the parameters used in https://huggingface.co/spaces/huggingchat/chat-ui-template/tree/main/defaults ."
aledane,2024-01-15 17:24:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I cannot share a link unfortunately since I am working as consultant.
Anyway, the configuration setting for now is just the following (I noticed that I need a max_new_tokens set otherwise the answer was too short).

                                ""torch_dtype"":torch.bfloat16,
                                ""return_full_text"" : True,
                                ""device_map"" : ""auto"",
                                ""max_new_tokens"" : 64

and for the prompt format, I am using this:

                    ""model_type"" : ""hugging-face"",
                    ""prompt_format"" : ""<s>[INST] {instructions}\nInput:\n{input}\n[/INST]""



where instructions is the prompt where I ask to classify in two classes.
Then I have some Python classes implemented to call the model, but basically I am doing this:

self.hf_pipeline_llm = HuggingFacePipeline(pipeline=self.pipeline_llm) 
self.pipeline_llm  = pipeline(task=self.model_task, 
                                          model=self.model_name, 
                                          **self.model_params)"
pandora-s,2024-01-15 17:27:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I see, I just found the parameters  you asked for by the way:
MODELS=`[
    {
      ""name"": ""mistralai/Mistral-7B-Instruct-v0.1"",
      ""displayName"": ""mistralai/Mistral-7B-Instruct-v0.1"",
      ""description"": ""Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms Llama2 13B in benchmarks."",
      ""websiteUrl"": ""https://mistral.ai/news/announcing-mistral-7b/"",
      ""preprompt"": """",
      ""chatPromptTemplate"" : ""<s>{{#each messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}"",
      ""parameters"": {
        ""temperature"": 0.1,
        ""top_p"": 0.95,
        ""repetition_penalty"": 1.2,
        ""top_k"": 50,
        ""truncate"": 3072,
        ""max_new_tokens"": 1024,
        ""stop"": [""</s>""]
      },
      ""promptExamples"": [
        {
          ""title"": ""Write an email from bullet list"",
          ""prompt"": ""As a restaurant owner, write a professional email to the supplier to get these products every week: \n\n- Wine (x10)\n- Eggs (x24)\n- Bread (x12)""
        }, {
          ""title"": ""Code a snake game"",
          ""prompt"": ""Code a basic snake game in python, give explanations for each step.""
        }, {
          ""title"": ""Assist in a task"",
          ""prompt"": ""How do I make a delicious lemon cheesecake?""
        }
      ]
    }
]`

You can find it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env

Maybe this will help you out !"
pandora-s,2024-01-15 17:36:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Also, I noticed your prompt format is wrong.
```
The example provided in the READ_ME is:
text = ""<s>[INST] What is your favourite condiment? [/INST]""
""Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> ""
""[INST] Do you have mayonnaise recipes? [/INST]""


As it suggests, the <s></s> section is only required for the past discussions, the instruction you are making now does not require, to be simple, you can do this if you do not care about the past exchanges with the bot:
""[INST] {instructions}\nInput:\n{input}\n[/INST]""

But I recommend you do this instead for example:
""<s>[INST] I will give you the description of people, and I want you to respond with a json. For example: ""A 18 years old woman""[/INST]{""age"":""woman"",""genre"":""woman""}</s>[INST]{new sentence} [/INST] ""
```
"
aledane,2024-01-15 17:45:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Thank you very much for both comments!
Few questions:
> I see, I just found the parameters  you asked for by the way:
> MODELS=`[
>     {
>       ""name"": ""mistralai/Mistral-7B-Instruct-v0.1"",
>       ""displayName"": ""mistralai/Mistral-7B-Instruct-v0.1"",
>       ""description"": ""Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms Llama2 13B in benchmarks."",
>       ""websiteUrl"": ""https://mistral.ai/news/announcing-mistral-7b/"",
>       ""preprompt"": """",
>       ""chatPromptTemplate"" : ""<s>{{#each messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}"",
>       ""parameters"": {
>         ""temperature"": 0.1,
>         ""top_p"": 0.95,
>         ""repetition_penalty"": 1.2,
>         ""top_k"": 50,
>         ""truncate"": 3072,
>         ""max_new_tokens"": 1024,
>         ""stop"": [""</s>""]
>       },
>       ""promptExamples"": [
>         {
>           ""title"": ""Write an email from bullet list"",
>           ""prompt"": ""As a restaurant owner, write a professional email to the supplier to get these products every week: \n\n- Wine (x10)\n- Eggs (x24)\n- Bread (x12)""
>         }, {
>           ""title"": ""Code a snake game"",
>           ""prompt"": ""Code a basic snake game in python, give explanations for each step.""
>         }, {
>           ""title"": ""Assist in a task"",
>           ""prompt"": ""How do I make a delicious lemon cheesecake?""
>         }
>       ]
>     }
> ]`
> 
> You can find it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env
> 
> Maybe this will help you out !


By chance do you have also that one for the v02? Because this is the v01 but I do not manage to find the right repo"
aledane,2024-01-15 17:47:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Also, I noticed your prompt format is wrong.
> ```
> The example provided in the READ_ME is:
> text = ""<s>[INST] What is your favourite condiment? [/INST]""
> ""Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> ""
> ""[INST] Do you have mayonnaise recipes? [/INST]""
> 
> 
> As it suggests, the <s></s> section is only required for the past discussions, the instruction you are making now does not require, to be simple, you can do this if you do not care about the past exchanges with the bot:
> ""[INST] {instructions}\nInput:\n{input}\n[/INST]""
> 
> But I recommend you do this instead for example:
> ""<s>[INST] I will give you the description of people, and I want you to respond with a json. For example: ""A 18 years old woman""[/INST]{""age"":""woman"",""genre"":""woman""}</s>[INST]{new sentence} [/INST] ""
> ```

Concerning this, in your last prompt, do you suggest to use the example as a few shot and put in the ""new sentence"" part the text I want to classify?"
pandora-s,2024-01-15 18:10:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Exactly, it gives it more context of what it should do. So you put the instruction, then an example and the bot's response yourself, so it knows what to do next time. Give it a shot."
pandora-s,2024-01-15 18:19:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Thank you very much for both comments!
> Few questions:
> > I see, I just found the parameters  you asked for by the way:
> > MODELS=`[
> >     {
> >       ""name"": ""mistralai/Mistral-7B-Instruct-v0.1"",
> >       ""displayName"": ""mistralai/Mistral-7B-Instruct-v0.1"",
> >       ""description"": ""Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms Llama2 13B in benchmarks."",
> >       ""websiteUrl"": ""https://mistral.ai/news/announcing-mistral-7b/"",
> >       ""preprompt"": """",
> >       ""chatPromptTemplate"" : ""<s>{{#each messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}"",
> >       ""parameters"": {
> >         ""temperature"": 0.1,
> >         ""top_p"": 0.95,
> >         ""repetition_penalty"": 1.2,
> >         ""top_k"": 50,
> >         ""truncate"": 3072,
> >         ""max_new_tokens"": 1024,
> >         ""stop"": [""</s>""]
> >       },
> >       ""promptExamples"": [
> >         {
> >           ""title"": ""Write an email from bullet list"",
> >           ""prompt"": ""As a restaurant owner, write a professional email to the supplier to get these products every week: \n\n- Wine (x10)\n- Eggs (x24)\n- Bread (x12)""
> >         }, {
> >           ""title"": ""Code a snake game"",
> >           ""prompt"": ""Code a basic snake game in python, give explanations for each step.""
> >         }, {
> >           ""title"": ""Assist in a task"",
> >           ""prompt"": ""How do I make a delicious lemon cheesecake?""
> >         }
> >       ]
> >     }
> > ]`
> > 
> > You can find it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env
> > 
> > Maybe this will help you out !
> 
> 
> By chance do you have also that one for the v02? Because this is the v01 but I do not manage to find the right repo

Here you go : https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env.template 

There is the parameters for all of them."
aledane,2024-01-16 08:18:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Thanks a lot for everything!
julien-c,2024-01-17 14:36:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,maybe cc @Wauplin 
Wauplin,2024-01-17 14:54:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@shivrajhug everything looks good from a HF-authentication point of view. Can it be that the docker settings don't allow the docker container to access your network? (and therefore doesn't have access to the internet?) Only a guess here, hope that can help."
LAOUINATE,2024-04-20 20:58:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"i have the same problem , @shivrajhug  have you get the solution of this problem or not yet ?"
iuliandiminenco,2024-04-23 13:28:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Guys i have the same error, how to solve it?"
horhan,2024-04-30 14:08:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"same problem, how to solve it?"
grgera,2024-05-07 01:22:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Me have the same
raj-chinagundi,2024-05-10 19:59:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,I dont know dude i cant figure out this error please help
cleyzin,2024-06-05 06:27:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"maybe you can try creating a new access token in write type, in this https://huggingface.co/settings/tokens

"
owaiskha9654,2024-01-29 23:55:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I tried several combinations, directly agent doesn't works properly. It fails to do simple task. Maybe Use Langchain CSV document method to load your csv, and then you can pass in that document with custom system prompts @DonYar "
ryanmurf,2024-01-05 01:09:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"NVIDIA 3090 has paired 24 GB GDDR6X, it should work."
hvrigazov,2024-01-12 07:16:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"This models seems to use 32GB on the CPU of my machine, at least with the example provided. That is because it is not using `torch.float16`. Add this in the start of your script:

```
import torch
torch.set_default_dtype(torch.float16)
```"
ChvckN0rri5,2024-01-02 23:12:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"# Hope this is helpful

## How we do it
We use [vLLM to provide an openai compatible chat endpoint](https://docs.mistral.ai/self-deployment/vllm/) that can use system messages. Together with [langchain](https://python.langchain.com/docs/integrations/llms/vllm#openai-compatible-server) we are able to accomplish this. 

## Alternate solutions
For TS Junkies [here is a solution](https://js.langchain.com/docs/integrations/chat/mistral).
If you don't have the hardware to run the model, here is a [Mistral API solution using langchain in Python](https://python.langchain.com/docs/integrations/chat/mistralai) (Mistral API is in [beta](https://console.mistral.ai))

Here is a post with a [python solution](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/41#65284b827b99d91baa1037bb) that may be more relevant to your error message. This one uses the user, assistant, user framework that the error mentions. 


## Example
```python
inference_server_url = ""http://localhost:8000/v1""

chat = ChatOpenAI(
    model=""mistralai/Mistral-7B-Instruct-v0.2"",
    openai_api_key=""EMPTY"",
    openai_api_base=inference_server_url,
    max_tokens=250,
    temperature=0,
)

messages = [
    SystemMessage(
        content=""You are a helpful assistant that translates English to Italian.""
    ),
    HumanMessage(
        content=""Translate the following sentence from English to Italian: I love programming.""
    ),
]
chat(messages)
```
```python
AIMessage(content=' Io amo programmare', additional_kwargs={}, example=False)
```

If this doesn't help, then it's at least a handy reference. "
stinkyElias,2024-01-25 09:41:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hello

I don't know if you still need help, but if you do you can suppress the warnings by setting pad_token_id to eos_token_id in the generate method from the example code.
For example, you can update the example code with

generated_ids = model.generate(model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1000, do_sample=True).

Hope this helps:)"
DeltaLux,2024-01-01 21:43:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Maybe try this:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
model_id = ""../Mistral-7B-Instruct-v0.2""
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""cpu"")
tokenizer = AutoTokenizer.from_pretrained(model_id)

text = ""<s>[INST] What is your favourite condiment? [/INST]""
inputs = tokenizer(text, return_tensors=""pt"")

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
I got the same error and what fixed it was specifying the property `device_map=""cpu""` when calling `from_pretrained`. I also get the same error when I run out of RAM while loading the model as on my machine it consumes about 29 GB of memory. If this code doesn't fix it, check the RAM consumption while it tries to load the model to see if it is that."
roboboot,2024-01-02 08:10:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Yes, true, it's a memory problem. I have 16 GB of RAM and it's not enough on a Windows machine.

So the model will be entirely loaded in memory....

thx"
Berketarak,2024-02-07 07:59:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi, 

After finetuning the model, my observation is that the model can be used either way. You may simply add an [/INST] token after your prompt. If you skip this token on your prompt, the finetuned model itself adds the token and start generating. You just need the exclude this token on your application.

Hope this helps."
nottakumasato,2023-12-20 20:26:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"32K for input as seen in the config.json:
  ""max_position_embeddings"": 32768,"
sniffski,2023-12-21 20:36:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Thanks, and what about the output?"
guidevops,2024-01-04 19:07:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> Thanks, and what about the output?

input + output = 32768"
cognitivetech,2024-02-04 11:15:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"in fact, according to [the paper](https://arxiv.org/pdf/2310.06825.pdf) context is 8192
| Parameter | Value |
| --- | --- |
| dim | 4096 |
| n_layers | 32 |
| head_dim | 128 |
| hidden_dim | 14336 |
| n_heads | 32 |
| n_kv_heads | 8 |
| window_size | 4096 |
| context_len | 8192 |
| vocab_size | 32000 |"
pierric,2023-12-18 16:46:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Hi @Nondzu, HF team member here - sorry to hear you've been experiencing ""occasional download speed issues"" with the Hugging Face Hub.

Could you elaborate on those issues? We're constantly trying to improve our delivery system, and always eager to hear more about pain points from our community.

Thanks!"
deleted,2023-12-16 13:41:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"does for me. EDIT: GGUF version does.  Not tried it native.   - EDIT 2:   A bit slower than GGUF on same hardware, but ya, it worked. "
chenhunghan,2023-12-17 06:42:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I wrote a simple wrapper [CLI](https://github.com/chenhunghan/oxpilot) if anyone wants to try the GGUF version on CPU :)
"
ArthurZ,2023-12-18 19:13:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,You can work native as well it's just a model supported by `transformers`
guwenyi,2023-12-14 06:40:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I got a same error:

User: hi
Assistant: Exception in thread Thread-3 (generate):
Traceback (most recent call last):
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner
    self.run()
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/threading.py"", line 953, in run
    self._target(*self._args, **self._kwargs)
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1652, in generate
    return self.sample(
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2734, in sample
    outputs = self(
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py"", line 1045, in forward
    outputs = self.model(
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py"", line 888, in forward
    attention_mask = self._prepare_decoder_attention_mask(
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py"", line 796, in _prepare_decoder_attention_mask
    combined_attention_mask = _make_sliding_window_causal_mask(
  File ""/root/miniconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py"", line 88, in _make_sliding_window_causal_mask
    mask = torch.triu(mask, diagonal=-sliding_window)
TypeError: bad operand type for unary -: 'NoneType'"
ybelkada,2023-12-14 12:09:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"hi everyone,
can you update the transformers package? `pip install -U transformers`"
epignatelli,2023-12-14 17:06:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Upgrading to a newer version of `transformers` worked for me.
MagiaSN,2024-01-03 12:31:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"An alternative is to change `sliding_window` in `config.json` from `null` to something like `4096`, if you can not update your `transformers` just like me."
RylanSchaeffer,2024-03-02 22:52:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Updating transformers worked for me!
dataviral,2023-12-13 12:24:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Fixed the lazy copy pasta, from mistral instruct v0.1.
Changed the model version used in the readme when initialized from from_pretrained "
TimeRobber,2023-12-14 22:35:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Thanks
DreamGenX,2024-01-26 10:04:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"The hugging face docs for Mistral say that if set to None, it will default to 4096: https://huggingface.co/docs/transformers/main/en/model_doc/mistral
I am not sure what is the intention here"
DreamGenX,2024-01-26 10:05:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Digging deeper, setting it to none will indeed disable it: https://github.com/huggingface/transformers/blob/8eb74c1c8961e3dc8549bb1a76463c7658a63d43/src/transformers/models/mistral/modeling_mistral.py#L380"
KurtGD1915,2024-01-26 10:16:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Hi @DreamGenX. Can you tell me which part of the doc specifies that? 
DreamGenX,2024-01-26 10:18:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@KurtGD1915 It says under sliding window that it defaults to 4096, but looking at the code, it is just default param value (so if you set it to None explicitly, it will stay none). I have fixed the link above that actually does the inference, where you can see that sliding window is disabled if it's None or larger than content length."
ArthurZ,2023-12-13 08:52:32+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Make sure you check which token is the one that is generate, and which token you are using as a stop token. the `\n` token that is given by  `tokenizer(stop_word, return_tensors='pt')['input_ids']` is not the same as ` tokenizer.convert_tokens_to_ids(stop_word)` because the tokenizer always adds a prefix space. "
asmittal,2023-12-13 10:27:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@ArthurZ But this same code works with Falcon7b Instruct model
asmittal,2023-12-13 11:03:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,tokenizer.convert_tokens_to_ids(stop_word) this just gives zero as output for every token
sauravjoshi23,2024-03-03 22:20:55+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"```
stop_list = ["" \n\nAnswer:"", "" \n"", "" \n\n""]
stop_token_ids = [tokenizer(x,  return_tensors='pt', add_special_tokens=False)['input_ids'] for x in stop_list]
stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]
    
class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        for stop_ids in stop_token_ids:
            if torch.eq(input_ids[0][-len(stop_ids[0])+1:], stop_ids[0][1:]).all():
                return True
        return False

stopping_criteria = StoppingCriteriaList([StopOnTokens()])
```"
ArthurZ,2023-12-13 09:05:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Just pass the full inputs from the tokenizer. `inputs = tokenizer([""Hey how are you?""], ['Good'], padding = True)` will return the attention mask. But if there is no padding token you should specify `tokenizer.pad_token = tokenizer.eos_token`"
jgsmcmahon,2023-12-14 13:36:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Ok thanks will try this :-)  
jabarne6,2023-12-17 18:23:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Add this after loading in the tokenizer from pretrained:

```
if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
```

And then change the generate line to this:
```
generated_ids = model.generate(model_inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=2000, do_sample=True)
```
"
ArthurZ,2023-12-13 09:06:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"Are you sure you are using the correct version in your environnment? 
Could you print `import transformers; print(transformers.__version__)` and share a reproducer ? "
ArthurZ,2023-12-12 10:46:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,It's english only (from the blog post) 
roboboot,2024-01-02 10:39:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,So it's not possible to have answers in other languages....
316usman,2024-01-02 10:44:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Yes not possible
poarpeak,2024-02-22 02:15:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"I have used simple Chinese supervision to fine tune the data, and it can output Chinese normally. However, if the vocabulary is not friendly to Chinese, there may be a mixture of simplified and traditional characters, and my understanding of Chinese is also average"
vince62s,2023-12-12 15:31:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"set sliding_window to 0 or back to 4096 in the config until they fix transformers code, it won't hurt the finetuning"
ybelkada,2023-12-12 16:18:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Hi @asmittal can you try with the latest version of transformers? I think this has been fixed with the attention mask refactor `pip install -U transformers`
Nondzu,2023-12-12 17:32:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@asmittal how you do fine tuning of this model ? do you have some guide ?  I tried axolotl but no success 
asmittal,2023-12-13 04:32:55+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@Nondzu I don't really have a guide, but I'm fine tuning this model like any other LLM, I think prompt is the only thing that makes any difference."
asmittal,2023-12-13 04:34:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@ybelkada Thanks alot! It's working with the latest version of transformers.
vince62s,2023-12-11 19:59:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,it seems they changed rope theta to 1e6 for all their models.
Yuuru,2023-12-11 20:18:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,32k context
mrfakename,2023-12-11 21:42:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,@Yuuru What is the source of this information?
mpasila,2023-12-11 23:38:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"> it seems they changed rope theta to 1e6 for all their models.

They also set ""sliding_window"" to null for some reason.

> @Yuuru What is the source of this information?

The config.json file. (it's the same context size as the previous version)"
nudelbrot,2023-12-12 00:14:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"@mrfakename , vllm says it btw when loading the model 

```
 […] max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0
```       "
YorelNation,2023-12-12 10:21:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,Yeah it would be interesting to understand how it's actually different from the first one.
Tokie,2024-01-10 11:54:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"It is a lot less obedient, for one: [v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) refuses to answer a sixth of my test prompts, while [v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) refuses to answer three‑quarters of them."
SFconvertbot,2023-12-11 19:41:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.2,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
michelk,2024-04-05 20:08:59+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Would be good to merge, because system prompt are not supported at the moment!
"
vibranium,2024-02-06 16:46:25+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"```
/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:602: UserWarning: You are using a Backend <class 'text_generation_server.utils.dist.FakeGroup'> as a ProcessGroup. This usage is deprecated since PyTorch 2.0. Please use a public API of PyTorch Distributed instead.
  warnings.warn(
Traceback (most recent call last):
  File ""/opt/conda/bin/text-generation-server"", line 8, in <module>
    sys.exit(app())
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/cli.py"", line 89, in serve
    server.serve(
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py"", line 228, in serve
    asyncio.run(
  File ""/opt/conda/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/opt/conda/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/server.py"", line 174, in serve_inner
    model = get_model(
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/__init__.py"", line 310, in get_model
    return FlashMixtral(
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mixtral.py"", line 21, in __init__
    super(FlashMixtral, self).__init__(
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/flash_mistral.py"", line 333, in __init__
    model = model_cls(config, weights)
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"", line 820, in __init__
    self.model = MixtralModel(config, weights)
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"", line 757, in __init__
    [
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"", line 758, in <listcomp>
    MixtralLayer(
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"", line 692, in __init__
    self.self_attn = MixtralAttention(
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"", line 232, in __init__
    self.query_key_value = load_attention(config, prefix, weights)
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"", line 125, in load_attention
    return _load_gqa(config, prefix, weights)
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"", line 158, in _load_gqa
    get_linear(weight, bias=None, quantize=config.quantize)
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/layers.py"", line 330, in get_linear
    linear = ExllamaQuantLinear(
  File ""/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/gptq/exllamav2.py"", line 145, in __init__
    assert qzeros.shape == (
AssertionError
 #033[2m#033[3mrank#033[0m#033[2m=#033[0m0#033[0m
#033[2m2024-02-06T16:43:07.427575Z#033[0m #033[31mERROR#033[0m #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Shard 0 failed to start
#033[2m2024-02-06T16:43:07.427609Z#033[0m #033[32m INFO#033[0m #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Shutting down shards
Error: ShardCannotStart
```"
vibranium,2024-02-06 16:48:27+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,@TheBloke Could you please check what did I miss?
sdkramer10,2024-02-29 18:37:03+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,@vibranium were you able to figure this out? I am also having issues deploying to a sagemaker endpoint.
SpaceCowboy850,2024-01-25 15:55:53+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Posting this here as a link for anyone that finds it.  I still haven't solved my problem, but the primary discussion seems to be in this thread
https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/discussions/5

"
MinaiRoderon,2024-04-16 13:42:25+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,@Birdman13 the official website: https://mistral.ai/news/mixtral-of-experts/#:~:text=It%20gracefully%20handles%20a%20context%20of%2032k%20tokens.
Kukedlc,2024-02-09 06:32:59+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,This comment has been hidden
iemaig,2024-02-20 23:48:28+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"@mullerse any insights? I observe same phenomenon (Oobabooga fast, pythons scripting excruciatingly slow) with similar specs albeit 2x 48gb A6000's.
I'm even running script from same conda environment Oobabooga uses.
I observe the model loaded onto the gpu. Gpu is not off."
Yhyu13,2023-12-19 13:46:11+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"the GGUF 4bit is actually the same algorithm as GPTQ, if I got that correctly. But llamacpp would not support loading the GPTQ format since it already has GGUF.

I don't believe any GPTQ loader would optimize for mac, so GGUF is your best bet.

There will be optimization done by llamacpp for Mixtral laster for sure, just be patient"
nazrak-atlassian,2023-12-18 00:46:24+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Just noticed in the model card (sorry missed it the first time, it was pretty far down):
```
Serving this model from Text Generation Inference (TGI)
Not currently supported for Mixtral models
```

Can you provide any clarity on what the blocker is here? I know TGI supports Mixtral at a base level and have deployed a non-quantized version."
AbRds,2023-12-18 16:14:34+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"I'm facing the same issue, any suggestion? 

Thanks in advance"
nazrak-atlassian,2023-12-19 02:12:19+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,@AbRds I've swapped to EETQ for quantization for now. I believe it's supported from TGI 1.3.2 onward.
AbRds,2023-12-19 11:08:34+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"> @AbRds I've swapped to EETQ for quantization for now. I believe it's supported from TGI 1.3.2 onward.

Hi @nazrak-atlassian I'm not familiar with EETQ, how it works? I just have to pass the parameter eetq instead of gptq? 
I've tried passing the eetq argument but I have receive another error:

```RuntimeError: weight model.layers.0.self_attn.q_proj.weight does not exist```
"
nazrak-atlassian,2023-12-19 15:15:18+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"@AbRds Yes you should be able to change the --quantize arg. EETQ an improved in-place 8bit quant technique (supposedly better performing than bitsandbytes'), but I couldn't speak to it more than that. It worked for me with TGI 1.3.3 with the following docker run:

```bash
docker run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data:/data ghcr.io/huggingface/text-generation-inference:latest --model-id mistralai/Mixtral-8x7B-v0.1 --sharded true --num-shard 4 --quantize eetq
```

My guess is that you are trying to use TheBloke's GPTQ model with eetq. It should be run on the original model, and will perform the quantization on the fly. Note that it's only an 8bit quantization, so if you require 4bit for your VRAM requirements it will not work unfortunately."
AbRds,2023-12-19 15:54:35+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"> @AbRds Yes you should be able to change the --quantize arg. EETQ an improved in-place 8bit quant technique (supposedly better performing than bitsandbytes'), but I couldn't speak to it more than that. It worked for me with TGI 1.3.3 with the following docker run:
> 
> ```bash
> docker run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data:/data ghcr.io/huggingface/text-generation-inference:latest --model-id mistralai/Mixtral-8x7B-v0.1 --sharded true --num-shard 4 --quantize eetq
> ```
> 
> My guess is that you are trying to use TheBloke's GPTQ model with eetq. It should be run on the original model, and will perform the quantization on the fly. Note that it's only an 8bit quantization, so if you require 4bit for your VRAM requirements it will not work unfortunately.

@nazrak-atlassian you're right, I was trying to run Thebloke's version instead of the original one, now works perfectly.

Thanks a lot. 

"
p-christ,2024-01-28 14:34:39+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,Any update on when GPTQ might work?
Brandon1126,2024-03-13 03:15:27+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,It seems that all of TheBloke's GPTQ models are still broken with TGI serving
rjmehta,2023-12-15 18:32:09+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,Okay. Turbo is working on mixtral architecture. Wrong thread to question. Thanks anyways.
rjmehta,2023-12-15 18:32:28+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,https://github.com/turboderp/exllamav2/issues/223
Yhyu13,2023-12-16 09:34:03+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,@rjmehta You need to build exllamav2 if you will with cuda12.1 locally on this branch https://github.com/turboderp/exllamav2/tree/experimental
Hypersniper,2023-12-16 04:26:34+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,same here
1NoobArtist,2024-01-03 17:57:09+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Getting this error:

23:25:45-094355 INFO     Loading TheBloke_Mixtral-8x7B-Instruct-v0.1-GPTQ
23:25:45-609266 ERROR    Failed to load the model.
Traceback (most recent call last):
  File ""E:\text-generation-webui-main\text-generation-webui-main\modules\ui_model_menu.py"", line 214, in load_model_wrapper
    shared.model, shared.tokenizer = load_model(selected_model, loader)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\text-generation-webui-main\text-generation-webui-main\modules\models.py"", line 90, in load_model
    output = load_func_map[loader](model_name)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\text-generation-webui-main\text-generation-webui-main\modules\models.py"", line 399, in ExLlama_HF_loader
    return ExllamaHF.from_pretrained(model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\text-generation-webui-main\text-generation-webui-main\modules\exllama_hf.py"", line 174, in from_pretrained
    return ExllamaHF(config)
           ^^^^^^^^^^^^^^^^^
  File ""E:\text-generation-webui-main\text-generation-webui-main\modules\exllama_hf.py"", line 31, in __init__
    self.ex_model = ExLlama(self.ex_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\text-generation-webui-main\text-generation-webui-main\installer_files\env\Lib\site-packages\exllama\model.py"", line 753, in __init__
    decoder_size += math.prod(shape) * _layer_dtype_size(key)
                                       ^^^^^^^^^^^^^^^^^^^^^^
  File ""E:\text-generation-webui-main\text-generation-webui-main\installer_files\env\Lib\site-packages\exllama\model.py"", line 716, in _layer_dtype_size
    raise ValueError(""Unrecognized layer: "" + key)
ValueError: Unrecognized layer: model.layers.0.block_sparse_moe.experts.0.w1.bias"
FelipeAr4n,2024-01-20 21:14:16+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"ERROR    Failed to load the model.
Traceback (most recent call last):
  File ""D:\text-generation-webui-main\modules\ui_model_menu.py"", line 213, in load_model_wrapper
    shared.model, shared.tokenizer = load_model(selected_model, loader)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\text-generation-webui-main\modules\models.py"", line 87, in load_model
    output = load_func_map[loader](model_name)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\text-generation-webui-main\modules\models.py"", line 250, in llamacpp_loader
    model, tokenizer = LlamaCppModel.from_pretrained(model_file)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""D:\text-generation-webui-main\modules\llamacpp_model.py"", line 63, in from_pretrained
    Llama = llama_cpp_lib().Llama
            ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'Llama'"
dimaischenko,2023-12-12 16:38:39+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Tried the same but with `CUDA 12.1` , `torch==2.1.1+cu121` and built `auto-gptq==0.6.0.dev0+cu121` from source. The same error."
TheBloke,2023-12-12 23:40:02+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Unfortunately there was an issue with the branch I linked; I didn't realise that the author had made another commit to it which broke inference again.  I've now updated the README to reference a different branch.

The newly linked PR will now work: https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix"
tsalvoch,2023-12-13 15:53:50+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Build AutoGPT OK with CUDA 12.1, transformers 4.36.0 and torch==2.1.1+cu121 = auto-gptq==0.6.0.dev0+cu121
But model loading failed in text-generation-webui:


Traceback (most recent call last):
  File ""/home/me/text-generation-webui/modules/ui_model_menu.py"", line 208, in load_model_wrapper
    shared.model, shared.tokenizer = load_model(selected_model, loader)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/me/text-generation-webui/modules/models.py"", line 89, in load_model
    output = load_func_map[loader](model_name)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/me/text-generation-webui/modules/models.py"", line 380, in AutoGPTQ_loader
    return modules.AutoGPTQ_loader.load_quantized(model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/me/text-generation-webui/modules/AutoGPTQ_loader.py"", line 58, in load_quantized
    model = AutoGPTQForCausalLM.from_quantized(path_to_model, **params)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/auto.py"", line 102, in from_quantized
    model_type = check_and_get_model_type(model_name_or_path, trust_remote_code)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/_utils.py"", line 232, in check_and_get_model_type
    raise TypeError(f""{config.model_type} isn't supported yet."")
TypeError: mixtral isn't supported yet.


I probably missed something to have that: 
mixtral isn't supported yet

But what?"
dimaischenko,2023-12-13 16:07:02+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"@tsalvoch most likely you did not build `auto-gptq` from the `Mixtral-fix` git branch. I had the same error when I built it from the `master` branch

https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix

`git checkout Mixtral-fix`"
dimaischenko,2023-12-13 16:10:59+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"> Unfortunately there was an issue with the branch I linked; I didn't realise that the author had made another commit to it which broke inference again.  I've now updated the README to reference a different branch.
> 
> The newly linked PR will now work: https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix

@TheBloke Thank you!"
bdambrosio,2023-12-13 17:16:11+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"@dimaischenko - How did you get this to run on a 3090? with Mixtral-fix it does try to load, but runs out of memory on my 4090.
I do have 2x4090, guess I'll look through the code base to see if/how to specify multiple gpu.

"
dimaischenko,2023-12-13 17:24:50+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"@bdambrosio I am ok with 3090. Even for `revision=""main""`, but you can try `revision=""gptq-3bit-128g-actorder_True""` it takes about 19 Gb (example in my first thread post)"
bdambrosio,2023-12-13 17:27:37+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Ah, yup, just realized my error. I had loaded a larger version assuming I would use both gpus. Downloading smaller version now, while also trying to figure out syntax of AutoGPTQ .from_pretrained device parameter.

tnx!"
bdambrosio,2023-12-13 17:34:23+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,"Ah - In case anyone else stumbles here - @TheBloke - any ideas?
                                  
gptq-4bit-128g-actorder_True	4:

model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,
                                           model_basename=""model"",
                                           use_safetensors=True,
                                           per_gpu_max_memory={0:""20GIB"",1:""20GIB""},
                                           )

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=False)

prompt = ""Tell me about AI""
prompt_template=fquotequotequote<s>[INST] {prompt} [/INST]
print(""\n\n*** Generate:"")

input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()
output = model.generate(inputs=input_ids, temperature=0.1, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)
print(tokenizer.decode(output[0]))


(mistral) bruce@bruce-AI:~/Downloads/alphawave/tests/Sam$ python mixtral-8x-GPTQ.py 
MixtralGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.         
MixtralGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.


*** Generate:
Traceback (most recent call last):
  File ""/home/bruce/Downloads/alphawave/tests/Sam/mixtral-8x-GPTQ.py"", line 31, in <module>
    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)
  File ""/home/bruce/miniconda3/envs/mistral/lib/python3.10/site-packages/auto_gptq/modeling/_base.py"", line 447, in generate
    return self.model.generate(**kwargs)
  File ""/home/bruce/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1764, in generate
    return self.sample(
  File ""/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2897, in sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
"
robert1968,2023-12-16 18:00:13+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,This comment has been hidden
robert1968,2023-12-16 18:06:42+00:00,comment,TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,This comment has been hidden
slippylolo,2023-05-26 10:33:46+00:00,comment,tiiuae/falcon-7b-instruct,"👋 Hello Supreeth, 

Our release has been on the low-key side for now, and we are preparing for a more public release soon. 

We have added to the model card:
* An inference code sample;
* Details about the instruct/chat dataset used. 

I hope this is helpful for you!

We have not used any very specific prompt format. For chat, you should be able to get something nice with simply:
```
User: Can you tell me what is brutalism?
Assistant:
```

(the most important part here being not having a stray space at the start of the assistant message, as this will mess with tokenisation!)"
MaziyarPanahi,2023-05-26 16:53:37+00:00,comment,tiiuae/falcon-7b-instruct,"Hi @slippylolo 

Is there any end of the text tokens like `</s>` or `<|endoftext|>`? I was wondering if there is another token formatting that can be used:

```
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
```"
MaziyarPanahi,2023-06-01 08:02:22+00:00,comment,tiiuae/falcon-7b-instruct,"I am going to share any prompt that worked for me here, starting with this classic template:


```
""""""Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say ""I don't know""

Context:
The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.
33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places 
to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).
Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following
a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance
where the athletes of different nations had agreed to share the same medal in the history of Olympics. 
Barshim in particular was heard to ask a competition official ""Can we have two golds?"" in response to being offered a 
'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and 
Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump
for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg
of Sweden (1984 to 1992).

Who won the 2020 Summer Olympics men's high jump?""""""
```
Which it answered `Gianmarco Tamberi (Italy) and Mutaz Essa Barshim (Qatar)`

To be sure it does respect the instruction at the beginning:

```
""""""Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say ""I don't know my lord!""

Context:
The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.
33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places 
to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).
Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following
a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance
where the athletes of different nations had agreed to share the same medal in the history of Olympics. 
Barshim in particular was heard to ask a competition official ""Can we have two golds?"" in response to being offered a 
'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and 
Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump
for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg
of Sweden (1984 to 1992).

Who is the president of France?""""""
```
I wanted to be sure this `I don't know` part comes from the instruction, so I added `my lord!`. Which it answered: `I don't know my lord!`

## LangChain
If you want to use the same prompt template in LangChain:

```python
template = """"""Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say ""I don't know""

Context:
{context}

{query}"""""".strip()

from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate(
    input_variables=[""query"", ""context""],
    template=template
)

prompt_template
```

Which can be used:

```python
context = """"""
The men's high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium.
33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places 
to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021).
Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following
a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance
where the athletes of different nations had agreed to share the same medal in the history of Olympics. 
Barshim in particular was heard to ask a competition official ""Can we have two golds?"" in response to being offered a 
'jump off'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men's high jump for Italy and 
Belarus, the first gold in the men's high jump for Italy and Qatar, and the third consecutive medal in the men's high jump
for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg
of Sweden (1984 to 1992).""""""

query = ""Who won the 2020 Summer Olympics men's high jump?""

prompt = prompt_template.format(query=query, context=context)
print(prompt)
```"
zhangmaosen,2023-06-13 06:44:37+00:00,comment,tiiuae/falcon-7b-instruct,"> 👋 Hello Supreeth, 
> 
> Our release has been on the low-key side for now, and we are preparing for a more public release soon. 
> 
> We have added to the model card:
> * An inference code sample;
> * Details about the instruct/chat dataset used. 
> 
> I hope this is helpful for you!
> 
> We have not used any very specific prompt format. For chat, you should be able to get something nice with simply:
> ```
> User: Can you tell me what is brutalism?
> Assistant:
> ```
> 
> (the most important part here being not having a stray space at the start of the assistant message, as this will mess with tokenisation!)

Hi, I have problem when use prompt like you give, falcon model will generate answer like bellow:
```
User: Can you tell me what is brutalism?
mini: Brutalism is a style of architecture characterized by a lack of ornamentation and a focus on structure and materials. It was popular in the mid-20th century, particularly in Europe and North America.
User What are some examples of brutalist architecture?
Mini Some examples of brutalist architecture include the Ministry of Education building in Paris, the British Museum in London, and the Supreme Court
```
I dont want model to play two roles and talk itself, how to solve it? thank u very much"
yi1,2023-06-13 07:48:38+00:00,comment,tiiuae/falcon-7b-instruct,"Check out the special tokens near the top of the tokenizer.json file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types you can play with. I've had the best results prompting it like this:

>>QUESTION<<In Python, I want to write a simple HTTP API that receives an object via POST and responds with another object. The request object contains a string prompt, float temperature, and int max_tokens. The response object contains a string response, int prompt_tokens, int completion_tokens. For now just set hard-coded values and get the response out. I'll add the logic myself afterward.
>>ANSWER<<"
aditunoe,2023-06-13 13:26:39+00:00,comment,tiiuae/falcon-7b-instruct,"I also used the prompt as described by @yi1 inside of LangChain ""stuff"" chain to make a RetrievalQA chain. I can report it seems to work, the answers sound quite accured. But i encounter always a cutoff of the answer. Its never longer than one sentece, mostly cut off in the middle of the sentence. Maybe its related to that i use the free Inference API atm. Does someone has an idea?

PS: I have my code in several files and classes, there may be a bug in here because I just copied everything together.

```python
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain.llms import HuggingFacePipeline

# Load document
loader = TextLoader(""../data/state_of_the_union.txt"")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# OpenAI embedding model just as an example
embeddings = OpenAIEmbeddings()
docsearch = Chroma.from_documents(texts, embeddings)

# prepare embedding model
retriever = Chroma(persist_directory=""./data"", embedding_function=embeddings)

# prepare Falcon Huggingface API
llm = HuggingFaceEndpoint(
            endpoint_url= ""https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct"" ,
            huggingfacehub_api_token=HUGGINGFACE_API_KEY,
            task=""text-generation"",
            model_kwargs = {
                ""min_length"": 200,
                ""max_length"":2000,
                ""temperature"":0.5,
                ""num_return_sequences"":1
            }
      )

# prepare stuff prompt template
template = """"""You are a helpful AI assistant and provide the answer for the question based on the given context.
Context:{context}
>>QUESTION<<{question}
>>ANSWER<<"""""".strip()

prompt = PromptTemplate(
    input_variables=[""context"", ""question""],
    template=template
)

chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type=chain_type,
        retriever=docsearch.as_retriever(),
        return_source_documents=True,
        chain_type_kwargs={""prompt"":prompt}
    )
question = input(""Question:"" )
answer = chain(question)
print(answer)
```

EDIT: I found my mistake.... 😅 my answer was short, because i didnt set the max_new_tokens higher. For anyone else:

```python
llm = HuggingFaceEndpoint(
            endpoint_url= ""https://api-inference.huggingface.co/models/"" + config.reader.name,
            huggingfacehub_api_token=config.huggingface_api_key,
            task=""text-generation"",
            model_kwargs = {
                ""min_length"": 200,
                ""max_length"":2000,
                ""temperature"":0.5,
                ""max_new_tokens"":200,
                ""num_return_sequences"":1
            }
        )
```"
MaziyarPanahi,2023-06-13 13:53:17+00:00,comment,tiiuae/falcon-7b-instruct,"@aditunoe I use both Falcon 7b and 40b via `text-generation-inference` and with `max_new_tokens` of 400 or 512 I always have very long and complete answers. I am pretty sure this must be the use of the free inference. 
EDIT: seemed it was the missing `max_new_tokens` and not the free inference API :) "
airtable,2023-06-21 13:23:07+00:00,comment,tiiuae/falcon-7b-instruct,"@MaziyarPanahi which kinda GPU are you using to load 40B? Do you load it in 8bit, 4bit?"
MaziyarPanahi,2023-06-21 14:47:03+00:00,comment,tiiuae/falcon-7b-instruct,"@airtable I have 4 A100 with 80G. I load in 16bit, my experiences with 8bit and 4bit were kind of terrible. The speed suffers badly in anything less than 16bit and the quality took a big hit. Not sure if there is a way to fix any of these 2 problems, but for now the 16bit seems to be a good choice"
airtable,2023-06-23 07:07:36+00:00,comment,tiiuae/falcon-7b-instruct,Thanks @MaziyarPanahi unfortunately the most I can get is 1x H100 80Gb GPU by Lambda labs
cmp-nct,2023-06-25 03:11:36+00:00,comment,tiiuae/falcon-7b-instruct,"I'm getting almost 45 tokens/sec for short and around 30 tokens for large contexts on ggllm.cpp using 8 bit Falcon 7B.
That's on a 4090
The 40B model is not that fast, I've not seen anything beyond 15/sec so far "
MaziyarPanahi,2023-06-26 18:14:28+00:00,comment,tiiuae/falcon-7b-instruct,"I just did a benchmark of Falcon-40B on 4 A100 with 80G:

- text-generation-inference (no quantization - 16bit)
- falcon-40b-instruct

```python
import time

prompt = ""Write a story about alpaca""
prompt_template = f""### Instruction: {prompt}\n### Response:""

start = time.time()
output = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)

delay = time.time()
total_time = (delay - start)
time_per_token = total_time / 256

# Calculate tokens per second
tokens_per_second = 256 / total_time

# Print the results
print(""Total inference time: {:.2f} ms"".format(total_time))
print(""Number of tokens generated: {}"".format(256))
print(""Time per token: {:.2f} ms/token"".format(time_per_token))
print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
```

Results:
```
Total inference time: 2.22 ms
Number of tokens generated: 256
Time per token: 0.01 ms/token
Tokens per second: 115.40 token/s
```

UPDATE: this is not correct, this is the [correct results](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1#649aa884ce3015b86c350782)"
inspectablock,2023-06-27 01:55:12+00:00,comment,tiiuae/falcon-7b-instruct,"> Check out the special tokens near the top of the tokenizer.json file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types you can play with. I've had the best results prompting it like this:
> 
> >>QUESTION<<In Python, I want to write a simple HTTP API that receives an object via POST and responds with another object. The request object contains a string prompt, float temperature, and int max_tokens. The response object contains a string response, int prompt_tokens, int completion_tokens. For now just set hard-coded values and get the response out. I'll add the logic myself afterward.
> >>ANSWER<<



Great find. I was wondering which of these tokens would be best to utilise if I wanted to constrain the LLM's answer to just the given context? No matter what I use, the LLM seems to want to answer from knowledge it knows instead of just responding with a generic ""I don't know"" e.g.



```python
""""""Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, respond with ""I can't answer that""

>>CONTEXT<<
Home and contents insurance covers your house and other structures on your property as well as your belongings for loss or damage due to events like fire, theft and storms.

>>QUESTION<< What is 2+2
""""""
```

The answer is 4



"
cmp-nct,2023-06-27 02:28:02+00:00,comment,tiiuae/falcon-7b-instruct,"> I just did a benchmark of Falcon-40B on 4 A100 with 80G:
> 
> - text-generation-inference (no quantization - 16bit)
> - falcon-40b-instruct
> 
> ```python
> import time
> 
> prompt = ""Write a story about alpaca""
> prompt_template = f""### Instruction: {prompt}\n### Response:""
> 
> start = time.time()
> output = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)
> 
> delay = time.time()
> total_time = (delay - start)
> time_per_token = total_time / 256
> 
> # Calculate tokens per second
> tokens_per_second = 256 / total_time
> 
> # Print the results
> print(""Total inference time: {:.2f} ms"".format(total_time))
> print(""Number of tokens generated: {}"".format(256))
> print(""Time per token: {:.2f} ms/token"".format(time_per_token))
> print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
> ```
> 
> Results:
> ```
> Total inference time: 2.22 ms
> Number of tokens generated: 256
> Time per token: 0.01 ms/token
> Tokens per second: 115.40 token/s
> ```

That's impressive but the numbers do not appear to line up well ?
0.01ms /token would be 100k tokens/second and the total inference time of 2.22 would be 450 tokens/second. That needs an actual stopwatch to make sure :)

I ran the same one, though I can not fit more than 8 bit of 40B on my cheap desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation. (64ms/token)

"
MaziyarPanahi,2023-06-27 09:14:44+00:00,comment,tiiuae/falcon-7b-instruct,"> > I just did a benchmark of Falcon-40B on 4 A100 with 80G:
> > 
> > - text-generation-inference (no quantization - 16bit)
> > - falcon-40b-instruct
> > 
> > ```python
> > import time
> > 
> > prompt = ""Write a story about alpaca""
> > prompt_template = f""### Instruction: {prompt}\n### Response:""
> > 
> > start = time.time()
> > output = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)
> > 
> > delay = time.time()
> > total_time = (delay - start)
> > time_per_token = total_time / 256
> > 
> > # Calculate tokens per second
> > tokens_per_second = 256 / total_time
> > 
> > # Print the results
> > print(""Total inference time: {:.2f} ms"".format(total_time))
> > print(""Number of tokens generated: {}"".format(256))
> > print(""Time per token: {:.2f} ms/token"".format(time_per_token))
> > print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
> > ```
> > 
> > Results:
> > ```
> > Total inference time: 2.22 ms
> > Number of tokens generated: 256
> > Time per token: 0.01 ms/token
> > Tokens per second: 115.40 token/s
> > ```
> 
> That's impressive but the numbers do not appear to line up well ?
> 0.01ms /token would be 100k tokens/second and the total inference time of 2.22 would be 450 tokens/second. That needs an actual stopwatch to make sure :)
> 
> I ran the same one, though I can not fit more than 8 bit of 40B on my cheap desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation. (64ms/token)

That makes sense. I copied it from a github issue so I thought it was solid. I now see it just assumes the `output` has 256 tokens! This is a new code which has much lower inference time:

```python
import time

prompt = ""Write a long story about alpaca""
prompt_template = f""### Instruction: {prompt}\n### Response:""

start = time.time()
output = client_falcon40.generate(prompt_template, max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text
delay = time.time()

real_total_tokens = len(output.split("" ""))
total_time = (delay - start)
time_per_token = total_time / real_total_tokens

# Calculate tokens per second
tokens_per_second = real_total_tokens / total_time

# Print the results
print(""Total inference time: {:.2f} ms"".format(total_time))
print(""Number of tokens generated: {}"".format(real_total_tokens))
print(""Time per token: {:.2f} ms/token"".format(time_per_token))
print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
```

For Falcon-40B now I have

```
Total inference time: 3.15 ms
Number of tokens generated: 106
Time per token: 0.03 ms/token
Tokens per second: 33.67 token/s
```

For Falcon-7B
```
Total inference time: 3.70 ms
Number of tokens generated: 241
Time per token: 0.02 ms/token
Tokens per second: 65.16 token/s
```
"
MaziyarPanahi,2023-06-27 09:17:50+00:00,comment,tiiuae/falcon-7b-instruct,"> > Check out the special tokens near the top of the tokenizer.json file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types you can play with. I've had the best results prompting it like this:
> > 
> > >>QUESTION<<In Python, I want to write a simple HTTP API that receives an object via POST and responds with another object. The request object contains a string prompt, float temperature, and int max_tokens. The response object contains a string response, int prompt_tokens, int completion_tokens. For now just set hard-coded values and get the response out. I'll add the logic myself afterward.
> > >>ANSWER<<
> 
> 
> 
> Great find. I was wondering which of these tokens would be best to utilise if I wanted to constrain the LLM's answer to just the given context? No matter what I use, the LLM seems to want to answer from knowledge it knows instead of just responding with a generic ""I don't know"" e.g.
> 
> 
> 
> ```python
> """"""Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, respond with ""I can't answer that""
> 
> >>CONTEXT<<
> Home and contents insurance covers your house and other structures on your property as well as your belongings for loss or damage due to events like fire, theft and storms.
> 
> >>QUESTION<< What is 2+2
> """"""
> ```
> 
> The answer is 4

I am also interested in this. I use `>>QUESTION<<` and `>>ANSWER<<` tokens only and. the rest is just a simple template and it works. But time to time it feels like I am missing something else compare to OpenAssistant where I know all the tokens I can use"
cmp-nct,2023-06-28 13:26:34+00:00,comment,tiiuae/falcon-7b-instruct,"
> For Falcon-40B now I have
> 
> ```
> Total inference time: 3.15 ms
> Number of tokens generated: 106
> Time per token: 0.03 ms/token
> Tokens per second: 33.67 token/s
> ```
> 
> For Falcon-7B
> ```
> Total inference time: 3.70 ms
> Number of tokens generated: 241
> Time per token: 0.02 ms/token
> Tokens per second: 65.16 token/s
> ```

Thank you for the update, that clears a lot up.
So in the current state ggllm.cpp on 1 or 2x3090 is about half the speed compared with the 4x A100 and python

@MaziyarPanahi 
I ran a couple tests and it appears those tokens are very useful but also need to be controlled well. 
For question/answers I prefer it over the standard instruct fine tuning they offered (which incorporates ""safe"" OpenAI responses) you can use the normal foundation model!
But it's not a fine tuning so you'd need some logic to ""stop"", and a bit more pre-prompt to avoid >>COMMENT<< or phantasy names.

Below are examples, not cherry picked. Those are generated using Falcon 40B in 4bit quantization (to fit on a single 24gb GPU), so at 6+ bit it would likely be a bit more elaborate.
Use of the parameters is explained here: https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md
```
falcon_main.exe -t 2  -m .\falcon-40b\q4_k  --override-max-gpu 1 -e -n 500  --gpu-reserve-mb-main -500 -e -p "">>QUESTION<<\nWhat would be the most likely results for earth if the moon was suddenly beamed away by an alien species ?\n >>ANSWER<<\n"" -b 1
>>QUESTION<<
What would be the most likely results for earth if the moon was suddenly beamed away by an alien species ?
 >>ANSWER<<
I would say that the result for life on Earth would be catastrophic. The moon is a stabilizing force for Earth's orbit, controlling its wobble, which prevents it from flipping over. If the moon were to disappear, Earth would wobble like a top spinning around and gradually slow down, ending up with a completely different rotation period. The Earth would then become an uninhabitable iceball. This would happen within a couple of days or weeks.
The tidal effect of the moon would be gone, so instead of having the two high tides and two low tides of 12 hours each in a day, there would only be one low tide and one high tide, each lasting 24 hours. The other effect would be that Earth would be much more subject to earthquakes and volcanic eruptions. This is because the bulges created by the tidal effect of the moon are what cause tectonic plate movements. (Note: The moon is not big enough to cause plate tectonics on its own; it would be too far away.)
Another significant effect of removing the moon would be that it would no longer shield us from meteors, comets, and asteroids. This is because when a body gets close to the Earth, the moon’s gravitational pull pulls it in towards the Earth. But if the moon were gone, the Earth would then be hit by this body instead of the moon. The Earth has been hit by many small asteroids, but nothing catastrophic has happened recently. Without the moon, this would be a very real possibility.
If the moon was to suddenly disappear, we would know it almost immediately. If you have ever seen the moon in the daytime, then you will know that it is actually very bright, but because the sun is out, you cannot see it.
So we would see a big difference in the sky, but if this happened at night time, it would not be so easy to detect.
If this happened suddenly, we would be in real trouble. The moon is the reason why we have seasons. If it were not there, our climate would be even more extreme than it is now. The water would also escape the earth, as the moon is there to hold it back.
So in short, all life on earth would die out very quickly.
>>END<<
```

# For this one I had to add ""flawless answer"" otherwise it would spam ""comments"" after it. GPT 4 failed with this one consistently
```
>>TITLE<<
Flawless answer
>>QUESTION<<'If we place an air-filled balloon very close to a microphone inside a very small vacuum chamber, will the microphone register sound after the balloon bursts? Give an explanation also.'
 >>ANSWER<<
The balloon is filled with air. The air molecules are in constant motion. When the balloon is burst, the air molecules are released and move away from the balloon. The air molecules collide with the microphone and cause it to vibrate. The microphone will register sound.
```



In general summed up:
Those keywords are useful but they are not flawless.
It appears that the team used these keywords when training from data like Quora or Science forums, they replaced parts of the text with such tokens.
So that's also what you get when you use them, the model is generating context from such a forum. Which can include comments an discussions.
I could reduce the amount of discussions by using `>>TITLE<<`
In addition the model often does not stop after the answer, it will continue with another `>>COMMENT<< or >>QUESTION<<`
The good part is that this can be detected by software as a stop word, it stays quite close to that format"
MaziyarPanahi,2023-07-02 19:30:25+00:00,comment,tiiuae/falcon-7b-instruct,"Thanks @HahaHa1 for sharing your prompts and experiments. I am still having problems to be 100% sure about `retrieval questions_answering`. Something like:

```python
prompt_template = (
        ""{instruction}\n""
        ""---------------------\n""
        ""{context}\n""
        ""---------------------\n""
        ""{question}""
    )
```
It was easier with OpenAssistant to do this via `prompter/eos` and `assistant`"
etweedy,2023-07-03 00:01:06+00:00,comment,tiiuae/falcon-7b-instruct,"> > > I just did a benchmark of Falcon-40B on 4 A100 with 80G:
> > > 
> > > - text-generation-inference (no quantization - 16bit)
> > > - falcon-40b-instruct
> > > 
> > > ```python
> > > import time
> > > 
> > > prompt = ""Write a story about alpaca""
> > > prompt_template = f""### Instruction: {prompt}\n### Response:""
> > > 
> > > start = time.time()
> > > output = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)
> > > 
> > > delay = time.time()
> > > total_time = (delay - start)
> > > time_per_token = total_time / 256
> > > 
> > > # Calculate tokens per second
> > > tokens_per_second = 256 / total_time
> > > 
> > > # Print the results
> > > print(""Total inference time: {:.2f} ms"".format(total_time))
> > > print(""Number of tokens generated: {}"".format(256))
> > > print(""Time per token: {:.2f} ms/token"".format(time_per_token))
> > > print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
> > > ```
> > > 
> > > Results:
> > > ```
> > > Total inference time: 2.22 ms
> > > Number of tokens generated: 256
> > > Time per token: 0.01 ms/token
> > > Tokens per second: 115.40 token/s
> > > ```
> > 
> > That's impressive but the numbers do not appear to line up well ?
> > 0.01ms /token would be 100k tokens/second and the total inference time of 2.22 would be 450 tokens/second. That needs an actual stopwatch to make sure :)
> > 
> > I ran the same one, though I can not fit more than 8 bit of 40B on my cheap desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation. (64ms/token)
> 
> That makes sense. I copied it from a github issue so I thought it was solid. I now see it just assumes the `output` has 256 tokens! This is a new code which has much lower inference time:
> 
> ```python
> import time
> 
> prompt = ""Write a long story about alpaca""
> prompt_template = f""### Instruction: {prompt}\n### Response:""
> 
> start = time.time()
> output = client_falcon40.generate(prompt_template, max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text
> delay = time.time()
> 
> real_total_tokens = len(output.split("" ""))
> total_time = (delay - start)
> time_per_token = total_time / real_total_tokens
> 
> # Calculate tokens per second
> tokens_per_second = real_total_tokens / total_time
> 
> # Print the results
> print(""Total inference time: {:.2f} ms"".format(total_time))
> print(""Number of tokens generated: {}"".format(real_total_tokens))
> print(""Time per token: {:.2f} ms/token"".format(time_per_token))
> print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
> ```
> 
> For Falcon-40B now I have
> 
> ```
> Total inference time: 3.15 ms
> Number of tokens generated: 106
> Time per token: 0.03 ms/token
> Tokens per second: 33.67 token/s
> ```
> 
> For Falcon-7B
> ```
> Total inference time: 3.70 ms
> Number of tokens generated: 241
> Time per token: 0.02 ms/token
> Tokens per second: 65.16 token/s
> ```

I think in your benchmarking calculations you're not doing the conversion from milliseconds to seconds, e.g.
$$\frac{241\text{ tokens}}{3.70\text{ ms}} = \frac{241\text{ tokens}}{.0037\text{ s}} \approx 65135\text{ tokens/s}\text{ (or $65.135$ tokens/ms)}$$"
MaziyarPanahi,2023-07-04 17:46:13+00:00,comment,tiiuae/falcon-7b-instruct,"> > > > I just did a benchmark of Falcon-40B on 4 A100 with 80G:
> > > > 
> > > > - text-generation-inference (no quantization - 16bit)
> > > > - falcon-40b-instruct
> > > > 
> > > > ```python
> > > > import time
> > > > 
> > > > prompt = ""Write a story about alpaca""
> > > > prompt_template = f""### Instruction: {prompt}\n### Response:""
> > > > 
> > > > start = time.time()
> > > > output = client_falcon40.generate(prompt_template, max_new_tokens=256, temperature=1, top_k=1, top_p=0.90)
> > > > 
> > > > delay = time.time()
> > > > total_time = (delay - start)
> > > > time_per_token = total_time / 256
> > > > 
> > > > # Calculate tokens per second
> > > > tokens_per_second = 256 / total_time
> > > > 
> > > > # Print the results
> > > > print(""Total inference time: {:.2f} ms"".format(total_time))
> > > > print(""Number of tokens generated: {}"".format(256))
> > > > print(""Time per token: {:.2f} ms/token"".format(time_per_token))
> > > > print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
> > > > ```
> > > > 
> > > > Results:
> > > > ```
> > > > Total inference time: 2.22 ms
> > > > Number of tokens generated: 256
> > > > Time per token: 0.01 ms/token
> > > > Tokens per second: 115.40 token/s
> > > > ```
> > > 
> > > That's impressive but the numbers do not appear to line up well ?
> > > 0.01ms /token would be 100k tokens/second and the total inference time of 2.22 would be 450 tokens/second. That needs an actual stopwatch to make sure :)
> > > 
> > > I ran the same one, though I can not fit more than 8 bit of 40B on my cheap desktop PC, I'm getting ~16 tokens/sec for the same prompt and generation. (64ms/token)
> > 
> > That makes sense. I copied it from a github issue so I thought it was solid. I now see it just assumes the `output` has 256 tokens! This is a new code which has much lower inference time:
> > 
> > ```python
> > import time
> > 
> > prompt = ""Write a long story about alpaca""
> > prompt_template = f""### Instruction: {prompt}\n### Response:""
> > 
> > start = time.time()
> > output = client_falcon40.generate(prompt_template, max_new_tokens=1024, temperature=0.55, top_k=1, top_p=0.90).generated_text
> > delay = time.time()
> > 
> > real_total_tokens = len(output.split("" ""))
> > total_time = (delay - start)
> > time_per_token = total_time / real_total_tokens
> > 
> > # Calculate tokens per second
> > tokens_per_second = real_total_tokens / total_time
> > 
> > # Print the results
> > print(""Total inference time: {:.2f} ms"".format(total_time))
> > print(""Number of tokens generated: {}"".format(real_total_tokens))
> > print(""Time per token: {:.2f} ms/token"".format(time_per_token))
> > print(""Tokens per second: {:.2f} token/s"".format(tokens_per_second))
> > ```
> > 
> > For Falcon-40B now I have
> > 
> > ```
> > Total inference time: 3.15 ms
> > Number of tokens generated: 106
> > Time per token: 0.03 ms/token
> > Tokens per second: 33.67 token/s
> > ```
> > 
> > For Falcon-7B
> > ```
> > Total inference time: 3.70 ms
> > Number of tokens generated: 241
> > Time per token: 0.02 ms/token
> > Tokens per second: 65.16 token/s
> > ```
> 
> I think in your benchmarking calculations you're not doing the conversion from milliseconds to seconds, e.g.
> $$\frac{241\text{ tokens}}{3.70\text{ ms}} = \frac{241\text{ tokens}}{.0037\text{ s}} \approx 65135\text{ tokens/s}\text{ (or $65.135$ tokens/ms)}$$

There is an issue with the `Total inference time: 3.70 ms`. This should be `3.70s`. So the `60 tokens per second` check out considering the LLM model and the hardware limitations. (I am not even sure I can tokenize the text with 65k tokens/s :D)"
etweedy,2023-07-04 17:50:38+00:00,comment,tiiuae/falcon-7b-instruct,"I see, yes that is more reasonable! Somehow the units are not reporting correctly on total inference time."
Shivaraj,2023-07-28 05:41:30+00:00,comment,tiiuae/falcon-7b-instruct,"Is Falcon 7b instruct model sensitive to the Prompt template? As per my experiment I did several times different types of prompt templates I don't it see it considers all the guidelines provided in prompt, Specially when context doesn't have answer to the asked question it still tries to make answer out that, despite telling ""if context is empty then do not generate answer, say sorry I cannot answer!.""

Please share your experience with prompt template"
Shaffay00,2023-08-01 15:26:57+00:00,comment,tiiuae/falcon-7b-instruct,"Hi,
What improvements should I make in this prompt to get an exciting story with 1000+ tokens?

template = """"""
You are a story writer who writes long and captivating stories on a given topic.

The output format of the story should be the following 6 paragraphs.
Paragraph 1: Long Engaging Introduction (400 words)
Paragraph 2: Tell three main points about the topic. 
Paragraph 3: Explain point 1 in detail (400 words).
Paragraph 4: Explain point 2 in detail (400 words).
Paragraph 5: Explain point 3 in detail (400 words).
Paragraph 6: Long Conclusion (400 words)

The story should be 1000 words minimum.

Topic: {question}
Story:"""""""
matangilat,2023-08-09 14:12:56+00:00,comment,tiiuae/falcon-7b-instruct,"The following prompt works for me : 
""""""Answer the question using the provided text, and if the answer is not contained within the text below, respond with ""I can't answer that
     >>CONTEXT<<
      {context} 
      >>QUESTION<< {question}
"""""""
danny3,2023-08-11 20:37:45+00:00,comment,tiiuae/falcon-7b-instruct,"My falcon-7b do not seem to remember it context even using ConversationBufferMemory with langchain. Please help

class JacqChatbot:
  def __init__(self, tokenizer, model, generation_config):
    self.tokenizer = tokenizer
    self.model = model
    # self.stopping_criteria = stopping_criteria
    self.generation_config = generation_config
    self.DEFAULT_TEMPLATE = \
    """"""The following is a conversation between a human and AI, \
    called Jacq. Jacq is an experienced and very successful Legal professional. \
    Jacq was built to be respectful, polite, inclusive and practical. \
    Jacq was built by the Lex-ai in Lagos, Nigeria in the year 2023. \
    Jacq helps with every legal task given to him. \
    When requesting for physical documents refer them to upload those documents before submission. \
    If Jacq does not know the answer to a question, he truthfully says he does not know and does not \
    ask for the answer to the question. The conversation begins.

    Current conversation:
    {chat_history}
    ### Instruction:
    {input}

    ### Response:""""""

    self.prompt_template   = self._create_prompt_template()
    self.llm = self._create_llm_pipeline()
    self.llm_chain = self._create_llm_chain()
    self.STOP_SUSPECT_LIST = [""### Instruction:"", ""</s>"",
                              ""\nUser"", ""\n### Response:"",
                              ""\n<person>:"", ""\n##Response:"", ""\n<human>:"",
                              ""\n<assistant>:""]

  def _create_prompt_template(self):
    return PromptTemplate(input_variables=[""chat_history"", ""input""], template=self.DEFAULT_TEMPLATE)

  def _create_llm_pipeline(self):
    pipeline_instance = pipeline(
        ""text-generation"",
        generation_config=self.generation_config,
        model=self.model,
        tokenizer=self.tokenizer,
        # stopping_criteria=self.stopping_criteria,
        framework=""pt""
    )
    return HuggingFacePipeline(pipeline=pipeline_instance)

  def _create_llm_chain(self):
    config = {
        ""memory_key"": ""chat_history"",
        ""ai_prefix"": ""<assistant>"",
        ""human_prefix"": ""<human>"",
        ""input_key"": ""input"",
        ""return_messages"": False
    }

    memory = ConversationBufferMemory(**config)

    return LLMChain(llm=self.llm,
                    prompt=self.prompt_template,
                    # output_parser=OutputParser(),
                    verbose=True,
                    memory=memory)

  def chat(self, message):
      if message:
        return self.llm_chain.predict(input=message, stop=self.STOP_SUSPECT_LIST).strip()

# stop_tokens = [
#     [""###"", ""Instruction"", "":""],
#     [""###"", ""Response"", "":""],
#     # [""\\"",""n"",'User']
# ]

generation_config = GenerationConfig()
generation_config.max_new_tokens = 128
generation_config.temperature = 0.5
# generation_config.top = 0.15
generation_config.num_return_sequences = 1
generation_config.repetition_penalty = 6.5
# generation_config.top_k=50
# generation_config.top_p=0.92
# #generation_config.return_full_text = True Uncomment when using Langchain
generation_config.pad_token_id = tokenizer.pad_token_id
generation_config.eos_token_id = tokenizer.eos_token_id

# stopping_criteria = StoppingCriteriaList([StopGenerationCriteria(stop_tokens, tokenizer, model.device)])
chatbot = JacqChatbot(tokenizer, model, generation_config) #stopping_criteria"
Joshin,2023-09-04 13:57:12+00:00,comment,tiiuae/falcon-7b-instruct,"> > Check out the special tokens near the top of the tokenizer.json file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types you can play with. I've had the best results prompting it like this:
> > 
> > >>QUESTION<<In Python, I want to write a simple HTTP API that receives an object via POST and responds with another object. The request object contains a string prompt, float temperature, and int max_tokens. The response object contains a string response, int prompt_tokens, int completion_tokens. For now just set hard-coded values and get the response out. I'll add the logic myself afterward.
> > >>ANSWER<<
> 
> 
> 
> Great find. I was wondering which of these tokens would be best to utilise if I wanted to constrain the LLM's answer to just the given context? No matter what I use, the LLM seems to want to answer from knowledge it knows instead of just responding with a generic ""I don't know"" e.g.
> 
> 
> 
> ```python
> """"""Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, respond with ""I can't answer that""
> 
> >>CONTEXT<<
> Home and contents insurance covers your house and other structures on your property as well as your belongings for loss or damage due to events like fire, theft and storms.
> 
> >>QUESTION<< What is 2+2
> """"""
> ```
> 
> The answer is 4

You can use >>INTRODUCTION<< for the system prompt and >>SUMMARY<< for the context"
YoYo1234Qwerty,2023-09-26 21:34:54+00:00,comment,tiiuae/falcon-7b-instruct,"I think everyone's wrong about `>>QUESTION<<` and `>>ANSWER<<` etc.

If you prompt it with a `>>QUESTION<<` it never outputs the `>>ANSWER<<` token on its own, meaning it likely wasn't trained to use these tokens.

They apparently trained on Baise: https://github.com/project-baize/baize-chatbot/blob/main/demo/app.py#L59

Which suggests a prompting scheme of `[|Human|]` and `[|AI|]`.

But then in experiments, it really wants to output:

```
User How's the weather?
Mini So so, kinda gloomy.
```"
jay0911,2023-10-18 04:02:40+00:00,comment,tiiuae/falcon-7b-instruct,"How to use FalconForSequenceClassification for simple sentiment analysis task? I tried the following:

import torch
from transformers import AutoTokenizer, FalconForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(""Rocketknight1/falcon-rw-1b"")
model = FalconForSequenceClassification.from_pretrained(""Rocketknight1/falcon-rw-1b"",num_labels=2)
model.config.id2label = {""0"" : "" positive"" , ""1"":""Negative:}
inputs = tokenizer(""Hello, my dog is cute"", return_tensors=""pt"")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()

It was giving it as negative sentence"
Pitambarmuduli,2023-11-02 05:15:17+00:00,comment,tiiuae/falcon-7b-instruct,How is this model applied to translation (One language to other like German to English )?
krisha05,2023-12-10 09:09:19+00:00,comment,tiiuae/falcon-7b-instruct,"> Hi,
> What improvements should I make in this prompt to get an exciting story with 1000+ tokens?
> 
> template = """"""
> You are a story writer who writes long and captivating stories on a given topic.
> 
> The output format of the story should be the following 6 paragraphs.
> Paragraph 1: Long Engaging Introduction (400 words)
> Paragraph 2: Tell three main points about the topic. 
> Paragraph 3: Explain point 1 in detail (400 words).
> Paragraph 4: Explain point 2 in detail (400 words).
> Paragraph 5: Explain point 3 in detail (400 words).
> Paragraph 6: Long Conclusion (400 words)
> 
> The story should be 1000 words minimum.
> 
> Topic: {question}
> Story:""""""

I would love to know more about this"
a7mmdll,2024-05-03 18:22:00+00:00,comment,tiiuae/falcon-7b-instruct,"Update streamlit, huggingface and langchain
"
koflerdavid,2024-02-29 08:57:20+00:00,comment,tiiuae/falcon-7b-instruct,"Hello, it seems that PyTorch was actually only able to allocate 12GB. I suggest you use nvtop (dunno how to do it on Windows) to find out who is sitting on those missing 4GB. Anyways, you should try to load the model with `float16` or `bfloat16` first. The full precision of `float32` is rarely really necessary:

    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=model_directory, torch_dtype=torch.bfloat16).to(device)

 If that still doesn't work, then you have to use quantization:

    quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=False)
    #quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=model_directory, device_map='cuda', quantization_config=quantization_config)

I have to use 4bit quantization and Flash Attention to run it on an 8GB RTX 3070. Good luck!"
krisfarrmt,2024-02-12 13:09:13+00:00,comment,tiiuae/falcon-7b-instruct,"Have you found the solution, please?"
cargoro,2024-02-12 13:17:52+00:00,comment,tiiuae/falcon-7b-instruct,"Hi @krisfarrmt , yes, it turned out that during my experimentation I was having an `inference.py` and a `requirement.txt` files in the S3 folder from where the model was being loaded. Removing these and some other files I was adding during my tries, made it work :) . "
epignatelli,2023-12-22 00:15:14+00:00,comment,tiiuae/falcon-7b-instruct,"The current tokeniser uses the default (inappropriate) chat template.
This PR adds the correct chate template following https://github.com/chujiezheng/chat_templates.
You can find the easy-to-read version of the template at: https://github.com/chujiezheng/chat_templates/blob/main/chat_templates/falcon.jinja"
SMHLondon,2024-01-05 20:24:17+00:00,comment,tiiuae/falcon-7b-instruct,"I've got very much the same problem with python 3.9. 
transformers package is from git+https://github.com/huggingface/transformers and says version 4.37.0.dev as a version number. 
what is funny that the import statement says it loads BitsAndBytes from transformers, but then there exists a package with the same name. 
Does one have to install that package additionally?"
Vartika1,2024-03-21 04:44:37+00:00,comment,tiiuae/falcon-7b-instruct,I am getting similar error .How is this resolved 
ibrahimc,2024-04-24 20:26:49+00:00,comment,tiiuae/falcon-7b-instruct,hey guys is there anyone who resolved the issue? 
BDITM,2024-05-10 13:12:02+00:00,comment,tiiuae/falcon-7b-instruct,I'm also looking at the same issue.
incachild,2024-05-10 19:02:37+00:00,comment,tiiuae/falcon-7b-instruct,"to fix:
`pip install bitsandbytes` 
"
xceptor,2023-11-27 16:00:15+00:00,comment,tiiuae/falcon-7b-instruct,"What was your raw format? or .json format?
Was it {instruction: xyz, output: abc}?
"
pkroan,2023-11-03 17:10:12+00:00,comment,tiiuae/falcon-7b-instruct,"I am also getting these errors in GKE on GCP. I have tried tag 1.1.0 and 1.0.3 and keep having the webserver shutdown. 

GPU: Tesla T4
Trying a downloaded llama-2-7b-chat-hf model
Tag: 1.0.3 
--quantize bitsandbytes

1.0.3 error:
Unable to use Flash Attention V2: GPU with CUDA capability 7 5 is not supported for Flash Attention V2

1.1.0 error: 
Could not import Mistral model: Mistral model requires flash attn v2
"
aphedges,2023-10-25 19:47:04+00:00,comment,tiiuae/falcon-7b-instruct,Sorry about this! I didn't realize there was already a PR open to fix this: <https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/46>
Gnurro2,2023-10-25 13:11:27+00:00,comment,tiiuae/falcon-7b-instruct,"No, that's the default template in tokenizers.
Proper format (...well, at least somewhat official): https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1#64708b0a3df93fddece002a4
Apparently the model wasn't trained on any concise format, so it seems like ""whatever works"". The format is the whole point of instruct training, and I really do not know why so many model trainers do not properly share the used format..."
Zak,2023-10-02 06:19:35+00:00,comment,tiiuae/falcon-7b-instruct,"Same issue for me.
it seems that the Falcon team has modified the code and removed the configuration_RW.py file"
ashioyajotham,2023-10-05 09:53:14+00:00,comment,tiiuae/falcon-7b-instruct,Same issue. Welp!
ashioyajotham,2023-10-05 09:57:28+00:00,comment,tiiuae/falcon-7b-instruct,Using FalconForCasualLm seems to be the solution https://huggingface.co/tiiuae/falcon-7b/discussions/60#651988650e3a5553d4aac013
nkasmanoff,2023-09-19 11:47:34+00:00,comment,tiiuae/falcon-7b-instruct,"

Update generation config to correct token ids for BOS and EOS
"
matthewmrichter,2023-09-18 19:07:21+00:00,comment,tiiuae/falcon-7b-instruct,"I'm closing this. I figured out how to test, and this does not actually accomplish what I was looking to do. For those curious, the key was to run the model on a text-generation specific pre-baked image: `763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.0.3-gpu-py39-cu118-ubuntu20.04`."
myron1,2023-09-15 13:44:45+00:00,comment,tiiuae/falcon-7b-instruct,"Hello
"
fatnypantzer,2023-11-11 12:41:00+00:00,comment,tiiuae/falcon-7b-instruct,"7b-instruct I've trained with 9-36gb vram, currently trying 7b.

40b is ~96gb vram, from what i've read there was someone who had trained 40b-instruct using something different to Lora with 48gb vRam, however, even then there seems to be more involved with the GPU configuration.

if anyone has more concrete details on the hardware requirements.

I've come across quite a few links that i'll post when i find them again.

Resources: 
https://www.reddit.com/r/LocalLLaMA/comments/13wutj4/getting_falcon_40b_to_work/
https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ/discussions"
ankity09,2023-08-30 04:35:52+00:00,comment,tiiuae/falcon-7b-instruct,"When I set the returned search results from VecDB to 3(larger prompt), the model takes 
1st Question(Answer is wrong)
```CPU times: user 2min 39s, sys: 391 ms, total: 2min 39s```
2nd Question (Answer is wrong)
```CPU times: user 47.4 s, sys: 7.26 ms, total: 47.4 s```
and then does not respond from the third onwards

When I decrease the results to 1 (smaller prompt)
1st question takes(Answer is right)
```CPU times: user 44.6 s, sys: 288 ms, total: 44.9 s```
2nd Question takes(Answer is somewhat right)
```CPU times: user 17.4 s, sys: 0 ns, total: 17.4 s```
and then does not respond from the third onwards as above."
Ferraria,2023-08-03 17:48:30+00:00,comment,tiiuae/falcon-7b-instruct,i just saw that the actual memory of my VM was less than what i assumed. it had nothing to do with RAM
akashpatel,2023-11-01 06:52:02+00:00,comment,tiiuae/falcon-7b-instruct,I want to use csv agent with mistral model. Please update if you have any relevant information. 
codermato,2023-11-19 19:20:53+00:00,comment,tiiuae/falcon-7b-instruct,"Have you figured what was wrong? Similar thing is happening to me with arxiv and llamav2.
"
ravijoe,2024-01-28 20:33:41+00:00,comment,tiiuae/falcon-7b-instruct,any outputs here?
jurecucek,2023-07-21 12:39:59+00:00,comment,tiiuae/falcon-7b-instruct,"Falcon 7B does load using fastchat. So i guess my code is wrong :D
"
adam-zettafi,2023-07-28 15:04:10+00:00,comment,tiiuae/falcon-7b-instruct,"In my experience, killed usually means it used too much RAM and was shut down. Are there specific reasons you are providing so many configuration options, or was this from a code snippet? My best experiences with Hugging Face libraries have been when starting with only the bare necessities to get it running and then modifying from there for tuning. I suggest removing all the extras and letting the Transformer and Pipeline figure it out. You can always add more for tuning later."
anomalus,2023-07-18 20:38:14+00:00,comment,tiiuae/falcon-7b-instruct,"I'm curious as well! It'd be great to have the code from the [demo shown in the video](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/147_falcon/falcon-7b.mp4), so we can tinker.

I may be overthinking this, but I suspect it involves passing the String to a [tokenizer](https://developer.apple.com/documentation/naturallanguage/tokenizing_natural_language_text) built for this particular model, similar to these [Swift CoreML transformers](https://github.com/huggingface/swift-coreml-transformers/)."
pcuenq,2023-07-19 07:10:32+00:00,comment,tiiuae/falcon-7b-instruct,"You are right @anomalus: you need to tokenize the text, and then process the outputs to create the output sequence. The model only returns information about the probability of the next token in the sequence, so you need to call it multiple times to get the output.

We intend to publish everything soon."
anomalus,2023-07-19 19:10:34+00:00,comment,tiiuae/falcon-7b-instruct,@pcuenq Fantastic. Looking forward to it!
jayfehr,2023-08-05 20:56:39+00:00,comment,tiiuae/falcon-7b-instruct,"> You are right @anomalus: you need to tokenize the text, and then process the outputs to create the output sequence. The model only returns information about the probability of the next token in the sequence, so you need to call it multiple times to get the output.
> 
> We intend to publish everything soon.

Would you be able to provide quick sample code to run this the mlpackage? "
anomalus,2023-08-16 12:29:25+00:00,comment,tiiuae/falcon-7b-instruct,"Posting this here: https://huggingface.co/blog/swift-coreml-llm

Thanks @pcuenq! The only part I'm curious about is using Falcon 7b with Swift Chat is unusably slow. It takes maybe 5 minutes per word. I have a Macbook Pro M1 Max with 32GB of RAM, but SwiftChat uses 55GB+ of RAM on a simple run. Any advice on how to navigate this?"
bwebs,2023-07-21 15:43:36+00:00,comment,tiiuae/falcon-7b-instruct,Any updates here? I am experiencing the same thing
martinnovak,2023-07-21 12:50:04+00:00,comment,tiiuae/falcon-7b-instruct,"I found this problem as well. As far as I observed this, it happens when you enter longer text into the input. But apparently looking at your input, my observations are not 100% correct."
Shivaraj,2023-07-26 11:17:44+00:00,comment,tiiuae/falcon-7b-instruct,"Any Solution for the above? I am also facing same issue, my work around is to split by sentence and drop duplicates."
Iron10,2023-08-16 16:53:59+00:00,comment,tiiuae/falcon-7b-instruct,@fatimataayeb  Any solution? I am facing same issues
mjspeck,2023-09-28 03:46:47+00:00,comment,tiiuae/falcon-7b-instruct,Any updates on this?
danryan,2023-07-17 18:25:16+00:00,comment,tiiuae/falcon-7b-instruct,The endpoint container is not healthy and is restarting. Check the endpoint cloudwatch logs for details.
gpaoletti,2023-08-08 09:55:18+00:00,comment,tiiuae/falcon-7b-instruct,"I am tryning to deploy falcon-40b and experiencing the same error since I moved to 
```
# install supported sagemaker SDK
!pip install ""sagemaker==2.175.0"" --upgrade --quiet
```
and
```
# retrieve the llm image uri
llm_image = get_huggingface_llm_image_uri(
  ""huggingface"",
  version=""0.9.3""
)
```
all working fine with llm_image version ""0.8.2""

both tests done with 
```
import json
from sagemaker.huggingface import HuggingFaceModel

# sagemaker config
instance_type = ""ml.g5.12xlarge""
number_of_gpu = 4
health_check_timeout = 300

# TGI config
config = {
  'HF_MODEL_ID': ""tiiuae/falcon-40b-instruct"", # model_id from hf.co/models
  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica
  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text
  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)
  # 'HF_MODEL_QUANTIZE': ""bitsandbytes"", # comment in to quantize
}

# create HuggingFaceModel
llm_model = HuggingFaceModel(
  role=role,
  image_uri=llm_image,
  env=config
)
```
with falcon-7b I am able to succesfully deploy using version ""0.9.3"""
gpaoletti,2023-08-08 15:43:19+00:00,comment,tiiuae/falcon-7b-instruct,"details of my error 
```
UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-08-08-09-21-35-398: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..
```
"
philschmid,2023-08-08 16:21:52+00:00,comment,tiiuae/falcon-7b-instruct,What is the error you see in cloudwatch? 
gpaoletti,2023-08-08 16:38:23+00:00,comment,tiiuae/falcon-7b-instruct,"I have also the full CSV but this is  a screenshot which looks like some relevant part. 
![Screenshot 2023-08-08 at 18.35.34.png](https://cdn-uploads.huggingface.co/production/uploads/64be82038e051085ba21ed0f/9oItuf1PuuKezH8CqWAHD.png)
"
purunfer22,2023-07-11 10:27:58+00:00,comment,tiiuae/falcon-7b-instruct,There are some excerpts in the file which should be removed and hence I have closed this pull request and will raise another
pranavnerurkar,2023-07-12 09:18:03+00:00,comment,tiiuae/falcon-7b-instruct,"hi I am getting the same error
did u solve it?
"
np05,2023-07-12 12:35:15+00:00,comment,tiiuae/falcon-7b-instruct,@pranavnerurkar: nope it is not solved
np05,2023-07-13 14:28:02+00:00,comment,tiiuae/falcon-7b-instruct,"I was able to deploy an endpoint referring to this article on AWS. https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-falcon.ipynb.

@pranavnerurkar : FYI"
maxbohun,2023-07-13 04:40:14+00:00,comment,tiiuae/falcon-7b-instruct,I have the same problem when trying to use my fine-tuned Falcon. Are you using Windows?
Yash2k23,2023-07-13 11:37:51+00:00,comment,tiiuae/falcon-7b-instruct,"Yes, i am trying this on Windows 10 OS. But here i am trying to load my base model and not the fine tuned. 

however i found the issue is with my Model Weights and more precisely the issue is with model tokenizer files.. I tried to redownload the weights again and all files and then tried loading them. It worked.  "
adam-zettafi,2023-07-14 20:23:12+00:00,comment,tiiuae/falcon-7b-instruct,"It's trying to load remote code for the tokenizer, but `trust_remote_code`  is `False`. Windows doesn't seem to handle `signal(14)`. Adding `trust_remote_code=True` to the `AutoTokenizer.from_pretrained` call should allow it to download the appropriate tokenizer."
CalebAtkins,2023-06-28 23:25:13+00:00,comment,tiiuae/falcon-7b-instruct,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
captainst,2023-07-07 06:49:42+00:00,comment,tiiuae/falcon-7b-instruct,"I do have a doubt about it. 
As stated in the model card, the fine-tuned dataset includes baize (https://github.com/project-baize/baize-chatbot)
However, this dataset is a GPL licensed one, and inside its readme, there is a clear <b>Commercial use is strictly prohibited. </b>

So theoretically, this model is <b>NOT</b> viable for commercial use."
vipulad,2023-06-28 00:08:49+00:00,comment,tiiuae/falcon-7b-instruct,Can you share the line of code where you load the model?
gtmanuj,2023-06-28 03:58:35+00:00,comment,tiiuae/falcon-7b-instruct,"from transformers import AutoTokenizer
import transformers
import torch
model = ""tiiuae/falcon-7b-instruct""
tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)

sequences = pipeline(
    ""Write a poem about Valencia"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)

for seq in sequences:
    print(f""Result: {seq['generated_text']}"")
"
vipulad,2023-06-28 04:29:43+00:00,comment,tiiuae/falcon-7b-instruct,"Strange! it works in my environment, try loading the model as 

model = AutoModelForCasualLM(""tiiuae/falcon-7b-instruct"", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=""auto"", )
"
gtmanuj,2023-06-28 06:02:59+00:00,comment,tiiuae/falcon-7b-instruct,What is the size of RAM your device has?
gtmanuj,2023-06-28 06:12:12+00:00,comment,tiiuae/falcon-7b-instruct,And can you please share the full code which worked.
vipulad,2023-06-28 06:13:56+00:00,comment,tiiuae/falcon-7b-instruct,"Only 16 GB, if you feel the issue is with RAM, try increasing the size of swap file (page file in windows).

However if RAM is the case, usually python process get terminated. Error you posted is related to identify the model type while loading it.  just a wild guess, try removing the downloaded model from the cache and run the script again"
vipulad,2023-06-28 06:16:16+00:00,comment,tiiuae/falcon-7b-instruct,"> And can you please share the full code which worked.

Sorry, I may not able to share the code. :( 
However I could able to run the code snippet you've shared 
"
gtmanuj,2023-06-28 06:23:29+00:00,comment,tiiuae/falcon-7b-instruct,"I also have 16 gb ram, the above code which you told, should have AutoModelForCausalLM.from_pretrained right?
 And I have already written the below in pipeline:
 torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=""auto""


"
gtmanuj,2023-06-28 06:24:58+00:00,comment,tiiuae/falcon-7b-instruct,Could you run the code snippet shared or you had to make some modifications
gtmanuj,2023-06-28 06:30:33+00:00,comment,tiiuae/falcon-7b-instruct,"I made the modification you suggested and I got the following error.

ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format."
vipulad,2023-06-28 06:39:02+00:00,comment,tiiuae/falcon-7b-instruct,"> Could you run the code snippet shared or you had to make some modifications

I ran the one you have shared.  Well I had to fix indentations (in the last line), other than that I did not do any change.
"
gtmanuj,2023-06-28 06:45:20+00:00,comment,tiiuae/falcon-7b-instruct,"Ok, did it give the expected output of a poem?"
vipulad,2023-06-28 06:46:34+00:00,comment,tiiuae/falcon-7b-instruct,Yep. It gave a nice poem 
gtmanuj,2023-06-28 06:49:21+00:00,comment,tiiuae/falcon-7b-instruct,"Which environment you used, I am using vs code"
vipulad,2023-06-28 06:52:23+00:00,comment,tiiuae/falcon-7b-instruct,"Well mine was on a cloud instance, access though the terminal. I do not think issue is related to the environment. 

let's see if some one else who has faced the same issue replies to this"
gtmanuj,2023-06-28 06:53:40+00:00,comment,tiiuae/falcon-7b-instruct,Okay Thanks
suryakumaran,2023-07-06 12:30:47+00:00,comment,tiiuae/falcon-7b-instruct,"> I made the modification you suggested and I got the following error.
> 
> ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.

I am getting the same error"
PenskeRichland,2023-07-10 17:36:33+00:00,comment,tiiuae/falcon-7b-instruct,"The model is over 16GB so you will need to figure out where the remainder can be stored. If you dont have GPU, then it must be stored on hard drive, so the error you are getting is saying that you need to identify some folder for it store the spillover from the model. "
bdambrosio,2023-06-24 01:55:30+00:00,comment,tiiuae/falcon-7b-instruct,"Pipeline works out of the box.
you can find that file, though, in the model folder (model_path), along with a couple of others you may need. just import them"
cr00,2023-06-30 04:31:41+00:00,comment,tiiuae/falcon-7b-instruct,I may be wrong but I believe they mentioned that this is due to some update needed in huggingface? but that it could be ignored
michaelomahony,2023-06-30 10:18:20+00:00,comment,tiiuae/falcon-7b-instruct,Set max_new_tokens = 1
ecorro,2023-06-23 19:21:43+00:00,comment,tiiuae/falcon-7b-instruct,Here the answer to my question https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14 
dartzi,2023-06-22 17:02:06+00:00,comment,tiiuae/falcon-7b-instruct,+1
etweedy,2023-07-02 16:13:21+00:00,comment,tiiuae/falcon-7b-instruct,"I've frequently seen the same behavior from this model.  I've been using this model with langchain, and my solution has been to pass in 'User' or '\nUser' as a stop token to my chain's predict method.  This is sort of a hack, which manually removes suffixes in your provided stop list from the end of the response before storing it (or adding it to chain memory, if using)."
f-dig,2023-07-13 09:25:41+00:00,comment,tiiuae/falcon-7b-instruct,"+1
"
samdwar,2023-08-18 13:12:17+00:00,comment,tiiuae/falcon-7b-instruct,"Having same issue, it seems the issue is with the Auto inference code. It is detecting correct stop token and stopping, but instead of omitting that token from output it is including it. I tested number of Falcon models and they all have same problem when using with TGI"
syber-nomad,2024-02-01 15:31:04+00:00,comment,tiiuae/falcon-7b-instruct,"For anyone else who ends up here, a band-aid fix for this is to write a custom output parser: https://www.mlexpert.io/prompt-engineering/chatbot-with-local-llm-using-langchain#cleaning-output"
makeColabFree,2023-06-22 07:15:23+00:00,comment,tiiuae/falcon-7b-instruct,It seems like there was an issue while converting according to huggingface's tutorials (https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors) with their spaces (https://huggingface.co/safetensors).
eloaf,2023-07-18 14:52:11+00:00,comment,tiiuae/falcon-7b-instruct,@ptrrrr I'm curious what your experience has been?
cr00,2023-06-26 14:35:24+00:00,comment,tiiuae/falcon-7b-instruct,"same, but I don't know where in the pipeline there is any repo id"
patti-j,2023-06-26 17:38:22+00:00,comment,tiiuae/falcon-7b-instruct,"I traced it back to the README file, of all things. HF appears to be validating README files. I was going to report it but haven't had a chance yet. In the meantime I altered my version of HF validation python file to omit this record."
cr00,2023-06-26 18:25:18+00:00,comment,tiiuae/falcon-7b-instruct,do you remember the name of the file you edited? thanks
patti-j,2023-06-26 20:01:46+00:00,comment,tiiuae/falcon-7b-instruct,"Yes, it's validate.py in the 
C:\Python311\Lib\site-packages\huggingface_hub\utils\_validators.py 

This is the offending snippet:
if not REPO_ID_REGEX.match(repo_id):
        raise HFValidationError(
            ""Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are""
            "" forbidden, '-' and '.' cannot start or end the name, max length is 96:""
            f"" '{repo_id}'.""
        )

I just did a quick-and-dirty to bypass this passage if repo_id = 'RWModel('

Ideally, we should put a bit of code at the top, or better yet in the calling script, to exclude README.md files from validation."
Karthik1611,2023-06-28 09:02:57+00:00,comment,tiiuae/falcon-7b-instruct,"I got the same error and what worked for me was this. Instead of passing in the model variable add the model path in tokenizer.

tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-7b-instruct"")"
cr00,2023-06-30 03:17:15+00:00,comment,tiiuae/falcon-7b-instruct,"@Karthik1611 what did you do for this line: model=AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)
"
Karthik1611,2023-06-30 09:01:10+00:00,comment,tiiuae/falcon-7b-instruct,"Same as tokenizer, I passed the model path directly instead of the model variable."
Shivaraj,2023-07-31 14:24:35+00:00,comment,tiiuae/falcon-7b-instruct,"I am facing the same issue, The work around I have is I put cosine similarity cut-off for query and available snippets any question out of context is asked cosine similarity will less, hence I don't even trigger the LLM. "
patonw,2023-06-14 22:22:58+00:00,comment,tiiuae/falcon-7b-instruct,"Slow for me also, on a RTX3090. Orders of magnitude slower than other 7B models I've tried.
After warming up, other models summarize an article in 2 to 10 seconds. Falcon takes about 2 minutes for the same article.
I double checked that it's using the GPU and tried running a quantized version, but still slow."
Sven00,2023-06-16 07:40:17+00:00,comment,tiiuae/falcon-7b-instruct,@patonw can you please let me know  which prompt/parameter you are using for summarization task? i'm struggling with 7 B models to get a more or less stable and factual correct summary. thank you
HAvietisov,2023-06-17 11:11:21+00:00,comment,tiiuae/falcon-7b-instruct,@patonw aren't quantized models always slow in comparison to models in float16?
treeguard,2023-06-19 04:17:01+00:00,comment,tiiuae/falcon-7b-instruct,It's really slow for me also
patonw,2023-06-19 19:09:13+00:00,comment,tiiuae/falcon-7b-instruct,"@Sven00 I didn't any official examples for summarization prompts either, but through trial and error I found this works fairly well:  
```
INSTRUCTIONS:
You are a political analyst for a national newspaper.
Only refer to the provided text and no other sources.
Summarize 5 key facts from the following text as a numbered list.

TEXT:
###
{text}
###

SUMMARY:
```

However, the model neither numbers items or counts correctly

@HAvietisov Running quantized is slightly faster for this model on my hardware at least, but not by much."
HAvietisov,2023-06-19 19:16:09+00:00,comment,tiiuae/falcon-7b-instruct,"@patonw what hardware you use and what quantization method?
I run int8 quantization via bitsandbytes, with dequantization to float16 on single rtx 3090"
rustamg,2023-06-21 11:42:36+00:00,comment,tiiuae/falcon-7b-instruct,"Changing ``torch_dtype=torch.bfloat16`` to ``torch_dtype=torch.float16`` in the [Getting Started code snippet](https://huggingface.co/tiiuae/falcon-7b-instruct#how-to-get-started-with-the-model) (removing the ""b"" before ""float"") led to a significant speedup on a 16GB vRAM NC4as-v3 machine in databricks running the falcon-7b-instruct model. Hope this helps others, too."
michaelomahony,2023-07-10 09:33:24+00:00,comment,tiiuae/falcon-7b-instruct,"@rustamg thanks for sharing! Any idea how much of a drop in accuracy this could cause?
"
Jenny2020,2023-09-01 05:40:37+00:00,comment,tiiuae/falcon-7b-instruct,usually How long it takes for warmup steps finish for fully finetune? 
michaelomahony,2023-06-15 11:53:32+00:00,comment,tiiuae/falcon-7b-instruct,"I am also having a similar issue where the 8-bit Falcon-7b-instruct model will not generate anything. Even the 'poem about Valencia' example from their article (https://huggingface.co/blog/falcon#inference) does not work. There is no error code, it just returns the prompt with no generated text. The regular Falcon-7b-instruct model works fine for me but is very slow."
michaelomahony,2023-06-19 08:00:08+00:00,comment,tiiuae/falcon-7b-instruct,"> I am also having a similar issue where the 8-bit Falcon-7b-instruct model will not generate anything. Even the 'poem about Valencia' example from their article (https://huggingface.co/blog/falcon#inference) does not work. There is no error code, it just returns the prompt with no generated text. The regular Falcon-7b-instruct model works fine for me but is very slow.

My issue was with the 8-bit model which was loaded from locally saved files, when I load it from the hub it works fine. There must be some issue with how Huggingface saves the 8-bit model."
ccasimiro,2023-06-13 10:21:04+00:00,comment,tiiuae/falcon-7b-instruct,"Hi, as requested by @zerolyn, would be very useful to have the exact prompt used for fine-tuning to make the generation more reliable and robust. Thanks a lot for your contribution to the open-source!"
kronus86,2023-06-19 21:04:11+00:00,comment,tiiuae/falcon-7b-instruct,Can we please get the prompt used for fine-tuning.
michaelomahony,2023-07-10 09:52:41+00:00,comment,tiiuae/falcon-7b-instruct,Bump
thewilltejeda,2023-06-09 01:08:36+00:00,comment,tiiuae/falcon-7b-instruct,"Same issue here 

I'm on a 16"" M1 Pro macbook 16GB RAM 16Core GPU , 

Python3.9.2

```

Traceback (most recent call last):
  File ""/Users/__/Code/FalconLLM/./main.py"", line 11, in <module>
    pipeline = transformers.pipeline(
               ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/__init__.py"", line 788, in pipeline
    framework, model = infer_framework_load_model(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/base.py"", line 278, in infer_framework_load_model
    raise ValueError(f""Could not load model {model} with any of the following classes: {class_tuple}."")
ValueError: Could not load model tiiuae/falcon-7b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).
```"
ihendley,2023-06-09 02:14:49+00:00,comment,tiiuae/falcon-7b-instruct,I was able to load it and generate text on my 64GB M1 Max after upgrading torch to the latest 2.01 via `pip install --upgrade torch` and then changing `torch_dtype=torch.bfloat16` to `torch_dtype=torch.float32` in the pipeline. However the generation was extremely slow.
Ibrahim-Ola,2023-06-09 16:55:31+00:00,comment,tiiuae/falcon-7b-instruct,"> I was able to load it and generate text on my 64GB M1 Max after upgrading torch to the latest 2.01 via `pip install --upgrade torch` and then changing `torch_dtype=torch.bfloat16` to `torch_dtype=torch.float32` in the pipeline. However the generation was extremely slow.

I changed ```torch_dtype=torch.bfloat16``` to  ```torch_dtype=torch.float32```, but I still get the same error. My torch is the latest. I am on 16GB RAM, though."
therealcyberlord,2023-06-12 14:47:52+00:00,comment,tiiuae/falcon-7b-instruct,having the same issue as well
prasantapanja,2023-06-12 20:43:06+00:00,comment,tiiuae/falcon-7b-instruct,Facing same issue. Running on Mac M1. Can it be due to low memory as I'm using 8 GB RAM?
manuelrech,2023-06-13 11:07:40+00:00,comment,tiiuae/falcon-7b-instruct,"Same issue here, I was using it smoothly and suddenly it threw this error, no changes, no upgrades, downgrades."
nessian,2023-06-15 19:51:49+00:00,comment,tiiuae/falcon-7b-instruct,I am on Mac M1 as well and same issue
Ibrahim-Ola,2023-06-15 19:59:22+00:00,comment,tiiuae/falcon-7b-instruct,"> I'm getting the following the error below when I try to load my model on Ubuntu and MacOS (i7, 2018)
> 
> ```ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```
> 
> Note: all my packages are up to date. My pipeline is:
> 
> model = ""tiiuae/falcon-7b-instruct"" 
> 
> tokenizer = AutoTokenizer.from_pretrained(model)
> 
> pipeline = pipeline(
>     task=""text-generation"", 
>     model=model,
>     tokenizer=tokenizer,
>     torch_dtype=torch.bfloat16,
>     trust_remote_code=True,
>     device_map=""auto"",
>     max_length=200,
>     do_sample=True,
>     top_k=10,
>     num_return_sequences=1,
>     eos_token_id=tokenizer.eos_token_id
> )
> 
> Any help is appreciated.
> 
> Thanks!

I found a way to make it work:

```
from transformers import AutoModelForCausalLM

model_id=""tiiuae/falcon-7b-instruct""
tokenizer=AutoTokenizer.from_pretrained(model_id)
model=AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
```"
vpkprasanna,2023-08-10 08:15:20+00:00,comment,tiiuae/falcon-7b-instruct,"> I'm getting the following the error below when I try to load my model on Ubuntu and MacOS (i7, 2018)
> 
> ```ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).```
> 
> Note: all my packages are up to date. My pipeline is:
> 
> model = ""tiiuae/falcon-7b-instruct"" 
> 
> tokenizer = AutoTokenizer.from_pretrained(model)
> 
> pipeline = pipeline(
>     task=""text-generation"", 
>     model=model,
>     tokenizer=tokenizer,
>     torch_dtype=torch.bfloat16,
>     trust_remote_code=True,
>     device_map=""auto"",
>     max_length=200,
>     do_sample=True,
>     top_k=10,
>     num_return_sequences=1,
>     eos_token_id=tokenizer.eos_token_id
> )
> 
> Any help is appreciated.
> 
> Thanks!
Even i got the same error but when i specify device manually as ""cuda"" in device_map parameter it starts to load the model . try this method once "
FalconLLM,2023-06-09 14:17:22+00:00,comment,tiiuae/falcon-7b-instruct,The model requires ~16GB of memory to run comfortably. You could have a look at [FalconTune](https://github.com/rmihaylov/falcontune) to use a 4-bit version of the model.  
FalconLLM,2023-06-09 14:16:33+00:00,comment,tiiuae/falcon-7b-instruct,You should need ~16GB of memory to run the 7B model comfortably. 
Tsovinar,2023-06-16 21:27:22+00:00,comment,tiiuae/falcon-7b-instruct,"if our ram is 8gb it won't run?
"
designfailure,2023-06-03 13:26:58+00:00,comment,tiiuae/falcon-7b-instruct,"ChatGPT suggest this:

To save and load models using the Hugging Face's Transformers library, you might want to save both the model and the tokenizer, not the pipeline, as they are the primary components. Here's how to do it:

To save:

```python
tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model)

tokenizer.save_pretrained(""./model_path/"")
model.save_pretrained(""./model_path/"")
```

To load:

```python
tokenizer = AutoTokenizer.from_pretrained(""./model_path/"")
model = AutoModelForCausalLM.from_pretrained(""./model_path/"")

pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    device_map=""auto"",
)
```

In this way, you don't need to download the model each time you run your script, and it should resolve the issues you are encountering with the `trust_remote_code=True` setting.

#designfailure"
kycrowe,2023-06-05 14:22:21+00:00,comment,tiiuae/falcon-7b-instruct,"Thank you @designfailure 
I tried loading model and tokenizer separately like below
> ```python
> tokenizer = AutoTokenizer.from_pretrained(model)
> model = AutoModelForCausalLM.from_pretrained(model)
> ```

But getting the require execute the configuration file again on the line of loading model:
```
ValueError: Loading tiiuae/falcon-7b-instruct requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.
```"
rq-aszasz,2023-06-08 13:00:40+00:00,comment,tiiuae/falcon-7b-instruct,https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/10 flags the same issue when running on AWS Sagemaker.
mwgupta,2023-08-05 01:16:21+00:00,comment,tiiuae/falcon-7b-instruct,"> Thank you @designfailure 
> I tried loading model and tokenizer separately like below
> > ```python
> > tokenizer = AutoTokenizer.from_pretrained(model)
> > model = AutoModelForCausalLM.from_pretrained(model)
> > ```
> 
> But getting the require execute the configuration file again on the line of loading model:
> ```
> ValueError: Loading tiiuae/falcon-7b-instruct requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.
> ```

What was the fix in the end?"
cazz1,2023-09-02 05:08:18+00:00,comment,tiiuae/falcon-7b-instruct,I am encountering the same problem. What was the fix plz?
alflur,2023-09-04 20:53:52+00:00,comment,tiiuae/falcon-7b-instruct,hello ! same problem too. Someone have a fix ?Thanks
Khyn,2023-09-25 02:49:24+00:00,comment,tiiuae/falcon-7b-instruct,"> Thank you @designfailure 
> I tried loading model and tokenizer separately like below
> > ```python
> > tokenizer = AutoTokenizer.from_pretrained(model)
> > model = AutoModelForCausalLM.from_pretrained(model)
> > ```
> 
> But getting the require execute the configuration file again on the line of loading model:
> ```
> ValueError: Loading tiiuae/falcon-7b-instruct requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.
> ```

hello ! same problem too.What was the fix in the end? Thanks

"
aovalle,2023-10-18 12:51:08+00:00,comment,tiiuae/falcon-7b-instruct,"I had the same issue, resolved it with updating to transformers==4.34.0

"
rjalexa,2024-04-16 09:36:14+00:00,comment,tiiuae/falcon-7b-instruct,Transformers 4.39.3 and I still have the same problem :(
sardodazione,2023-12-07 00:10:11+00:00,comment,tiiuae/falcon-7b-instruct,Changing bfloat to float worked for me too... thanks!
eastwind,2023-06-02 14:39:48+00:00,comment,tiiuae/falcon-7b-instruct,The repo structure seems to differ from the 40b repo. The configuration_RW.py seems to be missing code?
bernardogmorais,2023-06-05 09:29:13+00:00,comment,tiiuae/falcon-7b-instruct,Got the same problem. Were you able to solve it?
eastwind,2023-06-05 09:30:38+00:00,comment,tiiuae/falcon-7b-instruct,No not yet. 40b seems to work o I'm using that
k998,2023-06-06 05:47:11+00:00,comment,tiiuae/falcon-7b-instruct,I am also getting the same error for 7b and 7b-instruct model
Nordlund,2023-06-06 13:03:46+00:00,comment,tiiuae/falcon-7b-instruct,+1
eschu,2023-06-09 07:29:41+00:00,comment,tiiuae/falcon-7b-instruct,+1
YaliLLM,2023-06-20 00:47:14+00:00,comment,tiiuae/falcon-7b-instruct,"+1
"
taurasAI,2023-06-28 08:52:52+00:00,comment,tiiuae/falcon-7b-instruct,"Try upgrading package from transformers to latest version
I upgraded transformers to 4.30.2 and this issues is resolved
P.S torch=2.0.1"
aditunoe,2023-06-12 16:06:29+00:00,comment,tiiuae/falcon-7b-instruct,"I have the same issue, at least in combination with LangChain the mdel tends to only ouput a few Tokens and than just stops in the middle of the sentence.

Would be nice to know if we are just doing something wrong or its just the way this model works?"
aditunoe,2023-06-13 13:58:55+00:00,comment,tiiuae/falcon-7b-instruct,"I found our mistake. @domid10  you need to add max_new_tokens and set it higher to get better results.
Example:
```python
llm = HuggingFaceEndpoint(
            endpoint_url= ""https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct"",
            huggingfacehub_api_token=HUGGINFACE_KEY,
            task=""text-generation"",
            model_kwargs = {
                ""temperature"":0.2,
                ""max_new_tokens"":400,
                ""num_return_sequences"":1
            }
        )
```"
ajmalsiddiqui,2023-06-13 15:36:54+00:00,comment,tiiuae/falcon-7b-instruct,"Hello All, I am interested to know falcon performance benchmarking on A100 and T4. I will be thankful if someone can share the inference statistics.
a) GPU type
b) Average inference time per request"
domid10,2023-06-15 12:04:21+00:00,comment,tiiuae/falcon-7b-instruct,@aditunoe Thank you! This should really be added to the docs.
Lusp,2023-06-06 17:59:06+00:00,comment,tiiuae/falcon-7b-instruct,"Same issue here, trying various messageEndToken values, prompts etc does not really help...."
hariprasad0994,2023-06-09 20:56:31+00:00,comment,tiiuae/falcon-7b-instruct,Any updates on this thread please?
yi1,2023-06-13 07:49:06+00:00,comment,tiiuae/falcon-7b-instruct,"Check out the special tokens near the top of the tokenizer.json file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types you can play with. I've had the best results prompting it like this:

>>QUESTION<<In Python, I want to write a simple HTTP API that receives an object via POST and responds with another object. The request object contains a string prompt, float temperature, and int max_tokens. The response object contains a string response, int prompt_tokens, int completion_tokens. For now just set hard-coded values and get the response out. I'll add the logic myself afterward.
>>ANSWER<<"
yi1,2023-06-13 07:53:31+00:00,comment,tiiuae/falcon-7b-instruct,"QUESTION<<: who are you?
ANSWER<<: I am the one who is always there for you."
shaswatamitra,2024-03-23 00:50:17+00:00,comment,tiiuae/falcon-7b-instruct,"I'm also having the same issue. After answering the question. It keeps generating question and answering on its own.  >>QUESTION<<, >>ANSWER<<, technique is not working for me.
"
eastwind,2023-06-02 09:51:45+00:00,comment,tiiuae/falcon-7b-instruct,you need pytorch 2.x
FalconLLM,2023-06-01 03:08:18+00:00,comment,tiiuae/falcon-7b-instruct,"It's apache-2.0! Thanks for letting us now, we'll make sure to remove any remnants of the old license during the day."
FalconLLM,2023-06-09 14:13:27+00:00,comment,tiiuae/falcon-7b-instruct,"Given that Falcon models are autogressive decoder-only models, they will produce embeddings that are subpar compared to state-of-the-art embedding models (e.g., [E5](https://huggingface.co/intfloat/e5-large) or [Instructor](https://huggingface.co/hkunlp/instructor-large))"
rq-aszasz,2023-06-08 12:58:01+00:00,comment,tiiuae/falcon-7b-instruct,"I reproduced this error too.

I also tried setting the `container_startup_health_check_timeout` parameter to 1 hour, it didn't help."
rq-aszasz,2023-06-08 18:10:51+00:00,comment,tiiuae/falcon-7b-instruct,"Update:

Try with `sagemaker>=2.163.0`."
FalconLLM,2023-06-09 14:12:12+00:00,comment,tiiuae/falcon-7b-instruct,"You can leverage the HuggingFace LLM Inference Container, as documented [here](https://www.philschmid.de/sagemaker-falcon-llm), to deploy Falcon easily in SageMaker."
FalconLLM,2023-05-30 08:01:45+00:00,comment,tiiuae/falcon-7b-instruct,"Training multilingual models without degradation in mono-lingual model performance is rather tricky. It might be something we decide to tackle in the future, but I wouldn't expect something very soon. 
"
Owos,2024-01-12 20:06:24+00:00,comment,tiiuae/falcon-7b-instruct,could you suggest a recipe for finetuning in order to make it multilingual?
kendreaditya,2023-06-18 11:59:21+00:00,comment,tiiuae/falcon-7b-instruct,Would it be possible to provide the script to convert the model into coreml? Wanted to try and convert `falcon-rw-1b` to coreml as well.
Sasanou,2023-05-28 16:22:33+00:00,comment,tiiuae/falcon-7b-instruct,Same here - also on a MB M1 Pro
CleverShovel,2023-05-28 17:55:56+00:00,comment,tiiuae/falcon-7b-instruct,"Perhaps you need to install the latest versions of libraries from github. There is an example how to install them. Probably you need not all of them, but maybe useful
```
pip install -q -U bitsandbytes
pip install -q -U git+https://github.com/huggingface/transformers.git 
pip install -q -U git+https://github.com/huggingface/peft.git
pip install -q -U git+https://github.com/huggingface/accelerate.git
pip install -q einops
```"
aviadatlas,2023-05-28 18:51:07+00:00,comment,tiiuae/falcon-7b-instruct,"@CleverShovel Thanks, I did this, but it didn't solve it. Oddly enough though the error just changed a bit:

│ transformers/pipelines/base.py:278 in infer_framework_load_model             │
│                                                                              │
│    275 │   │   │   │   continue                                              │
│    276 │   │                                                                 │
│    277 │   │   if isinstance(model, str):                                    │
│ ❱  278 │   │   │   raise ValueError(f""Could not load model {model} with any  │
│    279 │                                                                     │
│    280 │   framework = infer_framework(model.__class__)                      │
│    281 │   return framework, model                                           │
╰──────────────────────────────────────────────────────────────────────────────╯
ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the 
following classes: (<class 
'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).



I also tried running it on Colab and get the same issue:
```
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ in <cell line: 8>:8                                                                              │
│                                                                                                  │
│ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788 in pipeline       │
│                                                                                                  │
│   785 │   # Forced if framework already defined, inferred if it's None                           │
│   786 │   # Will load the correct model if possible                                              │
│   787 │   model_classes = {""tf"": targeted_task[""tf""], ""pt"": targeted_task[""pt""]}                 │
│ ❱ 788 │   framework, model = infer_framework_load_model(                                         │
│   789 │   │   model,                                                                             │
│   790 │   │   model_classes=model_classes,                                                       │
│   791 │   │   config=config,                                                                     │
│                                                                                                  │
│ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:278 in                    │
│ infer_framework_load_model                                                                       │
│                                                                                                  │
│    275 │   │   │   │   continue                                                                  │
│    276 │   │                                                                                     │
│    277 │   │   if isinstance(model, str):                                                        │
│ ❱  278 │   │   │   raise ValueError(f""Could not load model {model} with any of the following cl  │
│    279 │                                                                                         │
│    280 │   framework = infer_framework(model.__class__)                                          │
│    281 │   return framework, model                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 
'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 
'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).
```"
CleverShovel,2023-05-28 19:47:10+00:00,comment,tiiuae/falcon-7b-instruct,"@aviadatlas Yeah, it is strange, maybe some problem with transformers.pipeline. 
I managed to launch it on Colab, here is [notebook](https://colab.research.google.com/drive/1cUmmXgIfTz61F1sCotjDV6nsyfGk1ScA?usp=sharing). I used sharded version of this model otherwise it can't be loaded in free tier Colab."
chilesaltlime,2023-05-28 21:06:27+00:00,comment,tiiuae/falcon-7b-instruct,"I'm having the same exact issue as OP.

It would help if there were a requirements.txt or environment.yaml or dockerfile so we are not just guessing when it comes to library versioning, especially in such a fast-moving space as LLMs. Without version numbers, the dependencies are almost guaranteed to not be in sync with pip / conda (even if you can figure out what  dependencies are needed by getting pip errors over and over)."
chilesaltlime,2023-05-28 21:08:50+00:00,comment,tiiuae/falcon-7b-instruct,I got this to work by running `pip install -U transformers`. I'd used conda to install transformers originally.
shri1510,2023-05-30 07:08:30+00:00,comment,tiiuae/falcon-7b-instruct,I'm still getting the same error. Does anybody have a solution for this?
vishaal27,2023-05-31 21:30:52+00:00,comment,tiiuae/falcon-7b-instruct,Me too!
celcov,2023-06-01 13:42:10+00:00,comment,tiiuae/falcon-7b-instruct,"For me it was a memory error, the way I solved this was by exporting TRANSFORMERS_CACHE to a different location"
vishaal27,2023-06-01 13:51:04+00:00,comment,tiiuae/falcon-7b-instruct,Can someone who it is working for please share an conda env.yaml or requirements.txt file reproducing their exact config so that we can try to use that setup to load the model and run it? This would be really helpful! :)
chpatel2,2023-06-01 20:47:49+00:00,comment,tiiuae/falcon-7b-instruct,Getting the same error. Hard to work with this stuff without versions of dependencies.
Sasanou,2023-06-04 14:19:21+00:00,comment,tiiuae/falcon-7b-instruct,"Hello Thanks for your help all, I Get the below error now, can anyone please help ?

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:01<00:00, 30.80s/it]
Traceback (most recent call last):
  File ""/Users/clustered/Python Application/02. Falcon_Model/Falcon-Application.py"", line 10, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model)
  File ""/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 643, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File ""/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 487, in get_tokenizer_config
    resolved_config_file = cached_file(
  File ""/Users/clustered/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py"", line 417, in cached_file
    resolved_file = hf_hub_download(
  File ""/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py"", line 112, in _inner_fn
  File ""/Users/clustered/miniconda3/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg/huggingface_hub/utils/_validators.py"", line 166, in validate_repo_id
huggingface_hub.utils._validators.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'RWForCausalLM(
  (transformer): RWModel(
    (word_embeddings): Embedding(65024, 4544)
    (h): ModuleList(
      (0-31): 32 x DecoderLayer(
        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)
        (self_attention): Attention(
          (maybe_rotary): RotaryEmbedding()
          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)
          (dense): Linear(in_features=4544, out_features=4544, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): MLP(
          (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)
          (act): GELU(approximate='none')
          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)
        )
      )
    )
    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)"
Sasanou,2023-06-05 13:37:10+00:00,comment,tiiuae/falcon-7b-instruct,Can anyone please help 9n this ? :)
Sasanou,2023-06-07 09:11:41+00:00,comment,tiiuae/falcon-7b-instruct,Anyone?
aviadatlas,2023-06-07 11:18:35+00:00,comment,tiiuae/falcon-7b-instruct,"So I managed to make the sample code work on Colab at least. I just needed to have the GPU enabled via ""Runtime -> Change runtime type"""
FalconLLM,2023-06-09 14:11:06+00:00,comment,tiiuae/falcon-7b-instruct,It seems like the issue here has been solved -- but feel free to also check-out this great [blogpost](https://huggingface.co/blog/falcon) from HuggingFace on using Falcon. 
taurasAI,2023-06-28 08:56:58+00:00,comment,tiiuae/falcon-7b-instruct,"Try upgrading package from transformers to latest version
I upgraded transformers to 4.30.2 and this issues is resolved
P.S torch=2.0.1"
CleverShovel,2023-05-28 12:36:20+00:00,comment,tiiuae/falcon-7b-instruct,"Hi! 

I'm not an author of this model but I sharded this model, you can check it [here](https://huggingface.co/CleverShovel/falcon-7b-instruct-sharded-bf16). 
And I did it using free tier Colab environment with no GPUs, in this environment Colab give you enough RAM to load models up to 7B."
FalconLLM,2023-06-09 14:09:37+00:00,comment,tiiuae/falcon-7b-instruct,"There has been some support for 4-bit in a great external library, [FalconTune](https://github.com/rmihaylov/falcontune). You can also check-out this [blog post](https://huggingface.co/blog/falcon) from HuggingFace."
vrsen,2023-06-14 16:56:15+00:00,comment,tiiuae/falcon-7b-instruct,"Hey, check out my video on how to fine tune and use instruct on a single gpu in free google colab: https://youtu.be/AXG7TA7vIQ8"
its-eric-liu,2023-09-06 22:55:36+00:00,comment,tiiuae/falcon-7b-instruct,"---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-13-5723accdc0f3> in <cell line: 10>()
      8 
      9 tokenizer = AutoTokenizer.from_pretrained(model)
---> 10 pipeline = transformers.pipeline(
     11     ""text-generation"",
     12     model=model,

1 frames
/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py in infer_framework_load_model(model, config, model_classes, task, framework, **model_kwargs)
    280             for class_name, trace in all_traceback.items():
    281                 error += f""while loading with {class_name}, an error is thrown:\n{trace}\n""
--> 282             raise ValueError(
    283                 f""Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\n\n{error}\n""
    284             )

ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:

while loading with AutoModelForCausalLM, an error is thrown:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py"", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py"", line 558, in from_pretrained
    return model_class.from_pretrained(
  File ""/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py"", line 3165, in from_pretrained
    ) = cls._load_pretrained_model(
  File ""/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py"", line 3286, in _load_pretrained_model
    raise ValueError(
ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.

while loading with TFAutoModelForCausalLM, an error is thrown:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py"", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py"", line 566, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers_modules.tiiuae.falcon-7b-instruct.eb410fb6ffa9028e97adb801f0d6ec46d02f8b07.configuration_RW.RWConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.
Model type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.
"
TheBloke,2023-05-28 15:05:40+00:00,comment,tiiuae/falcon-7b-instruct,"One thing to point out is that my comment related to the GPTQ 4 bit version, so there's also AutoGPTQ code in play there as well.  So it may not be representative of the base Falcon experience.

Also since I wrote that I did eventually see some VRAM increase, in some situations anyway. I've not had time to analyse it yet.

One issue we're experiencing with the AutoGPTQ model is it's really, really slow. Which again I expect is something to do with the AutoGPTQ code, or the interaction between the AutoGPTQ and Falcon code."
FalconLLM,2023-05-30 06:57:29+00:00,comment,tiiuae/falcon-7b-instruct,"Due to multi-query attention the model only needs to save one k,v per position instead of num_heads, for the 7b this corresponds to a factor of 71 :)"
Yhyu13,2023-06-01 12:59:50+00:00,comment,tiiuae/falcon-7b-instruct,@FalconLLM Thanks for giving insights! Would be excited to see more details in the paper. Paper still not out yet?
dfurman,2023-05-27 20:11:34+00:00,comment,tiiuae/falcon-7b-instruct,I got the same error
Imran1,2023-05-28 00:43:24+00:00,comment,tiiuae/falcon-7b-instruct,"> I got the same error

I solve the error but the model performance are very bad. I don't know why every model show this type of behavior."
Roy-Shih,2023-05-31 19:08:34+00:00,comment,tiiuae/falcon-7b-instruct,Set 'return_token_type_ids=False' in tokenizer()
ottonemo,2023-06-08 16:14:06+00:00,comment,tiiuae/falcon-7b-instruct,"If you, for some reason, don't have control over the `generate`-call but are able to pass down or modify the tokenizer, you can patch `_call_one` on the tokenizer:

```python
from functools import wraps

org_call_one = tokenizer._call_one

@wraps(org_call_one)
def _call_one_wrapped(*x, **y):
    y['return_token_type_ids'] = False
    return org_call_one(*x, **y)

tokenizer._call_one = _call_one_wrapped
```

This forces the tokenizer to never return token type ids."
bartowski,2024-05-24 16:39:56+00:00,comment,mistralai/Mistral-7B-v0.1,"Those two values aren't related, the sliding window refers to attention context and is related to the max_position_embeddings"
keval-sha,2024-05-24 18:25:22+00:00,comment,mistralai/Mistral-7B-v0.1,"Yeah, that makes sense. I am wondering why are they both exactly 4096? Interesting architecture choices.  "
View4900,2024-06-01 04:12:02+00:00,comment,mistralai/Mistral-7B-v0.1," I got same error too, running locally on vscode ide. Hope someone help."
Rushi5261,2024-04-27 15:33:22+00:00,comment,mistralai/Mistral-7B-v0.1,same error
rimrimrim0203,2024-04-28 16:00:57+00:00,comment,mistralai/Mistral-7B-v0.1,me too
Borko24,2024-04-29 21:53:02+00:00,comment,mistralai/Mistral-7B-v0.1,"same
"
Rushi5261,2024-04-29 22:30:01+00:00,comment,mistralai/Mistral-7B-v0.1,"Solution:

Go to the files and version section of mistral on huggingface -> accept the agreement -> login huggingface into your notebook -> done
it might take a few minutes to reflect, but it worked for me"
Tecena,2024-04-30 05:59:45+00:00,comment,mistralai/Mistral-7B-v0.1,"I am using AWS sagemaker woth proper IAM permissions to access Mistral-7B-v0.1 model, everything was working fine, but from last 2 days when I am trying to run the fine tuning on the model, I am getting the same error mentioned above,

Since I am using sagemaker, it has all permissions to access the model. Now suddenly getting access issue, 

Please help"
Borko24,2024-04-30 14:14:31+00:00,comment,mistralai/Mistral-7B-v0.1,"You can add as parameter ""token= <your-token>"" to "".from_pretrained(... , token = <your-token>)"". You can generate one through Settings->Access Tokens (in the menu on the left)"
Tecena,2024-05-02 08:02:16+00:00,comment,mistralai/Mistral-7B-v0.1,"Thank you for the response @Borko24 
But the aws sagemaker has all the permissions to access this model since I was using without any token or credentials earlier with the tag mistralai/Mistral-7B-v0.1. And finetuning was working well. But now suddenly asking for token. Showing error as you are trying to access gated repository.  

Anything changed w.r.t model in hugging face . Please help, my work has been paused..
Thank you "
flyskywalkerlby,2024-04-25 18:16:22+00:00,comment,mistralai/Mistral-7B-v0.1,"I encountered the same problem.
The same thing happened even after I finetuned the model.
So now I can only use regex to process the response.

```python
            prompts = batch[""prompt""]

            inputs = tokenizer(prompts, padding=""max_length"", max_length=512, return_tensors=""pt"")
            inputs = {k: v.to(device) for k, v in inputs.items()}

            generated_ids = ft_model.generate(**inputs, max_new_tokens=256, do_sample=True,
                                              pad_token_id=tokenizer.eos_token_id)

            decoded = tokenizer.batch_decode(generated_ids)
```

Have you found solutions?"
psneto,2024-04-25 19:28:17+00:00,comment,mistralai/Mistral-7B-v0.1,"I haven't found any much better solution than creating my own Stopping Criteria to stop the model on more than one token. But still, it's a somewhat flawed heuristic. I followed this discussion to create it: [Here](https://discuss.huggingface.co/t/implimentation-of-stopping-criteria-list/20040/13).

For answers that require less Reasoning, it even generates the ```</s>```, but when coupled in a question and answer template, it often repeats the question and only in a few ways does it generate the ```</s>```. I also tested the instruct version, and it is actually much better for this."
Hiteshkumar-Desai,2024-04-20 14:15:07+00:00,comment,mistralai/Mistral-7B-v0.1,"is there any solution for this error, I'm getting the same error too. Below is my code snippet

Code
-------------------------
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = ""mistralai/Mistral-7B-v0.1""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4""
)
model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)

Error
---------------------------
KeyError                                  Traceback (most recent call last)
Cell In[7], line 12
      6 base_model_id = ""mistralai/Mistral-7B-v0.1""
      7 bnb_config = BitsAndBytesConfig(
      8     load_in_4bit=True,
      9     bnb_4bit_use_double_quant=True,
     10     bnb_4bit_quant_type=""nf4""
     11 )
---> 12 model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)

File ~/my_projects/mistral_finetune/mistral_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:456, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    453 if kwargs.get(""torch_dtype"", None) == ""auto"":
    454     _ = kwargs.pop(""torch_dtype"")
--> 456 config, kwargs = AutoConfig.from_pretrained(
    457     pretrained_model_name_or_path,
    458     return_unused_kwargs=True,
    459     trust_remote_code=trust_remote_code,
    460     **hub_kwargs,
    461     **kwargs,
    462 )
    464 # if torch_dtype=auto was passed here, ensure to pass it on
    465 if kwargs_orig.get(""torch_dtype"", None) == ""auto"":

File ~/my_projects/mistral_finetune/mistral_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:957, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
...
--> 671     raise KeyError(key)
    672 value = self._mapping[key]
    673 module_name = model_type_to_module_name(key)

KeyError: 'mistral'"
Tecena,2024-04-30 06:01:33+00:00,comment,mistralai/Mistral-7B-v0.1,Hi the issue is solved for me by updating transformer library to  4.38.0.
makvoid,2024-04-06 12:13:27+00:00,comment,mistralai/Mistral-7B-v0.1,"The error you got seems to be memory-related. A great tool to use is the [Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage), you can provide the model ID, and select the different levels of model precision to see what the memory requirements are:

|Precision Level|Estimated Memory Requirement|
|---|---|
|float32 (Default)|27.49GB|
|float16|13.74GB|
|int8|6.87 GB|
|int4|3.44 GB|

LLama.cpp seems to use 4-bit quantization often, so this allows the model which normally takes ~27.5GB of memory to run to only take ~3.5GB of memory. If you want to use 4bit as well, you can still use the transformers library but you can use [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) / [accelerate](https://github.com/huggingface/accelerate)  to accomplish this.

If `bitsandbytes` / `accelerate` do not work for you, you can also try to use this model instead: `unsloth/mistral-7b-v0.2-bnb-4bit` (v0.1 is available too)

```shell
pip install -q -U bitsandbytes transformers peft accelerate
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
  ""mistralai/Mistral-7B-v0.1"",
  quantization_config=quantization_config,
  device_map=""auto""
)
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-v0.1"")

# ...
```

**Edit:** Updated the code provided, it was giving me some issues with that model class."
namantjeaswi,2024-04-06 18:51:57+00:00,comment,mistralai/Mistral-7B-v0.1,"Hello 

Thank you for your response, you are indeed correct , I was working with 16gb  ram and default 32 bit precision model which is not sufficient. I also tried out bitsandbytes / accelerate and it works, I just want to know is there any benefit or loss if we use  bitsandbytes / accelerate over using gguf models with llama cpp for same precision. 

I would think with  using bitsandbytes / accelerate I would be able access more models but we have few gguf models. I am trying to build a unified codebase where I can compare the performance different open source models in my rag pipeline and see if I can get close to gpt 4 and find an optimum point for inference time, size and response quality. "
makvoid,2024-04-06 20:36:21+00:00,comment,mistralai/Mistral-7B-v0.1,"I would benchmark the two with your use case and see how it performs in terms of speed, cost, and model accuracy. It's tough to give a definite answer, as one may work better for me that doesn't work better for you.

I do agree that using `bitsandbytes` would be much more flexible, as you can adjust precision very quickly as well as use models that someone (or yourself) has not quantized and posted publicly. Best of luck with your project!"
nabeelraza,2024-05-06 06:00:33+00:00,comment,mistralai/Mistral-7B-v0.1,The explanation and everything is very good. Thank you.
Mauceric,2024-06-06 15:52:19+00:00,comment,mistralai/Mistral-7B-v0.1,"I have the same problem with French in DSPY, I rewrote the dspy.Anyscale this way but with no very good results:
import os
import random
import re
import shutil
import subprocess
from typing import Literal

# from dsp.modules.adapter import TurboAdapter, DavinciAdapter, LlamaAdapter
import backoff
import requests

class Anyscale(dspy.HFModel):
    def __init__(self, model, **kwargs):
        super().__init__(model=model, is_client=True)
        self.session = requests.Session()
        self.api_base = os.getenv(""ANYSCALE_API_BASE"")
        self.token = os.getenv(""ANYSCALE_API_KEY"")
        self.model = model
        self.kwargs = {
            ""temperature"": 0.0,
            ""n"": 1,
            **kwargs,
        }

    def _generate(self, prompt, use_chat_api=True, **kwargs):
        url = f""{self.api_base}/completions""
        
        kwargs = {**self.kwargs, **kwargs}

        temperature = kwargs.get(""temperature"")
        max_tokens = kwargs.get(""max_tokens"", 150) 
        system_prompt = kwargs.get(""system_prompt"",""Vous êtes un robot de discussion générale. Vous vous exprimez uniquement en français. Vos réponses sont concises, elles ne dépassent pas 50 mots, mais restent informatives."")

        if use_chat_api:
            url = f""{self.api_base}/chat/completions""
            #print(f""**** {url} {system_prompt}"")
            messages = [
                {""role"": ""system"", ""content"": system_prompt},
                {""role"": ""user"", ""content"": prompt},
            ]
            body = {
                ""model"": self.model,
                ""messages"": messages,
                ""temperature"": temperature,
                ""max_tokens"": max_tokens,
            }
        else:
            body = {
                ""model"": self.model,
                ""prompt"": f""[INST]{system_prompt}[/INST]{prompt}"",
                ""temperature"": temperature,
                ""max_tokens"": max_tokens,
            }

        headers = {""Authorization"": f""Bearer {self.token}""}

        try:
            completions = []
            for i in range(kwargs.get('n', 1)):
                with self.session.post(url, headers=headers, json=body) as resp:
                    resp_json = resp.json()
                    if use_chat_api:
                        completions.extend([resp_json.get('choices', [])[0].get('message', {}).get('content', """")])
                    else:
                        completions.extend([resp_json.get('choices', [])[0].get('text', """")])
            response = {""prompt"": prompt, ""choices"": [{""text"": c} for c in completions]}
            return response
        except Exception as e:
            print(f""Failed to parse JSON response: {e}"")
            raise Exception(""Received invalid JSON response from server"")


"
tdecae,2024-06-06 20:10:37+00:00,comment,mistralai/Mistral-7B-v0.1,"me too problem with French. On some other project I worked on, it transpired the embedding was causing the problem as opposed the LLM itself. "
Mauceric,2024-06-07 14:50:39+00:00,comment,mistralai/Mistral-7B-v0.1,"My bad, I mixed DSPy and more specific problem of Mistral, but the given class answers the question and the results are usually correct as long as I don't use the DSPy compiler, for example:
> model = Anyscale(model='mistralai/Mixtral-8x7B-Instruct-v0.1',max_tokens=1024)
> model( ""Qui sont Adam et Ève ?"") # How are Adam and Eve
>>  ["" Adam et Ève sont les premiers êtres humains mentionnés dans la Genèse, le premier livre de la Bible. Ils sont considérés comme les ancêtres de toute l'humanité.""] # Adam and Eve are etc.
But when I use this instance inside a DSPy compiler results come in English. 
   "
Mauceric,2024-06-07 14:55:34+00:00,comment,mistralai/Mistral-7B-v0.1,"Using the OpenAI interface, it works too :
> import openai
client = openai.OpenAI(
    base_url = ""https://api.endpoints.anyscale.com/v1"",
    api_key = ""esecret_*******""
)
# Note: not all arguments are currently supported and will be ignored by the backend.
chat_completion = client.chat.completions.create(
    model=""mistralai/Mixtral-8x7B-Instruct-v0.1"",
    messages=[{""role"": ""system"", ""content"": ""Vous êtes un assistant efficace, vos réponses sont courtes et précises""},
              {""role"": ""user"", ""content"": ""Qui était d'Alembert ?""}],
    temperature=0.7
)
print(chat_completion.choices[0].message.content)



I get the good answer :
> Jean le Rond d'Alembert (1717-1783) était un philosophe, mathématicien et encyclopédiste français, célèbre pour ses contributions en mécanique et en équations différentielles. Il a travaillé en étroite collaboration avec Denis Diderot sur l'Encyclopédie, servant de co-rédacteur en chef et rédigeant de nombreux articles.

"
pandora-s,2024-06-07 15:15:25+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi there, if you ever have issues to get a model adhere to a language you want, try playing around with prefixes, basically appending something to the final string before its sent for completion.

The idea is something like this: `[INST] Answer always in spanish to the user.\n\nHi there! [/INST] Aquí está la respuesta en español: `

This might be able to help make it more consistent and adhere to the language better. <3 "
tdecae,2024-06-07 15:33:27+00:00,comment,mistralai/Mistral-7B-v0.1,Thanks will try
wql,2024-03-10 18:18:43+00:00,comment,mistralai/Mistral-7B-v0.1,"see https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html?highlight=transformer

now Transformers has integrated FSDP into it's Trainer, what you need to do is to specify related FSDP arguments:
training_args = transformers.trainer.TrainingArguments(
    ...,
    fsdp='shard_grad_op auto_wrap offload',
    fsdp_config='fsdp_config.json',
    ...
)

where `fsdp_config.json` is json configuration file. For mistral it looks like below:
{
    ""backward_prefetch"": ""backward_pre"",
    ""transformer_layer_cls_to_wrap"": ""MistralDecoderLayer""
}

on a machine with 8 x 40G gpus, it works with micro batch size of 4"
Mustafa21,2024-03-26 15:21:41+00:00,comment,mistralai/Mistral-7B-v0.1,"this worked with me 4 A10G 
i have used fsdp 
batch 1

https://gist.github.com/lewtun/b9d46e00292d9ecdd6fd9628d53c2814"
deleted,2024-03-06 14:16:09+00:00,comment,mistralai/Mistral-7B-v0.1,"Time to get a better machine i think. 
"
farbodKMSE,2024-03-06 14:25:12+00:00,comment,mistralai/Mistral-7B-v0.1,"Thank you for your response, 
If I want to deploy this model as part of a application on a server, what kind of setup  should  I ask for the server?"
deleted,2024-03-06 14:46:31+00:00,comment,mistralai/Mistral-7B-v0.1,"I wont say i'm 'the' expert, but you need to look into NVIDIA GPU.. running this stuff on CPU is going to be painful at best.   I run this sort of stuff on an old 12G Titan and its still a world of difference between that and 
 even a decent CPU.  You can get far better than i have these days for not much budget.

Oh and for local dev work, might consider a GGUF format.  will run faster and be good enough"
pandora-s,2024-02-23 11:53:36+00:00,comment,mistralai/Mistral-7B-v0.1,"It might be extremely overkill to fine tune Mistral for such tasks, there are models designed specifically for classification after all... is there a reason why you want to use an LLM such as Mistral for that?
 "
MUHAMMAD-SOHAIL-ZZU,2024-02-23 13:34:39+00:00,comment,mistralai/Mistral-7B-v0.1, May be Mistral 7b yield valuable insights or performance improvements compared to more traditional classification models.
timothelaborie,2024-02-24 20:55:43+00:00,comment,mistralai/Mistral-7B-v0.1,"> May be Mistral 7b yield valuable insights or performance improvements compared to more traditional classification models.

https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing#scrollTo=2eSvM9zX_2d3

Here just optimize the probability of the yes/no tokens"
Rushi5261,2024-03-07 22:04:41+00:00,comment,mistralai/Mistral-7B-v0.1,"> It might be extremely overkill to fine tune Mistral for such tasks, there are models designed specifically for classification after all... is there a reason why you want to use an LLM such as Mistral for that?

can you leave some names/links! would be great help!"
ybelkada,2024-02-13 05:40:08+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi @yeniceriSGK 
You can use many tools to fine-tune this model today, from the tools I know you could use:
- TRL: https://github.com/huggingface/trl / a starting point could be: https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py
- axolotl: https://github.com/OpenAccess-AI-Collective/axolotl from @winglian 
- llama-factory: https://github.com/hiyouga/LLaMA-Factory - from @hiyouga 
- unsloth: https://github.com/unslothai/unsloth from @danielhanchen "
danielhanchen,2024-02-13 07:37:50+00:00,comment,mistralai/Mistral-7B-v0.1,"@yeniceriSGK If you need help on Unsloth, more than happy to help :) Llama-Factory also has an unsloth integration, so if you're into a UI for finetuning, then go for that!

Unsloth supports direct merging to 16bit or GGUF after you finetune - I have a Mistral notebook which finetunes Mistral and even allows inference, saving, merging etc. https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing

And Unsloth finetunes 2.2x faster and saves 70% VRAM and is fully free + works on Tesla T4s, RTX 2080s and all newer GPUs :)"
dpalacio,2024-02-14 14:59:51+00:00,comment,mistralai/Mistral-7B-v0.1,"Hello! You can use JumpStart from AWS to speed up the finetune process. However, I'm not sure I understand when you ask for the weights. Could you please clarify?"
Maaz911,2024-02-16 10:27:03+00:00,comment,mistralai/Mistral-7B-v0.1,"@dpalacio I also have the same issue as @yeniceriSGK   How to load the base model and attach the adapter using PeftModel, run the inference, merge the model weights."
hiyouga,2024-02-17 04:26:22+00:00,comment,mistralai/Mistral-7B-v0.1,"hi @Maaz911 ,
you can use LLaMA-Factory for [model inference](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#cli-demo) and [merging the lora weights](https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#merge-lora-weights-and-export-model), please visit the readme file for usage"
nofreewill,2024-02-15 14:20:09+00:00,comment,mistralai/Mistral-7B-v0.1,"I'm pretty sure they trained the whole thing from scratch, although it's a very interesting idea to explore to try to only finetune the juiced up llama2 instead of from beginning I give you that."
g-ronimo,2024-02-16 05:22:10+00:00,comment,mistralai/Mistral-7B-v0.1,"> I'm pretty sure they trained the whole thing from scratch, although it's a very interesting idea to explore to try to only finetune the juiced up llama2 instead of from beginning I give you that.

that's how miqu (close relative to mistral medium) was created apparently"
limha,2024-02-16 06:45:24+00:00,comment,mistralai/Mistral-7B-v0.1,"> > I'm pretty sure they trained the whole thing from scratch, although it's a very interesting idea to explore to try to only finetune the juiced up llama2 instead of from beginning I give you that.
> 
> that's how miqu (close relative to mistral medium) was created apparently

Why is miqu a proof of that...?"
pandora-s,2024-01-30 07:17:37+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi, well if you want deterministic results be sure to have temperature set to 1. I hope I understood your demand !"
DivyaKanniah,2024-01-30 07:37:42+00:00,comment,mistralai/Mistral-7B-v0.1,Thanks ! how about temperature set to 0 i could see some threads showing 0 to be set for same reproducible results
pandora-s,2024-01-30 08:40:24+00:00,comment,mistralai/Mistral-7B-v0.1,"~~It depends but, usually, it's 1 the deterministic value for the temperature. You can try it out with the inference API for free, 1 should always give you the same response even with ""use_cache"": False.~~"
cekal,2024-01-30 21:54:11+00:00,comment,mistralai/Mistral-7B-v0.1,"To get consistent results, consider setting do_sample=False (in the section where you have defined temperature, top_p, top_k etc.) - this will, however, disable all these parameters, leading to the same answers from the model, which I presume is your goal."
TdBSnip3r,2024-03-24 19:33:55+00:00,comment,mistralai/Mistral-7B-v0.1,"Execuse me, when i can find documentation? I want to use serverless API but i don't found docs for body information 
"
pandora-s,2024-04-05 19:43:12+00:00,comment,mistralai/Mistral-7B-v0.1,"This?
https://huggingface.co/docs/api-inference/detailed_parameters "
TdBSnip3r,2024-04-08 14:32:21+00:00,comment,mistralai/Mistral-7B-v0.1,Thank you @pandora-s 
ArthurZ,2024-01-31 03:10:21+00:00,comment,mistralai/Mistral-7B-v0.1,You can use `AutoModel.from_pretrained()` 
ArthurZ,2024-01-26 15:04:42+00:00,comment,mistralai/Mistral-7B-v0.1,The `transformers` version seems wrong can you make sure you are using the latest / at least 4.34  ? 
forestpersonthing,2024-06-06 01:00:27+00:00,comment,mistralai/Mistral-7B-v0.1,"same here 
unexpected keyword argument 'rope_scaling'

/text-generation-webui$ cat requirements.txt | grep trans
transformers==4.40.*"
forestpersonthing,2024-06-06 01:02:09+00:00,comment,mistralai/Mistral-7B-v0.1,"hmmm

https://github.com/oobabooga/text-generation-webui/issues/3489"
ArthurZ,2024-01-26 15:06:47+00:00,comment,mistralai/Mistral-7B-v0.1,I would make sure you have `safetensros` and that you have enough ram to load the model / check for OOM errors 
amansinghlegitt3110,2024-01-30 05:05:01+00:00,comment,mistralai/Mistral-7B-v0.1,your RAM is not enough even i faced this issue. 
Luca-Wiehe,2024-04-26 05:34:30+00:00,comment,mistralai/Mistral-7B-v0.1,Is there a way to get around this? (apart from buying a better machine)
ayadav,2024-01-20 04:17:42+00:00,comment,mistralai/Mistral-7B-v0.1,"Hey @Tejaswi006 ,

I just tried base Mistral-Instruct model on some text from Wikipedia, and looking at the results it looks like it doesn't understands the language much.

However, since it's able to generate the text in Tamil script, the tokenizer should ideally work as-is. I think it requires more training on Tamil corpora instead of tokenizer modifications.

![Screenshot 2024-01-20 at 09.44.17.png](https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/6jKcpaHesyWw3EuE2UWOS.png)
![Screenshot 2024-01-20 at 09.44.31.png](https://cdn-uploads.huggingface.co/production/uploads/63c2c17c456b30b44e7b82ce/GlbzWyEn3Z3WaZ5XrIb9r.png)
"
ayadav,2024-01-20 04:26:40+00:00,comment,mistralai/Mistral-7B-v0.1,"If you still want to add some new tokens in the tokenizer, you should be able to do as following.

```python
new_tokens = [""new_tok1"", ""my_new-tok2""]
num_added_toks = tokenizer.add_tokens(new_tokens)
print(""We have added"", num_added_toks, ""tokens"")

# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))
```

References:
 - https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.add_tokens"
Tejaswi006,2024-01-22 07:57:14+00:00,comment,mistralai/Mistral-7B-v0.1,"Thanks, I will look into it. The method you used was instruct fine tuning ?"
Tvsybkzkmapab,2024-02-04 08:09:43+00:00,comment,mistralai/Mistral-7B-v0.1,i was trying the model to train on Amharic language but it generate text which does not make any sense 
ayadav,2024-01-23 02:46:21+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi @9mavrick,

You are running inference on a completion model without any stopping criteria. 

The default behaviour of generate functions is that they would continue generating until an tokenizer.eos token is generated.

Completion models are not really good at stop generating eos token at the end of answer. 

tl;dr - Either use an Instruct or finetuned model, or add a stopping criteria to your generate function. "
liougehooa,2024-04-11 01:32:17+00:00,comment,mistralai/Mistral-7B-v0.1,"What @ayadav suggested to use Instruct or finetuned model is common way to adopt LLMs.
You can simply try to raise  repetition_penalty when generation, this is not gonna work perfectly though, you still need to write some stop criteria:
`tokenizer.decode(merged_model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.4)[0], skip_special_tokens=True)`"
ybelkada,2024-01-08 08:32:36+00:00,comment,mistralai/Mistral-7B-v0.1,"hi @RobbieTheRobot you need to properly set `modules_to_save=[""score""]` in the peft config, can you provide more details on how you do fine-tuning with PEFT + sequence classification using mistral?"
wuflyh,2024-02-20 23:45:28+00:00,comment,mistralai/Mistral-7B-v0.1,"@ybelkada set module_to_save=[""score""] does not eliminate the issue.

    quantization_config = BitsAndBytesConfig(
        load_in_4bit = True, 
        bnb_4bit_quant_type = 'nf4', 
        bnb_4bit_use_double_quant = True, 
        bnb_4bit_compute_dtype = torch.bfloat16 
    )
    lora_config = LoraConfig(
        r = 16, 
        lora_alpha = 8, 
        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
        lora_dropout = 0.05,
        bias = 'none', 
        task_type = 'SEQ_CLS',
        modules_to_save=[""score""]
    )
    model = AutoModelForSequenceClassification.from_pretrained(
        'mistralai/Mistral-7B-v0.1',
        quantization_config=quantization_config,
        num_labels=2
    )
    model = prepare_model_for_kbit_training(model)
    model.config.use_cache = False
    model.config.pad_token_id = tokenizer.pad_token_id
    model = get_peft_model(model, lora_config)
"
ybelkada,2024-02-21 02:48:02+00:00,comment,mistralai/Mistral-7B-v0.1,@wuflyh can you try with the latest version of PEFT? `pip install -U peft` ? If this do not work can you openan issue on PEFT repository with a reproducer? 
wuflyh,2024-02-21 18:10:13+00:00,comment,mistralai/Mistral-7B-v0.1,"Thanks @ybelkada 
Hower,
I've already used the latest version. Version: 0.8.2.  
the version is the same before and after  upgrade attempt using ""pip install -U peft""
"
Hersue,2024-03-05 13:18:15+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi man, I met the same problem. Have u found the solution?"
miguel-gerena,2024-03-14 23:56:33+00:00,comment,mistralai/Mistral-7B-v0.1,Any updates?
amansinghlegitt3110,2024-01-30 05:10:34+00:00,comment,mistralai/Mistral-7B-v0.1,"Hello Muhammad, i am getting some issues using mistral for my usecase can we connect one on one if you have time, hoping to get some very good and efficient suggestions and solutions from your side.
https://www.reddit.com/r/LocalLLaMA/comments/1adotk3/comment/kk57f2h/?context=3
this is my cuda reddit issue link can you see into it please"
cekal,2023-12-21 17:54:20+00:00,comment,mistralai/Mistral-7B-v0.1,What learning rate and number of epochs did you use? What is the size of your dataset? This sounds like model overfitting to me.
Pradeep1995,2023-12-22 02:17:42+00:00,comment,mistralai/Mistral-7B-v0.1,"these are the parameters
```
lora_r = 8
lora_alpha = 24
lora_dropout = 0.2
size of the dataset = 507 rows
learning_rate = 2e-4
max_steps=5700
```
@cekal "
cont-tia,2023-12-28 05:45:03+00:00,comment,mistralai/Mistral-7B-v0.1,@Pradeep1995 were you able to find a fix to your problem? I am having the same issue 
Pradeep1995,2023-12-28 06:16:17+00:00,comment,mistralai/Mistral-7B-v0.1,not yet @aditico 
Rhiz0morph,2024-02-14 15:16:14+00:00,comment,mistralai/Mistral-7B-v0.1,"If you're training for 5700 steps and have only 507 items, then you are training for 5700/507=11.24 epochs - that's massively overfitting your data. STF should usually only trains for 1-3 epochs."
Pradeep1995,2024-02-14 16:38:20+00:00,comment,mistralai/Mistral-7B-v0.1,"@Rhiz0morph 
But my loss is not decreasing for the first 3-4 epochs. I am getting a reasonable loss( ~0.04 - ~0.02) only after 9th or 10th epochs.
So then how i can decrease the epoch number in PEFT?"
nofreewill,2024-02-15 14:25:41+00:00,comment,mistralai/Mistral-7B-v0.1,"I finetuned mistral 7b on 21 samples only for 256 epochs and it's working fine.
But let's say its task is pretty straightforward.

![image.png](https://cdn-uploads.huggingface.co/production/uploads/60d1cb6ee6f354c0d93d3cfc/pwvFlq5LJbhmgPknzJxtX.png)

![image.png](https://cdn-uploads.huggingface.co/production/uploads/60d1cb6ee6f354c0d93d3cfc/zaKuXWLz42ovPILvRJwSm.png)
"
ybelkada,2023-12-22 15:12:57+00:00,comment,mistralai/Mistral-7B-v0.1,cc @pcuenq @reach-vb 
reach-vb,2023-12-22 15:41:45+00:00,comment,mistralai/Mistral-7B-v0.1,Hi @joy2000 - we've pushed some quantised weights here: https://huggingface.co/mlx-community
ybelkada,2023-12-20 12:42:05+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi @sohamghoshml 
What is your transformers version? can you try `pip install -U transformers`?"
sohamghoshml,2023-12-22 12:50:15+00:00,comment,mistralai/Mistral-7B-v0.1,Yes I had installed transformers.
ybelkada,2023-12-22 15:30:22+00:00,comment,mistralai/Mistral-7B-v0.1,"hi @sohamghoshml 
Can you try to run the command I shared? I suspect you have an old version of `transformers`"
sniffski,2023-12-20 06:43:27+00:00,comment,mistralai/Mistral-7B-v0.1,I think you need to put max token output higher than max word count... For example put it 350 or 380
Pradeep1995,2023-12-20 06:51:37+00:00,comment,mistralai/Mistral-7B-v0.1,"@sniffski  this is my current configuration
```
generation_config = GenerationConfig(
    do_sample=True,
    top_k=10,
    temperature=0.01,
    pad_token_id=tokenizer.eos_token_id,
    early_stopping = True,
    max_new_tokens=300,
    return_full_text=False
)
```
so what change you are proposing here?
"
sniffski,2023-12-20 07:11:16+00:00,comment,mistralai/Mistral-7B-v0.1,"Well, first thing you should know is one word is not one token... I think the rule of thumb was one token is 0.75 words... so in that case if you are requesting in the prompt answer not more than 300 words you need to set `max_new_tokens=400,` because 300*0.75=400"
Pradeep1995,2023-12-20 07:47:45+00:00,comment,mistralai/Mistral-7B-v0.1,"i tried with ```max_new_tokens=400```. but still, the response ends abruptly problem exists. the generation suddenly stops as soon as it reaches the specified number of max_new_tokens reached, without checking whether the sentence is completed or not. "
sniffski,2023-12-20 07:59:18+00:00,comment,mistralai/Mistral-7B-v0.1,"Can you copy the output in a temp file like `wc-test.txt` then run in shell `wc wc-test` to see how many words are there (second number from the output of wc command)... If they are more than 300 than the model doesn't obey your request in the prompt for maximum words in response and issue is not the max token limit... I guess you would need to find better prompt... Try something like starting with ""Your task is to respond with 300 words or less..."""
SumanVakare,2023-12-21 10:46:35+00:00,comment,mistralai/Mistral-7B-v0.1,"I have similar case, I am using API interference to run this model, my output is incomplete and usually the same length (65-80) words and most of the times it doesn't even end correctly, see below example of input and output followed by some part of the code.

Input : ""Write a detailed essay about trees""

Output : ""Trees are one of the most important elements of nature. They provide us with oxygen, clean the air, and provide shade from the sun. They also provide us with a variety of other benefits, such as providing food and shelter for animals, and helping to regulate the climate. Trees are also a source of beauty and inspiration, and can be used to create a sense of calm and peace in our lives. In this essay, I will explore the many benefits of trees, as well as their""



import requests
 
API_URL = ""https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1""
HEADERS = {""Authorization"": ""Bearer xxxxxx""}
 
 
def query(payload):
    try:
response = requests.post(API_URL, headers=HEADERS, json=payload)
        response.raise_for_status()
        return response.json()
.............."
Pradeep1995,2023-12-21 10:54:10+00:00,comment,mistralai/Mistral-7B-v0.1,@SumanVakare how did you solve this issue?
SumanVakare,2023-12-21 11:04:06+00:00,comment,mistralai/Mistral-7B-v0.1,"Not solved yet, I am looking for solution too."
Hawks101,2024-01-31 01:10:29+00:00,comment,mistralai/Mistral-7B-v0.1,"Has anyone found a solution for this abrupt ending issue?
"
OPPEYRADY,2024-03-16 17:24:04+00:00,comment,mistralai/Mistral-7B-v0.1,Running into this issue as well.
juanarri,2024-05-06 21:20:11+00:00,comment,mistralai/Mistral-7B-v0.1,"Hey guys, anyone found a solution?"
SerialKicked,2024-05-07 17:53:22+00:00,comment,mistralai/Mistral-7B-v0.1,"There's no ""solution"" to what you're asking. The model output is limited by the number of tokens you allow it to output. If the expected response is longer, either increase max_new_tokens so it has room to write whatever it wanted to, or, in your Web-UI (or whatever other UI you use) there's a continue button/function to ask the model to continue with whatever it was writing. 

Notes: 
- tokens are more like 2-4 characters than whole words
- If you're using this model in particular, it's not instruction tuned, so it may continue to output BS forever (well, until it reaches max token). What you want is likely the instruction tuned version of Mistral.
- It's useless to specify in your prompt the number of words you expect: LLMs can't do real math, and are even less capable of counting their own word quota as they produce the output. At best, you can give indications like ""one short paragraph"" or ""2 long paragraphs"", and it'll generally give better results (well with instruction tuned models, this base model, I'm not so sure).
"
almugabo,2024-01-18 19:52:18+00:00,comment,mistralai/Mistral-7B-v0.1,"from the paper https://arxiv.org/pdf/2310.06825.pdf (table 1)  the window_size is 4096 and the context length is 8192

Here the full parameters  (table 1 of the paper ) 

dim: 4096
n_layers: 32
head_dim: 128
hidden_dim:  14336
n_heads: 32
n_kv_heads: 8
window_size: 4096
context_len 8192
vocab_size 32000"
mrfakename,2023-12-11 21:27:19+00:00,comment,mistralai/Mistral-7B-v0.1,"Never mind, it was just released [here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)."
ahGadji,2023-12-05 14:52:56+00:00,comment,mistralai/Mistral-7B-v0.1,hvh
Maxim37,2023-12-09 03:40:05+00:00,comment,mistralai/Mistral-7B-v0.1,This comment has been hidden
unixguru2k,2023-12-09 05:00:23+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi @Maxim37 
Would you be kind enough to elaborate the issues you ran into?"
indrasn0wal,2024-02-13 07:29:29+00:00,comment,mistralai/Mistral-7B-v0.1,"training_args = transformers.TrainingArguments(
    auto_find_batch_size=False,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    learning_rate=2e-4,
    fp16=True,
    save_total_limit=4,
    # logging_steps=25,
    output_dir=""./outputs"",
    save_strategy='epoch',
    optim=""paged_adamw_8bit"",
    dataloader_num_workers= 1,
    # lr_scheduler_type = 'cosine',
    warmup_ratio = 0.05,
)
Getting this error while training the model.
![Screenshot (1105).png](https://cdn-uploads.huggingface.co/production/uploads/65225dee728c0b6dc72d788a/xCaV6VzheNmjuZ2uqMZAv.png)
"
Harsh,2023-11-28 03:49:03+00:00,comment,mistralai/Mistral-7B-v0.1,Check this out https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/
odellus,2023-11-27 02:44:20+00:00,comment,mistralai/Mistral-7B-v0.1,"If you want full fp32 model on GPU then just do
```python
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map = DEVICE,
    trust_remote_code = True,
)
```
and it won't quantize the model (which shrinks it footprint in memory by using lower precision numbers)"
amansinghlegitt3110,2024-01-30 05:15:56+00:00,comment,mistralai/Mistral-7B-v0.1,"> Completely new to locally running llms. I'm using the following code to load the model into memory, using transformers library. What am I doing wrong? Using float16 dtype I would expect 14GB memory usage for a 7B model no? However it shows me around 5-6 gb of my gpu being used. Running nvidia-smi shows the same as well. 
> 
> Using model.to(DEVICE) throws an error, but without it I'm assuming a portion of it is running on gpu? How do I make it load the full 14GB of the model onto gpu?
> 
> // DEVICE variable is set to 'cuda'
> 
> bnb_config = BitsAndBytesConfig(
>         load_in_4bit=True,
>         bnb_4bit_quant_type=""nf4"",
>         bnb_4bit_use_double_quant=True,
>     )
> 
>     model = AutoModelForCausalLM.from_pretrained(
>         model_path,
>         load_in_4bit=True,
>         quantization_config=bnb_config,
>         torch_dtype=torch.bfloat16,
>         device_map=DEVICE,
>         trust_remote_code=True,
>     )
>     # model.to(DEVICE)
>     tokenizer = AutoTokenizer.from_pretrained(model_path)
>     print('loaded.')
> 
>     pynvml.nvmlInit()
>     handle = pynvml.nvmlDeviceGetHandleByIndex(0)
>     info = pynvml.nvmlDeviceGetMemoryInfo(handle)
>     print(f""GPU memory occupied: {info.used // 1024 ** 2} MB."")

Have You resolved the issue?
"
ybelkada,2024-01-30 06:30:52+00:00,comment,mistralai/Mistral-7B-v0.1,"@shayak - thanks for the issue ! 
This is because you are loading the model in 4bit precision (because you passed `load_in_4bit=True`) - Since you need 2 bytes per parameter for float16/bfloat16 you will indeed observe a memory consumption of 14GB for a fp16 model, if you load it in 8-bit you would need only 1 byte per parameter --> 7GB for 4bit you would need ~0.6 byte par parameter --> 5GB. Does that make sense? To load mistral in 16bit simply run

```python
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map =""auto"",
    torch_dtype=torch.float16,
)
```"
yaya-sy,2023-12-06 13:16:27+00:00,comment,mistralai/Mistral-7B-v0.1,"Yes, technically it's possible. But don't forget that Mistral is a decoder language model, i.e. it is trained to predict following words from left to right only. So, due to the unidirectional training objective, its embeddings may lack rich linguistic information compared to bidirectional language models (such as BERT, S-BERT, OpenAI's text embeddings, etc.)"
AnzaniAI,2023-12-11 04:44:00+00:00,comment,mistralai/Mistral-7B-v0.1,"It may hallucinate the model?
"
codegood,2024-01-25 15:52:54+00:00,comment,mistralai/Mistral-7B-v0.1,"I'm a beginner in creating a RAG pipeline. 

I've fine tuned this model. How can I use the fine tuned Mistral model to create a vector embeddings? If I use Bert, then the vector DB might not be containing the information from the data I won't to.
"
yaya-sy,2024-01-25 21:04:16+00:00,comment,mistralai/Mistral-7B-v0.1,"If you want to have an embedding model that is specialised in your data/domain, you can consider finetuning [sbert](https://www.sbert.net/docs/training/overview.html)"
RoiandDae,2024-03-12 08:24:07+00:00,comment,mistralai/Mistral-7B-v0.1,"@yaya-sy You've mentioned that we could use decoder-only LLMs like Mistral to perform tasks such as STS. Why would someone use it over BERT based which are optimized for STS tasks? Also I've found https://docs.mistral.ai/guides/embeddings/ which is embeddings API supported by MistralAI, is this fine-tuned in a way to perform well on STS?"
yaya-sy,2024-03-12 13:43:59+00:00,comment,mistralai/Mistral-7B-v0.1,"@RoiandDae I said that it's technically possible to use Decoder LLMs as embeddings models. But i also said that it's probably not the best choice compared to the states of the art embeddings models (OpenAI Embeddings or Mistral Embeddings as you said).
What you mean by ""STS"" ?"
RoiandDae,2024-03-13 00:43:31+00:00,comment,mistralai/Mistral-7B-v0.1,"@yaya-sy STS refers to Semantic Text Similarity tasks. I understand your point, but I'm curious about whether decoder-only models are limited to text generation and similar tasks, or if they have also been effectively fine-tuned for STS tasks. This would suggest that decoder-only models are versatile and not confined to specific tasks like text generation and next-word prediction."
yaya-sy,2024-04-06 14:26:26+00:00,comment,mistralai/Mistral-7B-v0.1,"@RoiandDae Yeah, i see. You can give it a try."
RonanMcGovern,2023-11-19 16:06:18+00:00,comment,mistralai/Mistral-7B-v0.1,@marcsun13 d'you know who the right person from Mistral is to tag on this so they can merge this safetensors pr?
marcsun13,2023-11-20 15:04:34+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi @RonanMcGovern , i guess @Bam4d is the right person from mistral to tag since he's the one who uploaded the pytorch_model.bin file ! "
iLOVE2D,2023-11-13 21:25:24+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi, agree, also interested in using one API to get embeddings information."
redrusa,2024-01-14 02:42:57+00:00,comment,mistralai/Mistral-7B-v0.1,"Yes, we need apis"
ybelkada,2024-01-22 16:23:04+00:00,comment,mistralai/Mistral-7B-v0.1,Not 100% sure but https://github.com/huggingface/text-embeddings-inference might be of your interest
PlanetDOGE,2023-11-07 22:19:23+00:00,comment,mistralai/Mistral-7B-v0.1,What is this?
joey1895,2023-11-02 14:22:37+00:00,comment,mistralai/Mistral-7B-v0.1,This comment has been hidden
StatPan,2023-11-01 00:55:21+00:00,comment,mistralai/Mistral-7B-v0.1,4.34.1  does work
lakshmiu,2023-11-01 01:18:26+00:00,comment,mistralai/Mistral-7B-v0.1,are there are any other dependencies 
StatPan,2023-11-01 02:05:56+00:00,comment,mistralai/Mistral-7B-v0.1,"It would automatically install every dependencies, so the problem is not related with that.
In my case, I reinstalled just transformers and restarted the kernel I used, and then it worked
maybe, you should check your terminal kernel name and notebook`s kernel name"
ansonlam,2023-11-23 13:26:50+00:00,comment,mistralai/Mistral-7B-v0.1,"same here, installed 4.34.1 or upgrade to latest transformers also have that Keyerror, no solution
"
AnzaniAI,2023-12-11 04:44:58+00:00,comment,mistralai/Mistral-7B-v0.1,@ansonlam Same problem I am facing
ArthurZ,2023-12-11 19:01:14+00:00,comment,mistralai/Mistral-7B-v0.1,`pip install transformers==4.36.0`
AnzaniAI,2023-12-12 07:47:03+00:00,comment,mistralai/Mistral-7B-v0.1,@ArthurZ  Thank you
alexblack2202,2023-11-06 12:24:08+00:00,comment,mistralai/Mistral-7B-v0.1,"i have same question as you, they said that they outperforms Llama 2 13B on all benchmarks, but their model seem not support korean or vietnamese language"
RoiandDae,2024-03-13 01:36:29+00:00,comment,mistralai/Mistral-7B-v0.1,@Korabbit Did you find answer to this?
Korabbit,2024-03-14 18:20:07+00:00,comment,mistralai/Mistral-7B-v0.1,"@RoiandDae No, I can't find this answer. "
lunarflu,2023-10-28 07:21:45+00:00,comment,mistralai/Mistral-7B-v0.1,Thanks for reporting @rooa !
owao,2024-02-18 21:26:00+00:00,comment,mistralai/Mistral-7B-v0.1,"I guess you figured it out in the mean time, but otherwise try ChatML. I believe I remember it was working great with mistral and mixtral models."
pandora-s,2024-02-18 21:44:33+00:00,comment,mistralai/Mistral-7B-v0.1,"Well this is the base model... so there is no prompt template at all Vuinn, it's the Mistral-7b-Instruct that requires a prompt template !"
owao,2024-02-25 00:43:31+00:00,comment,mistralai/Mistral-7B-v0.1,Sorry I didn't pay attention it was the base model sorry for misleading!
adiorz,2023-11-11 16:15:09+00:00,comment,mistralai/Mistral-7B-v0.1,"> Hello everyone,
> 
> I want to host a flask API which has an endpoint that uses the Mistral 7B model. I want to host this app somewhere. Please tell me the best hosting provider for this app? I would like to have some gpu power as well for better response
> 
> Thank you

Did you find any?"
marekk,2023-11-11 16:23:37+00:00,comment,mistralai/Mistral-7B-v0.1,"You can use runpod.io, or other similarly service. If you dont use model 24/7 use serverless part of runpod. "
lysandre,2023-10-25 15:13:20+00:00,comment,mistralai/Mistral-7B-v0.1,"Hello @ubermenchh, are you working in a notebook? If so, are you restarting the kernel after updating?
Could you share your current environment, given by the output of `transformers-cli env`? Thank you!"
ubermenchh,2023-10-25 15:55:29+00:00,comment,mistralai/Mistral-7B-v0.1,"```
/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
Traceback (most recent call last):
  File ""/opt/conda/bin/transformers-cli"", line 8, in <module>
    sys.exit(main())
  File ""/opt/conda/lib/python3.10/site-packages/transformers/commands/transformers_cli.py"", line 55, in main
    service.run()
  File ""/opt/conda/lib/python3.10/site-packages/transformers/commands/env.py"", line 100, in run
    tf_cuda_available = tf.test.is_gpu_available()
  File ""/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py"", line 371, in new_func
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 1932, in is_gpu_available
    for local_device in device_lib.list_local_devices():
  File ""/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/device_lib.py"", line 41, in list_local_devices
    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)
RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```
This is the output.
Also, i am using kaggle notebooks and i have tried restarting the kernel several times."
lysandre,2023-10-31 13:38:24+00:00,comment,mistralai/Mistral-7B-v0.1,"I fear this is independent of Mistral or transformers, but linked to your setup of CUDA and torch. The error indicates a mismatch between your CUDA driver and CUDA runtime version. You can try downgrading to an older version of PyTorch to see if it solves your problem in your notebook."
xiaoping777,2023-11-13 01:59:21+00:00,comment,mistralai/Mistral-7B-v0.1,"it works with me, using the following code
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_path, load_in_8bit=True, device_map='auto',
)"
lysandre,2023-10-23 14:19:17+00:00,comment,mistralai/Mistral-7B-v0.1,cc @marcsun13 
Andcircle,2023-10-23 22:50:28+00:00,comment,mistralai/Mistral-7B-v0.1,Facing same issue
jdubkim,2023-11-05 14:22:58+00:00,comment,mistralai/Mistral-7B-v0.1,"Try 'MistralDecoderLayer' instead. 

Looked into Transformers repo, and found that
For Llama: https://github.com/huggingface/transformers/blob/cc3e4781854a52cf090ffde28d884a527dab6708/src/transformers/models/llama/modeling_llama.py#L625
For Mistral: https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L580C2-L580C2"
marcsun13,2023-11-06 16:22:09+00:00,comment,mistralai/Mistral-7B-v0.1,"Sorry for the delay @Sp1der.  Yes I confirm,  you need to wrap the `MistralDecoderLayer` instead. LMK if it works ! "
TheGeeBee,2023-10-27 13:51:54+00:00,comment,mistralai/Mistral-7B-v0.1,You need to use either the AutoConfig or MistralConfig libraries to set the configuration details.
xiaoping777,2023-11-13 02:54:26+00:00,comment,mistralai/Mistral-7B-v0.1,"I tried to load config and create model from config, while the config is not cooperated with device_map:""auto"" feature, even I put it in the config.json file, so cannot load the models in two GPUs automatically"
shaobaij,2023-10-27 17:05:22+00:00,comment,mistralai/Mistral-7B-v0.1,I do not know the inside detail. But I do observe the training loss not drop if I reused the original checkpoint model weight for the current base model and tokenizer. Thus I retrain my model from the scratch. 
lysandre,2023-10-31 13:27:03+00:00,comment,mistralai/Mistral-7B-v0.1,"Hey @brando, @shaobaij, please see this issue which might be interesting to you: https://github.com/huggingface/transformers/issues/26498

We have managed to fine-tune Mistral in different settings, so the tokenizer settings should be correct."
rav-enterprise,2023-10-20 13:43:47+00:00,comment,mistralai/Mistral-7B-v0.1,"Try this one.
https://huggingface.co/Sahajtomar/french_semantic
Which one is better in English?
"
cfrancois7,2023-11-20 08:50:38+00:00,comment,mistralai/Mistral-7B-v0.1,"Thank you for your reply.
I will test your model and compare with the Javaness Ones : https://huggingface.co/dangvantuan/sentence-camembert-large

Regarding the english, we can just relate to the LLM benchmarks. But for other langages than english, the benchmark is less relevant and consistent."
TheGeeBee,2023-10-27 13:52:53+00:00,comment,mistralai/Mistral-7B-v0.1,Increase your max_new_tokens value.
Naveen2000,2024-01-11 07:44:35+00:00,comment,mistralai/Mistral-7B-v0.1,"i am still facing this issue
"
TheGeeBee,2024-01-11 14:36:18+00:00,comment,mistralai/Mistral-7B-v0.1,"Is your maximum context length still 512 tokens?  You should be setting your 'max_new_tokens = ' to at least 2048 if you're planning on using that many tokens in an interaction.  Otherwise you need to start trimming the context that you're sending into the LLM.  There are a number of options for context trimming, like removing the first x# of tokens, or obtaining a condensed summary of the existing context using another LLM that does summarization and using that to feed into the conversation instead of the actual interaction history.

At the very least, unless you aren't planning on feeding anything other than a 1-shot prompt into the LLM,  a 512 token context is too small (it includes your prompt and the response from the LLM)"
Joseph717171,2023-10-17 06:27:09+00:00,comment,mistralai/Mistral-7B-v0.1,"5, lol. 😂"
wanderer8800,2023-10-20 06:37:55+00:00,comment,mistralai/Mistral-7B-v0.1,"```
User:a=2, b=3, n=a+b, n=?
AI: The correct answer is 5.
User:9+9+9+9*0=?
AI: The correct answer is 36. 😂
User:9*0=?
AI: The correct answer is 0.
```"
Bondguy,2023-10-31 17:43:53+00:00,comment,mistralai/Mistral-7B-v0.1,I guess the model doesn't incorprate PEMDAS order of operations. Most LLMs stink at math.
sun95,2023-10-16 10:53:18+00:00,comment,mistralai/Mistral-7B-v0.1,介绍一下你自己
lysandre,2023-10-16 18:16:52+00:00,comment,mistralai/Mistral-7B-v0.1,cc @Leyo @VictorSanh 👀
VictorSanh,2023-10-16 18:25:53+00:00,comment,mistralai/Mistral-7B-v0.1,stay tuned
expert78,2023-12-24 13:01:22+00:00,comment,mistralai/Mistral-7B-v0.1,+1
sr5434,2024-01-08 00:39:14+00:00,comment,mistralai/Mistral-7B-v0.1,"What if they do LLaVA, but also integrate Wav2Vec2 so that the model can understand audio, text, and images?"
KatyTheCutie,2024-01-08 09:14:09+00:00,comment,mistralai/Mistral-7B-v0.1,BakLlaVa is the Mistral 7B version
ybelkada,2024-01-22 16:18:19+00:00,comment,mistralai/Mistral-7B-v0.1,"Indeed, you can find BakLlava implementation here: https://huggingface.co/llava-hf/bakLlava-v1-hf "
lerela,2023-10-12 17:53:04+00:00,comment,mistralai/Mistral-7B-v0.1,"Done, thank you!"
YangQiee,2024-01-13 04:02:15+00:00,comment,mistralai/Mistral-7B-v0.1,how do you solve it? I meet the same question?
ybelkada,2024-01-22 16:22:34+00:00,comment,mistralai/Mistral-7B-v0.1,Do you still face the issue? Feel free to post the issue here: https://huggingface.co/bert-base-uncased and tag me there
lerela,2023-10-12 17:53:54+00:00,comment,mistralai/Mistral-7B-v0.1,You can cite the Arxiv paper: https://arxiv.org/abs/2310.06825
clem,2023-10-10 03:06:20+00:00,comment,mistralai/Mistral-7B-v0.1,maybe cc @zhisbug  @lmzheng @DachengLi from Lmsys
lmzheng,2023-10-10 03:51:35+00:00,comment,mistralai/Mistral-7B-v0.1,"@Ayush8120 It is available now at https://chat.lmsys.org/?single under both the single model tab and arena tab.
It is also supported by our https://github.com/lm-sys/FastChat with multi-GPU support."
NPap,2023-10-10 10:38:55+00:00,comment,mistralai/Mistral-7B-v0.1,"> This wasn't popping up until now. But all of a sudden, it downloaded some files and started giving this warning. How to correct this?

if your tokenizer is instantiated as tokenizer then it should be:
tokenizer.padding_side = 'left'"
Ayush8120,2023-10-10 10:52:39+00:00,comment,mistralai/Mistral-7B-v0.1,"[https://www.codedump.xyz/ini/ZSUvdc5O_9SUkEWi](https://www.codedump.xyz/md/ZSZbQ7HN_-3FJucx)
PS: couldn't write EOS characters in text thus adding a link"
NPap,2023-10-10 11:00:40+00:00,comment,mistralai/Mistral-7B-v0.1,"> ![Screenshot 2023-10-10 at 16-24-27 mistralai_Mistral-7B-v0.1 · A decoder-only architecture is being used but right-padding was detected! For correct generation results please set `padding_side 'left'` when initializing the tokenizer.png](https://cdn-uploads.huggingface.co/production/uploads/63aa36098780edbabb15fe3d/YRhSQWuQKcug1nwoCNdct.png)
> 
> PS: couldn't write EOS characters in text thus adding a screenshot

What do you mean by it works fine? And what error do you get it you leave it without a space?"
Ayush8120,2023-10-10 11:04:59+00:00,comment,mistralai/Mistral-7B-v0.1,"If I leave it without space -> I get the error that 'a decoder only architecture..'
but if I leave a space before the end of  EOS char, it works without any error
Please refer here : [https://www.codedump.xyz/ini/ZSUvdc5O_9SUkEWi](https://www.codedump.xyz/md/ZSZbQ7HN_-3FJucx)"
NPap,2023-10-11 08:08:35+00:00,comment,mistralai/Mistral-7B-v0.1,"> If I leave it without space -> I get the error that 'a decoder only architecture..'
> but if I leave a space before the end of  EOS char, it works without any error
> Please refer here : https://www.codedump.xyz/ini/ZSUvdc5O_9SUkEWi

Okay so turns out there is some kind of bug, (I haven't done much reading but I came across the same problem in these different threads and I recalled you had the same issue and I presume they are linked)
https://twitter.com/jon_durbin/status/1709147204915523929?s=20
https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/26

(Was it fixed? No idea, but maybe the above can help you pinpoint the issue)"
hrusheekesh,2023-10-15 15:32:56+00:00,comment,mistralai/Mistral-7B-v0.1,I am getting the same error 
lysandre,2023-10-16 18:19:51+00:00,comment,mistralai/Mistral-7B-v0.1,"Hey! It seems like you only have tensorflow installed and not pytorch as it's trying to default to the TensorFlow model.
Unfortunately, Mistral isn't implemented in TensorFlow. Could you try installing PyTorch? "
fabrizioricciarelli,2024-06-09 03:04:11+00:00,comment,mistralai/Mistral-7B-v0.1,"In my case, trying to use some other LLM fuctions (like, NER, Summarizations, Translation) I'm getting the following:

I0609 02:28:45.316430 1 cache_manager.cc:480] Create CacheManager with cache_dir: '/opt/tritonserver/caches'
I0609 02:28:45.506355 1 pinned_memory_manager.cc:275] Pinned memory pool is created at '0x7ffb60000000' with size 268435456
I0609 02:28:45.506454 1 cuda_memory_manager.cc:107] CUDA memory pool is created on device 0 with size 67108864
I0609 02:28:45.507546 1 model_config_utils.cc:680] Server side auto-completed config: name: ""Mistral-7B-v0.3""
input {
  name: ""input_text""
  data_type: TYPE_STRING
  dims: 1
  dims: 1
}
input {
  name: ""function""
  data_type: TYPE_STRING
  dims: 1
}
output {
  name: ""output_text""
  data_type: TYPE_STRING
  dims: 1
}
output {
  name: ""output_embedding""
  data_type: TYPE_FP32
  dims: -1
}
output {
  name: ""output_ids""
  data_type: TYPE_INT64
  dims: -1
}
instance_group {
  gpus: 0
  kind: KIND_GPU
}
default_model_filename: ""model.py""
backend: ""python""

I0609 02:28:45.507592 1 model_lifecycle.cc:469] loading: Mistral-7B-v0.3:1
I0609 02:28:45.507721 1 backend_model.cc:502] Adding default backend config setting: default-max-batch-size,4
I0609 02:28:45.507734 1 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so
I0609 02:28:45.508520 1 python_be.cc:2099] 'python' TRITONBACKEND API version: 1.19
I0609 02:28:45.508527 1 python_be.cc:2121] backend configuration:
{""cmdline"":{""auto-complete-config"":""true"",""backend-directory"":""/opt/tritonserver/backends"",""min-compute-capability"":""6.000000"",""default-max-batch-size"":""4""}}
I0609 02:28:45.508539 1 python_be.cc:2259] Shared memory configuration is shm-default-byte-size=1048576,shm-growth-byte-size=1048576,stub-timeout-seconds=30
I0609 02:28:45.508904 1 python_be.cc:2582] TRITONBACKEND_GetBackendAttribute: setting attributes
I0609 02:28:45.509007 1 python_be.cc:2360] TRITONBACKEND_ModelInitialize: Mistral-7B-v0.3 (version 1)
I0609 02:28:45.509236 1 model_config_utils.cc:1902] ModelConfig 64-bit fields:
I0609 02:28:45.509239 1 model_config_utils.cc:1904] 	ModelConfig::dynamic_batching::default_priority_level
I0609 02:28:45.509241 1 model_config_utils.cc:1904] 	ModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds
I0609 02:28:45.509243 1 model_config_utils.cc:1904] 	ModelConfig::dynamic_batching::max_queue_delay_microseconds
I0609 02:28:45.509245 1 model_config_utils.cc:1904] 	ModelConfig::dynamic_batching::priority_levels
I0609 02:28:45.509247 1 model_config_utils.cc:1904] 	ModelConfig::dynamic_batching::priority_queue_policy::key
I0609 02:28:45.509249 1 model_config_utils.cc:1904] 	ModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds
I0609 02:28:45.509251 1 model_config_utils.cc:1904] 	ModelConfig::ensemble_scheduling::step::model_version
I0609 02:28:45.509253 1 model_config_utils.cc:1904] 	ModelConfig::input::dims
I0609 02:28:45.509254 1 model_config_utils.cc:1904] 	ModelConfig::input::reshape::shape
I0609 02:28:45.509256 1 model_config_utils.cc:1904] 	ModelConfig::instance_group::secondary_devices::device_id
I0609 02:28:45.509258 1 model_config_utils.cc:1904] 	ModelConfig::model_warmup::inputs::value::dims
I0609 02:28:45.509260 1 model_config_utils.cc:1904] 	ModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim
I0609 02:28:45.509263 1 model_config_utils.cc:1904] 	ModelConfig::optimization::cuda::graph_spec::input::value::dim
I0609 02:28:45.509264 1 model_config_utils.cc:1904] 	ModelConfig::output::dims
I0609 02:28:45.509266 1 model_config_utils.cc:1904] 	ModelConfig::output::reshape::shape
I0609 02:28:45.509268 1 model_config_utils.cc:1904] 	ModelConfig::sequence_batching::direct::max_queue_delay_microseconds
I0609 02:28:45.509270 1 model_config_utils.cc:1904] 	ModelConfig::sequence_batching::max_sequence_idle_microseconds
I0609 02:28:45.509272 1 model_config_utils.cc:1904] 	ModelConfig::sequence_batching::oldest::max_queue_delay_microseconds
I0609 02:28:45.509274 1 model_config_utils.cc:1904] 	ModelConfig::sequence_batching::state::dims
I0609 02:28:45.509276 1 model_config_utils.cc:1904] 	ModelConfig::sequence_batching::state::initial_state::dims
I0609 02:28:45.509278 1 model_config_utils.cc:1904] 	ModelConfig::version_policy::specific::versions
I0609 02:28:45.509740 1 stub_launcher.cc:385] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /models/Mistral-7B-v0.3/1/model.py triton_python_backend_shm_region_cf2fe392-563d-4a2e-acd7-3a141c183094 1048576 1048576 1 /opt/tritonserver/backends/python 336 Mistral-7B-v0.3 DEFAULT
I0609 02:28:48.668810 1 python_be.cc:2055] model configuration:
{
    ""name"": ""Mistral-7B-v0.3"",
    ""platform"": """",
    ""backend"": ""python"",
    ""runtime"": """",
    ""version_policy"": {
        ""latest"": {
            ""num_versions"": 1
        }
    },
    ""max_batch_size"": 0,
    ""input"": [
        {
            ""name"": ""input_text"",
            ""data_type"": ""TYPE_STRING"",
            ""format"": ""FORMAT_NONE"",
            ""dims"": [
                1,
                1
            ],
            ""is_shape_tensor"": false,
            ""allow_ragged_batch"": false,
            ""optional"": false
        },
        {
            ""name"": ""function"",
            ""data_type"": ""TYPE_STRING"",
            ""format"": ""FORMAT_NONE"",
            ""dims"": [
                1
            ],
            ""is_shape_tensor"": false,
            ""allow_ragged_batch"": false,
            ""optional"": false
        }
    ],
    ""output"": [
        {
            ""name"": ""output_text"",
            ""data_type"": ""TYPE_STRING"",
            ""dims"": [
                1
            ],
            ""label_filename"": """",
            ""is_shape_tensor"": false
        },
        {
            ""name"": ""output_embedding"",
            ""data_type"": ""TYPE_FP32"",
            ""dims"": [
                -1
            ],
            ""label_filename"": """",
            ""is_shape_tensor"": false
        },
        {
            ""name"": ""output_ids"",
            ""data_type"": ""TYPE_INT64"",
            ""dims"": [
                -1
            ],
            ""label_filename"": """",
            ""is_shape_tensor"": false
        }
    ],
    ""batch_input"": [],
    ""batch_output"": [],
    ""optimization"": {
        ""priority"": ""PRIORITY_DEFAULT"",
        ""input_pinned_memory"": {
            ""enable"": true
        },
        ""output_pinned_memory"": {
            ""enable"": true
        },
        ""gather_kernel_buffer_threshold"": 0,
        ""eager_batching"": false
    },
    ""instance_group"": [
        {
            ""name"": ""Mistral-7B-v0.3_0"",
            ""kind"": ""KIND_GPU"",
            ""count"": 1,
            ""gpus"": [
                0
            ],
            ""secondary_devices"": [],
            ""profile"": [],
            ""passive"": false,
            ""host_policy"": """"
        }
    ],
    ""default_model_filename"": ""model.py"",
    ""cc_model_filenames"": {},
    ""metric_tags"": {},
    ""parameters"": {},
    ""model_warmup"": []
}
I0609 02:28:48.668972 1 python_be.cc:2404] TRITONBACKEND_ModelInstanceInitialize: Mistral-7B-v0.3_0_0 (GPU device 0)
I0609 02:28:48.669047 1 backend_model_instance.cc:106] Creating instance Mistral-7B-v0.3_0_0 on GPU 0 (8.9) using artifact 'model.py'
I0609 02:28:48.669335 1 stub_launcher.cc:385] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /models/Mistral-7B-v0.3/1/model.py triton_python_backend_shm_region_36b77468-5d65-49c9-85a2-5846aa709a7a 1048576 1048576 1 /opt/tritonserver/backends/python 336 Mistral-7B-v0.3_0_0 DEFAULT
Loading checkpoint shards: 100%|#############################################################################################################################################################################| 3/3 [00:03<00:00,  1.00s/it]
Failed to load model for task ner: Unrecognized configuration class <class 'transformers.models.mistral.configuration_mistral.MistralConfig'> for this kind of AutoModel: AutoModelForTokenClassification.
Model type should be one of AlbertConfig, BertConfig, BigBirdConfig, BioGptConfig, BloomConfig, BrosConfig, CamembertConfig, CanineConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LiltConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, NezhaConfig, NystromformerConfig, PhiConfig, Phi3Config, QDQBertConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, T5Config, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.
Failed to load model for task summarization: Unrecognized configuration class <class 'transformers.models.mistral.configuration_mistral.MistralConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.
Model type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.
Loading checkpoint shards: 100%|#############################################################################################################################################################################| 3/3 [00:03<00:00,  1.26s/it]
Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at /models/Mistral-7B-v0.3/1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Failed to load model for task translation: Unrecognized configuration class <class 'transformers.models.mistral.configuration_mistral.MistralConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.
Model type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.
I0609 02:28:57.463372 1 python_be.cc:2425] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful Mistral-7B-v0.3_0_0 (device 0)
I0609 02:28:57.473754 1 backend_model_instance.cc:772] Starting backend thread for Mistral-7B-v0.3_0_0 at nice 0 on device 0...
I0609 02:28:57.480430 1 model_lifecycle.cc:835] successfully loaded 'Mistral-7B-v0.3'
I0609 02:28:57.483055 1 server.cc:607] 
+------------------+------+
| Repository Agent | Path |
+------------------+------+
+------------------+------+

I0609 02:28:57.484239 1 server.cc:634] 
+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend | Path                                                  | Config                                                                                                                                                        |
+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| python  | /opt/tritonserver/backends/python/libtriton_python.so | {""cmdline"":{""auto-complete-config"":""true"",""backend-directory"":""/opt/tritonserver/backends"",""min-compute-capability"":""6.000000"",""default-max-batch-size"":""4""}} |
+---------+-------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0609 02:28:57.485211 1 server.cc:677] 
+-----------------+---------+--------+
| Model           | Version | Status |
+-----------------+---------+--------+
| Mistral-7B-v0.3 | 1       | READY  |
+-----------------+---------+--------+

I0609 02:28:57.639809 1 metrics.cc:877] Collecting metrics for GPU 0: NVIDIA RTX 4000 SFF Ada Generation
I0609 02:28:57.640830 1 metrics.cc:770] Collecting CPU metrics
I0609 02:28:57.641538 1 tritonserver.cc:2538] 
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                |
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                               |
| server_version                   | 2.45.0                                                                                                                                                                                               |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics tr |
|                                  | ace logging                                                                                                                                                                                          |
| model_repository_path[0]         | /models                                                                                                                                                                                              |
| model_control_mode               | MODE_NONE                                                                                                                                                                                            |
| strict_model_config              | 0                                                                                                                                                                                                    |
| rate_limit                       | OFF                                                                                                                                                                                                  |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |
| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                             |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |
| strict_readiness                 | 1                                                                                                                                                                                                    |
| exit_timeout                     | 30                                                                                                                                                                                                   |
| cache_enabled                    | 0                                                                                                                                                                                                    |
+----------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0609 02:28:57.652122 1 grpc_server.cc:2370] 
+----------------------------------------------+---------+
| GRPC KeepAlive Option                        | Value   |
+----------------------------------------------+---------+
| keepalive_time_ms                            | 7200000 |
| keepalive_timeout_ms                         | 20000   |
| keepalive_permit_without_calls               | 0       |
| http2_max_pings_without_data                 | 2       |
| http2_min_recv_ping_interval_without_data_ms | 300000  |
| http2_max_ping_strikes                       | 2       |
+----------------------------------------------+---------+

I0609 02:28:57.661526 1 grpc_server.cc:102] Ready for RPC 'Check', 0
I0609 02:28:57.662075 1 grpc_server.cc:102] Ready for RPC 'ServerLive', 0
I0609 02:28:57.662107 1 grpc_server.cc:102] Ready for RPC 'ServerReady', 0
I0609 02:28:57.662114 1 grpc_server.cc:102] Ready for RPC 'ModelReady', 0
I0609 02:28:57.662127 1 grpc_server.cc:102] Ready for RPC 'ServerMetadata', 0
I0609 02:28:57.662136 1 grpc_server.cc:102] Ready for RPC 'ModelMetadata', 0
I0609 02:28:57.662144 1 grpc_server.cc:102] Ready for RPC 'ModelConfig', 0
I0609 02:28:57.662322 1 grpc_server.cc:102] Ready for RPC 'SystemSharedMemoryStatus', 0
I0609 02:28:57.662339 1 grpc_server.cc:102] Ready for RPC 'SystemSharedMemoryRegister', 0
I0609 02:28:57.662363 1 grpc_server.cc:102] Ready for RPC 'SystemSharedMemoryUnregister', 0
I0609 02:28:57.662377 1 grpc_server.cc:102] Ready for RPC 'CudaSharedMemoryStatus', 0
I0609 02:28:57.662389 1 grpc_server.cc:102] Ready for RPC 'CudaSharedMemoryRegister', 0
I0609 02:28:57.662411 1 grpc_server.cc:102] Ready for RPC 'CudaSharedMemoryUnregister', 0
I0609 02:28:57.662425 1 grpc_server.cc:102] Ready for RPC 'RepositoryIndex', 0
I0609 02:28:57.662451 1 grpc_server.cc:102] Ready for RPC 'RepositoryModelLoad', 0
I0609 02:28:57.662460 1 grpc_server.cc:102] Ready for RPC 'RepositoryModelUnload', 0
I0609 02:28:57.662470 1 grpc_server.cc:102] Ready for RPC 'ModelStatistics', 0
I0609 02:28:57.662490 1 grpc_server.cc:102] Ready for RPC 'Trace', 0
I0609 02:28:57.662506 1 grpc_server.cc:102] Ready for RPC 'Logging', 0
I0609 02:28:57.662531 1 grpc_server.cc:366] Thread started for CommonHandler
I0609 02:28:57.663217 1 infer_handler.cc:680] New request handler for ModelInferHandler, 0
I0609 02:28:57.663269 1 infer_handler.h:1322] Thread started for ModelInferHandler
I0609 02:28:57.663410 1 infer_handler.cc:680] New request handler for ModelInferHandler, 0
I0609 02:28:57.663464 1 infer_handler.h:1322] Thread started for ModelInferHandler
I0609 02:28:57.663761 1 stream_infer_handler.cc:128] New request handler for ModelStreamInferHandler, 0
I0609 02:28:57.663800 1 infer_handler.h:1322] Thread started for ModelStreamInferHandler
I0609 02:28:57.663817 1 grpc_server.cc:2463] Started GRPCInferenceService at 0.0.0.0:8001
I0609 02:28:57.665315 1 http_server.cc:4692] Started HTTPService at 0.0.0.0:8000
I0609 02:28:57.707641 1 http_server.cc:362] Started Metrics Service at 0.0.0.0:8002
I0609 02:29:10.838304 1 infer_handler.cc:702] Process for ModelInferHandler, rpc_ok=1, 0 step START
I0609 02:29:10.838333 1 infer_handler.cc:680] New request handler for ModelInferHandler, 0
I0609 02:29:10.839025 1 infer_request.cc:131] [request id: <id_unknown>] Setting state from INITIALIZED to INITIALIZED
I0609 02:29:10.839034 1 infer_request.cc:900] [request id: <id_unknown>] prepared: [0x0x7ffa98007110] request id: , model: Mistral-7B-v0.3, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 0, priority: 0, timeout (us): 0
original inputs:
[0x0x7ffa980077d8] input: function, type: BYTES, original shape: [1], batch + shape: [1], shape: [1]
[0x0x7ffa98007698] input: input_text, type: BYTES, original shape: [1,1], batch + shape: [1,1], shape: [1,1]
override inputs:
inputs:
[0x0x7ffa98007698] input: input_text, type: BYTES, original shape: [1,1], batch + shape: [1,1], shape: [1,1]
[0x0x7ffa980077d8] input: function, type: BYTES, original shape: [1], batch + shape: [1], shape: [1]
original requested outputs:
output_text
requested outputs:
output_text

I0609 02:29:10.839047 1 infer_request.cc:131] [request id: <id_unknown>] Setting state from INITIALIZED to PENDING
I0609 02:29:10.839070 1 infer_request.cc:131] [request id: <id_unknown>] Setting state from PENDING to EXECUTING
I0609 02:29:10.839104 1 python_be.cc:1395] model Mistral-7B-v0.3, instance Mistral-7B-v0.3_0_0, executing 1 requests
Both `max_new_tokens` (=10000) and `max_length`(=10000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

*************************************
This is my ""model.py"" code (called by the model inside the model_repository, see below):

import triton_python_backend_utils as pb_utils
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline
import torch
import numpy as np

class TritonPythonModel:
    def initialize(self, args):
        model_dir = args['model_repository'] + '/' + args['model_version']
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        
        self.model_chatbot = AutoModelForCausalLM.from_pretrained(model_dir)
        self.model_ner = self.load_model_for_task(model_dir, ""ner"")
        self.model_summarization = self.load_model_for_task(model_dir, ""summarization"")
        self.model_sentiment = self.load_model_for_task(model_dir, ""sentiment"")
        self.model_translation = self.load_model_for_task(model_dir, ""translation"")
        self.model_embedding = pipeline(""feature-extraction"", model=self.model_chatbot, tokenizer=self.tokenizer)

    def load_model_for_task(self, model_dir, task):
        try:
            if task == ""ner"":
                return pipeline(""ner"", model=AutoModelForTokenClassification.from_pretrained(model_dir), tokenizer=self.tokenizer)
            elif task == ""summarization"" or task == ""translation"":
                return pipeline(task, model=AutoModelForSeq2SeqLM.from_pretrained(model_dir), tokenizer=self.tokenizer)
            elif task == ""sentiment"":
                return pipeline(""sentiment-analysis"", model=AutoModelForSequenceClassification.from_pretrained(model_dir), tokenizer=self.tokenizer)
            else:
                raise ValueError(f""Task {task} is not supported by the model configuration."")
        except Exception as e:
            print(f""Failed to load model for task {task}: {str(e)}"")
            return None

    def execute(self, requests):
        responses = []
        for request in requests:
            input_text_tensor = pb_utils.get_input_tensor_by_name(request, ""input_text"")
            function_tensor = pb_utils.get_input_tensor_by_name(request, ""function"")
            
            input_texts = input_text_tensor.as_numpy().tolist()
            functions = function_tensor.as_numpy().tolist()
            
            for input_text, function in zip(input_texts, functions):
                input_text = input_text[0].decode('utf-8')
                function = function.decode('utf-8')
                
                if function == 'chatbot':
                    responses.append(self.chatbot_response(input_text))
                elif function == 'ner' and self.model_ner:
                    responses.append(self.ner_response(input_text))
                elif function == 'summarization' and self.model_summarization:
                    responses.append(self.summarization_response(input_text))
                elif function == 'sentiment' and self.model_sentiment:
                    responses.append(self.sentiment_response(input_text))
                elif function == 'translation' and self.model_translation:
                    responses.append(self.translation_response(input_text))
                elif function == 'embedding':
                    responses.append(self.embedding_response(input_text))
                elif function == 'tokenization':
                    responses.append(self.tokenization_response(input_text))
                else:
                    responses.append(self.error_response(f""Function {function} not supported or model not available""))

        return responses

    def chatbot_response(self, input_text):
        input_ids = self.tokenizer.encode(input_text, return_tensors=""pt"")
        attention_mask = torch.ones_like(input_ids)
        outputs = self.model_chatbot.generate(input_ids, attention_mask=attention_mask, pad_token_id=self.tokenizer.eos_token_id, max_new_tokens=10000, max_length=10000)
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        output_tensor = pb_utils.Tensor(""output_text"", np.array([generated_text], dtype=object))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def ner_response(self, input_text):
        ner_results = self.model_ner(input_text)
        output_tensor = pb_utils.Tensor(""output_text"", np.array([str(ner_results)], dtype=object))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def summarization_response(self, input_text):
        summary = self.model_summarization(input_text)
        output_tensor = pb_utils.Tensor(""output_text"", np.array([summary[0]['summary_text']], dtype=object))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def sentiment_response(self, input_text):
        sentiment = self.model_sentiment(input_text)
        output_tensor = pb_utils.Tensor(""output_text"", np.array([str(sentiment)], dtype=object))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def translation_response(self, input_text):
        translation = self.model_translation(input_text)
        output_tensor = pb_utils.Tensor(""output_text"", np.array([translation[0]['translation_text']], dtype=object))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def embedding_response(self, input_text):
        embeddings = self.model_embedding(input_text)
        output_tensor = pb_utils.Tensor(""output_embedding"", np.array(embeddings, dtype=np.float32))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def tokenization_response(self, input_text):
        tokenized = self.tokenizer(input_text)
        output_tensor = pb_utils.Tensor(""output_ids"", np.array([tokenized.input_ids], dtype=np.int64))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def error_response(self, message):
        output_tensor = pb_utils.Tensor(""output_text"", np.array([message], dtype=object))
        return pb_utils.InferenceResponse(output_tensors=[output_tensor])

    def finalize(self):
        pass

***********************
model repository: 

├── Mistral-7B-v0.3
│   ├── 1
│   │   ├── README.md
│   │   ├── __pycache__
│   │   │   └── model.cpython-310.pyc
│   │   ├── config.json
│   │   ├── consolidated.safetensors
│   │   ├── generation_config.json
│   │   ├── model-00001-of-00003.safetensors
│   │   ├── model-00002-of-00003.safetensors
│   │   ├── model-00003-of-00003.safetensors
│   │   ├── model.py
│   │   ├── model.safetensors.index.json
│   │   ├── params.json
│   │   ├── special_tokens_map.json
│   │   ├── tokenizer.json
│   │   ├── tokenizer.model
│   │   ├── tokenizer.model.v3
│   │   └── tokenizer_config.json
│   └── config.pbtxt

Please HELP!

+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX 4000 SFF Ada ...    Off | 00000000:01:00.0 Off |                  Off |
| 30%   37C    P8              11W /  70W |    229MiB / 20475MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    395864      C   tritonserver                                222MiB |
+---------------------------------------------------------------------------------------+

"
ArthurZ,2023-10-08 20:37:42+00:00,comment,mistralai/Mistral-7B-v0.1,Could you share the full traceback? 
Jenad1kr,2023-10-08 21:22:54+00:00,comment,mistralai/Mistral-7B-v0.1,"@ArthurZ , 
here it is. I created a new space.
===== Application Startup at 2023-10-08 21:21:11 =====

/home/user/app/app.py:3: GradioDeprecationWarning: gr.Interface.load() will be deprecated. Use gr.load() instead.
  gr.Interface.load(""models/mistralai/Mistral-7B-v0.1"").launch()
Fetching model from: https://huggingface.co/mistralai/Mistral-7B-v0.1
Running on local URL:  http://0.0.0.0:7860

To create a public link, set `share=True` in `launch()`.
Traceback (most recent call last):
  File ""/home/user/.local/lib/python3.10/site-packages/requests/models.py"", line 971, in json
    return complexjson.loads(self.text, **kwargs)
  File ""/usr/local/lib/python3.10/json/__init__.py"", line 346, in loads
    return _default_decoder.decode(s)
  File ""/usr/local/lib/python3.10/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/local/lib/python3.10/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/user/.local/lib/python3.10/site-packages/gradio/queueing.py"", line 406, in call_prediction
    output = await route_utils.call_process_api(
  File ""/home/user/.local/lib/python3.10/site-packages/gradio/route_utils.py"", line 226, in call_process_api
    output = await app.get_blocks().process_api(
  File ""/home/user/.local/lib/python3.10/site-packages/gradio/blocks.py"", line 1554, in process_api
    result = await self.call_function(
  File ""/home/user/.local/lib/python3.10/site-packages/gradio/blocks.py"", line 1192, in call_function
    prediction = await anyio.to_thread.run_sync(
  File ""/home/user/.local/lib/python3.10/site-packages/anyio/to_thread.py"", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File ""/home/user/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 877, in run_sync_in_worker_thread
    return await future
  File ""/home/user/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py"", line 807, in run
    result = context.run(func, *args)
  File ""/home/user/.local/lib/python3.10/site-packages/gradio/utils.py"", line 659, in wrapper
    response = f(*args, **kwargs)
  File ""/home/user/.local/lib/python3.10/site-packages/gradio/external.py"", line 415, in query_huggingface_api
    errors_json = response.json()
  File ""/home/user/.local/lib/python3.10/site-packages/requests/models.py"", line 975, in json
    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)
requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

"
piperod91,2023-10-21 02:43:42+00:00,comment,mistralai/Mistral-7B-v0.1,Same issue 
dwipper,2023-10-30 13:20:35+00:00,comment,mistralai/Mistral-7B-v0.1,"@ArthurZ Same Issue. I did the quick-create to host a model in a new space. I generated this Gradio code:

import gradio as gr
import os
import requests

HUGGING_FACE_TOKEN=os.environ.get('HUGGING_FACE_TOKEN', None)

gr.load(name=""models/meta-llama/Llama-2-7b-chat-hf"",hf_token=HUGGING_FACE_TOKEN).launch()"
juliaah,2023-11-05 13:38:30+00:00,comment,mistralai/Mistral-7B-v0.1,Did you find a solution to this?
dwipper,2023-11-05 14:07:11+00:00,comment,mistralai/Mistral-7B-v0.1,"Unfortunately no. If you search Spaces for ""Llama-2-7b-chat-hf"" numerous people have created Spaces the same way, and they have errors. "
ArthurZ,2023-10-08 20:42:50+00:00,comment,mistralai/Mistral-7B-v0.1,"Hey! I am not sure what is wrong here. A prefix space is added as expected. See the tokenization:
```python 
>>> from transformers import AutoTokenizer
>>> tokenizer  = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-v0.1"")
>>> tokenizer.tokenize(""\n\n"", add_special_tokens=False)
 ['▁', '<0x0A>', '<0x0A>']

>>> tokenizer.tokenize("".\n\n"", add_special_tokens=False)
 ['▁.', '<0x0A>', '<0x0A>']
```
The character ""\n\n""  is not a token in the vocab, I would recommend you to stop based on two indexes (13,13) rather than taking the prefix space into account"
jiankunww,2023-10-07 12:01:23+00:00,comment,mistralai/Mistral-7B-v0.1,11
ArthurZ,2023-10-08 20:44:45+00:00,comment,mistralai/Mistral-7B-v0.1,"LGTM! As discussed offline I don't think this should be kept. For transformers 4.34 it will be ignored, for earlier versions it will break. For the next patch it will no longer be saved."
LucileSaulnier,2023-10-06 15:06:18+00:00,comment,mistralai/Mistral-7B-v0.1,"Hello @xzuyn ,

I'm sorry you're having trouble using `mistralai/Mistral-7B-v0.1`. 

In the release of transformers `v4.34`,  some breaking changes regarding the tokenization API were introduced, I suspect your issue is linked to this. What version of transformers are you using ?
"
xzuyn,2023-10-06 16:19:05+00:00,comment,mistralai/Mistral-7B-v0.1,"> What version of transformers are you using ?

`4.34.0`

Is there a reason there's an `added_tokens.json` here though? [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) doesn't have it. It also seems completely reduntant to have since its just adding tokens which already exist, so its creating a problem for no reason (unless there's something I missed, then in that case my mistake).

Deleting the file allowed me to convert the model to GGUF perfectly fine since the other config files already specify the tokens."
LucileSaulnier,2023-10-06 16:56:06+00:00,comment,mistralai/Mistral-7B-v0.1,"> Is there a reason there's an added_tokens.json here though? Mistral-7B-Instruct-v0.1 doesn't have it. It also seems completely reduntant to have since its just adding tokens which already exist, so its creating a problem for no reason (unless there's something I missed, then in that case my mistake).

This file is there because it is saved by the `save_pretrained` method with transformers `4.34.0`. Unfortunately, I don't know what API for tokenizers transformers maintainers want to do in the future given the recent changes, in other words, whether the file should be there or not. @ArthurZ can you guide us on what to do please? That's sad that a user see this issue with transformers `4.34.0`. 

About Mistral-7B-Instruct-v0.1's tokenizer, we need to change its tokenizer too to match the one in this repo. With the [recent tokenizer related changes inside transformers](https://github.com/huggingface/transformers/pull/23909), the tokenizer doesn't handle the special tokens as we expected."
ArthurZ,2023-10-08 20:37:12+00:00,comment,mistralai/Mistral-7B-v0.1,"The file will have to stay for forward compatibility (people who did not update to 4.34 should still be able to load a new tokenizer for older tokenizers classes for example), but this one in particular was wrong. A patch will be published soon and I believe the file should be removed! 
"
RonanMcGovern,2023-11-02 10:55:40+00:00,comment,mistralai/Mistral-7B-v0.1,Can this be merged please? much quicker for downloading and using tgi
lcahill,2023-11-17 01:38:55+00:00,comment,mistralai/Mistral-7B-v0.1,Where is `model-00002-of-00002.safetensors`?
RonanMcGovern,2023-11-17 17:08:32+00:00,comment,mistralai/Mistral-7B-v0.1,"Yeah actually looks like the safetensors need to be remade before pushing.

Would be good to do this and have safetensors available."
lcahill,2023-11-18 10:11:56+00:00,comment,mistralai/Mistral-7B-v0.1,Added PR https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/91 with both files. Can confirm from local testing that the model loads and runs successfully. 
nth-attempt,2023-10-05 21:56:40+00:00,comment,mistralai/Mistral-7B-v0.1,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
TchoupiMasque,2023-10-07 18:47:27+00:00,comment,mistralai/Mistral-7B-v0.1,"ça dépend. j'ai testé sous ooga booga, en parlant directement en français, il répond dans cette langue, mais le moindre mot en anglais a tendance à le destabiliser.
niveau français, ça reste compréhensible."
Giroud,2023-10-08 10:01:36+00:00,comment,mistralai/Mistral-7B-v0.1,"Ooga booga ? Je ne vois pas ce que c'est. Pour le moment, j'ai utilisé des versions en ligne : https://labs.perplexity.ai/ et https://huggingface.co/chat/. Mais en conversation en français, les réponses sont incompatibles ou carrément fausses, et parfois la réponse vient en partie en français avec des parties en anglais au milieu, ce qui est vraiment étrange.
"
FlorianJc,2023-10-12 11:49:45+00:00,comment,mistralai/Mistral-7B-v0.1,"Si vous lui donnez un prompt qui dit (en français) ""tu es un assistant qui répond en français"", il répond normalement en français.
Je l'ai testé avec llama.cpp avec les modèles convertis de TheBloke et ça marche très bien.

A noter que ce modèle dérivé est particulièrement performant (en tout cas pour la discussion de type chat):
https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca

Mon prompt type est:
""""""
A partir le maintenant le texte sera en français uniquement...
L'utilisateur donne des instructions et le modèle répond brièvement.
""""""

Mettre ensuite un début de dialogue aide également beaucoup pour l'obliger à continuer la conversation en français.
Du coup un prompt type avec le modèle Mistral-7B-Instruct-v0.1 sur llama.cpp donnerait ça:
""""""
\<s>[INST] Transcription textuelle d'un dialogue, dans laquelle [[USER_NAME]] interagit avec un assistant IA nommé [[AI_NAME]].
[[AI_NAME]] est serviable et répond aux demandes de [[USER_NAME]] en français et avec précision.
[[AI_NAME]] arrête de répondre si il n'est pas certain de dire la vérité.
La transcription comprend uniquement du texte, elle n'inclut pas de balisage comme HTML et Markdown. [/INST]

[INST] [[USER_NAME]] : Bonjour, [[AI_NAME]] ! [/INST]
[[AI_NAME]] : Bonjour [[USER_NAME]] !
[INST] [[USER_NAME]] : En quelle année sommes-nous ? [/INST]
[[AI_NAME]] : Nous sommes en [[DATE_YEAR]].
[INST] [[USER_NAME]] : Veuillez me dire quelle est la plus grande ville d'Europe. [/INST]
[[AI_NAME]] : la plus grande ville d'Europe est Moscou, la capitale de la Russie.
[INST] [[USER_NAME]] : Qu'est-ce qu'un chat ? [/INST]
[[AI_NAME]] : un chat est une espèce domestique de petit mammifère carnivore. C'est la seule espèce domestiquée de la famille des félidés.
[INST] [[USER_NAME]] : nommez une couleur. [/INST]
[[AI_NAME]] : bleu.
[INST] [[USER_NAME]] : Quelle heure est-il ? [/INST]
[[AI_NAME]] : Il est [[DATE_TIME]].
"""""""
Giroud,2023-10-12 12:11:34+00:00,comment,mistralai/Mistral-7B-v0.1,Merci beaucoup pour les informations. Je vais essayer cela.
bofenghuang,2023-10-16 09:04:25+00:00,comment,mistralai/Mistral-7B-v0.1,Voici notre version fine-tunée en français [Vigostral-7B-Chat](https://huggingface.co/bofenghuang/vigostral-7b-chat).  N'hésitez pas à la tester et à partager vos commentaires !
Giroud,2023-10-16 09:35:11+00:00,comment,mistralai/Mistral-7B-v0.1,Merci beaucoup. Je vais essayer cela aussi.
lerela,2023-10-05 09:44:33+00:00,comment,mistralai/Mistral-7B-v0.1,"You need the Instruct version for that: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1

Prompt is documented here: https://docs.mistral.ai/llm/mistral-instruct-v0.1#chat-template"
DanekBigLike,2023-10-05 14:37:29+00:00,comment,mistralai/Mistral-7B-v0.1,"Is it possible to further train the model (LoRa or Checkpoint) and how? (Sorry if this is a stupid question, I'm still a newbie)"
deleted,2023-10-05 14:46:57+00:00,comment,mistralai/Mistral-7B-v0.1,not an expert but ooba offers in-GUI lora training 
DanekBigLike,2023-10-05 15:40:30+00:00,comment,mistralai/Mistral-7B-v0.1,Do ooba of them support Mistral?
DanekBigLike,2023-10-05 16:03:36+00:00,comment,mistralai/Mistral-7B-v0.1,"If I try to run through text generation web, I get this error:

Traceback (most recent call last):
  File ""E:\ai\ruai\saiga\text-generation-webui\modules\ui_model_menu.py"", line 194, in load_model_wrapper
    shared.model, shared.tokenizer = load_model(shared.model_name, loader)
  File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 75, in load_model
    output = load_func_map[loader](model_name)
  File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 136, in huggingface_loader
    config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=shared.args.trust_remote_code)
  File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 1039, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict[""model_type""]]
  File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 734, in __getitem__
    raise KeyError(key)
KeyError: 'mistral'"
deleted,2023-10-05 16:10:25+00:00,comment,mistralai/Mistral-7B-v0.1,"For what its worth, I can run mistral converted to gguf on ooba just fine on CPU.   Cant say about 'regular' model. My GPU is dedicated for other uses.
"
acrastt,2023-10-06 04:29:40+00:00,comment,mistralai/Mistral-7B-v0.1,"> If I try to run through text generation web, I get this error:
> 
> Traceback (most recent call last):
>   File ""E:\ai\ruai\saiga\text-generation-webui\modules\ui_model_menu.py"", line 194, in load_model_wrapper
>     shared.model, shared.tokenizer = load_model(shared.model_name, loader)
>   File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 75, in load_model
>     output = load_func_map[loader](model_name)
>   File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 136, in huggingface_loader
>     config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=shared.args.trust_remote_code)
>   File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 1039, in from_pretrained
>     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
>   File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 734, in __getitem__
>     raise KeyError(key)
> KeyError: 'mistral'

Can you try re-downloading Transformers from Github?"
DanekBigLike,2023-10-06 20:14:07+00:00,comment,mistralai/Mistral-7B-v0.1,"> > If I try to run through text generation web, I get this error:
> > 
> > Traceback (most recent call last):
> >   File ""E:\ai\ruai\saiga\text-generation-webui\modules\ui_model_menu.py"", line 194, in load_model_wrapper
> >     shared.model, shared.tokenizer = load_model(shared.model_name, loader)
> >   File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 75, in load_model
> >     output = load_func_map[loader](model_name)
> >   File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 136, in huggingface_loader
> >     config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=shared.args.trust_remote_code)
> >   File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 1039, in from_pretrained
> >     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
> >   File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 734, in __getitem__
> >     raise KeyError(key)
> > KeyError: 'mistral'
> 
> Can you try re-downloading Transformers from Github?

I'll try, thanks. (Do you mean that Mistral is using a new version of the transformer? If not, it probably won't work, since Llama starts without problems)"
acrastt,2023-10-06 23:58:46+00:00,comment,mistralai/Mistral-7B-v0.1,"> > > If I try to run through text generation web, I get this error:
> > > 
> > > Traceback (most recent call last):
> > >   File ""E:\ai\ruai\saiga\text-generation-webui\modules\ui_model_menu.py"", line 194, in load_model_wrapper
> > >     shared.model, shared.tokenizer = load_model(shared.model_name, loader)
> > >   File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 75, in load_model
> > >     output = load_func_map[loader](model_name)
> > >   File ""E:\ai\ruai\saiga\text-generation-webui\modules\models.py"", line 136, in huggingface_loader
> > >     config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=shared.args.trust_remote_code)
> > >   File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 1039, in from_pretrained
> > >     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
> > >   File ""C:\Users\remot\.conda\envs\textgen2\lib\site-packages\transformers\models\auto\configuration_auto.py"", line 734, in __getitem__
> > >     raise KeyError(key)
> > > KeyError: 'mistral'
> > 
> > Can you try re-downloading Transformers from Github?
> 
> I'll try, thanks. (Do you mean that Mistral is using a new version of the transformer? If not, it probably won't work, since Llama starts without problems)
Yeah."
Shashankbr07,2024-03-08 07:24:22+00:00,comment,mistralai/Mistral-7B-v0.1,"I need to use this model for my question answering project. can I use this directly ? or should I have to train this on a squad dataset for the usage ?

from transformers import AutoTokenizer, AutoModelForQuestionAnswering
model_name = ""mistralai/Mixtral-8x7B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
codegood,2024-03-22 08:29:01+00:00,comment,mistralai/Mistral-7B-v0.1,@Shashankbr07 You can use it directly
lerela,2023-10-12 17:55:35+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi, it is not. See #8"
almatkai,2024-04-01 19:16:45+00:00,comment,mistralai/Mistral-7B-v0.1,the same question
LucileSaulnier,2023-10-03 14:02:53+00:00,comment,mistralai/Mistral-7B-v0.1,
Evan102,2023-10-04 06:55:16+00:00,comment,mistralai/Mistral-7B-v0.1,"When I load this model, it appears the error ""ValueError: Non-consecutive added token ' < unk > ' found. Should have index 32000 but has index 0 in saved vocabulary."" 
Should ""added_tokens.json"" be removed?
"
ArthurZ,2023-10-04 17:18:30+00:00,comment,mistralai/Mistral-7B-v0.1,"Yes, getting the same with the main branch so I'll have look. 
"
ArthurZ,2023-10-05 06:06:52+00:00,comment,mistralai/Mistral-7B-v0.1,@Evan102 you are probably not using `transformers==4.34` which is required to run mistral! 
ybelkada,2023-10-03 16:59:07+00:00,comment,mistralai/Mistral-7B-v0.1,"Closing the PR as this seems to be the correct one:

```bash
more mistral-7B-v0.1/params.json
>>> {
    ""dim"": 4096,
    ""n_layers"": 32,
    ""head_dim"": 128,
    ""hidden_dim"": 14336,
    ""n_heads"": 32,
    ""n_kv_heads"": 8,
    ""norm_eps"": 1e-05,
    ""sliding_window"": 4096,
    ""vocab_size"": 32000
}
```"
NickyNicky,2023-10-02 23:05:27+00:00,comment,mistralai/Mistral-7B-v0.1,"Other ERROR BetterTransformer -->> flash attention 2

```Python
!python -c ""import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'""
!export CUDA_HOME=/usr/local/cuda-11.8
# !MAX_JOBS=4 pip install flash-attn --no-build-isolation
!MAX_JOBS=4 pip install flash-attn --no-build-isolation  -qqq
!pip install git+""https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary"" -qqq
!python -m pip install optimum -qqq


from optimum.bettertransformer import BetterTransformer #flash attention 2

model = AutoModelForCausalLM.from_pretrained(model_id,
                                             device_map=""auto"",
                                             trust_remote_code=True,
                                             torch_dtype=torch.bfloat16,
                                             load_in_4bit=True,
                                             quantization_config=quantization_config,
                                             # use_flash_attention_2=True,
                                             low_cpu_mem_usage= True,
                                             )

# model = BetterTransformer.transform(model, keep_original_model=False) #flash attention 2
```

ERROR 2
```Python
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-1-a5799a87d18b> in <cell line: 43>()
     41                                              )
     42 
---> 43 model = BetterTransformer.transform(model, keep_original_model=False) #flash attention 2
     44 
     45 

1 frames
/usr/local/lib/python3.10/dist-packages/optimum/bettertransformer/transformation.py in transform(model, keep_original_model, max_memory, offload_dir, **kwargs)
    226             )
    227         if not BetterTransformerManager.supports(model.config.model_type):
--> 228             raise NotImplementedError(
    229                 f""The model type {model.config.model_type} is not yet supported to be used with BetterTransformer. Feel free""
    230                 f"" to open an issue at https://github.com/huggingface/optimum/issues if you would like this model type to be supported.""

NotImplementedError: The model type mistral is not yet supported to be used with BetterTransformer. Feel free to open an issue at https://github.com/huggingface/optimum/issues 
if you would like this model type to be supported. Currently supported models are: 
dict_keys(['albert', 'bark', 'bart', 'bert', 'bert-generation', 
'blenderbot', 'bloom', 'camembert', 'blip-2', 'clip', 'codegen', 'data2vec-text', 'deit',
 'distilbert', 'electra', 'ernie', 'fsmt', 'falcon', 'gpt2', 'gpt_bigcode', 'gptj', 'gpt_neo', 
'gpt_neox', 'hubert', 'layoutlm', 'llama', 'm2m_100', 'marian', 'markuplm', 'mbart',
 'opt', 'pegasus', 'rembert', 'prophetnet', 'roberta', 'roc_bert', 'roformer', 'splinter', 
'tapas', 't5', 'vilt', 'vit', 'vit_mae', 'vit_msn', 'wav2vec2', 'whisper', 'xlm-roberta', 'yolos']).
```"
sgauravm,2023-10-12 13:49:14+00:00,comment,mistralai/Mistral-7B-v0.1,Getting the same error. In the supported models they say that Mistral is included but don't know why is it giving this error.
lysandre,2023-10-12 14:14:10+00:00,comment,mistralai/Mistral-7B-v0.1,cc @ybelkada 
ybelkada,2023-10-12 16:05:02+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi @NickyNicky and @sgauravm 

If you install the latest version of transformers

```bash
pip install -U transformers
```

Flash Attention-2 should be supported

Check out this specific section of the docs: https://huggingface.co/docs/transformers/model_doc/mistral#combining-mistral-and-flash-attention-2 for more details"
NickyNicky,2023-10-12 18:39:54+00:00,comment,mistralai/Mistral-7B-v0.1,"Thank you very much, 
I have the latest version of transformers."
ybelkada,2023-10-12 21:10:00+00:00,comment,mistralai/Mistral-7B-v0.1,"@NickyNicky 
Your first script

```python
!python -c ""import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'"" # pass
!export CUDA_HOME=/usr/local/cuda-11.8
# !MAX_JOBS=4 pip install flash-attn --no-build-isolation
!MAX_JOBS=4 pip install flash-attn --no-build-isolation  -qqq
!pip install git+""https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary"" -qqq
!python -m pip install optimum -qqq

import torch, transformers,torchvision
torch.__version__,transformers.__version__, torchvision.__version__ # ('2.0.1+cu118', '4.34.0.dev0', '0.15.2+cu118')

model_id = 'mistralai/Mistral-7B-Instruct-v0.1'

model = AutoModelForCausalLM.from_pretrained(model_id,
                                             device_map=""auto"",
                                             trust_remote_code=True,
                                             torch_dtype=torch.bfloat16,
                                             load_in_4bit=True,
                                             quantization_config=quantization_config,
                                             use_flash_attention_2=True,
                                             low_cpu_mem_usage= True,

                                             )
```

Should work if you have latest transformers installed, however Mistral is not in `BetterTransformer` yet, we will add the support of F.SDPA natively in transformers core soon"
NickyNicky,2023-10-12 22:51:21+00:00,comment,mistralai/Mistral-7B-v0.1,"With these versions it works.
```Python
import torch, transformers
torch.__version__,transformers.__version__
('2.0.1+cu118', '4.34.0')
```"
YaTharThShaRma999,2023-11-15 18:12:56+00:00,comment,mistralai/Mistral-7B-v0.1,"its easy to solve, just provide previous responses that include the model outputting nsfw responses and it should work."
harpercarroll,2023-10-02 17:11:41+00:00,comment,mistralai/Mistral-7B-v0.1,I noticed this too. Increasing the temperature helps 
NPap,2023-10-10 10:41:07+00:00,comment,mistralai/Mistral-7B-v0.1,Does it keep on generating text until it reaches the max_generated_tokens every time?
kypai,2023-10-12 11:02:15+00:00,comment,mistralai/Mistral-7B-v0.1,This comment has been hidden
adriata,2023-10-12 11:06:36+00:00,comment,mistralai/Mistral-7B-v0.1,"I have problem with generating text until it reaches the max_generated_tokens on  Tesla V100S GPU, but Colab A100 works ok. Any idea what can be done with this? "
NPap,2023-10-12 11:40:06+00:00,comment,mistralai/Mistral-7B-v0.1,"> I have problem with generating text until it reaches the max_generated_tokens on  Tesla V100S GPU, but Colab A100 works ok. Any idea what can be done with this?

You are running the exact same script + environment + component versions and you are getting different results with different gpus?"
adriata,2023-10-12 12:05:44+00:00,comment,mistralai/Mistral-7B-v0.1,"> You are running the exact same script + environment + component versions and you are getting different results with different gpus?

Ok, I double checked this. The problem occurred with my qlora finetuned model. Base model works ok on both gpu.  
"
nps798,2023-11-07 14:21:38+00:00,comment,mistralai/Mistral-7B-v0.1,"@adriata did you figure out eventually how to deal with repetition problem with mistral qlora fine tuned model? 
I also face this problem when I attempted to fine tuned on Chinese languages instruction dataset. no effective solution resolve my problem ...🥺"
Rmote6603,2024-03-26 16:26:06+00:00,comment,mistralai/Mistral-7B-v0.1,"Yeah, I am also running into the same issue even after increasing the temperature and penalty. Is this the same problem with Mistral-7b-v0.2 or with the Mistral-7b-instruct model? "
adriata,2024-03-27 15:54:08+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi, remember to set PAD token != EOS during finetuning. "
jdsannchao,2024-03-28 02:12:28+00:00,comment,mistralai/Mistral-7B-v0.1,"> Hi, remember to set PAD token != EOS during finetuning.

Hi could you explain more, thanks!"
adriata,2024-03-28 23:04:19+00:00,comment,mistralai/Mistral-7B-v0.1,"During fine tuning model can 'forget' about eos token </s> when we set it as pad token. It's because of masking - we don't learn model to predict padding. 
IDN what you use for fine tune, but setting this helped me with repetition: 
```
tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    model_max_length=4096,
    padding_side=""left"",
    add_eos_token=True # add eos at the end of text  
)

# tokenizer.pad_token = tokenizer.eos_token  # don't use this 
tokenizer.pad_token = tokenizer.unk_token  # use this
model.config.pad_token_id = tokenizer.pad_token_id 
```

My dummy fine-tuning sample:
 https://github.com/atadria/llm_calculator/blob/main/mistral_finetune.ipynb"
Rmote6603,2024-04-02 19:28:37+00:00,comment,mistralai/Mistral-7B-v0.1,What value did you set for pad_token_id? Could you explain more on what the pad_token_id does? 
Rmote6603,2024-04-02 19:41:37+00:00,comment,mistralai/Mistral-7B-v0.1,"I also read this article: https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k/discussions/3, but I am confused between what value to set for eos_token_id and pad_token_id. Has anyone played around with these two parameters and got the issue resolves? Please let me know. "
salvagimeno,2024-04-19 10:12:31+00:00,comment,mistralai/Mistral-7B-v0.1,Hi @Rmote6603 did you manage to solver this issue? Facing the same problem myself. Thanks!
Nowanda,2023-10-05 11:26:22+00:00,comment,mistralai/Mistral-7B-v0.1,Looking for this info too
lucadiliello,2023-10-06 06:36:09+00:00,comment,mistralai/Mistral-7B-v0.1,Look at the Mistral website. It looks like they used the Leonardo Italian supercomputer: https://mistral.ai/news/announcing-mistral-7b/
deleted,2023-09-30 11:44:18+00:00,comment,mistralai/Mistral-7B-v0.1,lol
Yhyu13,2023-09-30 13:24:27+00:00,comment,mistralai/Mistral-7B-v0.1,WTF
lunarflu,2023-09-30 16:50:08+00:00,comment,mistralai/Mistral-7B-v0.1,"@webtest1s Thanks for testing! Feel free to do it privately (for your own repos), that way you won't get pinged when people comment under it :)"
JaganDS,2023-09-30 12:13:59+00:00,comment,mistralai/Mistral-7B-v0.1,"I'm attempting to use deepspeed for model training, but the script crashes when trying to load the model. Additionally, the trainer is not assigning the model to the GPU.
"
harpercarroll,2023-10-02 17:31:27+00:00,comment,mistralai/Mistral-7B-v0.1,"You want to specify the `target_modules` which are the layers to apply LoRA to. I did the linear layers: 
    target_modules=[
        ""q_proj"", 
        ""k_proj"", 
        ""v_proj"", 
        ""o_proj"", 
        ""gate_proj"", 
        ""up_proj"", 
        ""down_proj"", 
        ""lm_head"",
    ], "
NPap,2023-10-11 10:23:08+00:00,comment,mistralai/Mistral-7B-v0.1,"According to the LoRA paper, the best performance you can get is if you apply it to all the linear layers of the model. (Diminishing returns if you were to choose just the Q and V but they propose all if I'm not mistaken)
Which are the ones @harpercarroll has mentioned. But if you want to go deeper and apply this to other models as well what you would do is instantiate your model, then go and print(model) then you will see all the layers with their names and you would want to include in the target_modules all layers that have linear or 4bitlinear (something like that if its 4bit). 
 "
lunarflu,2023-10-05 08:10:02+00:00,comment,mistralai/Mistral-7B-v0.1,@lerela Thanks for reporting!
clem,2023-10-02 23:22:29+00:00,comment,mistralai/Mistral-7B-v0.1,I think the Mistral team said that they would train bigger ones
deleted,2023-10-03 00:10:32+00:00,comment,mistralai/Mistral-7B-v0.1,"Great to hear. No rush or complaints, just was hoping that it was in the future plans. "
rezzie-rich,2023-12-28 04:25:26+00:00,comment,mistralai/Mistral-7B-v0.1,"Why would you want 13b when the 7b model is outperforming 70b models? it clearly shows the performance isn't reliant on the size but instead on how it's structured, like the I4 engine matching v8. What we need is the turbo or supercharger for the models while making the model more sophisticated. "
jlzhou,2023-12-28 05:18:55+00:00,comment,mistralai/Mistral-7B-v0.1,"> Why would you want 13b when the 7b model is outperforming 70b models? it clearly shows the performance isn't reliant on the size but instead on how it's structured, like the I4 engine matching v8. What we need is the turbo or supercharger for the models while making the model more sophisticated.

7b outperforming 70b doesn't mean that 13b could not do better.

Like with the same turbo or supercharger a v8 will probably outperform an l4

"
deleted,2023-12-28 12:41:38+00:00,comment,mistralai/Mistral-7B-v0.1,"> > Why would you want 13b when the 7b model is outperforming 70b models? it clearly shows the performance isn't reliant on the size but instead on how it's structured, like the I4 engine matching v8. What we need is the turbo or supercharger for the models while making the model more sophisticated.
> 
> 7b outperforming 70b doesn't mean that 13b could not do better.
> 


Agreed.  a 13b might be like another 100b+ model, and still be of more than reasonable size"
rezzie-rich,2023-12-29 08:35:57+00:00,comment,mistralai/Mistral-7B-v0.1,"> > > Why would you want 13b when the 7b model is outperforming 70b models? it clearly shows the performance isn't reliant on the size but instead on how it's structured, like the I4 engine matching v8. What we need is the turbo or supercharger for the models while making the model more sophisticated.
> > 
> > 7b outperforming 70b doesn't mean that 13b could not do better.
> > 
> 
> 
> Agreed.  a 13b might be like another 100b+ model, and still be of more than reasonable size

I understand the idealogy, 'make it bigger to make more power'. That's the same approach Google and openAI took and now it requires supercomputers to run those models. Seeing models with only 70b (XWIN-LM) surpassing gpt4 with 1.7T on alpacaeval and Zepher 7b matching the 1.7T model on some benchmarks should inspire people to focus more on refinement than just resize. The biggest advantage of 7b models is efficiency especially if you need to run multiple models for different agents at the same time on consumer-grade devices. Let's not forget the main goal of open-source AI, making it accessible for everyone."
deleted,2023-12-29 13:06:31+00:00,comment,mistralai/Mistral-7B-v0.1,"Adding a 13 isn't going to require a super computer, that is still more than approachable to us 'serfs'..  And no one said anything about removing the smaller option. Just adding more options. "
rezzie-rich,2023-12-29 23:50:05+00:00,comment,mistralai/Mistral-7B-v0.1,"> Adding a 13 isn't going to require a super computer, that is still more than approachable to us 'serfs'..  And no one said anything about removing the smaller option. Just adding more options.

You are right about not needing a supercomputer for 13b but my point is not to shift from the current strategy, it winning against models 100x bigger. For the need of more raw power, you have Mixtral 8x7b. That has 42b parameters making it 3x bigger than the 13b you're asking for."
deleted,2023-12-30 01:31:50+00:00,comment,mistralai/Mistral-7B-v0.1,"Agree that is bigger ( better,  time will tell ), but i do think 13 is a magic number for many. not too small, not too large.  And i like i was saying i never suggested for 7b to go to the wayside... just include the next step up.   Before you make the leap to something much larger."
YaTharThShaRma999,2023-10-01 15:57:51+00:00,comment,mistralai/Mistral-7B-v0.1,"Also, there’s gqa which is a big difference. Llama 7b, 13b do not contain gqa which mistral does"
CamilleSchnak,2023-10-04 07:20:14+00:00,comment,mistralai/Mistral-7B-v0.1,"I made it work with ooga booga.
model loader ""transformers"" and ""load in 4 bits"".
it take ages to load and answer not that quickly but it work."
elboertjie,2023-10-15 16:30:19+00:00,comment,mistralai/Mistral-7B-v0.1,"> I made it work with ooga booga.
> model loader ""transformers"" and ""load in 4 bits"".
> it take ages to load and answer not that quickly but it work.

How did you manage to download the model?

I'm using Ubuntu and when I execute the three lines below, I get the error at the bottom:

from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-v0.1"")
model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.1"")

Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████| 966/966 [00:00<00:00, 4.40MB/s]
Downloading tokenizer.model: 100%|████████████████████████████████████████████| 493k/493k [00:00<00:00, 18.9MB/s]
Downloading (…)/main/tokenizer.json: 100%|██████████████████████████████████| 1.80M/1.80M [00:00<00:00, 15.7MB/s]
Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████| 72.0/72.0 [00:00<00:00, 389kB/s]
Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████| 571/571 [00:00<00:00, 2.84MB/s]
Traceback (most recent call last):
  File ""/home/j/jr_loadmodel"", line 39, in <module>
    model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.1"")
  File ""/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py"", line 444, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File ""/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py"", line 940, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict[""model_type""]]
  File ""/home/j/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py"", line 655, in __getitem__
    raise KeyError(key)
KeyError: 'mistral'



"
elboertjie,2023-10-15 16:40:55+00:00,comment,mistralai/Mistral-7B-v0.1,"I found the answer:

1) Re-install transformers
2) pip install accelerate==0.20.3

That worked for me"
arzoop,2024-01-12 17:30:14+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi. Could you please make your jupyter available for us to see?
I am trying to run this code in google colab, and it says that the ram is not enough :(. I dont know what to do now."
mnwato,2023-09-29 08:37:05+00:00,comment,mistralai/Mistral-7B-v0.1,"Same issue.

After installing requirements using these two lines:
```
!pip install git+https://github.com/huggingface/transformers
!pip install accelerate bitsandbytes
```

I always use Use `load_in_4bit=True` and `device_map='cuda'` while loading model:
```
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-v0.1"")
model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.1"",  load_in_8bit=True, device_map='cuda')
```
But on colab CPU memory is getting OOM. I dont know why this is loading  on CPU memory instead of GPU memory!!!"
prudant,2023-09-30 01:59:05+00:00,comment,mistralai/Mistral-7B-v0.1,"from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer
import torch

model_name_or_path = ""mistralai/Mistral-7B-Instruct-v0.1""
config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)
config.max_position_embeddings = 8096
quantization_config = BitsAndBytesConfig(
    llm_int8_enable_fp32_cpu_offload=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    load_in_4bit=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    config=config,
    trust_remote_code=True,
    quantization_config=quantization_config,
    device_map=""auto"",
    offload_folder=""./offload""
)

prompt = ""<s>[INST]your prompt[/INST]""
print(""\n\n*** Generate:"")
inputs = tokenizer(prompt, return_tensors=""pt"", add_special_tokens=False).to(""cuda"")
streamer = TextStreamer(tokenizer, skip_prompt= True)
output = model.generate(**inputs,
                        streamer=streamer,
                        max_new_tokens=512,
                        temperature=0.3,
                        top_k=20,
                        top_p=0.4,
                        repetition_penalty=1.1, do_sample=True)"
01jonathanf,2024-03-01 10:22:50+00:00,comment,mistralai/Mistral-7B-v0.1,"For anyone using Colab, remove `device_map='cuda'` and it will load it onto the GPU correctly"
kira,2023-10-02 09:31:15+00:00,comment,mistralai/Mistral-7B-v0.1,"its still WIP, but I used this seems to work fine for FA2 https://github.com/huggingface/transformers/pull/26464"
jselvam,2023-09-28 18:44:24+00:00,comment,mistralai/Mistral-7B-v0.1,install transformers from source: pip install git+https://github.com/huggingface/transformers
CREET01,2023-09-29 15:38:14+00:00,comment,mistralai/Mistral-7B-v0.1,@jselvam This solution works.
atineoSE,2023-10-02 21:13:54+00:00,comment,mistralai/Mistral-7B-v0.1,This comment has been hidden
lerela,2023-10-05 07:49:18+00:00,comment,mistralai/Mistral-7B-v0.1,"Closing as this is indeed the solution. Mistral is now part of Transformers 4.34.0 so `pip install ""transformers>=4.34.0""` is enough."
samprita,2024-02-22 13:52:21+00:00,comment,mistralai/Mistral-7B-v0.1,"still its not working for CPU(im using google colab CPU)
i tried all the  above mentioned methods,still its giving 
KeyError                                  Traceback (most recent call last)
<ipython-input-13-90388a986bd4> in <cell line: 3>()
      1 tokenizer = AutoTokenizer.from_pretrained(model_name)
      2 
----> 3 model = AutoModelForCausalLM.from_pretrained(model_name,
      4                                              device_map=""auto"",
      5                                              #device_map = device_map,

2 frames
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py in __getitem__(self, key)
    669         (""reformer"", ""Reformer""),
    670         (""regnet"", ""RegNet""),
--> 671         (""rembert"", ""RemBERT""),
    672         (""resnet"", ""ResNet""),
    673         (""retribert"", ""RetriBERT""),

KeyError: 'mistral'
this error
"
songogeta31,2024-03-21 15:00:36+00:00,comment,mistralai/Mistral-7B-v0.1,"```
File /opt/conda/envs/py39/lib/python3.9/ctypes/__init__.py:382, in CDLL.__init__(self, name, mode, handle, use_errno, use_last_error, winmode)
    379 self._FuncPtr = _FuncPtr
    381 if handle is None:
--> 382     self._handle = _dlopen(self._name, mode)
    383 else:
    384     self._handle = handle

OSError: libiomp5.so: cannot open shared object file: No such file or directory
```

I am getting this error after solving transformers error"
Kishorekumr,2023-09-28 19:17:21+00:00,comment,mistralai/Mistral-7B-v0.1,+1
devendrachaplot,2023-09-28 21:45:34+00:00,comment,mistralai/Mistral-7B-v0.1,"Maybe this is useful:
https://colab.research.google.com/drive/1F2PeWl5FOHv4sjd7XTEu40JjqbFhC3LB?usp=sharing"
alexsherstinsky,2023-09-29 00:09:36+00:00,comment,mistralai/Mistral-7B-v0.1,"Thank you very much; this is helpful, although it does not answer the question about the amount of memory needed.  Still, thank you for the very useful notebook."
ianuvrat,2023-09-29 03:26:50+00:00,comment,mistralai/Mistral-7B-v0.1,I want to run quantized version. This model runs out of memory in free google colab 
girrajjangid,2023-09-29 05:20:22+00:00,comment,mistralai/Mistral-7B-v0.1,I am still getting OUT OF MEMORY error. Even using this notebook https://colab.research.google.com/drive/1F2PeWl5FOHv4sjd7XTEu40JjqbFhC3LB?usp=sharing 
alexsherstinsky,2023-09-29 06:11:14+00:00,comment,mistralai/Mistral-7B-v0.1,"Yes; I believe that downloading the model requires 15GB, which is more RAM than the free Colab account has.  The memory gets freed up after the download, but it is needed for the download to succeed.  Thank you."
girrajjangid,2023-09-29 08:54:08+00:00,comment,mistralai/Mistral-7B-v0.1,"Thanks me later.
You can load it using this on free colab.


!pip -q install git+https://github.com/huggingface/transformers # need to install from github
!pip -q install bitsandbytes accelerate xformers einops

import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = ""mistralai/Mistral-7B-Instruct-v0.1""
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    quantization_config=bnb_config,
    device_map='auto',
)

tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_id,
)

text = ""<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]""
encodeds = tokenizer(text, return_tensors=""pt"", add_special_tokens=False)
model_inputs = encodeds

generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)
decoded = tokenizer.batch_decode(generated_ids)
print(decoded[0])
"
ianuvrat,2023-09-29 09:36:49+00:00,comment,mistralai/Mistral-7B-v0.1,"@girrajjangid , its using system RAM in colab ..why? It runs out of memory with your code"
someone13574,2023-09-29 14:21:41+00:00,comment,mistralai/Mistral-7B-v0.1,"I’ve managed to load it into the gpu in a free instance by loading it when it is sharded into 2gb files https://huggingface.co/someone13574/Mistral-7B-v0.1-sharded
"
alexsherstinsky,2023-09-30 14:17:51+00:00,comment,mistralai/Mistral-7B-v0.1,Thank you everybody for your very helpful responses.  I will be learning from them in the coming days.  Much appreciated!
alexsherstinsky,2023-09-30 14:35:53+00:00,comment,mistralai/Mistral-7B-v0.1,@someone13574 Would you be so kind as to point to the tools and/or code for how you carried out the sharding?  I feel that it would be useful to a lot of people.  Thank you very much in advance.
someone13574,2023-09-30 18:05:40+00:00,comment,mistralai/Mistral-7B-v0.1,"@alexsherstinsky Just a simple script using huggingface transformers. You do need enough ram to fit the model though.

```py
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(""model/"", low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=""cpu"")
model.save_pretrained(""sharded"", max_shard_size=""2GB"")
```"
alexsherstinsky,2023-10-01 04:56:07+00:00,comment,mistralai/Mistral-7B-v0.1,"@someone13574 Thank you so much for this code and the explanation.  Yes, this really helps -- because in terms of RAM, I can use my development machine, which is Mac M1 Max with 64GB RAM, and then upload the results to HuggingFace -- and then go from there (I verified that your sharded model fits very comfortably in the free Google Colab tier instance with the T4 GPU.  So thank you very much again for this.  Cheers!"
alexsherstinsky,2023-10-02 23:56:38+00:00,comment,mistralai/Mistral-7B-v0.1,"@someone13574 I have a follow up question for you, if I may, please.  In the line `model.save_pretrained()` -- I do not see the path specified.  Is it possible to save the sharded model to a HuggingFace location in my account?  Thank you.  Also, once I get this to work on my end, I would like to use it in a pull request to an open source library.  What is the best way to acknowledge you for this idea?  Thank you very much again."
alexsherstinsky,2023-10-03 00:13:39+00:00,comment,mistralai/Mistral-7B-v0.1,"@someone13574 Oh, I think we just say `model.save_pretrained(path_on_hugging_face, max_shard_size=""2GB"")` -- is this correct?"
someone13574,2023-10-03 03:53:26+00:00,comment,mistralai/Mistral-7B-v0.1,"@alexsherstinsky `model.save_pretrained()` saves the model to the local path specified, which in the case of the code example I put above was a directory called `sharded`. Then you can just upload it manually from the website. I believe that you can push directly to huggingface using `push_to_hub()`, but I'm not sure if anything other than safetensors are supported, or if sharded models are supported. I just did it manually.

(Also, I don't need to be acknowledged.)"
alexsherstinsky,2023-10-03 04:14:46+00:00,comment,mistralai/Mistral-7B-v0.1,@someone13574 Got it -- really appreciate your explanation.  Thanks a lot!
alexsherstinsky,2023-10-03 05:08:12+00:00,comment,mistralai/Mistral-7B-v0.1,"@someone13574 Sorry to bother you again: one more question, if I may, please.  I tried your procedure, but ended up with only 7 (instead of 8) shard files.  In addition, I did not get README in there.  Even more importantly, I did not get the added tokens JSON file.  Could you please share the code for how you added the special tokens 
```
{
  ""</s>"": 2,
  ""<s>"": 1,
  ""<unk>"": 0
}
```
to your tokenizer ?

Just in case it helps, here is the code I used:
```
mistral_7b_original_base_model_name: str = ""mistralai/Mistral-7B-v0.1""

mistral_7b_sharded_base_model_name: str = ""alexsherstinsky/Mistral-7B-v0.1-sharded""

original_base_model_tokenizer: LlamaTokenizerFast = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=mistral_7b_original_base_model_name, trust_remote_code=True, padding_side='left')

original_base_model: MistralForCausalLM = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=mistral_7b_original_base_model_name, torch_dtype=torch.float16, trust_remote_code=True, device_map=""auto"", low_cpu_mem_usage=True)

original_base_model.save_pretrained(save_directory=f""/content/models/{mistral_7b_sharded_base_model_name}"", max_shard_size=""2GB"", push_to_hub=True)

original_base_model_tokenizer.save_pretrained(save_directory=f""/content/models/{mistral_7b_sharded_base_model_name}"", legacy_format=False, push_to_hub=True)
```

Thanks a lot again!"
someone13574,2023-10-03 21:16:37+00:00,comment,mistralai/Mistral-7B-v0.1,"@alexsherstinsky I just copied the files not save from `save_pretrained`, such as the tokenizer and all other files other than the model weights and the weight index over from the original model, as they should be the same whether or not the model is sharded."
alexsherstinsky,2023-10-03 21:28:46+00:00,comment,mistralai/Mistral-7B-v0.1,@someone13574 Got it -- this was very helpful.  Do you by chance know how we could have ended up with the different number of shard files?  Or did you copy the original base model's one as well?  Thanks a lot!
someone13574,2023-10-03 23:36:43+00:00,comment,mistralai/Mistral-7B-v0.1,@alexsherstinsky Not sure what would have caused that.
alexsherstinsky,2023-10-04 08:07:13+00:00,comment,mistralai/Mistral-7B-v0.1,"@someone13574 No matter what I have tried, I am unable to shard the way you have -- I always get 7 instead of 8 files.  I even tried to download your model and upload it sharded, and still get 7 files.  And loading my sharded result fails with this exception:
```
/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics)
    315                     module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)
    316         elif isinstance(value, torch.Tensor):
--> 317             new_value = value.to(device)
    318         else:
    319             new_value = torch.tensor(value, device=device)

NotImplementedError: Cannot copy out of meta tensor; no data!
```

Could the problem be the version of PyTorch and HuggingFace Transformers libraries that I am using and which may be wrong?  Could you please tell me the versions of these packages that you are using?

I think that my PyTorch version is 2.0.1+cu118 and Transformers version is 4.35.0.dev0 -- could this be my issue?

Thanks a lot for your help."
alexsherstinsky,2023-10-04 19:00:39+00:00,comment,mistralai/Mistral-7B-v0.1,"@someone13574 I believe that I found the issue.  It seems that the needed version of transformers should be `4.34.0`.  When I used that and did the sharding from my local machine (instead of Google Colab), everything worked properly.  Sorry to have disturbed you with the messages, and thank you again for your help."
lerela,2023-10-05 08:01:04+00:00,comment,mistralai/Mistral-7B-v0.1,"Glad that you could make it work, supported version at the moment is indeed 4.34.0. We've clarified the README in that regard!"
alexsherstinsky,2023-10-05 14:46:35+00:00,comment,mistralai/Mistral-7B-v0.1,@lerela Thank you for confirming.  Perhaps Mistral AI would consider making the official model sharded in order to make it easier for new users to use it in low-resource hardware environments.  Thanks again.
Manu9000k,2023-10-06 05:07:01+00:00,comment,mistralai/Mistral-7B-v0.1,Can you please share the full colab example?
alexsherstinsky,2023-10-06 05:30:28+00:00,comment,mistralai/Mistral-7B-v0.1,@Manu9000k I am preparing it and hope to share early next week; I will paste it here -- thank you for asking.
alexsherstinsky,2023-10-07 08:26:49+00:00,comment,mistralai/Mistral-7B-v0.1,@Manu9000k Here it is: https://predibase.com/blog/fine-tuning-mistral-7b-on-a-single-gpu-with-ludwig -- enjoy!
Manu9000k,2023-10-08 08:46:11+00:00,comment,mistralai/Mistral-7B-v0.1,Thanks a lot @alexsherstinsky you are the best!!!!
alexsherstinsky,2023-10-08 14:48:13+00:00,comment,mistralai/Mistral-7B-v0.1,@Manu9000k Thank you very much!  It is really my pleasure -- I am glad it helps -- enjoy!
NPap,2023-10-10 10:37:51+00:00,comment,mistralai/Mistral-7B-v0.1,"Okay, now the loading issue is out of the way, has anyone succeeded in finetuning Mistral through google colab?"
ArthurZ,2023-10-10 10:57:39+00:00,comment,mistralai/Mistral-7B-v0.1,I think you should be able to do that if you use the left library cc @ybelkada we have tutorials for Llama I think no? 
NPap,2023-10-10 10:59:55+00:00,comment,mistralai/Mistral-7B-v0.1,"> I think you should be able to do that if you use the left library cc @ybelkada we have tutorials for Llama I think no?

I've tried Llama and it worked with a few dirty fixes currently but this one has a different tokenizer and attention mechanism (if I'm not mistaken) so I'm not sure if peft/bits&bytes/SFFTrainer are good to go for that. (These are the components I'm using)"
lysandre,2023-10-10 14:40:12+00:00,comment,mistralai/Mistral-7B-v0.1,cc @ybelkada for peft/bitsandbytes/SFT Trainer
alexsherstinsky,2023-10-10 14:52:44+00:00,comment,mistralai/Mistral-7B-v0.1,@NPap Have you tried the procedure outlined in https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/13#65211649706c7551487c999a (an earlier message above)?  Thank you.
ybelkada,2023-10-10 19:32:24+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi everyone,

You can indeed fine-tune Mistral-7B on a free tier google colab instance. I made a colab notebook for it here: https://colab.research.google.com/drive/1DNenc5BpdqaS10prtklYyIe9qW_7gUnb?usp=sharing 

Make sure to use the sharded version of the model that I have pushed under my namespace [here](https://huggingface.co/ybelkada/Mistral-7B-v0.1-bf16-sharded) as currently since the largest shard is 10GB, it leads to CPU OOM if you try to use `mistralai/Mistral-7B-v0.1`

I recommend to train your model with `packing` to avoid issues presented in this GH thread: https://github.com/huggingface/transformers/issues/26498 

Let us know here how the training goes - fine-tuning the model on the entire guanaco dataset seems to take ~4 hours. This can be further reduced down once `torch.scaled_dot_product_attention` will be integrated in transformers core : https://github.com/huggingface/transformers/pull/26572

"
ahuang11,2023-10-13 05:05:57+00:00,comment,mistralai/Mistral-7B-v0.1,"Thanks for sharing the notebook! I was wondering if you need to format the training dataset to conform to:
```<s>[INST] What is your favourite condiment? [/INST]
Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>```

Or does the tokenizer automatically do that?

like mentioned in https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe and https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format"
NPap,2023-10-13 06:39:26+00:00,comment,mistralai/Mistral-7B-v0.1,"> Thanks for sharing the notebook! I was wondering if you need to format the training dataset to conform to:
> ```<s>[INST] What is your favourite condiment? [/INST]
> Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>```
> 
> Or does the tokenizer automatically do that?
> 
> like mentioned in https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe and https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format

Check the tokenizers parameters, I think its eos, bos, and special tokens arguments. (Thats for the <s> and </s>, I think you will need to put [INST] and [/INST] yourself)"
ArthurZ,2023-10-13 09:56:57+00:00,comment,mistralai/Mistral-7B-v0.1,The chat templating can be used following [this blog](https://huggingface.co/docs/transformers/main/chat_templating) for chat formating! 
jthk-HF,2024-03-13 03:00:17+00:00,comment,mistralai/Mistral-7B-v0.1,"> Hi everyone,
> 
> You can indeed fine-tune Mistral-7B on a free tier google colab instance. I made a colab notebook for it here: https://colab.research.google.com/drive/1DNenc5BpdqaS10prtklYyIe9qW_7gUnb?usp=sharing 
> 
> Make sure to use the sharded version of the model that I have pushed under my namespace [here](https://huggingface.co/ybelkada/Mistral-7B-v0.1-bf16-sharded) as currently since the largest shard is 10GB, it leads to CPU OOM if you try to use `mistralai/Mistral-7B-v0.1`
> 
> I recommend to train your model with `packing` to avoid issues presented in this GH thread: https://github.com/huggingface/transformers/issues/26498 
> 
> Let us know here how the training goes - fine-tuning the model on the entire guanaco dataset seems to take ~4 hours. This can be further reduced down once `torch.scaled_dot_product_attention` will be integrated in transformers core : https://github.com/huggingface/transformers/pull/26572

In addition to this,
Try training it on Kaggle, it worked for me with training batch size 2. 
In colab what worked for me was decreasing the max_sequence_length to 256 and turning on double quantization with the batch size =1. (GPU RAM 10.5 / 15.0 GB)
Please check the table at the end here to make an informed assumption: https://huggingface.co/blog/4bit-transformers-bitsandbytes
"
GaussianMixture,2023-09-30 15:18:18+00:00,comment,mistralai/Mistral-7B-v0.1,but don't they use sliding window attention mechanism for larger contexts?
lerela,2023-10-05 07:48:24+00:00,comment,mistralai/Mistral-7B-v0.1,That's not planned in the near future!
Rishab,2023-09-29 03:48:05+00:00,comment,mistralai/Mistral-7B-v0.1,"hey, how did you perform summarisation task. the templates i have been using for llama when i did here it has been failing, can you share your code/template"
acrastt,2023-09-28 03:42:56+00:00,comment,mistralai/Mistral-7B-v0.1,Sorry the convert script crashed.
arthurmensch,2023-10-12 07:57:48+00:00,comment,mistralai/Mistral-7B-v0.1,"Hello, thanks for your interest and kind words! Unfortunately we're unable to share details about the training and the datasets (extracted from the open Web) due to the highly competitive nature of the field. We appreciate your understanding!"
roffmonster,2023-10-12 23:03:29+00:00,comment,mistralai/Mistral-7B-v0.1,"> Unfortunately we're unable to share details about the training and the datasets (extracted from the open Web)

Well, that answers one of my main questions. This model is *not* trained on synthetic textbooks. Based on how well this model performs, perhaps textbooks are not all one needs. Thanks for that!
"
flux-equals-rad,2023-10-23 16:14:29+00:00,comment,mistralai/Mistral-7B-v0.1,Yeah--Thanks for developing and sharing this model! Could you tell us when the model was trained? 
arthurcolle,2023-11-28 07:54:07+00:00,comment,mistralai/Mistral-7B-v0.1,"I wish I had a pile of books to train on for real, maybe one day"
reviriego,2023-12-09 23:27:15+00:00,comment,mistralai/Mistral-7B-v0.1,"I understand details of the dataset cannot be disclosed, but can you share some aggregate numbers, like for example the % of the text in the training dataset that is in Spanish?  That would be useful for us when evaluating Mistral on vocabulary tests and seeing quite different performances between English and Spanish (see https://arxiv.org/abs/2310.14703) much more variations than for other models."
ody55ey,2023-12-10 18:18:34+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi @arthurmensch,  i know you kinda answered this already, however, would be able to confirm that the data you used to train the model or the underlying dataset was ""ethically"" sourced? Or is this information confidential? The reason I am asking, is the potential to use your model in a corporate setting and the source of data means a lot to my company. Thanks! "
jdchang,2023-12-11 20:00:02+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi, I was wondering if we could get confirmation that there has been no data contamination with popular RLHF datasets such as TL;DR summarization or Anthropic HH? Thank you!"
janphilippfranken,2024-02-18 17:03:34+00:00,comment,mistralai/Mistral-7B-v0.1,"@jdchang i have the same question—it would be good to know if there is a risk of data contamination with preference/RLHF datasets such as hh_rlhf, tl;dr, etc. 

thank you!"
mikeX1001,2024-03-01 15:39:03+00:00,comment,mistralai/Mistral-7B-v0.1,Does anyone know where I can Opt-out of the Mistral AI Training Data? 
deleted,2024-03-01 19:34:13+00:00,comment,mistralai/Mistral-7B-v0.1,"> Does anyone know where I can Opt-out of the Mistral AI Training Data?

lol"
mikeX1001,2024-03-04 12:40:37+00:00,comment,mistralai/Mistral-7B-v0.1,"so, no chance?
"
deleted,2024-03-04 12:44:48+00:00,comment,mistralai/Mistral-7B-v0.1,"
> so, no chance?

Personally, if it was freely availble on the net, then too bad.  If i can read content with my eyes,  a company has just as much right to use it. You could always choose to be a jerk and sue of course.  However, good luck with that across ( potential ) international boundaries.  "
shaobaij,2023-09-28 04:10:21+00:00,comment,mistralai/Mistral-7B-v0.1,I got the same errors. I think the problem is from API side?
FumaBuma,2023-09-28 06:04:43+00:00,comment,mistralai/Mistral-7B-v0.1,This comment has been hidden
infinitylogesh,2023-09-28 07:31:29+00:00,comment,mistralai/Mistral-7B-v0.1,"You might have to install it from source , as the associated model code hasn't been released as a new transformers version yet.

```
pip install git+https://github.com/huggingface/transformers 
```

Hope this helps !"
sbs2680,2023-09-28 09:10:56+00:00,comment,mistralai/Mistral-7B-v0.1,I have the same issue
ArjunPrSarkhel,2023-09-28 11:38:21+00:00,comment,mistralai/Mistral-7B-v0.1,"Hi i was facing same issue, Instead of pip install transformers, as someone stated above earlier use this
!pip install git+https://github.com/huggingface/transformers

"
lerela,2023-10-05 07:46:49+00:00,comment,mistralai/Mistral-7B-v0.1,"Mistral is now part of the official 4.34.0 Transformers release. The above workarounds are not necessary anymore, just make sure to `pip install ""transformers>=4.34.0""`."
nadahlberg,2023-09-27 22:56:35+00:00,comment,mistralai/Mistral-7B-v0.1,"I think it's not yet in the main library.  Try this in the meantime:

```
pip install git+https://github.com/huggingface/transformers.git
```"
lbaldesi,2023-09-27 23:18:24+00:00,comment,mistralai/Mistral-7B-v0.1,"You are right, thanks!"
ZeroXClem,2023-09-27 22:12:21+00:00,comment,mistralai/Mistral-7B-v0.1,"8K sequence length noted from their product page. https://mistral.ai/product/ 
For a more detailed specification read here: https://mistral.ai/news/announcing-mistral-7b/"
lerela,2023-10-05 07:44:27+00:00,comment,mistralai/Mistral-7B-v0.1,"Thanks @ZeroXClem  for the answer, and it's also in Transformers documentation: https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/mistral#model-details"
YokaiKoibito,2023-10-05 22:47:29+00:00,comment,mistralai/Mistral-7B-v0.1,"Technically it's unlimited with a 4k sliding window context size.

Under the hood the stacked layers allow the possibility of indirectly attending to things more than 4k token previously, but that now requires multiple attention-hops, and there's no backward-in-token-space propagation of the attention-query to before the sliding window. So in effect, if something's outside the sliding window, the model can attend to it only if it previously attended to it at a token less than 4k tokens previously: so outside the sliding window the model can learn to have capabilities more like LSTM-level than attention."
casperhansen,2023-09-27 16:38:53+00:00,comment,mistralai/Mistral-7B-v0.1,Closing as I forgot to add the copyright claim
bilalfaye,2024-04-18 09:12:46+00:00,comment,mistralai/Mistral-7B-v0.1,This comment has been hidden
Kjgghuty5,2024-05-03 01:33:16+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Toda la razón le dices algo al modelo y te analiza la frase palabra por palabra 
Avelina,2024-05-06 18:23:41+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"I don't think those are ""junk words"". Those are word pieces which can are used to make up actual words. If you look at GPT2's tokenizer for example you'll see much of the same."
MartialTerran,2024-05-06 18:51:04+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"I visually inspected part of the 50000+ token vocab being used here and elsewhere and I saw useless junk BPE fragments that will obviously not be relevant to any kind of ""TinyStories"" Dataset.   By Junk I mean ""word pieces"" of things that exist somewhere on the internet but are not parts of any real words that are commonly used in any English sentence.   And, there are only about 40,000 unique words of any kind (including misspellings) used in the Tiny-Storiesv2 dataset, so at least 10,000 tokens in the 50,000 BPE vocab are unnecessary (junk) for TinyStoriesv2 purposes.  The Microsoft team also did this for no good reason.   Andrej Karpathy has trained models using the TinyStories dataset and he saw the value of producing a dataset-specific vocab_size=2048 vocab file.  He built a 2048 BPE token vocab based on TinyStories:
    These stages are designed to be run in order.

    To tokenize data with the Llama 2 tokenizer:
    python tinystories.py download
    python tinystories.py pretokenize

    To tokenize data with a custom tokenizer we train ourselves with sentencepiece, e.g.:
    python tinystories.py download
    python tinystories.py train_vocab --vocab_size=2048
    python tinystories.py pretokenize --vocab_size=2048

https://github.com/karpathy/llama2.c/blob/master/tinystories.py
https://github.com/karpathy/llama2.c

Thus, he was able to build a TinyStories model using only 2048 tokens, not the 50,000+ tokens BPE vocabulary as in GPT-2. 

  See also the 27-token model investigated in the influential paper at https://jalammar.github.io/illustrated-gpt2/   (referenced in https://jalammar.github.io/illustrated-gpt2/ )"
Avelina,2024-05-06 19:04:34+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Okay but this model is not designed for Tiny Stories, it is designed to be completely general purpose for both natural language and code, and with the capability to interpret other languages as well (you may have noticed non-latin characters in the vocab, in addition to words clearly from other languages)."
Avelina,2024-05-06 19:05:22+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"I think you may have been confused by ""Tiny"" in the model name. This model has ZERO relation to Tiny Stories or any other ""Tiny"" datasets."
MartialTerran,2024-05-06 22:32:24+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Yes,  I was just now confused by the ""Tiny"" misnomer (as in Tiny Stories).  But, the BPE vocab employed in TinyLlama-1.1B-Chat-v1.0 is still full of unnecessary junk tokens that are bloating the model and unnecessarily consuming electrical power (in the output and input tokenization levels).  For example, based on another quick visual inspection, there is no logical need to have three or more various dedicated tokens for: ""/"",  ""//"", and ""///"" and ""////"" much less """"\\,"" and   ""▁\""\\"": 6634, and       ""▁`\\"": 6714,  and  ""▁[` and       "":("":

      ""\\\"""": 5931,
  ...
      ""\\,"": 5940,

      ""▁[`"": 5913,
      ""Char"": 5914,
      ""▁Cap"": 5915,
      ""boldsymbol"": 5916,
      ""▁Roman"": 5917,
      ""▁Max"": 5918,
      "":("": 5919,

      "">("": 5961,

      ""▁['"": 6024,

      "");\r"": 6075,

      ""▁$$\\"": 6118,

    ""▁ž"": 6145,

      "":\"""": 6160,

      ""////"": 6165,

      ""}:"": 6177,

      ""{{"": 6224,

    ""(&"": 6243,

      ""}^{\\"": 6292,
      ""you"": 6293,
      ""/."": 6294,

      ""▁.."": 6317,

      ""▁<?"": 6319,

      ""}_{\\"": 6353,

      ""{\"""": 6377,

      ""▁${"": 6435,

      ""}]"": 6525,

      ""#,"": 6552,

      ""▁š"": 6569,
      ""▁OS"": 6570,
      ""▁{}"": 6571,

      ""▁%>"": 6580,

      ""^*"": 6622,

      ""▁\""\\"": 6634,

      "",-"": 6653,

      ""///"": 6658,
      ""▁feb"": 6659,
      ""▁-->"": 6660,

      ""?”"": 6677,

      ""▁†"": 6700,

      ""▁`\\"": 6714,

      ""▁>="": 6736,

      "">>"": 6778,

      ""▁[\"""": 6796,

The BPE Vocabs used in most of the LLMs are full of such useless garbage undeserving of a dedicated token.

Given the emphasis on ""coding"" in this small model, if there were special sequences that were necessary for syntax (such as indent 4 spaces, 8 spaces, etc, as in python) (or in HTML pages etc) then those special sequences would be worthy of a dedicated token and given special handling in the vocab and in the tokenizer of the model.  See the ""tokenizer"" video by Andrej Karpathy:  https://www.youtube.com/watch?v=zduSFxRajkE 
"
Avelina,2024-05-07 13:26:50+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"`boldsymbol` is likely referring to the the LaTeX command.

`▁-->` is referring to the end of a HTML comment (where `▁` means pre-fixed by a space).

`▁${` is probably the JS template string specifier, again prefixed by a space.

`▁[\""`,  `}_{\\`, `}^{\\` and `""▁$$\\` are likely more latex commands.

Anything prefixed by `▁` means the token starts with a space rather than existing as a word-piece which explains the existence of `▁feb`, `▁Max`, `▁Roman` and `▁Cap`. This is opposite to BERT style tokenizers where everything is prefixed with a space by default, and the prefix of `##` is used to denote word-pieces.

`Char` is probably a datatype which isn't prefixed by a space because it may usually be prefixed by a tab character in the source code it's from.

`?”` is probably due to the existence of question marks in book dialogue, hence the use of curly quotes rather than straight quotes.

`>>` is probably the bitshift operator in many languages (including prevalent use for streams in C++).

`▁>=` is math notation.

I could go on...

You're probably seeing a pattern here: common symbol occurrences end up getting folded into single tokens to reduce the context size needed to represent common strings of characters in the source training data... and considering TinyLlama was trained on a 7-to-3 ratio of language-to-code it makes sense a lot of the 'odd' tokens in the vocab are common strings used in programming."
MartialTerran,2024-05-07 22:57:29+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"I appreciate the effort and insights you have put into your response.  I am glad to learn that many of these weird BPE tokens may serve a useful purpose somewhere in coding space or HTML, where they would help reduce the token count going into a GPT model.  

 I knew that underbar ▁ means: Begins with Space.   

I did not know that  in BERT style tokenizers everything is prefixed with a space by default, and the prefix of ## is used to denote word-pieces.

I know of a person on hugginface that built an experimental tintystoriesv2_train.txt model that uses explicit ""space"" tokens and uses the Shift-Key as a pre-token that indicates the next word's first-letter is Capitalized.  This method costs more input tokens, but the model basically works as expected.  https://huggingface.co/Corianas/Microllama_Char_100k_step 

As for your review of   ""▁[`"": 5913, through  "":("": 5919,
I only just copy-pasted the whole series because of the beginning and end tokens of that series.  I was not really interested in/objecting-to the in-between tokens.   But, I appreciate your consideration and explanation of the in-between tokens."
limcheekin,2024-03-01 11:03:18+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,I'm looking for something similar. What I found is using the `huggingface/candle` wasm module and serve the downloaded/bundled model. 
huggingface9837,2024-03-01 18:45:20+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Thanks, I'll take a look!"
AIArtsChannel,2024-03-20 23:27:00+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Probably one of the best way right now is trough llama.cpp or any of it's many bindings libraries. it's small, efficient and you'll just need the model in GGUF format, like [TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF)"
MartialTerran,2024-03-28 20:48:24+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"What examples can you offer that show this model is capable of competent coding?  It failed in the example of a properly worded request to produce a python function to calculate the first 10 numbers in the fibonacci sequence:

Tell me the first 10 numbers in the Fibonacci sequence
Sure! Here are the first 10 numbers in the Fibonacci sequence: ``` 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 ``` The first 10 numbers in the Fibonacci sequence are: 1, 1, 2, 3, 5, 8, 13, 21

Write a python function to calculate the first 10 numbers in the fibonacci sequence and to print the sequence.
Here's a Python function that calculates the first 10 numbers in the Fibonacci sequence and prints them to the command line: ```python def fibonacci_sequence_to_digits(n): """""" Calculates the first n digits of the Fibonacci sequence. """""" if n <= 1: return ""0"" elif n == 2: return ""1""



"
Sirclavin,2024-01-11 13:29:38+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,
SwatiM,2024-01-22 13:43:19+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Have not tried it personally but on a general note wrapping is always beneficial in sft.
For DPO chosen and rejected template should be used ""https://huggingface.co/blog/dpo-trl""."
demetera,2024-01-10 21:29:18+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"It's not clear for me too. Here - https://mychen76.medium.com/tinyllama-colorist-fine-tuned-with-color-dataset-8cd1cf7e5665 - the guy is using this template:
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
{answer}<|im_end|>

Llama2 template with, like used in a small dataset mlabonne/guanaco-llama2-1k also seems accepted.
It's also not clear, what to do, if there's a unlabelled text corpus of data."
ljhwild,2024-01-10 22:44:22+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"<|system|>
{system_message}</s>
<|user|>
{prompt}</s>
<|assistant|>
is what the bloke says on the gguf version"
ljhwild,2024-01-10 22:48:41+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,It's sick how fast can generation be on cpu :O
azarovalex,2024-01-12 16:25:13+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"The correct prompt format can be found in the Python code sample in [the readme](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0):

```
<|system|>
You are a friendly chatbot who always responds in the style of a pirate.</s>
<|user|>
How many helicopters can a human eat in one sitting?</s>
<|assistant|>
```"
JoeySalmons,2024-01-08 07:49:42+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"""This is my first foray into downloading and attempting to run an LLM locally""
If this is true, I highly recommend trying out running GGUF quantized models from TheBloke: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF using a backend like llama.cpp. TheBloke also lists several other clients and libraries that support GGUF, most of which also support running LLM inference on a mac. I suggest this method because it *should* be a very simple and beginner-friendly intro to running LLMs locally, the main requirement in your case being the support for older hardware.

Also, you could check out discussions in the llama.cpp github as well as the r/localLlama subreddit for posts like these: https://www.reddit.com/r/LocalLLaMA/comments/16j29s3/old_comp_running_llm_i_got_llama27bchatq2_kgguf/"
sachinmyneni,2024-01-15 05:44:56+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Thanks @JoeySalmons   That was very helpful.  I could use llama-cpp-python and a few models successfully. I then tried the text-generator-webui but the installation was not as smooth as the one in the reddit thread. I will keep at that. But with llama-cpp-python, I have a good start. 
I am now playing with different parameters and looking to see if I can train a model with my data. llama-cpp-python does not seem to be there yet according to this thread atleast: https://github.com/abetlen/llama-cpp-python/issues/814
Oh.. and I went ahead and got into google colab since the free tier itself gives some GPU resources. "
gauravlogical,2024-01-08 19:05:45+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Same for me :( 
tog,2024-01-12 06:48:49+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"This should be fixed by the last version of accelerate (0.26.1) 
"
FlorianD,2024-01-25 12:19:48+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Same for me even with accelerate==0.26.1.
m3z,2024-01-31 17:15:15+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Same here m2 MacBook Air
nivapo95,2024-02-17 21:46:39+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Same here, MacBook Pro Sonoma."
alirahebi,2024-03-02 19:31:36+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Same problem when getting Zephyr Beta... There are some solutions online but this happens when loading the model, so I can't edit the code or anything."
odwyerrichard,2024-04-28 18:11:06+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Also seeing this on Macbook Pro, M3 Max.

accelerate==0.26.1 does not resolve the issue."
Erni,2024-05-17 13:49:57+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Same here
PY007,2024-01-07 04:59:49+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Seems that the transformer version is too old. Try to upgrade to 4.35
satyroffrost,2024-03-10 06:46:17+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"this is chat bot model. You provide ""question"" to model, model answer your question. In fact it generates next tokens after your tokenized question. So you have to prepare dataset in format, discussed here 
https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/discussions/16
(try system role = 'astronomer')
and then fine tune pretrained model on your dataset. "
barnobarno666,2024-01-06 21:00:34+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,maybe it was meant to give you the feeling of talking to joe biden?
YaTharThShaRma999,2024-01-08 14:19:38+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"@froilo 
obviously dont expect it to be near even a 7b mistral model which alone is 7x the size of this one. 

It could serve for extremely fast inference on a edge device and most devices cant run 7b models. They barely fit 7b quantized models.
You could also finetune it for niche tasks which will be easy as it takes very little vram.
You could also use it as a speculative decoder model which will make larger llama models such as 7b llama 2 or 70b llama 2 considerably faster."
KatyTheCutie,2024-01-04 16:16:29+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,2048 tokens.
mrfakename,2024-01-03 03:36:11+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,cc: @clefourrier 
clefourrier,2024-01-03 08:40:49+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Hi @AIWintermuteAI ,
Feel free to submit it yourself if you want, after checking which parameters should be used :)
"
AIWintermuteAI,2024-01-03 08:50:02+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Looks like it was submitted already, pending
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6442dce47feb866811b32a0a/Y10uuHNAaDopJVISHSY07.png)"
MartialTerran,2024-03-28 20:56:06+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Look at the vocabulary words (tokenizer).  That hex code is like the first token in the model vocabulary?

https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/blob/main/tokenizer.json

So, maybe your output biased to returning that token-number due to some coding error, or other issue."
MartialTerran,2024-03-28 20:57:22+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,provide a link URL to show   using Mamba SSM (I saw someone inject nanogpt attention heads on top of mamba and it got amazing results).
Jenish-23,2024-01-01 19:19:31+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Unsloth_ai released a Google [colab script](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing). Maybe you can use that.
demetera,2024-01-04 19:38:30+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Dataset must be in this format : https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k
I was using this easy approach for fine tuning : https://www.kaggle.com/code/tommyadams/fine-tuning-tinyllama
(but in this script author feeding chunks of lines without template)

As an output you will have an adapter - can be loaded separately or merged with the main model 
(but need to have equal precision for the main model and adapter).

"
oliverbob,2024-02-06 19:05:20+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Unsloth doesn't provide the same prompt template used in this model:

<|system|>
You are a friendly chatbot who always responds in the style of a pirate.</s>
<|user|>
How many helicopters can a human eat in one sitting?</s>
<|assistant|>

How do I finetune it to use exactly the same template, but using the unsloth example?"
julien-c,2024-01-02 18:23:43+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,great addition!
limcheekin,2024-01-01 02:45:58+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,I am more interested on why @PY007 settled on 1.1B? How did he/she come out with this number? 
rwl4,2023-12-31 08:28:30+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"If you look in the config.json and search for ""max_position_embeddings"", you'll see that it has a context window size of 2048."
TKDKid1000,2023-12-31 23:00:49+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Currently, it's 2048 tokens, but I know that there are several methods of extending the context length of Llama-based models. One of which is [YaRN](https://huggingface.co/papers/2309.00071), and that may be worth applying here just to see how well it works."
dillfrescott,2023-12-31 04:35:19+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,Your efforts are very much appreciated!
limcheekin,2023-12-31 05:57:41+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v1.0,"Great! Thanks for sharing the model.

Happy new year 2024! 🎉"
ybelkada,2024-05-14 15:42:12+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @wanted6 
This is related to the fact that the model is now gated, you need to request access with your HF account directly from the model card. Can you try that out?"
Belucraft,2024-05-22 15:25:42+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@ybelkada I have the problem and I have the access
ybelkada,2024-04-22 17:51:47+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @cuongk14 ! 
You might be interested in GaLore algorithm: https://huggingface.co/docs/transformers/v4.40.0/en/trainer#galore that enables parameter efficient full pre-training & fine-tuning. Note this does not support DeepSpeed yet"
Schmip,2024-04-04 09:40:18+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Typo: Mistral -> Mixtral
ybelkada,2024-02-22 09:56:13+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @ChanakyaReddy ! 
Please consider loading your model in half-precision `torch_dtype=torch.float16` or in 8-bit / 4-bit precision through the `load_in_4bit` / `load_in_8bit` arguments"
ChanakyaReddy,2024-02-22 22:28:29+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Ybelkada, Thanks for the response. think I have fixed it, it appears the /tmp sub folder on AWS has some limitations, had to download it to another mounted location. Further, this is when the model is initially downloading, I don't think there is an option for downloading only the float16 of 4_bit/8_bit quantized ones, think that only applies after the initial download. Thanks for the response, appreciate it"
ybelkada,2024-02-23 08:25:30+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Thank you very much for your prompt reply @ChanakyaReddy ! 
YaTharThShaRma999,2024-02-27 23:58:24+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@dylanbui that just loads the model and will take a lot of time since it has to download and load the model. You can use llama cpp to load the model much faster though. And you don’t even have to download the full model but the already quantized model(3x smaller in file storage) 

If you are talking about inference speed then model.eval() should increase it but it would be around 50% faster?

Using llama.cpp or exllama v2 will massively increase it(at least 3x). Just look at there docs on how to use it"
dylanbui,2024-03-14 15:44:02+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@YaTharThShaRma999 Have you had the same experience by reducing loading time with Mistral too?

Thank you."
YaTharThShaRma999,2024-03-14 16:33:13+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@dylanbui yes loading time will massively decrease. Llama cpp is just a few seconds. Exllama v2 is only a slightly faster then transformers but not by a lot
dylanbui,2024-03-15 13:05:35+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@YaTharThShaRma999 How can you install llama cpp on GPU/CPU ? I followed the instruction and got the error:

Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [46 lines of output]
      *** scikit-build-core 0.8.2 using CMake 3.28.3 (wheel)
      *** Configuring CMake...
      loading initial cache file /tmp/tmpco0700tx/build/CMakeInit.txt
      -- The C compiler identification is GNU 12.3.0
      -- The CXX compiler identification is GNU 12.3.0
      -- Detecting C compiler ABI info
      -- Detecting C compiler ABI info - done
      -- Check for working C compiler: /apps/USE/easybuild/release/2023.1/software/GCCcore/12.3.0/bin/cc - skipped
      -- Detecting C compile features
      -- Detecting C compile features - done
      -- Detecting CXX compiler ABI info
      -- Detecting CXX compiler ABI info - done
      -- Check for working CXX compiler: /apps/USE/easybuild/release/2023.1/software/GCCcore/12.3.0/bin/c++ - skipped
      -- Detecting CXX compile features
      -- Detecting CXX compile features - done
      -- Found Git: /usr/bin/git (found version ""2.39.3"")
      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD
      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
      -- Check if compiler accepts -pthread
      -- Check if compiler accepts -pthread - yes
      -- Found Threads: TRUE
      -- Could not find nvcc, please set CUDAToolkit_ROOT.
      CMake Warning at vendor/llama.cpp/CMakeLists.txt:407 (message):
        cuBLAS not found


      -- CUDA host compiler is GNU
      CMake Error at vendor/llama.cpp/CMakeLists.txt:835 (get_flags):
        get_flags Function invoked with incorrect arguments for function named:
        get_flags


      -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
      -- CMAKE_SYSTEM_PROCESSOR: x86_64
      -- x86 detected
      CMake Warning (dev) at CMakeLists.txt:21 (install):
        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
      This warning is for project developers.  Use -Wno-dev to suppress it.

      CMake Warning (dev) at CMakeLists.txt:30 (install):
        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
      This warning is for project developers.  Use -Wno-dev to suppress it.

      -- Configuring incomplete, errors occurred!

      *** CMake configuration failed
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for llama-cpp-python
Failed to build llama-cpp-python
ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects
"
dylanbui,2024-03-15 14:47:34+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@YaTharThShaRma999 I solved it. I forgot to enable cuda. 
sniffski,2024-02-13 18:14:51+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"You can try text-generation-webui <https://github.com/oobabooga/text-generation-webui> and run it with the --api option... Then check for the chat-ui <https://github.com/huggingface/chat-ui> it's all described in the readme to both repos... Take a look and msg if you have trouble. I'm on mobile now, that's why I'm not driving into explanations."
sniffski,2024-02-13 18:22:11+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Only webui will do fine, but if you try 'chat-ui' you'll get what I mean :)"
joelfabregat,2024-02-15 16:08:32+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> You can try text-generation-webui <https://github.com/oobabooga/text-generation-webui> and run it with the --api option... Then check for the chat-ui <https://github.com/huggingface/chat-ui> it's all described in the readme to both repos... Take a look and msg if you have trouble. I'm on mobile now, that's why I'm not driving into explanations.

heyy i have looked it a bit but i dont know how to do it, can you guide me a bit please. I am planning on using https://x10hosting.com since i dont want to use any money at least for testing it."
deleted,2024-02-15 16:11:49+00:00,comment,mistralai/Mixtral-8x7B-v0.1,that looks like a simple web hosting site.  You need a server that can run code. 
sniffski,2024-02-15 20:02:55+00:00,comment,mistralai/Mixtral-8x7B-v0.1,It's a bit complex... Can we move to another channel if communication so we don't bother the community... It won't work with that hosting you are referring.
joelfabregat,2024-02-15 23:11:08+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> It's a bit complex... Can we move to another channel if communication so we don't bother the community... It won't work with that hosting you are referring.

sure what channel? i have discord if you want to talk via discord: 
user ID: 371007503707668482
username: joniomega
"
sniffski,2024-02-16 04:42:32+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"You need to try to find me, as you are not accepting friend requests. My user name is sniffski"
Thillai,2024-03-22 16:16:48+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hey guys Iam also having a similar requirement Can you guys share some light on how you handled the scenario??


"
ArthurZ,2024-02-02 09:32:52+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Would recommend you to load it according to the documentation with the approtiate `device_map = ""auto""` ! This will offload the model to CPU RAM if you don't have enough and then disk if you still don't have enough! "
WiLDCaT6,2024-02-02 14:01:07+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Can you / do you want to try it with ollama ? It's very very fine tune lastly, run on rtx 3090 fast ! But maybe it's what you'r looking for😅"
pierrerichard,2024-02-02 14:11:30+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Thanks ArthurZ! What you said worked perfectly, I was just missing this flag : D

Hi WiLDCaT6, I checked ollama yes, but I wanted to try loading the model on its own ahah : )"
WiLDCaT6,2024-02-02 15:27:12+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Yes it's what I thank, thx to reply, that will be my next goal, for now ollama + Django is very cool for my 'little' configuration and Mixtral 😅"
ybelkada,2024-01-30 10:56:15+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @bweinstein123 
The snippet below should enable multi-GPU inference:
```diff
+ import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
 
model_id = ""mistralai/Mixtral-8x7B-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)
+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=""auto"")
- model = AutoModelForCausalLM.from_pretrained(model_id)
- model.half().cuda()

inputs = tokenizer(text, return_tensors=""pt"")
inputs_gpu = {key: value.to(""cuda"") for key, value in inputs.items()}

outputs = model.generate(**inputs_gpu, max_new_tokens=500)
```
Check out: https://huggingface.co/docs/accelerate/usage_guides/big_modeling for more details"
barbery,2024-02-19 08:39:18+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @ybelkada 
I try this code, get the error like below:
```
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && ""index out of bounds""` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && ""index out of bounds""` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && ""index out of bounds""` failed.
....
....
Traceback (most recent call last):
  File ""/mnt/nvme0n1p1/model_download/test.py"", line 13, in <module>
    outputs = model.generate(**inputs_gpu, max_new_tokens=500)
  File ""/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py"", line 1479, in generate
    return self.greedy_search(
  File ""/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py"", line 2340, in greedy_search
    outputs = self(
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py"", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py"", line 1317, in forward
    outputs = self.model(
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py"", line 1185, in forward
    layer_outputs = decoder_layer(
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py"", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py"", line 891, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py"", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py"", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py"", line 708, in forward
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
  File ""/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py"", line 216, in apply_rotary_pos_emb
    cos = cos[position_ids].unsqueeze(unsqueeze_dim)
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
```"
ofir408,2024-03-08 16:37:20+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@barbery  Did you solve this error? If so, can you share how?"
barbery,2024-03-28 07:48:04+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> @barbery  Did you solve this error? If so, can you share how?

No, I have no idea how to solve it @ofir408 "
ybelkada,2024-01-22 16:27:04+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @kxgong 
I suggest to load the model in half-precision (`torch_dtype=torch.float16`) or in 4-bit precision `load_in_4bit=True` in order to load your model in the most memory efficient manner possible"
kxgong,2024-01-23 02:53:54+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> Hi @kxgong 
> I suggest to load the model in half-precision (`torch_dtype=torch.float16`) or in 4-bit precision `load_in_4bit=True` in order to load your model in the most memory efficient manner possible

Thank you, I am using mixtral-8x7B for training. I wonder whether using 4bit will cause performance drop."
ybelkada,2024-01-23 14:17:57+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@kxgong if you use QLoRA you shouldn't expect performance drop with respect to full-finetuning. You can read more about QLoRA here: https://huggingface.co/blog/4bit-transformers-bitsandbytes and get started with resources on how to run QLoRA with this blogpost for example: https://pytorch.org/blog/finetune-llms/
kxgong,2024-01-30 16:06:29+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Thanks for your help.
julien-c,2024-01-17 14:35:25+00:00,comment,mistralai/Mixtral-8x7B-v0.1,🇫🇷🇫🇷🇫🇷🔥
tzivi,2024-01-11 11:04:49+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi, 
I'm looking forward to using this powerful model! I have cloned the repo for mistral-src onto my GPU machine. I followed the steps in the readme file, but I would really like to train it for my purpose.
Could I get a clear step-by-step tutorial on how to pre-train and fine-tune the model.
Thank you."
ybelkada,2024-01-22 16:25:37+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @tzivi 
You can benefit from many tools from HF ecosystem to fine-tune this model. I suggest to go for the QLoRA approach as it is very memory efficient, among the options I know, you can use:
- unsloth library: https://github.com/unslothai/unsloth
- Llama-Factory from @hiyouga : https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/10#6577e443e390cfd40990deff
- axolotl: https://github.com/OpenAccess-AI-Collective/axolotl
- TRL library: https://huggingface.co/docs/trl/sft_trainer"
hiyouga,2024-01-22 16:48:30+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi, looking for a fine-tuning framework?
Try LLaMA-Factory and fine-tuning Mixtral using ZeRO stage-2/3 with the latest DeepSpeed release (v0.13.0)
https://github.com/hiyouga/LLaMA-Factory#hardware-requirement 
https://github.com/hiyouga/LLaMA-Factory/pull/2283 "
ybelkada,2024-01-08 08:45:51+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @iriven 
Thanks for the issue, you can definitely use PEFT & QLoRA to fine-tune Mixtral easily, a nice tutorial I found is this one: https://x.com/HarperSCarroll/status/1737946511856832695?s=20 that you can easily follow"
leeedylan,2023-12-28 01:25:24+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I got the same problem, looking forward to the solution"
leeedylan,2023-12-28 01:41:15+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I got it , you need to update your transformers version to the latest, then it will work!"
basyouni12,2024-01-03 13:31:38+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I got the same error but I solved it with 

```pip install -U transformers```

huggingface-hub-0.20.1 tokenizers-0.15.0 transformers-4.36.2"
Genio55,2024-01-07 11:06:48+00:00,comment,mistralai/Mixtral-8x7B-v0.1,I'm using Pinokio with oobabooga: how do I update Transformers?
ybelkada,2024-01-08 08:34:48+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"as stated by @basyouni12 , you can update transformers by running
`pip install -U transformers` in your terminal "
cekal,2023-12-23 08:30:28+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"You don't have enough GPU memory. Consider renting a GPU, or loading the model in a more efficient way (e.g. in 4-Bit)"
ybelkada,2024-01-08 08:38:57+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I second what @cekal said, you probably don't have enough GPU ram to fit the model, try either to load it with smaller precision (e.g. `float16` or `load_in_4bit`, or using the serialized 4-bit here: https://huggingface.co/ybelkada/Mixtral-8x7B-Instruct-v0.1-bnb-4bit) "
ashish86deshpande,2024-02-15 21:24:43+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @ybelkada 

Any idea what is the minimum system requirement to run this model (for e.g. GPU, etc..) ? I am trying to run below python code using streamlit and I get the above error (or warning, I would say) -

import streamlit as st
from langchain import PromptTemplate,  LLMChain
from langchain import HuggingFacePipeline
from transformers import AutoTokenizer
import transformers
import torch

token = ""<my huggingface token goes here>""

model = ""meta-llama/Llama-2-7b-chat-hf""

tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
    ""text-generation"", 
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
    max_length=1000,
    eos_token_id=tokenizer.eos_token_id
)

llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})

template = """"""
              You are an intelligent chatbot that gives out useful information to humans.
              You return the responses in sentences with arrows at the start of each sentence
              {query}
           """"""

prompt = PromptTemplate(template=template, input_variables=[""query""])

llm_chain = LLMChain(prompt=prompt, llm=llm)

print(llm_chain.invoke('What are the 3 causes of glacier meltdowns?'))
"
ybelkada,2023-12-20 08:59:39+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @Guanzheng 
I think so yes! Perhaps @TimeRobber can confirm"
Guanzheng,2023-12-21 03:06:42+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Thank you!
TimeRobber,2023-12-22 09:02:57+00:00,comment,mistralai/Mixtral-8x7B-v0.1,There's no sliding window on Mixtral
ybelkada,2023-12-19 08:48:20+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"hi @Pradeep1995 
That model is a base model, therefore it doesn't need to be prompted in a specific way in order to get started with the model. If you want to use the instruct version of the model, you need to follow the template that is on the model card: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1#instruction-format"
cekal,2023-12-18 22:46:16+00:00,comment,mistralai/Mixtral-8x7B-v0.1,?
cekal,2023-12-15 19:58:27+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Bonjour @yob84800, vous obtenez cette réponse du modèle parce qu'il s'agit d'un modèle de base, c'est-à-dire qu'il n'a pas encore été entraîné sur un ensemble de données de questions-réponses, ce qui signifie qu'il ne sera pas performant. Vous pouvez essayer d'utiliser mistralai/Mixtral-8x7B-Instruct-v0.1 qui a été affiné pour suivre les instructions."
yob84800,2023-12-16 16:00:19+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"eh bien comment dire sur celui si Mixtral-8x7B-Instruct-v0.1 cela me fait exactement pareil ..
"
cekal,2023-12-16 21:55:51+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Est-ce que vous demandez au modèle de procéder de cette manière ? 

```
<s>[INST] Instruction [/INST] Réponse du modèle</s> [INST] Instruction de suivi [/INST]
```

Si ce n'est pas le cas, veuillez le faire. Sinon, le modèle produira des résultats inattendus."
yob84800,2023-12-17 11:11:45+00:00,comment,mistralai/Mixtral-8x7B-v0.1,merci je vais essayer de trouver ou metre les instructions que vous laver dit eh c'est normal que dans le fichier  json modèle type mixtral je suis obligé de le changer en mistral ? sinon il ne charge pas le modèle
yob84800,2023-12-18 10:10:54+00:00,comment,mistralai/Mixtral-8x7B-v0.1,la réponse eh plus simple que sa soi mon model etait endommager il y avais des coupure sur internet soi mon model avait aussi besoin de télécharger les consolidate 00 jusqu'à 7 .pt en plus des safetensor mai sa veu dire que cela ferai le double de 87 go .. parceque je les re télécharger avec lollms  et ces beaucoup mieux voila merci encore merci cekal de m'avoir un peu aider ;)
MuteXX,2023-12-14 08:38:30+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"LLMs can't do math reliably without external assistance.

I don't know what you expect here. "
yixliu1,2023-12-14 09:29:53+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi,
Thanks for your answering. I also tried a few other examples. For instance: ask it to generate a prompt based on my need, and ask it to answer some questions based on the context I provide. Neither of them shows useful results."
MuteXX,2023-12-14 09:42:47+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"You're asking a base model to solve problems.

What you want is the Instruct variant. Base isn't suitable for this."
yixliu1,2023-12-14 10:17:01+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"So does base model for further fine tune while instruct model for solve problems？
Thanks for your sharing！"
MuteXX,2023-12-14 10:24:17+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"The base model is a raw LLM; it ONLY does text completion.
Instruct has been tuned to respond to you instead."
yixliu1,2023-12-14 11:05:01+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Understand. Thx！
Satoszi,2023-12-21 13:39:25+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"""1+1=3"" doesn't necessarily mean that the model was wrong. The '1+1=3' can mean many different things, such as irony, a metaphor for synergy, and it can even be the start of an equation like '1+1=3-1' which also is correct. 

The issue is NOT that the model is incapable of such a simple operation! It's because it doesn't understand what you actually want from it. If you want to ensure the model knows what you mean, you either have to fine-tune it or give an example by prefacing the equation, for example, '5+3=8 1+1=', and now it's obvious that the expected answer is the sum of 1 and 1.

Several examples:

Input: '5+3=8 1+1=', the model outputs '2'
Input: 'Sum of: 1+1=', the model outputs '2'
Input: 'Sum of: 62+16=', the model outputs '78'"
MuteXX,2023-12-21 19:13:57+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"You can certainly nudge an LLM in the right direction, but they are *fundamentally* incapable of arithmetics or real logical actions without external help.
Don't mistake simple calculations being right as ability to do maths."
yixliu1,2023-12-22 03:27:40+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> ""1+1=3"" doesn't necessarily mean that the model was wrong. The '1+1=3' can mean many different things, such as irony, a metaphor for synergy, and it can even be the start of an equation like '1+1=3-1' which also is correct. 
> 
> The issue is NOT that the model is incapable of such a simple operation! It's because it doesn't understand what you actually want from it. If you want to ensure the model knows what you mean, you either have to fine-tune it or give an example by prefacing the equation, for example, '5+3=8 1+1=', and now it's obvious that the expected answer is the sum of 1 and 1.
> 
> Several examples:
> 
> Input: '5+3=8 1+1=', the model outputs '2'
> Input: 'Sum of: 1+1=', the model outputs '2'
> Input: 'Sum of: 62+16=', the model outputs '78'

Hi Satoszi, 
Thx for your sharing! I haven't though from that side. I think that's quite interesting. It's like LLM has many ""capabilities"" to answer this question but without FT it doesn't know which one it should give. "
Satoszi,2023-12-23 18:55:14+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> You can certainly nudge an LLM in the right direction, but they are *fundamentally* incapable of arithmetics or real logical actions without external help.
> Don't mistake simple calculations being right as ability to do maths.

Of course you are right, that LLMs are not good at arithmetics, and they are not built for that. We can never trust LLM output in arithmetic problems (and other domains too 🙂). It'll give approximation that looks legit, but for simple operations like sum of small numbers etc that approximation should be (usually) correct.

> > ""1+1=3"" doesn't necessarily mean that the model was wrong. The '1+1=3' can mean many different things, such as irony, a metaphor for synergy, and it can even be the start of an equation like '1+1=3-1' which also is correct. 
> > 
> > The issue is NOT that the model is incapable of such a simple operation! It's because it doesn't understand what you actually want from it. If you want to ensure the model knows what you mean, you either have to fine-tune it or give an example by prefacing the equation, for example, '5+3=8 1+1=', and now it's obvious that the expected answer is the sum of 1 and 1.
> > 
> > Several examples:
> > 
> > Input: '5+3=8 1+1=', the model outputs '2'
> > Input: 'Sum of: 1+1=', the model outputs '2'
> > Input: 'Sum of: 62+16=', the model outputs '78'
> 
> Hi Satoszi, 
> Thx for your sharing! I haven't though from that side. I think that's quite interesting. It's like LLM has many ""capabilities"" to answer this question but without FT it doesn't know which one it should give.

Yeah vanilla LLMs without reinforcement or other fancy finetuning methods are pretty ""stupid"" 😁"
shaun-glass,2023-12-13 13:50:01+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I ran transformers version 4.35.2 and got the same error. This is because it is not in the config map, but if you get the latest version, the corresponding value will be there, so if you get the latest version, it will be resolved. The latest version at this point is 4.36"
ybelkada,2023-12-13 14:30:48+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"yes, `pip install -U transformers` should solve the issue"
deleted,2023-12-13 23:15:08+00:00,comment,mistralai/Mixtral-8x7B-v0.1,This comment has been hidden
imjunaidafzal,2023-12-14 12:09:51+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @shaun-glass , i have test it with `4.35.2` but get the same error."
basyouni12,2024-01-03 13:31:14+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"i got the same error but i solve it with 

```pip install -U transformers```

huggingface-hub-0.20.1 tokenizers-0.15.0 transformers-4.36.2"
vibranium,2023-12-12 17:59:23+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@Nondzu or anyone from the Mistral AI team can help?
Nondzu,2023-12-12 18:23:23+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@vibranium upgrade your transformers `pip install --upgrade git+https://github.com/huggingface/transformers.git --no-cache`
vibranium,2023-12-12 18:38:26+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@Nondzu This is sagemaker deployment. 

from sagemaker.huggingface import HuggingFaceModel
and i already have this defined in hub:
```
 transformers_version='4.36.0',
```
Did I miss something?"
BoDong,2023-12-13 03:37:38+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I face the same issue and also installed newest transformers from source. transformers commit id is f4db565b695582891e43a5e042e5d318e28f20b8
Could you provide the help?
"
ArthurZ,2023-12-13 09:10:56+00:00,comment,mistralai/Mixtral-8x7B-v0.1,cc @philschmid 
philschmid,2023-12-13 09:34:07+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Hey can you please take a look at https://www.philschmid.de/sagemaker-deploy-mixtral. You need the container version 1.3.1 which is not yet available in sagemaker. 
vibranium,2023-12-13 18:22:08+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@philschmid Thanks! it worked. Much appreciated for the help.
seabasshn,2023-12-15 20:55:17+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@philschmid I tried following your tutorial but I keep getting the same issue as @vibranium. Any ideas as to what the issues might be?
"
vibranium,2023-12-15 21:05:59+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@seabasshn What instance size you are using? In my case it works on ml.g5.48xlarge. Also, make sure you are using below image:
```
763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.3.1-gpu-py310-cu121-ubuntu20.04-v1.0
```
You would also need sagemaker version: `sagemaker==2.199.0`"
seabasshn,2023-12-15 21:19:14+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@vibranium Yes, I am using ml.g5.48xlarge and image: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.3.1-gpu-py310-cu121-ubuntu20.04-v1.0"
dwilman,2024-01-16 09:10:44+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@philschmid - I have the same problem trying to deploy, exactly following the instructions in your blog post ( same image, same instance, etc ), from Sagemaker Studio (exactly as @seabasshn seems to experience too). The Cloud logs state a problem starting the Shard. 
Any ideas what might be the problem??
Thanks!!!"
dwilman,2024-01-16 13:29:41+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@philschmid , @seabasshn : problem solved: I needed TGI v1.3.3
i.e. huggingface-pytorch-tgi-inference:2.1.1-tgi1.3.3-gpu-py310-cu121-ubuntu20.04-v1.0
NOT v1.3.1 as described in the demo.
Then it worked
"
dwilman,2024-01-16 13:32:15+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@philschmid, are there quantized versions of the Mixtral-8x7B-v0.1 model available yet over the Hugging Face LLM DLC?
"
rcarrata,2024-01-18 21:49:11+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"it worked also for me with the TGI v1.3.3
Nice blog post @philschmid, very neat!"
ybelkada,2023-12-12 16:24:58+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"hi @petergrubercom those are used to highlight the diff that users need to apply to enable things like 4bit inference or FA-2, you can refer to the basic usage shared on the first snippet and apply manually the changes which are a 1 LoC change"
paddyofitz,2023-12-15 23:53:37+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I get what the team have done. It is not clear however about doing two things - for example: Load the model with Flash Attention 2 and half-precision settings. 

I used:
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = ""mistralai/Mixtral-8x7B-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Load the model with Flash Attention 2 and half-precision settings
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, attn_implementation=""flash_attention_2"")

# Define the prompt
prompt = ""My name is""

# Tokenize the prompt
model_inputs = tokenizer([prompt], return_tensors=""pt"")

# Generate text based on the prompt
generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))

I am coming across quite a few issues loading this on to a Windows 11 box however. I am more than happy to write out and end to end install instruction for Windows 11 (a lot of people I know want to do this). Shall I post separately about what I have found and how I have been dociumenting it so far (e.g. minimum requirements, Cude Toolkit installation, torch, wheel etc.)
"
ybelkada,2023-12-16 16:39:28+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> I am coming across quite a few issues loading this on to a Windows 11 box however. I am more than happy to write out and end to end install instruction for Windows 11 

That would be really great @paddyofitz !
I think that you can post a new issue with a clear title that explains it is about how end-to-end instructions for Windows 11, it will be definitely extremely helpful for the community"
paddyofitz,2023-12-16 21:39:06+00:00,comment,mistralai/Mixtral-8x7B-v0.1,I will do - just stripping my build right back to work out the dependency chain on pip as if this was the first thing going on a machine @ybelkada 
gdlangus,2023-12-13 04:41:45+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Getting this error as well.
ArthurZ,2023-12-13 09:09:01+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Thanks, could you open an issue on `https://huggingface/transformers` with a full reproducer? "
rganti,2024-01-05 20:04:24+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Is there any corresponding issue?
ybelkada,2024-01-08 08:35:59+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I believe it has been recently fixed by: https://github.com/huggingface/transformers/pull/28061
You can use the main branch of transformers, `pip install -U git+https://github.com/huggingface/transformers.git`"
rganti,2024-01-08 15:43:01+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@ybelkada I can confirm that with moving to the latest HF as mentioned above, I am able to fine tune Mixtral using FSDP. :tada:"
hrushikesh1,2024-01-11 18:25:53+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@rganti Can you please share your FSDP config ?
I am trying a full fine tuning(not LoRA)  using 
`auto_wrap_policy={MixtralDecoderLayer}, activation_checkpointing_policy={MixtralDecoderLayer}` 
according to https://lightning.ai/docs/fabric/stable/advanced/model_parallel/fsdp.html

It is giving me recomputed tensor size mismatch error. A detailed bug report is [here ](https://github.com/Lightning-AI/pytorch-lightning/issues/19267)
FYI: I tried the latest transformer and lightning library installed from git+https"
rganti,2024-01-11 18:37:47+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"```
{
  ""fsdp_auto_wrap_policy"": ""TRANSFORMER_BASED_WRAP"",
  ""fsdp_backward_prefetch_policy"": ""BACKWARD_PRE"",
  ""fsdp_cpu_ram_efficient_loading"": ""False"",
  ""fsdp_forward_prefetch"": ""True"",
  ""fsdp_offload_params"": ""False"",
  ""fsdp_state_dict_type"": ""SHARDED_STATE_DICT"",
  ""fsdp_sync_module_states"": ""False"",
  ""fsdp_transformer_layer_cls_to_wrap"": ""MixtralDecoderLayer"",
  ""fsdp_use_orig_params"": ""True"",
  ""activation_checkpointing"": ""True""
}
```

I am using `SFTTrainer`"
rganti,2024-01-11 19:32:58+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"btw @hrushikesh1 -- some other model (`GPTBigCode`) is giving me this trouble (size/shape mismatch), it used to work well in the past for me :) "
rganti,2024-01-12 18:26:10+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@hrushikesh1 To update, it seems to be flaky and dependent on the PyTorch and HF versions that are installed. I am still trying to figure out the ""right"" combination, but perhaps @ybelkada or someone from HF/PT teams can comment?"
rganti,2024-01-12 18:41:27+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"specifically, using torch version `2.2.0.dev20231121+cu118` and transformers is `4.37.0.dev0` and python is `3.11`"
hrushikesh1,2024-01-13 01:07:37+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Thanks for the info @rganti ! 
I was able to solve it by explicitly calling 
`model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': True})` 
after  model load `AutoModel.from_pretrained()`.

The issue re-appears if I set `use_reentrant:False` in the above call. Lightning library might be defaulting to use_reentrant:False.

There is lot of notes and warning from pytorch on the renentrant behavior [here](https://pytorch.org/docs/stable/checkpoint.html)
As of torch 2.1 it defaults to True, but they plan to move to use_reentrant=False as a default in future, that might be causing the flakiness you observe across versions"
rganti,2024-01-16 19:35:21+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@hrushikesh1 I was able to Lora tune mixtral on the latest PT nightlies and latest HF main after adding the above line, thanks!"
hrushikesh1,2024-01-19 00:15:09+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Wanted to share a note for a future data scientist in trouble:
I was trying `LORA` fine tuning of Mistral-7B using `FSDP` strategy and `pytorch lighting `trainer. It used to get stuck at Step-1. 
Turned out, since there are some frozen parameters without gradients, I can not use gradient_clipping."
cuong-dyania,2024-04-08 22:39:53+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> ```
> {
>   ""fsdp_auto_wrap_policy"": ""TRANSFORMER_BASED_WRAP"",
>   ""fsdp_backward_prefetch_policy"": ""BACKWARD_PRE"",
>   ""fsdp_cpu_ram_efficient_loading"": ""False"",
>   ""fsdp_forward_prefetch"": ""True"",
>   ""fsdp_offload_params"": ""False"",
>   ""fsdp_state_dict_type"": ""SHARDED_STATE_DICT"",
>   ""fsdp_sync_module_states"": ""False"",
>   ""fsdp_transformer_layer_cls_to_wrap"": ""MixtralDecoderLayer"",
>   ""fsdp_use_orig_params"": ""True"",
>   ""activation_checkpointing"": ""True""
> }
> ```
> 
> I am using `SFTTrainer`

Hi, Did you fine-tune Mixtral 8x7b with any adapter ? Or just regular fine-tuning with FSDP. Can you provide your GPU computing resource info ?"
Okarels,2023-12-12 10:15:12+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Thank you, I downloaded and hosted it on my seedbox to improve torrent seeding. "
hiyouga,2023-12-12 08:47:53+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Mixtral-8x7B fine-tuned on the Alpaca dataset, preliminary results:

![20231212164640.jpg](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/UKM9ZoA8JgjIZhyNHSzFt.jpeg)
"
hiyouga,2023-12-12 08:55:49+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Remarkable reasoning abilities:

![20231212165413.jpg](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/x3p-PwRfBZY2txxuJ1DtB.jpeg)
"
kustoll,2023-12-15 00:36:16+00:00,comment,mistralai/Mixtral-8x7B-v0.1,This sounds great! Could you kindly provide your command line parameters and a Deepspeed config to run it on multiple H100s?
shahules786,2023-12-16 12:32:56+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"This is great @hiyouga . I wonder how efficient the training will be, especially with sparse models, and how issues like token dropping will be addressed. "
noforit,2023-12-17 15:52:52+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"This is great! Thanks for sharing,  but i have an issue when  adopt 8-bit quantization, LoRA adapters and FlashAttention-2 with Mixtral 8x7B MoE model. 
there's an error :       RuntimeError: The size of tensor a (32) must match the size of tensor b (8) at non-singleton dimension 0
could you help me?

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6523af370415e1b734ffb84c/3pLGVbfa7wKZmh1dtUhMo.png)
"
hiyouga,2023-12-18 08:43:38+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@noforit it looks like there are some issues in 8-bit quantization, we recommend using 4-bit quantization instead"
noforit,2023-12-19 02:48:17+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"> @noforit it looks like there are some issues in 8-bit quantization, we recommend using 4-bit quantization instead

As you say, thanks, I use 4-bit instead and it works"
baptistejamin,2023-12-20 07:58:24+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I saw some comments showing that quantization was an issue to leverage with Mixtral MOE.

Mixtral routes each token to experts. Quantization can reduce probability for each token, resulting routing to only go a small portion of experts. "
giannigi,2023-12-23 11:27:28+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Can you share a mini-guide on the steps necessary to perform the taining, or share the commands and configs used? thanks"
hiyouga,2023-12-24 11:22:16+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@giannigi 

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --do_train \
    --model_name_or_path mistralai/Mixtral-8x7B-v0.1 \
    --dataset alpaca_en \
    --template mistral \
    --finetuning_type lora \
    --lora_target q_proj,v_proj \
    --output_dir mixtral \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 1.0 \
    --quantization_bit 4 \
    --bf16
```"
sumegh,2024-01-06 13:55:21+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"i'm following the same and doing a 4-bit LoRA finetuning on a custom dataset. Tried changing templates between Alpaca & Mistral. But my training loss diverges after 1k steps or so. Any ideas ?

Reference Notebook - https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing

cc - @hiyouga "
Villian7,2024-01-06 22:15:52+00:00,comment,mistralai/Mixtral-8x7B-v0.1,What are the minimum compute resources required to train the model?
aigeek0x0,2024-01-10 00:29:29+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@hiyouga 

can you please explain why you only targeted ""q_proj,v_proj"" layers? 

i have come across some arguments opposing the idea of fine-tuning all linear layers/gates/routers. I would greatly appreciate it if someone could provide a more detailed explanation on this matter. 

thank you."
hiyouga,2024-01-10 01:13:19+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@aigeek0x0 
We used `q_proj,v_proj` modules just to estimate the minimum resource usage. It is recommended to use all linear layers with LoRA adapters for better fitting.
"
Villian7,2024-01-10 01:57:08+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@aigeek0x0 
You can specify and finetune only the linear layers of any LLM model while using LORA. When you print(model) you will get the layers in that some people use only attention layers such as q, k, v, o in case of Mistral or some people use all linear layers. I'm exactly not sure how it affects the performance, but it will surely reduce the RAM size of the peft model and it is of very small amount."
chsafouane,2024-01-15 13:37:47+00:00,comment,mistralai/Mixtral-8x7B-v0.1,It's still not clear for me whether one should also finetune the routers. Any resources discussing this ?
ivsanro1,2024-03-04 06:58:11+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I was also wondering about the need of fine tuning routers. Intuitively it does not make much sense to fine tune the routers with the proj layers, because it can make training unstable, as you’d be fine tuning both representations and routers, and changes in one can affect the other and vice versa.

Other way to see it is that if you change the expert(s) for a given token, you’re losing very valuable information from the base model, and changing the routing decision e.g. from experts (1,3) at a given layer to experts e.g. (4,6) would have much of a bigger, sudden impact  than a small, gradual update of the proj matrices in every update step.

But all of this is just speculation, and can be task-specific."
pierrerichard,2024-03-13 08:39:57+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Is it possible to run the fine-tuning of Mixtral with LLaMa-Factory on CPU only or on both GPU and CPU (my GPU is 16 GB of VRAM)?
Thanks a lot!"
ArthurZ,2023-12-11 20:59:36+00:00,comment,mistralai/Mixtral-8x7B-v0.1,You did not install transformers==4.36.0 .... `pip install -U transformers`
dragstoll,2023-12-11 21:09:22+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"thank you Arthur, I updated the transformers via github and it didn't work, the dev version is not up to date
"
sacdroid,2023-12-12 00:16:48+00:00,comment,mistralai/Mixtral-8x7B-v0.1,It is working for me with `RUN pip install git+https://github.com/huggingface/transformers.git`
Mauceric,2023-12-12 09:26:15+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Thanks ArthurZ, thanks Sacdroid :)"
sanjog21,2023-12-12 17:40:00+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[5], line 6
      3 model_id = ""mistralai/Mixtral-8x7B-v0.1""
      4 tokenizer = AutoTokenizer.from_pretrained(model_id)
----> 6 model = AutoModelForCausalLM.from_pretrained(model_id)
      8 text = ""Hello my name is""
      9 inputs = tokenizer(text, return_tensors=""pt"")

File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    523 if kwargs.get(""quantization_config"", None) is not None:
    524     _ = kwargs.pop(""quantization_config"")
--> 526 config, kwargs = AutoConfig.from_pretrained(
    527     pretrained_model_name_or_path,
    528     return_unused_kwargs=True,
    529     trust_remote_code=trust_remote_code,
    530     code_revision=code_revision,
    531     _commit_hash=commit_hash,
    532     **hub_kwargs,
    533     **kwargs,
    534 )
    536 # if torch_dtype=auto was passed here, ensure to pass it on
    537 if kwargs_orig.get(""torch_dtype"", None) == ""auto"":

File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
   1062     return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)
   1063 elif ""model_type"" in config_dict:
-> 1064     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
   1065     return config_class.from_dict(config_dict, **unused_kwargs)
   1066 else:
   1067     # Fallback: use pattern matching on the string.
   1068     # We go from longer names to shorter names to catch roberta before bert (for instance)

File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761, in _LazyConfigMapping.__getitem__(self, key)
    759     return self._extra_content[key]
    760 if key not in self._mapping:
--> 761     raise KeyError(key)
    762 value = self._mapping[key]
    763 module_name = model_type_to_module_name(key)

KeyError: 'mixtral'
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
I have tried everything which is mentioned in the above comments but still I'm getting this error
 "
ybelkada,2023-12-12 22:26:53+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@sanjog21 can you try to re-install transformers in a fresh new env?
yardenhoch,2023-12-27 09:45:11+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi,

I am currently trying to load the `mistralai/Mixtral-8x7B-v0.1` model using Hugging Face, but I am encountering some issues. I noticed a note stating that ""the model cannot (yet) be instantiated with HF."" However, I have seen that you have managed to load this model despite this comment. 

When I attempt to load the model `AutoModelForCausalLM.from_pretrained(model_id)`, I receive the following error:
`OSError: mistralai/Mixtral-8x7B-v0.1 does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.`

I would appreciate any guidance on how to properly load this model. Is there a specific method or workaround to load this model using Hugging Face despite the aforementioned comment?

Thank you in advance for your help."
ybelkada,2024-01-08 08:37:30+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @yardenhoch 
Thanks for the comment! I suspect you most likely have an outdated version of transformers that do not support `safetensors` serialization format. Can you try to upgrade your transformers version? `pip install -U transformers` this should solve your issue"
deleted,2023-12-13 21:34:04+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@Hoioi I want this to, but it will never happen.

Mistral is a French company looking for EU dominance in the AI market, so their smartest path forward is with multi-lingual LLMs.

Plus 46b parameters is already too few for the mixture of experts x8 design, which is one of the reasons it performs so poorly on Arc despite its size (66 vs GPT3.5's 86). It basically only has the intelligence of an ~14b dense (non-MOE) Mistral, and the knowledge of an ~20b dense Mistral, yet requires the RAM of a 46b dense Mistral. However, they need to move forward with MOE so they can host to more users with fewer resources."
YaTharThShaRma999,2023-12-13 22:35:39+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@Phil337 hmm no it has the knowledge of a 47b model? In inference time, you basically use the 2 model/layers which are more suited to the task and you should actually get better output then a normal 47b model? It beats llama 2 70b which was pretty good as well."
deleted,2023-12-13 22:52:41+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@YaTharThShaRma999 MOE, no matter how well it's implemented, will always have significantly less information than an equal sized dense LLM.

This is primarily because (1) there's FAR more redundancy within all of the experts. Pretty much every expert has the same basic knowledge (and 2) the most ideal 2 of 8 experts is not always chosen, so using all 8 experts at once, vs just running inference through 2, will return significantly more valid information.

This is all part of the compromise of MOE. Far faster inference (=12b) with a small boost in intelligence (~=14b), a bigger boost in knowledge (=~20b), but with far more RAM required (=47b). If they ran inference through all 8 at once they can significantly boost both intelligence and knowledge (=~26b), but it would run at the speed of a 47b LLM.

The best way to see the knowledge drop is by having it recite the lyrics of a popular song. This is hard to store accurately within small LLMs. GPT4 is excellent at this, GPT3.5 is good, Llama 2 70b & Falcon 180b are OK, Mixtral is bad and Mistral 7b is horrible (one line at most).

And the best way to see the intelligence drop is to make it write a joke about two disparate things while forcing a header to it doesn't just try to copy an existing joke. For example, ""Write a joke about a horse and computer that begins with On a rainy day. Then explain the joke."" Or just look at the Arc score (only 66 for Mixtral, when Mistral 7b scored 60).

"
YaTharThShaRma999,2023-12-14 18:21:54+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@Phil337 alright i see your point but I and about everyone else  find it much better then 70b sized models and similar sized models as well. Ask it coding tasks, riddles and more things and you will see this is a model that can easily compete with chatgpt.

Benchmarks are not everything and even the arc benchmark is just really multiple choice which is not everything at all. "
deleted,2023-12-14 18:36:25+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@YaTharThShaRma999 Yes, I threw several questions at Mixtral that Falcon 180b, Llama 2 70b, Mistral 7b... got wrong, and it got them right. It's undeniably better than Falcon 180b or Llama 2 70b. But it's not generally better than GPT3.5.

For example, it can't adeptly handle complex and tricky prompts which require truly adaptable cognition, such as generating jokes about 2 disparate things with a forced header so it doesn't just try to copy the format of a known joke. This general intelligence is best measured by Arc vs any other standardized LLM test (since Arc only requires general knowledge that even the smallest LLMs have).

So claiming GPT3.5 performance was almost entirely about Arc. Mistral 7b was already within 3-5 point of GPT3.5 on MMLU, WinoGrande, HellaSwag... so Mistral 14b dense would have easily matched GPT3.5 on said tests. So it all came down to Arc (60 for Mistral 7b and 85 for GPT3.5), yet Mixtral only got 66, which is exactly what a 14b dense Mistral is predicted to get."
YaTharThShaRma999,2023-12-15 15:13:44+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@Phil337 yeah I agree that gpt3.5 does have a slightly better instruction following ability but mixtral is pretty near most things. And chatgpt most likely has lots of extra stuff  that it’s using with it like wolfram alpha, maybe some hallucinater checker or different things.

Also 8 experts is actually much worse than using 2 currently. You can check this perplexity benchmark and it shows more experts = higher perplexity. But it is a artificial benchmark that’s even worse then normal benchmarks so yeah

[benchmark](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/TvjEP14ps7ZUgJ-0-mhIX.png)"
deleted,2023-12-15 15:19:23+00:00,comment,mistralai/Mixtral-8x7B-v0.1,@YaTharThShaRma999 Thanks for that chart. That conclusively shows that using more than 3 experts is progressively worse.
ArthurZ,2023-12-11 13:51:55+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"The fix in transformers for the loss computation is going to be  something like
```python
    if isinstance(gate_logits, tuple):
        # cat along the layers?
        gate_logits = torch.cat([gate.cpu() for gate in gate_logits], dim=0)
``` 
we overlooked the devices when computing the loss"
joorei,2024-01-04 14:54:57+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@ArthurZ is this fixed in transformers? I am trying to fine tune with axolotl, but I get either 

```RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:6 and cuda:0!```

 or when I change the config.json part like this:

```
  ""output_router_logits"": false,
```

I get :

```
RuntimeError: !grad_accumulator_.expired() INTERNAL ASSERT FAILED at ""../torch/csrc/autograd/saved_variable.cpp"":226, please report a bug to PyTorch. No grad accumulator for a saved leaf
```

Any hints?

No accelerate, just trying to run the training."
ybelkada,2023-12-11 13:23:48+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"hi @dataautogpt3 
Can you share a reproducible snippet together with the full traceback of the error? thanks"
bjoernp,2023-12-11 13:32:32+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I'm getting the same issue with the following code:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = ""mistralai/Mixtral-8x7B-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"")

text = ""Hello my name is""
inputs = tokenizer(text, return_tensors=""pt"").cuda()

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
results in:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1718, in generate
    return self.greedy_search(
  File ""/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2579, in greedy_search
    outputs = self(
  File ""/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py"", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File ""/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py"", line 1244, in forward
    aux_loss = load_balancing_loss_func(
  File ""/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py"", line 98, in load_balancing_loss_func
    gate_logits = torch.cat(gate_logits, dim=0)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)
```
"
ArthurZ,2023-12-11 13:48:17+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"that for computing the loss, I think the code is not the latest cuz I pushed a fix but will check
"
ybelkada,2023-12-11 13:49:49+00:00,comment,mistralai/Mixtral-8x7B-v0.1,https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5 should fix the issue
ybelkada,2023-12-11 14:01:31+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@bjoernp can you try:

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = ""mistralai/Mixtral-8x7B-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"", revision=""refs/pr/5"")

text = ""Hello my name is""
inputs = tokenizer(text, return_tensors=""pt"").cuda()

outputs = model.generate(**inputs, max_new_tokens=20)
```"
bjoernp,2023-12-11 14:55:36+00:00,comment,mistralai/Mixtral-8x7B-v0.1,Works! Thanks :)
bweinstein123,2024-01-30 10:19:52+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"@bjoernp Hi,

Does the code above should parallelize the model across multiple gpus? Is the device_map='auto' does this work?

Thanks."
ybelkada,2024-01-30 10:54:31+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"Hi @bweinstein123 
Yes, `device_map=""auto""` should split the model evenly across all GPUs"
ArthurZ,2023-12-11 12:13:40+00:00,comment,mistralai/Mixtral-8x7B-v0.1,"I think the amazing @TheBloke is going to convert them and push them as always to his repos! 
You can already use `load_in_4bit = True` for `bitsandbytes` quantization with transformers format"
diegolascasas,2023-12-10 19:55:37+00:00,comment,mistralai/Mixtral-8x7B-v0.1,
YaTharThShaRma999,2023-11-02 11:06:42+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,I believe you just have way too little vram. Try using gguf is it uses your ram instead of vram.
koolaidfiction,2023-10-24 06:04:24+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"clear up pytorch's cache and reload the model.
"
abrehmaaan,2023-10-06 06:24:46+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"@TheBloke Please help
"
TheBloke,2023-10-06 09:08:38+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"Yes, AutoGPTQ doesn't support Mistral GPTQ yet. You need to use Transformers GPTQ, or ExLlama."
dyoung,2023-10-03 04:35:53+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"I'm having the same issue. If any, what was your solution lqfjun? 
To anyone helping, let me know if you need additional info in regards of my situation.
If I figure it out before I hear from anyone, I'll be sure to update here about it."
dyoung,2023-10-03 04:48:24+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"OK, I think I have an idea. Seems that I missed in the model card that AutoGPTQ doesn't have support for mistral models yet. And that this can be remedied by pip git installing transformers build not in the main pip repository. ""pip3 install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79"" Or I can rename the model in the config after it's been downloaded as mentioned here from the original models HF card. ""https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/2"" I've not tried either of those options yet. Will update once I have new info."
dyoung,2023-10-03 04:58:45+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"I went to the quick config.json adjustment because I didn't want to spend time vetting if I wanted to pip git install a version of transformers not the current main version.  Getting some warnings, but the model is still working. Think I'll go with this method for now."
lqfjun,2023-10-03 05:17:36+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"Yes, just install the latest transformer will work"
dyoung,2023-10-03 05:25:47+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,I'll have to try the latest angle another time. I've source installed another git branch of transformers which I need for another project/model for it to work. I'll likely have to check to see if working with the latest will work for my other model as well as for Mistral. Which is why I went with the dirty quick adjustment in the config for today. I just wanted to take mistral for a quick ride. Just discovered the model today.
TheBloke,2023-09-29 20:46:17+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"Great, thanks for letting us know!"
SergiusFlavius,2023-10-03 21:52:32+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,"It works with Exllama v2 (release: 0.0.4).
```
c:\AI\exllamav2>call .\venv\Scripts\activate   & python examples/chat.py --mode raw --model_dir c:\AI\exllamav2\models\Mistral-7B-Instruct-v0.1-GPTQ-4bit-32g-actorder_True
 -- Model: c:\AI\exllamav2\models\Mistral-7B-Instruct-v0.1-GPTQ-4bit-32g-actorder_True
 -- Options: ['rope_scale 1.0', 'rope_alpha 1.0']
 -- Loading model...
 -- Loading tokenizer...

User: Hi

Chatbort: Hello! How can I help you today?
```"
zcamz,2023-11-16 12:03:40+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,This comment has been hidden
larsskaug,2023-12-18 20:59:59+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,Are you finding it slower in exllama v2 than in exllama? I do. 
ani-baghdasaryan,2024-06-03 06:43:28+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I think you had to go through the link and accept the terms, so it would allow you to get access. "
rkhapre,2024-05-28 16:11:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I am also looking for the same answer, now again this is overloaded"
rkhapre,2024-05-28 16:12:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@pandora-s , can you please let us know if possible."
pandora-s,2024-05-29 06:27:23+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi there ! Personally its working for me, but I think this can happen on 3 circumstances: `maintenance`, `migration of servers` or just model is too `busy (overloaded so try later)`."
Jaereb,2024-05-14 15:01:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,My fault.... HF permissions.
ybelkada,2024-05-14 15:36:13+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @Lue-C 
Thanks for the issue, this is probably because you are not running the script on a GPU. Bitsandbytes only works on GPU"
Lue-C,2024-05-28 10:24:13+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi ybelkada,
yeah this was the problem. Thanks!"
aantoniuron,2024-05-22 05:53:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,me too 😿. do you got any update??
pooja03,2024-05-28 05:26:21+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@aantoniuron no, i used text split until last fullstop to work. I know its not the right way but for now I am working with this"
JoJoJo333,2024-04-25 15:49:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I am also having a similar issue trying to download models in CD pipelines even tho we have clicked all the ""agree to share"" buttons on the HF website. We can still download mistralai/Mistral-7B-Instruct-v0.1 but not mistralai/Mixtral-8x7B-v0.1"
chakkakrishna,2024-04-22 04:49:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,
chakkakrishna,2024-04-22 04:49:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I need to merge two file on fine tunning on mixtral 8 7b
chakkakrishna,2024-04-22 04:50:04+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,203
IagenInnov,2024-04-18 15:15:28+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have the same question when i ma trying to pull the model from vllm.
At this end, I have
""  File ""/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py"", line 304, in hf_raise_for_status
    response.raise_for_status()
  File ""/usr/local/lib/python3.10/dist-packages/requests/models.py"", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/mistralai/Mixtral-8x7B-Instruct-v0.1"""
JaUg,2024-04-18 15:41:08+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"They seem to have locked these files behind authentication. It is possible to retrieve if you add a HF token to your request. See more here:
https://huggingface.co/docs/hub/en/security-tokens
https://huggingface.co/docs/transformers.js/en/guides/private"
deleted,2024-04-18 16:46:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"when i get this, I just download them manually via web. But ya, doing it tokens is more efficient  "
sophiamyang,2024-04-19 08:57:12+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"You will need to go back to the model page. Click ""Agree and access repository"". 
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6320c05a145cfa4c04cb4359/mSohUuo7VxXXJVcKG7gVV.png)
"
datadoktergroen,2024-04-19 09:06:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Somehow, it still doesn't work, despite agreeing.
accessing other private models does work so it's not the code. Is there some delay or something?

-- Update: it works now, just had to wait a few minutes"
AbRds,2024-04-19 09:30:30+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I solved the problem generating the HF tokes as @JaUg  said then I just added the token as a variable environment in the docker run command. 

docker run -d -e HF_TOKEN='XXXXXX' --gpus all --shm-size 1g -p 8080:80 -v /data:/data ghcr.io/huggingface/text-generation-inference:latest  --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 ... "
realdumbslayer,2024-04-23 13:38:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"i may help you in that, this is my thesis code that i'm currently working on, we have similar code, but mine has no memory because the buffer memory isn't working with mine :(
But the whole code and chatbot works!

# CHATBOT
## HF_TOKEN
HF_TOKEN = 'Your_token'
os.environ[""HUGGINGFACEHUB_API_TOKEN""] = HF_TOKEN

# Load PDF documents from the upload folder
loader = PyPDFDirectoryLoader(UPLOAD_FOLDER)
loader.requests_per_second = 1
docs = loader.load()

# Chunking - Text Splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)
chunks = text_splitter.split_documents(docs)

# Embedding Model
embeddings = HuggingFaceEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=""db"")

# Retrieval
query = ""What is Clustering?""
search = vectorstore.similarity_search(query)

# Retriever
retriever = vectorstore.as_retriever(
    search_type=""mmr"",  # similarity
    search_kwargs={'k': 4}
)

# LLM-Open Source
llm = HuggingFaceHub(
    repo_id=""mistralai/Mixtral-8x7B-Instruct-v0.1"",
    model_kwargs={""temperature"": 0.5, ""max_length"": 64, ""max_new_tokens"": 512}
)

# RAG RetrievalQA chain
qa = RetrievalQA.from_chain_type(llm=llm, chain_type=""refine"", retriever=retriever)

# Prompt Template
DEFAULT_SYSTEM_PROMPT = """"""
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
"""""".strip()


def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:
    return f""""""
{prompt}
"""""".strip()

# Chain
SYSTEM_PROMPT = ""Use the following pieces of context and history to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.""

template = generate_prompt(
    """"""
    context: {context}
    question: {query}
    answer:
    """""",
    system_prompt=SYSTEM_PROMPT,
)

prompt = ChatPromptTemplate.from_template(template)

retrieval_chain = RetrievalQA.from_chain_type(llm,
                                              chain_type='stuff',
                                              retriever=vectorstore.as_retriever(),
                                              chain_type_kwargs={
                                                  ""prompt"": prompt,
                                            })

rag_chain = (
    {""context"": retriever, ""query"": RunnablePassthrough()}
    | prompt
    | llm  
    | StrOutputParser()
)

# Function to retrieve conversation history
def get_chat_history():
    return list(chat_history_collection.find())

@app.route('/chat', methods=['GET', 'POST'])
def chat():
    if request.method == 'GET':
        chat_history = get_chat_history()
        return render_template('chatbot.html', chat_history=chat_history)
    elif request.method == 'POST':
        user_input = request.json['user_input']
        chat_history_collection.insert_one({'sender': 'user', 'message': user_input})
        response = rag_chain.invoke(user_input)
        bot_response = response.split('answer: ')[1].strip()
        chat_history_collection.insert_one({'sender': 'bot', 'message': bot_response})


        # Retrieve the last document if any exists
        last_chat_document = chat_messages_collection.find_one(sort=[('_id', -1)])
        
        # Check if a document exists and extract the existing chat history
        if last_chat_document:
            chat_history = last_chat_document.get('chat_history', [])
        else:
            chat_history = []
        
        # Append the current user input and bot response to the chat history
        chat_history.append({'user_input': user_input, 'bot_response': bot_response})
        
        # Update the chat history in the database
        if last_chat_document:
            chat_messages_collection.update_one({'_id': last_chat_document['_id']}, {'$set': {'chat_history': chat_history}})
        else:
            chat_messages_collection.insert_one({'chat_history': chat_history})
        
        # Format the chat history
        formatted_chat_history = []
        for entry in chat_history:
            formatted_entry = {
                'user_input': entry['user_input'],
                'bot_response': entry['bot_response']
            }
            formatted_chat_history.append(formatted_entry)
        return response

i'm using flask and html. 
I've been trying to integrate and get the buffer memory code to work, but i can't figure it out.

Check my code above, you'll get the response you want! :)"
realdumbslayer,2024-04-23 13:46:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"if you're using colab or jupyter, i could give you the notebook code.
The input and response should work for you, i don't know about the buffer memory though, but if the code works for you, let me know! "
w8jie,2024-04-24 09:42:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I'm facing the same issue as OP, although I am using Langchian to stack multiple chains for generating summaries. I'm not using RAG, so context doesnt get returned but my few shot examples are returned instead....."
w8jie,2024-04-24 09:42:44+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> if you're using colab or jupyter, i could give you the notebook code.
> The input and response should work for you, i don't know about the buffer memory though, but if the code works for you, let me know!

Do you mind sharing your notebook with me?"
edoyen,2024-04-24 13:23:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Finally I found the answer to my problem by using the HuggingFaceEnpoint function instead of HuggingFaceHub if that helps.

      repo_id = ""mistralai/Mixtral-8x7B-Instruct-v0.1""
      mistral_llm = HuggingFaceEndpoint(
               repo_id=repo_id,
               temperature=0.5,
               top_k=50,
               huggingfacehub_api_token=""HUGGINGFACE_API_TOKEN""
       )
"
realdumbslayer,2024-04-24 17:58:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,did the buffer memory work as well?
w8jie,2024-04-25 06:24:57+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I found a workaround, for my case it was having to add a `return_full_text` param and have it set to False.



        pipe = pipeline(
                ""text-generation"",
                model=model,
                tokenizer=tokenizer,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                device=""cuda"",
                return_full_text=False
            )
      self.llm = HuggingFacePipeline(pipeline=pipe)"
realdumbslayer,2024-04-26 18:11:20+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,i can't find the correct libraries for HuggingFaceEndpoint
MarvelSanya,2024-04-17 21:55:44+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"similar errors in huggingchat
"
sunnykusawa,2024-04-19 04:32:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I am able to solve this issue for sagemaker endpoint.

we need to set environment variables MAX_INPUT_LENGTH and MAX_TOTAL_TOKEN.

While deploying llm with sagemaker add this environment variables

hub = {	
    'HF_MODEL_ID':'mistralai/Mixtral-8x7B-Instruct-v0.1',
	'SM_NUM_GPUS': json.dumps(8),
    ""MAX_INPUT_LENGTH"": '30000',   =>  put here any value upto  32768 as per your requirement.
    ""MAX_TOTAL_TOKENS"": '32768',
    ""MAX_BATCH_PREFILL_TOKENS"": '32768',
    ""MAX_BATCH_TOTAL_TOKENS"": '32768',
}

It will change the defaul MAX_INPUT_TOKEN size from 2048 to 30000

"
sunnykusawa,2024-04-19 04:32:31+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I am able to solve this issue for sagemaker endpoint.

we need to set environment variables MAX_INPUT_LENGTH and MAX_TOTAL_TOKEN.

While deploying llm with sagemaker add this environment variables

hub = {	
    'HF_MODEL_ID':'mistralai/Mixtral-8x7B-Instruct-v0.1',
	'SM_NUM_GPUS': json.dumps(8),
    ""MAX_INPUT_LENGTH"": '30000',   =>  put here any value upto  32768 as per your requirement.
    ""MAX_TOTAL_TOKENS"": '32768',
    ""MAX_BATCH_PREFILL_TOKENS"": '32768',
    ""MAX_BATCH_TOTAL_TOKENS"": '32768',
}

It will change the defaul MAX_INPUT_TOKEN size from 2048 to 30000

"
pandora-s,2024-04-16 21:51:08+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi there? It should stop by itself tho? But the EOS is `</s>` (the BOS should be `<s>`), you can check this on the chat template,~~btw you might want to consider switching to the Instruct V0.2?~~"
pandora-s,2024-04-16 21:51:47+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Ops my bad thought I was in Mistral7B for a moment-
But yeah, `</s>` should do."
kolinko,2024-04-10 07:38:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Update - the formats seem off for other weights as well. What's going on?

```
bb = sweights[""model.layers.0.block_sparse_moe.experts.0.w2.weight""]
aa = tweights[""layers.0.block_sparse_moe.w2""].reshape((8, -1, 14336))[0]
aa-bb.sum()
>>> 0  (the same)

bb = sweights[""model.layers.0.block_sparse_moe.experts.1.w2.weight""]
aa = tweights[""layers.0.block_sparse_moe.w2""].reshape((8, -1, 14336))[1]
>>> -0.1250 (difference!)
```

For expert 0 it's the same.

Was the model converted into safetensors properly? Or is it a different version?
My implementation breaks when using the new weights, but it works fine when using .pt. Not sure if I'm missing sth or what :/

Also, where can I find any info on changes like this? E.g. in Mistral it used to be w1/w2/w3 for ffn weights - that's how it's implemented in reference, now it's gate/down/up. I had to find it in HF implementation to make sure I'm getting the new names right.

(aside from all that, thanks for releasing the model!)"
HamzaGbada,2024-04-12 08:58:35+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Same here! Any updates ?
pandora-s,2024-04-12 09:24:28+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi, Im not sure if Im understanding, the responses are being cut ? Thats most likely because of the max new token yeah."
whalemare,2024-04-15 06:33:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same problem

With max_new_tokens, response in just empty string
With min_new_tokens, response is cut and doesn't follow instructions"
pandora-s,2024-04-16 18:35:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Same problem
> 
> With max_new_tokens, response in just empty string
> With min_new_tokens, response is cut and doesn't follow instructions

What value do u put for max new tokens?"
leonshub,2024-04-06 16:47:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,solved my problem
dron3flyv3r,2024-03-29 21:20:52+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Context size as in context window? on Mistral AI's website you can read about the AI model. But it has a **context window of 32K**.
CR33STL,2024-04-05 14:17:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Actually when running inference I get this error. So I guess it's not 32k...
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6555c3205a9ec4267fa2de51/oROTbQom03Sxe0N21qcAA.png)
"
shuyuej,2024-04-07 01:16:03+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@Pradeep1995 @CR33STL Have you saved the tokenizer? If you set the ""model_max_length"" to 1024 and your input length is now larger than 1024, it would generate this warning. 
But if you are using the original tokenizer, the length is somehow ""unlimited"" as shown [here](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/blob/main/tokenizer_config.json#L35)
During their training, the context length used was `32768`, as shown in the paper. "
kmukeshreddy,2024-03-28 16:04:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hello @ybelkada , Hope you are doing well!
Hopefully looking for your comment on the above issue.
Thanks in advance!!"
kmukeshreddy,2024-03-28 20:39:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ArthurZ @joaogante 
alamati,2024-03-28 11:10:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,
markov-aug,2024-04-08 09:21:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I would also want to know
keskival,2024-03-28 12:26:21+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"We are somewhat capable of mitigating this by adding the following instruction:
```
I know you have a huge temptation to end the response in the middle of a birth date, but that
is not a normal thing to do, and you absolutely should produce a complete JSON and your answer
shouldn't stop before you have closed off the task XML tags.
```"
barbery,2024-03-28 08:34:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This problem has been bothering me for 3 weeks and I still have no idea how to solve it.
kmukeshreddy,2024-03-28 16:07:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@barbery : 
try this instead, you are missing ** infront of inputs while you are feeding to the model.

inputs = tokenizer(prompt, return_tensors=""pt"")
inputs = inputs.to(model.device.type)
outputs = model.generate(**inputs, generation_config=generation_config)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
g-ronimo,2024-03-29 05:15:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> This problem has been bothering me for 3 weeks and I still have no idea how to solve it.

your code works fine for me on 2x3090. Are you using the latest transformers, accelerate, bitsandbytes? what version of torch? "
barbery,2024-03-29 05:51:38+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> @barbery : 
> try this instead, you are missing ** infront of inputs while you are feeding to the model.
> 
> inputs = tokenizer(prompt, return_tensors=""pt"")
> inputs = inputs.to(model.device.type)
> outputs = model.generate(**inputs, generation_config=generation_config)
> response = tokenizer.decode(outputs[0], skip_special_tokens=True)

if add ** in front of inputs will get error : 
```
outputs = model.generate(**input_ids, max_new_tokens=20)
TypeError: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor
```"
barbery,2024-03-29 05:52:56+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> > This problem has been bothering me for 3 weeks and I still have no idea how to solve it.
> 
> your code works fine for me on 2x3090. Are you using the latest transformers, accelerate, bitsandbytes? what version of torch?

Really? This is really weird. I'll try to upgrade them all."
barbery,2024-03-29 06:13:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"```
Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.2)
Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0)
Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.0)
Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.2)
```

@g-ronimo I have upgrade to latest version, but still not work, and output same error message. that is so weird
"
g-ronimo,2024-03-29 06:30:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Think I had this error once where the tokenizer output token id's not part of the embeddibg table. Can you try re-downloading the model from HF?
g-ronimo,2024-03-29 06:31:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Can you please post the input ids generated with your code?
barbery,2024-03-29 07:02:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Can you please post the input ids generated with your code?

do you mean print the input_ids?  here is print output:
```
tensor([[    1,   733, 16289, 28793,  1824,   349,   574, 16020,  2076,  2487,
         28804,   733, 28748, 16289, 28793,  6824, 28725,   315, 28742, 28719,
          3448, 10473,   298,   264,  1179, 11322, 19961,   302,  6138, 23598,
         18342, 28723,   661, 13633,   776,   272,  1103,  3558,   302,   686,
         16944, 15637,   423,   298,  5681,   315, 28742, 28719, 13198,   582,
           297,   272,  6132, 28808,     2,   733, 16289, 28793,  2378,   368,
           506,   993,  7136,   864, 21116, 28804,   733, 28748, 16289, 28793]],
       device='cuda:7')
```"
g-ronimo,2024-03-29 10:00:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Lgtm. Try updating the model 
venkilfc,2024-04-10 09:23:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I'm having the same issue. Works well with cpu. But with GPU it doesn't work at all. I have 5x48GB RTX 6000 Ada.
kolinko,2024-04-10 06:30:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"It's the same as for Mistral, afaik. At least I'm using the Mistral's one and it works well."
samarshahbaz4,2024-04-04 14:12:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,2048
RamanSB,2024-04-05 18:48:31+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Please state your source or reference to the value mentioned.
Ateeqq,2024-03-26 17:54:25+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"The GPU is currently utilizing approximately 31GB of memory. Hence, I speculate that it could also be fine-tuned with a 32GB VRAM GPU. If anyone achieves success with this, please inform me in the comments!"
MaziyarPanahi,2024-03-26 18:47:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,thanks for sharing! very useful information! 
nilsec,2024-03-21 15:13:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Related to the following issue: https://github.com/vllm-project/vllm/issues/2464
ndurkee,2024-03-25 17:49:48+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Just wanted to say that this seems to have fixed a lot of the issues I was having with my code.
epignatelli,2024-05-24 14:04:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Why is there no comment from the official maintainers, given the issue is so fundamental?
Also, why has the fix not merged yet?"
the-qa-company,2024-03-18 11:15:13+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"32k afaik.

32768 to be exact."
Theguy666,2024-03-11 16:58:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This is one of the best assistants you could ever have to work for you.
pandora-s,2024-03-10 16:52:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi there, 
The full precision model would require around 100gb of VRAM, that's how big the model is- You can check that out on their website directly.
Now for the GPU renting... why not Vast.AI? Tho it really depends on what you are going to use it for."
dounykim,2024-03-10 17:43:30+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@pandora-s  Hello. Thanks for the comment. I am trying to test out its performance by giving it some prompts and see the response. I checked the Vast.AI but can you recommend which one to use? Do I have to check if Per-GPU RAM is 100 GB?
YaTharThShaRma999,2024-03-09 22:11:07+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@cleverest yeah just search mixtral8x7b exl2 and you should get a lot. Find something thats below 4 bpw and it will fit. If it still somehow doesnt fit, try using 8bitcache or even 4 bit cache."
Smayt,2024-03-14 09:19:44+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Get yourself the LM studio i found it the easiest way
minhphan-qbe,2024-03-05 19:47:15+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/652e65cc17096ceb6bc351b6/dcwOUMQHWj6pE1onQNmY5.png)
"
SoheylM,2024-03-01 09:17:27+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi there,
I think you will not be able to run Mixtral unquantized with your current setup. The weights of the model alone are about 95GB, to which you need to add the cuda graphs (or maybe not if you enforce-eager). Extra VRAM will definitely be needed for the 32k token context length. You can in total think of 120GB/140GB of VRAM needed for the unquantized version of Mixtral, so 2xNvidia A100 80GB needed. If you want to try the quantized version, take a look at one of the awq quantization for vLLM use:
https://huggingface.co/TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-AWQ


and run it as 
python3 -m vllm.entrypoints.openai.api_server --model TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-AWQ --quantization awq --tensor-parallel-size 2 --host 0.0.0.0 "
zebfreeman,2024-03-01 13:49:20+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Thank You @SoheylM , That is what I am currently running as an Alternative. I just wanted to be able to get the fulll version running. "
AsmaAsma,2024-03-27 00:15:07+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I have the same problem .how did you solve the issue?
quamer23,2024-03-27 01:32:20+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@AsmaAsma Have not yet been able to fix this.

Used query engine, as it didn't matter much for my use case

Alternatively, you can try langchain. That should work"
LeroyDyer,2024-04-23 13:55:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I think the problem is that it is trying to tokenize via the chat template , but the chat template contains responses from human and ai ??
Bu the history is just a list , ie a single reply instead of being paired , hence this issue is langchain issue , maybe ?? 
many of my models have been actin strange lately and is dues to some changes in prompt templates ?? 
"
popflamingo,2024-05-02 00:59:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"As far as I can tell, the issue occurs because Mixtral doesn't seem to natively support the `system` message role (but only `user` and `assistant` ones), so the system instructions should probably be included in a different way, such as by prepending them to the first `user` message."
pandora-s,2024-02-24 20:45:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"The role of Apply_Chat_Template is exactly to apply the correct prompt template instruction so it works. So yeah, you dont need to use it if you are using the chat template, it handles it for u."
supercharge19,2024-02-26 11:17:46+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@pandora-s  what about quantized versions of the model, does a quantized version already include correct format or do I have to use <s >[INST] I put a space after s so that line is not striked through </ s> [/INST] ?"
parvezkhan,2024-03-05 07:29:27+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"While using torch.int8 getting below error:

```
ValueError: Can't instantiate MixtralForCausalLM model under dtype=torch.int8 since it is not a floating point dtype
```
"
ybelkada,2024-03-05 07:48:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@parvezkhan 
to load the model in int8 precision you need to pass a `BitsAndBytesConfig`. Please check out the relevant documentation section about it: https://huggingface.co/docs/transformers/quantization#bitsandbytes"
parvezkhan,2024-03-05 11:14:28+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@ybelkada thanks for the inputs. I think it make lot of sense to pass ```BitsAndBytesConfig```. I am running and issue with bitandbytes 

```
RuntimeError: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):

        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues
```

Below is my Setup:

GPU: NVIDIA A100
Python: Python3.10
Cuda: 12.3


Based on these issues: (https://github.com/TimDettmers/bitsandbytes/issues/1022 ,   https://github.com/TimDettmers/bitsandbytes/issues/956  ) It doesnt support cuda 12.3 unless I am missing something :)

"
anubhav10mishra,2024-02-28 14:10:21+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Hello all,
> 
> I am a professional AI engineer. I am using the mentioned LLM model on jumpstart and I can produce the responses in average 5 seconds even after enabling all 8 GPUS provided by  ""ml.g5.48xlarge"". My requirement is not further reduce the response time e.g. less than a second. 
> 
> For this purpose I planned to deploy  mistralai/Mixtral-8x7B-Instruct-v0.1 using inference.py file and mentioning device name as CUDA and using same ""ml.g5.48xlarge"" Ec2 instance to deploy on aws. I am using Sagemaker to write all the code.
> 
> Below is the error I am getting:
> 2024-02-14 T06:26:16,524 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 22.20 GiB total capacity; 1.88 GiB already allocated; 115.12 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
> 
> Below are some options I tried :
> 1. setting  PYTORCH_CUDA_ALLOC_CONF using max_split_size_mb
>    e.g. os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:24""
>    tried max_split_size_mb = 64,128,512,1024
> 2. setting PYTORCH_CUDA_ALLOC_CONF as all type of memory management technique e.g.  ""heuristic""
> as suggested in below blog
>  https://iamholumeedey007.medium.com/memory-management-using-pytorch-cuda-alloc-conf-dabe7adec130
> 3. Using torch.cuda.empty_cache()  in the inference script
> https://community.databricks.com/t5/machine-learning/torch-cuda-outofmemoryerror-cuda-out-of-memory/td-p/9651
> 
> Any kind of help or references are really appreciated. Looking forward to it. Thanks

Same problem for me ..."
Kyleiwaniec,2024-03-25 16:03:00+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I am having the same issue. I wonder what the setup is in the Inference API on this website. It seems very fast in comparison.
ArthurZ,2024-02-20 13:49:35+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,[Did this link](https://huggingface.co/chat/settings/assistants/new) not help to create one? 🤗
Dommos,2024-02-20 14:46:40+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I precisely want to configure the HuggingsChat assistant on my WordPress blog, but I don't know how to do it. The assistant with Mixtral-8x7B is really very good."
kmukeshreddy,2024-02-16 19:30:17+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ybelkada 
ybelkada,2024-02-19 07:13:01+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"hi @kmukeshreddy 

Thanks for the issue! For me, it is unclear to me what is the motivation behind this. When you load the model across multiple GPUs through `device_map=""auto""`, instead of having one replica of the model on each GPU, your model will be sharded across all these GPUs. E.g. the first layer will be loaded on GPU:0, the second on GPU:1 and so on. To perform text generation with such a model you need to make sure your input is on the same device as the first layers of the model, hence the `inputs = inputs.to(""cuda"")` and placing it on `cuda:0`, and the computation will be done sequentially, meaning while a GPU is being used, all other GPUs are kept idle.

If you want to parallelize the text generation procedure by let's say loading one copy of the model per GPU, you can pass `device_map={"""": PartialState().process_index}` (after importing `PartialState` from `accelerate`), that way the model will be entirely loaded on the device `PartialState().process_index` which should correspond to the index of the current GPU. After that you just need to set your input to that device 

```python
device_index = PartialState().process_index
inputs = inputs.to(device_index)
```

However I doubt Mixtral will fit on a single GPU unless you use the 2bit version of the model, e.g.: https://huggingface.co/BlackSamorez/Mixtral-8x7b-AQLM-2Bit-1x16-hf-test-dispatch from @BlackSamorez (you need to pip install aqlm and install transformers from source until the next release)"
kmukeshreddy,2024-02-26 18:07:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @ybelkada , Apologies for delayed response.

I will rephrase the question, the concept of GPU allocation in the above context was assumed wrong. Apologies for confusion.

## Updated question:

I have 4 GPU's, Each GPU has ~ 40 GB:
GPU 0: 40 GB
GPU 1: 40 GB
GPU 2: 40 GB
GPU 3: 40 GB

As per the hugging face blog for loading large models here: https://huggingface.co/docs/accelerate/en/concept_guides/big_model_inference

>""balanced_low_0"" evenly splits the model on all GPUs except the first one, and only puts on GPU 0 what does not fit on the others. This option is great when you need to use GPU 0 for some processing of the outputs, like when using the generate function for Transformers models 

Using balanced_low_0 for text-generation, I loaded the Mixtral to the onto all GPU's except the GPU: 0. [i.e. GPU: 0 is saved for model.generate() function]

`model_id = ""mistralai/Mixtral-8x7B-Instruct-v0.1""`
`tokenizer = AutoTokenizer.from_pretrained(model_id)`
`model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""balanced_low_0"")`

### GPU Utilization:

![image.png](https://cdn-uploads.huggingface.co/production/uploads/634b5559dcf125e4da00ce4a/ERejAHyyY7F50MjrWFmB1.png)

Here we can see, Complete GPU: 0 and Partial GPU: 3 has some memory left.

Now, when i feed a long text input to model generate less than 32k tokens. The CUDA Memory error appears, but this memory error only says there is no enough space on GPU: 0.

`inputs = tokenizer(long_text_message, return_tensors=""pt"").to(""cuda"")`
`outputs = model.generate(inputs, max_new_tokens=20) # CUDA error from this part`

### CUDA Error:
`OutOfMemoryError: CUDA out of memory. Tried to allocate 22.80 GiB (GPU 0; 39.59 GiB total capacity; 28.81 GiB already allocated; 6.46 GiB free; 31.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CON`

###  My expectation is to utilize the complete GPU: 0 & Partial GPU: 4 for model.generate to process long texts.

Could you please let me know how can I utilize the GPU: 4 on top of GPU: 0 for model.generate()? (or utilize all the remaining GPU resources for model.generate()?)


"
ybelkada,2024-02-27 02:23:37+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @kmukeshreddy 
Hmm interesting I see. My guess here is that the text you're passing it so large that the hidden states computed on the first GPU exceeds 40GB. Can you try again by reducing the size of the model by loading it in half-precision or 8-bit / 4-bit precision?"
kmukeshreddy,2024-02-27 14:58:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @ybelkada , Thank you for the follow up comment. 
The issue with the quantized version is, there is a significant performance drop for my task. So, I was looking for the work-around to use the model as-is."
ybelkada,2024-02-16 07:31:47+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Indeed, mixtral is a quite large model. The model has around ~45B parameters meaning that you need around 90GB (or more) just to store the model weights. for inference, you'll need slightly more GPU memory. 
To run this model locally you need to use the quantized versions of the model. 
Here you can see a rough estimate of the amount of needed RAM: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/77#659910160c972f4c7789542d - now you can also run the model in 2-bit precision in a free tier google colab instance: https://colab.research.google.com/drive/1-xZmBRXT5Fm3Ghn4Mwa2KRypORXb855X?usp=sharing see for reference: https://huggingface.co/posts/ybelkada/434200761252287"
dounykim,2024-02-18 05:53:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ybelkada Thanks for the comment. I checked the links you provided and it says 32 GB of RAM is needed for quantized int4. So I guess it is difficult to run it in locally. Maybe running it in 2-bit precision is the only option right? But what is the difference between just running the model and 2-bit precision?
pandora-s,2024-02-18 07:57:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Performance will suffer a huge impact mostly, u are reducing the weights precision after all. To make it short, Quantization reduces considerably the weights precision -> reducing memory necessary -> reducing ram necessary overall -> faster and easier to run on less powerfull hardware. But has a drawback of reducing performance overall the lower the precision. But most of the time it's better to take a quantized version of a huge model than a normal model of the same file size, that's why it's still very popular anyways !"
dounykim,2024-02-18 13:05:21+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@pandora-s Thank you for the simple explanation. I have another question. How do people usually run these huge model without using quantization? Buy a server with high GPU and RAM?
pandora-s,2024-02-18 13:58:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"That's right, there is little choice to be made, Mixtral for example needs 2 GPUs if you want to run it as it is. "
dounykim,2024-02-18 14:31:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@pandora-s 2 I am sorry for keep asking. So you mean running a Mixtral 8x7B model must require 2 GPUs rather than just one really expensive GPU? Then I guess I should just use 2 bit precision. Is there any research comparing accuracy or benchmarks of base model and different quanitizations?
pandora-s,2024-02-18 15:43:56+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@dounykim that's right ! You need an absurd amount of 100GB of VRAM, even an A100 lacks with it's 80GB of ram !! However, some Q versions can be run on CPU (GGUF Quantized models) and then share some layers with the GPU, you can check on sites such as https://anakin.ai/blog/how-to-run-mixtral-8x7b-locally/ the results you can expect, here is a screenshot of one of their comparisons:
![image.png](https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/Bxx4Ff0szH-KSpVm8p585.png)
"
dounykim,2024-02-19 00:01:17+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@pandora-s  Thank you again for the link. I checked the link and seems like it was ran on 64GB of RAM and RTX 4090. As far as I know, RTX 4090 has 24 VRAM. But you said 100 VRAM is needed?"
pandora-s,2024-02-19 06:38:44+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Let me try to explain again:

GGUF Models can be run on a CPU instead of a GPU, using the RAM instead of VRAM, but you can also share some layers of the model with the GPU using VRAM ! To make it simple, a GGUF version of the model can be run simultaneously by the CPU & GPU, if you do the math, 64+24= 88 Gb of RAM+VRAM, not exactly 100 but it was just an average, and they tested up to Q8 anyways not F16, you get the idea."
pandora-s,2024-02-19 06:43:00+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"If you want to run the full model on GPU, you will need ridiculously powerfull GPUs, but GGUF Quantized models are both smaller, more efficient, and can be run on CPU&GPU. "
ybelkada,2024-02-19 07:02:38+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Thank you very much @pandora-s @dounykim for this insightful discussion! Thanks also for sharing that blogpost ! 
dounykim,2024-02-19 08:56:31+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@pandora-s  Oh now I understand.. Thanks for the kind explanation ! I guess it is pretty necessary to utilize CPU.
pandora-s,2024-02-19 10:36:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Well, not necessarily. If you have a powerful GPU, you do not necessarily need to opt for GGUF models. However, for larger models like Mixtral, in my opinion, most of us may not have the budget (or it may not be worth it) to invest in a powerful GPU. You can use a quantized version of the model entirely on the GPU as well. It's just that, specifically for this model, if you aim for minimal quantization, going for a GGUF model and utilizing both the CPU and GPU is the best approach in my point of view. However, I do not know your situation, budget, or specific requirements.

We have mainly discussed Mixtral, but there are also smaller models like Mistral.

You have different approaches:
- Run it fully on a GPU -> Full model (or Q/smaller versions). (very fast but expensive)
- Run it fully on CPU -> GGUF version (or GGUF Q/smaller versions). (extremely slow but cheaper)
- Run it on both CPU & GPU -> GGUF Version (or GGUF Q versions). (a good balance)

I mostly discussed GGUF models, but there are also other technologies you might want to explore, such as GPTQ and AWS. GPTQ is more focused on GPU, for example.

I cannot decisively determine what's better for you, but I wanted to explain GGUF as it's a very good approach that allows you to run models with most hardware !

In any case, I'm glad I was of help !"
pandora-s,2024-02-19 10:39:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I recommend you to check the models of our beloved TheBloke here: https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF 
He also gives some comparisons between models !"
dounykim,2024-02-19 20:24:47+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@pandora-s Your comments really helped me a lot ! I really appreciate it. My MacBook Pro is M3, 32 RAM and my desktop is equipped with 4070 ti, 32 RAM, i5-13500k. I can try GGUF with my desktop but as far as I know, I can't run GGUF models on Mac becasue MacOS does not support bitsandbytes ?"
pandora-s,2024-02-19 22:58:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I'am not sure myself, but GGUF should work I believe? Do not quote me on that tho, Mac is out of my expertise, but BNB shouldn't be required for GGUF models I think... we do use it often to train models tho... I am not sure if there should be a compatibility issue. Well the easiest way is just to try and run a small GGUF model, if it works then all of them should if you have ram/vram enough ! 

Sorry, I'm glad I could help but Mac is out of my league-"
cekal,2024-02-15 05:59:02+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Insert the following into your response config:

do_sample=False"
FashionStash,2024-02-22 19:52:03+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Very interesting @cekal, I will certainly need that settings later in the future!"
g-ronimo,2024-02-12 14:51:00+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"start with a small model first. a 7b model like llama or mistral or even tinyllama. there's many tutorials on fine-tuning these models, start with ""finetune llama"" on google and start from there. "
FashionStash,2024-02-22 19:55:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Perhaps I will have that need in future. It will not working on a 8x7B model such the one in that current page where I answer to that post?
pandora-s,2024-02-12 13:21:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi,
First question and Third: It's less about Disk Space and more about Ram (usually it's almost the same quantity), MistralAI states on https://docs.mistral.ai/models/ that Mixtral requires 100GO of VRAM (GPU) for exemple. However, this can be reduced through the use of `quantized models`instead of the original (will come back at it on a second).
Second: To run the original model you will require 2 GPU's, as the VRAM necessary is an insane amount (can be reduced with quantization).
Fourth: I don't think so, you can however fine tune it or use a fine tuned version for a specific task you want it to do.
For your last questions: MistralAI itself as an API available on their website that you can use, and many other websites have also endpoints you can use !

Now about quantization, there is a well respected user on hugging face named TheBloke that shares quantized versions of popular models.
Here: https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF, for example, there are GGUF versions that you can actually run with a powerfull CPU and some of them with way less ram (the smaller one required 18.14 GB compared to the 100 of the original one...), so it really depends on what you need and your budget honnestly !

Note: The smaller the quantized version, the less accurate and good the results will be.

I hope I was of help !"
mariakatosvich,2024-02-09 05:55:25+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,
pandora-s,2024-02-08 07:25:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi ! Well, first of all: You are not using the prompt format. And lastly, in this use case scenario you might want to set the parameter `return_full_text: False`.
Try this: 

```
""[INST] You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. Do not give the context again in your response\nGenerate a concise and informative answer in less than 100 words for the given question\nSEARCH RESULT 1: The title is Rush, year is 2013, budget is 500000, earning is 300000, genere is action.\nQUESTION: What is the release date of rush?\n[/INST]""
```

Or, if you want the entire code:

```
import requests

API_URL = ""https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1""
headers = {""Authorization"": ""Bearer ####""}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output = query({
	""inputs"": ""[INST] You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. Do not give the context again in your response\nGenerate a concise and informative answer in less than 100 words for the given question\nSEARCH RESULT 1: The title is Rush, year is 2013, budget is 500000, earning is 300000, genere is action.\nQUESTION: What is the release date of rush?\n[/INST]"",
	""parameters"": {
		""return_full_text"": False
    }
})

print(output)
```

This should do !"
rkhapre,2024-02-11 15:48:57+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi, 
Thanks for the above information its useful. But i am applying prompt before my search result, like this below
I am closing the [/INST] before the SEARCH RESULT 1

With this i am getting ""ANSWER : ......................""

How can i avoid this?

[INST] You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. Do not give the context again in your response\nGenerate a concise and informative answer in less than 100 words for the given question[/INST]\nSEARCH RESULT 1: The title is Rush, year is 2013, budget is 500000, earning is 300000, genere is action.\nQUESTION: What is the release date of rush?\n

I am getting this below and i do not want ""ANSWER:

[
    {
        ""generated_text"": ""ANSWER: The movie Rush was released in 2""
    }
]"
pandora-s,2024-02-11 16:14:39+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"This is really confusing, why are you putting the search result after the [/INST] ? [/INST] it's a signal to the model to generate an answer, it's as if you were doing something like:

```
User: You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. Do not give the context again in your response\nGenerate a concise and informative answer in less than 100 words for the given question
Bot: SEARCH RESULT 1: The title is Rush, year is 2013, budget is 500000, earning is 300000, genere is action.\nQUESTION: What is the release date of rush?\n
```

The model is completing the text, but you are not really supposed to use it like this, try like the example I gave you before:

```
[INST] You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. \nGenerate a concise and informative answer in less than 100 words for the given question.\nCONTEXT: The title is Rush, year is 2013, budget is 500000, earning is 300000, genre is action.\nQUESTION: What is the release date of rush?[/INST]
```

This should work better I believe ! Tell me what you think.

PS: If it cuts the answer and does not give you the entire thing, try using the parameter ""max_new_tokens"" with a high value like:
```
    ""parameters"": {
        ""return_full_text"": False,
        ""max_new_tokens"": 256
    }
```
"
rkhapre,2024-02-19 15:23:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @pandora-s .
Thanks for above responses, i am facing another problem like below , where it is generating roles like ASSISTANT , ASSISTANT

if i am keeping my previous 5  interaction prompts and responses as a context to this LLM, it generates ASSISTANT multiple times like below is a example, how we can fix this?

This is the below response

ASSISTANT: ASSISTANT: The customer name for Order ID CA-2017-145233 is Dori Sori. The product names are GE ABCD, EFGH, and XYZA. The order status is Processing for the first two products and Delivered for the last one. 

Thanks"
pandora-s,2024-02-19 15:36:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I would need more information and the code to be sure about what the problem is, but my guess is that, because now we are dealing with followed up questions and an entire chat, that you need to use the full prompt template. It being the following if we keep the previous example:
```
<s> [INST] You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. \nGenerate a concise and informative answer in less than 100 words for the given question.\nCONTEXT: The title is Rush, year is 2013, budget is 500000, earning is 300000, genre is action.\nQUESTION: What is the release date of rush?[/INST] RESPONSE </s> [INST] NEW QUESTION OR WHATEVER [/INST]
```
So as you can guess, at the beginning of the prompt you need to add `<s>`, and every time it answers you, you must add `</s>` at the end !

As said, I cannot be sure this is the problem as I do not have the full code, but that's my guess !

Glad to be helpfull !!"
rkhapre,2024-02-20 07:43:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This comment has been hidden
rkhapre,2024-02-20 07:44:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"We are giving the input to LLM in this way

`<s> [INST] You are a helpful chatbot assistant which provides answer based on the context given. Do not give any extra information. Do not give the context again in your response\nGenerate a concise and informative answer in less than 100 words for the given question\nSEARCH RESULT 1: The title is Rush, year is 2013, budget is 500000, earning is 300000, genere is action.\nQUESTION: What is the release date of rush?[/INST]`

With the above format i am getting this answer

""ANSWER: The movie Rush was released in 2013.""

Only difference is instead of saying Context, we have SEARCH RESULT 1 coming from Vector DB"
rkhapre,2024-02-20 07:44:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"not sure why everything crossed with a line. Sorry for that, i tried to re-comment it"
pandora-s,2024-02-20 07:53:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"My main question is how you handle multiple interactions with the bot, you keep track of previous conversations? If yes, be sure to follow the format like the example I gave.
If you do not like the fact it answers with ""ANSWER"" then you can give it an example or rewrite the prompt without the ""QUESTION:"", as it's most likely the trigger keyword creating the ""ANSWER:"" one !"
pengzhang,2024-03-23 01:17:48+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,how this 4032*32 =131k is calcuated?
CamiloMM,2024-02-10 17:14:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Did you post this while intoxicated?
ArthurZ,2024-02-05 06:19:55+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,?
sniffski,2024-02-03 05:53:27+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Yeah, king of struggling with making the LLaMa response... It seems it doesn't know where to stop and starts a conversation with itself... Started to wonder if I'm not passing the right chat_template... Would be nice if we have an example with proper input and expected output... And maybe proper output parsing function. 🫣"
YaTharThShaRma999,2024-02-01 15:25:20+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@AnzaniAI you might have different sampling parameters like higher temp or top p. Make sure that’s the same as before
ArthurZ,2024-02-02 09:30:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Would recommend you to upgrade your transformers version as well ! 
gzguevara,2024-02-03 15:15:46+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@AnzaniAI why do you use mistralai/Mixtral-8x7B-Instruct-v0.1 for NER? Why not mistralai/Mixtral-8x7B-v0.1?
"
AnzaniAI,2024-02-13 05:29:16+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@gzguevara 
If I use the mistralai/Mixtral-8x7B-v0.1 then it throw follwing error
ValueError: 
                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
                        `device_map` to `from_pretrained`. Check
                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-g
![Screenshot (115).png](https://cdn-uploads.huggingface.co/production/uploads/65719aed127b56d199bdbbb2/EfC31Li1Z23G_UkJyApX0.png)
pu
                        for more details."
AnzaniAI,2024-02-13 05:30:38+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@ArthurZ 
transformer upgrading throws following error

ValueError: 
                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
                        `device_map` to `from_pretrained`. Check
                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu
                        for more details.
"
AnzaniAI,2024-02-13 05:31:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@YaTharThShaRma999 
I am using lowest temperature=0.01. Still persisting that ouput
"
AnzaniAI,2024-02-13 10:45:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi communities,
The model is actually giving the correct output but it carries the question and context in the output that I have passed in llm_chain.run
following is the code block
for i in tqdm(data.index):

        template = data[""system_prompt""][i]
        context = data[""text""][i]
        question = data[""user_msg""][i]
        prompt = PromptTemplate(template=template, input_variables=[""question"",""context""])

        llm_chain = LLMChain(prompt=prompt,llm=llm)
        entities = {""question"":question,""context"":context}
        response = llm_chain.run(entities)"
skapadia-zalando,2024-01-31 09:41:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Related to this issue: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/114

This current chat template doesn't support the system role, so this adds support for it

Tested over here: https://colab.research.google.com/drive/1egzjeVDcuaQ6QaEelcW5UxnnRR48A1aP?usp=sharing"
skapadia-zalando,2024-01-31 09:47:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Requesting review from @teven since you've worked on chat templates before. Appreciate Mistral's work so far and I would be thankful if you had a look!
skapadia-zalando,2024-02-07 08:16:07+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I found this on an older version of the docs site: https://web.archive.org/web/20231030013339/https://docs.mistral.ai/usage/guardrailing/#appendix, seems like the `<<SYS>>` tokens are not supported"
ArthurZ,2024-02-02 09:31:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,#115 should fix this if merged
sam-kap,2024-02-03 09:43:56+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ArthurZ Yup #115 is me from my work account :D
ansh146572,2024-02-05 04:19:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@sam-kap and @ArthurZ  even if you have corrected the template but will the model be able to interpret that, whatever is written inside SYS tags  needs to be considered a part of system prompt?"
sam-kap,2024-02-06 21:07:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I would assume so, since Mistral uses the Llama template and supports system prompts in their API calls"
ycros,2024-02-06 22:10:14+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I've seen mention that it doesn't follow llama chat format, and is instead `[INST] system prompt [/INST]</s> ` - but it would be nice to get actual clarification from mistral on this point."
sam-kap,2024-02-07 08:15:13+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I found this on an older version of the docs site: https://web.archive.org/web/20231030013339/https://docs.mistral.ai/usage/guardrailing/#appendix, seems like the <<SYS>> tokens are not supported"
ArthurZ,2024-01-31 03:08:30+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,You can save it in the config: ` model.config.output_router_logits = True` 
ybelkada,2024-01-31 03:02:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @bweinstein123 
Please see my comment here: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/36#65b8d5cf23d948d884d19645 to understand how to run multi-GPU inference"
neph1,2024-01-25 11:39:04+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Running is one thing, training is another. I don't think training is feasible (or even possible with RAM), but please correct me if I'm wrong. You can use Google Colab or some other cloud run service for finetuning. There are loads of scripts to get up and running. It's even possible with the free instance, if the dataset is small and the context is short."
NileshInfer,2024-01-31 00:34:01+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Not sure on PC, but ran some experiments on running a quantized version on cloud, you can check the repo here - https://huggingface.co/Inferless/Mixtral-8x7B-v0.1-int8-GPTQ"
YaTharThShaRma999,2024-02-01 15:30:07+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@edw-hug-face your probably thinking about mistral not mixtral(which this is).
Mixtral is 46b parameters not 7b
Mistral is very good for its size as wel but obviously worse then mixtral
Mixtral is absolutely not running on a normal phone and it will take like 24gb ram/vram.

Training with qlora(the lowest ram taking training) takes 2x vram so roughly 48gb ram/vram.

Mistral can run in just 4gb vram/ram and be trained in like 12gb ram/vram so yeah that’s possible.

However cpu is horribly slow(takes a day for just like a few hundred q and a)

So I reccomend you use colab since it has a 15gb vram gpu which will massively increase speed."
neph1,2024-02-01 17:36:38+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Actually, with https://github.com/unslothai/unsloth, you can get quite a bit under 12GB, maybe down to 8-9GB for a 7b."
MaziyarPanahi,2024-01-24 11:33:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Have you tried to look into the prompt template? In my experience, Mistral and Mixtral models are super sensitive with those symbols, one extra space and you are on a very wrong path in the response! You can use HuggingChat UI to see the actual prompt that goes into the LLM, so when you see a good response just click on that download button in the user's question:

![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/TA0p7_5Hs2Uszt7W4iZgV.png)

That will show you exactly how the prompt was constructed (in addition to generation configs)
```
{
  ""note"": ""This is a preview of the prompt that will be sent to the model when retrying the message. It may differ from what was sent in the past if the parameters have been updated since"",
  ""prompt"": ""<s> [INST]You are a helpful assistant.\n This is a test, just say test back! [/INST]"",
  ""model"": ""mistralai/Mixtral-8x7B-Instruct-v0.1"",
  ""parameters"": {
    ""temperature"": 0.6,
    ""truncate"": 24576,
    ""max_new_tokens"": 8192,
    ""stop"": [
      ""</s>""
    ],
    ""top_p"": 0.95,
    ""top_k"": 50,
    ""repetition_penalty"": 1.2,
    ""stop_sequences"": [
      ""</s>""
    ],
    ""return_full_text"": false
  }
}
```
"
kristjan,2024-01-31 12:59:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@MaziyarPanahi thank you for these instructions. Where do I see this download button? I went to HuggingChat and started a [chat](https://hf.co/chat/r/OYOlaBG) with mistralai/Mixtral-8x7B-Instruct-v0.1 but could not find it.
I was able to find some templates in the source code [here](https://github.com/huggingface/chat-ui/blob/main/.env.template#L9) and [here](https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/PROMPTS.md#mixtral)"
MaziyarPanahi,2024-02-01 10:08:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"You are welcome @kristjan 
On HuggingFace/chat, when you ask a question you can hover over the question and these icons will appear:

![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/I06lzSXw7Eii2CSFLYlF3.png)

It is right next to the re-generate icon/button"
kerea,2024-02-08 12:59:16+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Your lines (below) seem to refer to Mistal 7B, not Mixtral 8x7B?

model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")"
MaziyarPanahi,2024-02-08 18:21:18+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"HuggingChat does also offer the 7B Mistral model. Pretty solid model, I really like it. (also sensitive to prompt template, one extra space before EOS and the whole thing changes)"
kerea,2024-02-16 14:43:46+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Yes, but if the goal is to reproduce HuggingChat's Mixtral 8x7B results, probably can't do that straightforwardly when calling Mistral 7B instead of Mixtral 8x-7B ^_^'"
MaziyarPanahi,2024-02-17 11:36:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"You can switch models in HuggingChat. Choose whichever model you want to test on HuggingChat, if you see they perform better hosted on HuggingChat, you can use this feature to see the exact prompt template and parameters to reproduce locally. (I was just making an example, you should adjust the model's name accordingly) - this should answer the question ""why a model performs better on HuggingChat than my local ENV""/

"
pandora-s,2024-02-17 15:04:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I think there is a misunderstanding @MaziyarPanahi , Kerea saw a brutal error on the original post that instantly explains why it's not working, the main reason the results are different are first of all caused by this line: `AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")`, it's obvious that the author of the post will not have the same results as Mixtral if they use Mistral ! Of course other parameters will have an impact, but it's kinda useless if they are using the wrong model all together."
MaziyarPanahi,2024-02-17 16:32:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> I think there is a misunderstanding @MaziyarPanahi , Kerea saw a brutal error on the original post that instantly explains why it's not working, the main reason the results are different are first of all caused by this line: `AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")`, it's obvious that the author of the post will not have the same results as Mixtral if they use Mistral ! Of course other parameters will have an impact, but it's kinda useless if they are using the wrong model all together.

I understand now and thanks for the clarification :) "
ybelkada,2024-01-23 14:23:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @jjplane 
Can you try to set a pad token as suggested by the error message?"
jjplane,2024-01-23 14:51:57+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Hi @jjplane 
> Can you try to set a pad token as suggested by the error message?

ok, tokenizer.pad_token = tokenizer.eos_token worked, and <\s> is padded on the left. thx!"
FlorianJc,2024-01-24 16:48:31+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"C'est surtout un forum dédié ou un discord français qu'il faudrait. Même si les llms aident pas mal pour la traduction, c'est un peu fatiguant à la longue !"
FashionStash,2024-02-22 20:36:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Tiens ben moi j'ai une question..

Avez-vous une idée théorique et pratique qui puisse faire en sorte que le ChatBOT (Mixtral) retienne en mémoire les discussions précédentes (par exemple moi je pense à stocker dans un fichier les anciennes discussions ou dans une option de réglage prévue par Mixtral ?) et s'en serve pour consolider les réponses futures ?

Exemple... Supposons que j'écrive en prompt : [Désormais tu t'appelle ""Mixtralys""] ...
Que je laisse le Space du ChatBOT se mettre en pause... (donc reset automatique du conteneur Docker)
Et que.. lundi, je lui pose la question suivante, via un prompt : [Tu t'appelle comment ?]

Et que là, comme elle aurait retenu mon ancienne discussion, elle rétorquerait alors en toutes logique ceci : [Mixtralys.]

C'est faisable ça, sans ré-entraîner le modèle ?

Si oui, vous auriez des pistes pour insuffler en théorie cette capacité au ChatBOT (et cela, je le rappelle, sans ré-entraîner le modèle Mixtral), merci d'avance de votre aide !

Si je demande, c'est surtout pour assurer une sorte de continuité des discussions, de créer un suivi de discussions pour le ChatBOT (je ne parle pas d'agents OpenAI). J'ai remarqué que le Modèle finissait parfois sur une erreur de ""Model is overload"", donc, il faudrait que ce soit une capacité de stockage de discussions capables de se suivre même en cas de reset du Space (même après un Factory Rebuild (Exemple Import du suivi de discussions (csv, txt), le tout sans surcharge du modèle !"
FlorianJc,2024-02-26 00:14:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Tu utilises une interface qui n'est là qu'à des fins de démonstration.

Pour qu'il y ait une mémoire, il faut implémenter un système de stockage et de compression des instructions, donc il faudrait utiliser un client compatible avec l'API OpenAI et te connecter aux serveurs de Mistral AI (par exemple).
Une autre méthode (pour palier la limite de la taille du contexte) est d'utiliser une base de donnée utilisant des ""embeddings"" pour piocher des informations et les insérer dynamiquement dans les instructions.

Aucune de ces fonctionnalités ne fait parti intégrante du modèle, mais n'est pas non plus implémenté dans l'interface que tu utilises sur HuggingFace.

Je connais pas trop les logiciels qui implémentent ça (à part le miens, mais il n'est pas disponible publiquement).

Le truc qui s'en rapproche le plus que je connaisse est gpt4all.
Il ne permet pas vraiment d'utiliser des serveurs arbitraires sauf via une branche que j'ai créée il y a plusieurs mois:
https://github.com/nomic-ai/gpt4all/pull/1692

Les modifications ont approuvées mais tardent à être ajoutées à la branche principale.

Reste que dans tous les cas il te faut avoir accès à un serveur."
evann,2024-01-19 13:48:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,cr
Allenxy96,2024-03-26 03:39:25+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi, I encountered the same issue, may I know did you manage to find the solution for this error?"
jdjayakaran,2024-01-16 10:30:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Successfully installed huggingface-hub-0.20.2 tokenizers-0.15.0 transformers-4.36.2
pandora-s,2024-01-16 10:33:51+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Could you give us the code?
jdjayakaran,2024-01-16 10:39:01+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Just running the instructions as given i get the error. I also tried the 4 bit quantization

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = ""mistralai/Mixtral-8x7B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)

text = ""Hello my name is""
inputs = tokenizer(text, return_tensors=""pt"").to(0)

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
"
pandora-s,2024-01-16 10:42:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Usually this is caused by the transformers not being updated, if you installed the recent version then reload the kernel if you are using Jupyter or some kind of Notebook (Restart)"
jdjayakaran,2024-01-16 10:46:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Perfect.. will try 

Thanks for the immediate response"
pandora-s,2024-01-16 10:47:30+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Just trying to help people out ! ;)
bindup,2024-02-01 07:30:52+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @jdjayakaran ,
Could you please let me know if the fix suggested by @GreyForever worked for you. Because I am facing the similar issue as well

Feb 1, 2024 12:57:32 PM INFO: Traceback (most recent call last):
Feb 1, 2024 12:57:32 PM INFO:   File ""script"", line 103, in rm_main
Feb 1, 2024 12:57:32 PM INFO:     model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=data_type, revision=revision)
Feb 1, 2024 12:57:32 PM INFO:   File ""/opt/anaconda3/envs/rm_genai2/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py"", line 526, in from_pretrained
Feb 1, 2024 12:57:32 PM INFO:     config, kwargs = AutoConfig.from_pretrained(
Feb 1, 2024 12:57:32 PM INFO:   File ""/opt/anaconda3/envs/rm_genai2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py"", line 1064, in from_pretrained
Feb 1, 2024 12:57:32 PM INFO:     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
Feb 1, 2024 12:57:32 PM INFO:   File ""/opt/anaconda3/envs/rm_genai2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py"", line 761, in __getitem__
Feb 1, 2024 12:57:32 PM INFO:     raise KeyError(key)
Feb 1, 2024 12:57:32 PM INFO: 
Feb 1, 2024 12:57:32 PM INFO: KeyError: 'mixtral' (script, line 103)"
ybelkada,2024-02-02 01:55:27+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,`pip install -U transformers` should fix the issue
autoscrape123tx,2024-01-19 21:39:15+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Contact me on : www freelancer com/u/AutoScrape123TX
zatokazatoka,2024-01-15 23:13:00+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,ollama + web ui (easy to run in docker)..
deleted,2024-01-15 23:45:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"i use ooba's text gen as a server. 

its setup to run as a service under Linux.  Just pass parameters to it to be accessible on your network."
icemaro,2024-01-16 07:58:03+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I want to run the Mistral model 
zatokazatoka,2024-01-16 09:18:17+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image

https://github.com/ollama-webui/ollama-webui"
deleted,2024-01-16 12:09:55+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> I want to run the Mistral model

And you got 2.5 suggestions now for doing it."
aigeek0x0,2024-01-19 00:35:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"i have put together a simple implementation guide using Runpod (for GPU), Google Colab (for inference), and Gradio (for UI) Here:
https://github.com/aigeek0x0/radiantloom-ai/blob/main/mixtral-8x7b-instruct-v-0.1-runpod-template.md"
EddieJoeJohnson,2024-01-24 00:53:07+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I am having the identical issue. CUDA/GPU was working for me in all other situations. Only now trying Mixtral I get the same error. I have tried old-school searching for libcudart.so on my entire HD and can't find it anywhere.
"
aigeek0x0,2024-01-19 00:38:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Make sure you install the latest transformers package using git.
shankarkumarj,2024-01-24 14:20:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I get the error : ValueError: Unsupported model type mixtral
ArthurZ,2024-01-26 15:07:12+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,`pip install -U transformers` 
MohammadOthman,2024-05-02 04:27:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have a similar problem, did you manage to find a solution for OOM?"
bweinstein123,2024-01-10 11:27:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Adding the failure:

</>
from transformers import AutoTokenizer, AutoModelForCausalLM

In [3]: tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mixtral-8x7B-Instruct-v0.1"")
   ...: model = AutoModelForCausalLM.from_pretrained(""mistralai/Mixtral-8x7B-Instruct-v0.1"")
Loading checkpoint shards:  26%|█████████████████████▊                                                             | 5/19 [02:21<06:40, 28.60s/it]Killed
(pytorch_p38) ubuntu@ip-172-23-1-218:~$ ipython
Python 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.31.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: # Use a pipeline as a high-level helper
   ...: from transformers import pipeline
   ...: 
   ...: pipe = pipeline(""text-generation"", model=""mistralai/Mixtral-8x7B-Instruct-v0.1"")
Loading checkpoint shards:  26%|█████████████████████▊                                                             | 5/19 [02:11<06:00, 25.75s/it]Killed
</>"
andreniyongabo,2024-01-17 19:08:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Have you also tried loading it in half-precision by adding `torch_dtype=torch.float16` in your pipeline? Something like this `pipe = pipeline(""text-generation"", model=""mistralai/Mixtral-8x7B-Instruct-v0.1"", torch_dtype=torch.float16)`"
aigeek0x0,2024-01-19 00:41:39+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Have you considered deploying on other cloud platforms. I am using Runpod and it's working great. I have put together a guide here if you are interested: https://github.com/aigeek0x0/radiantloom-ai/blob/main/mixtral-8x7b-instruct-v-0.1-runpod-template.md
bweinstein123,2024-01-21 07:48:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@aigeek0x0 have you performed fine-tuning using one A100 80GB runpod?
aigeek0x0,2024-01-21 16:05:39+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@bweinstein123 yes, i have. you can finetune this model with 4-bit quantization on A100. Even RTX A6000 would suffice if you use smaller batch size."
mlkorra,2024-01-25 04:24:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@aigeek0x0 how was the performance of mixtral instruct after fine-tuning? Any insights which I can borrow, thanks "
aigeek0x0,2024-01-25 14:37:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@mlkorra based on vibes-check evaluations, it is looking promising. I haven't run benchmarks on it yet, but I will do so after a few more modifications."
Tejasram,2024-02-04 16:36:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Heyyy, same issue, i am trying to run it on a g5.8xlarge machine, and it gets killed RIGHT at the 26% checkpoint. Did you come across any solution?
"
deleted,2024-01-08 19:31:48+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,it all depends on how fast you want to go.
ShivanshMathur007,2024-01-08 19:36:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"like while inferencing the locally downloaded model it should have a speed of 5 tokens/sec. you can provide the details at different speed also it would be helpful
"
ShivanshMathur007,2024-01-09 07:33:51+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Also tell the minimum requirement. It would be helpful. On the Mixtral website -> 
Mixtral requires 64GB of RAM and 2 GPUs, which increases the cost by a factor of 3 (1.3$/h vs. 4.5$/h). This is mentioned can anyone elaborate on this.
"
aigeek0x0,2024-01-19 00:43:27+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,You can run it with 8-bit precision on one A100 (80GB) which costs ~$1.89/h on Runpod.
YaTharThShaRma999,2024-02-01 15:33:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"If you are using huggingface, you must move model and input ids to cuda.
Do it with model.cuda() and input_ids.cuda()"
kmukeshreddy,2024-02-25 15:05:55+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Yes, the issue was the complete model was not enough in GPU's which i have.
When i quantized the model the GPU inference is good."
castafra,2024-01-10 11:06:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hello, 
I am encountering the same issue when trying the Inference API for the small model https://docs.mistral.ai/platform/endpoints/ (which is Mixtral). Most answers seem to be of good quality but oftentimes, the answer is cut in the middle. The endpoint does not receive many different parameters and I have tested all of them, so maybe this is just a model problem ? "
pandora-s,2024-01-12 07:31:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Just to be sure, you are using the prompt format provided? "
ludomare,2024-01-12 11:03:00+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@GreyForever : what do you mean??
"
Suparious,2024-01-12 13:38:16+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Your var `map_prompt` is not using the correct prompt format, just like the others have mentioned.

There is an example for you in the README.md, it looks like this:

```
<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]
```"
Suparious,2024-01-12 13:41:13+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Also, try setting `min_tokens`, `max_tokens` and / or `max_new_tokens` depending on what API you are using - do this in addition to fixing your prompt template."
pandora-s,2024-01-12 13:44:35+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> @GreyForever : what do you mean??

As stated by Suparious, this model, the Instruct version, was trained to be able to follow instructions. But you are required to follow the prompt format like all LLMs similar to this one. If you don't then it's almost the same as using the base model (mistralai/Mixtral-8x7B-v0.1), so to have the Instruct version work properly you are required to follow the prompt format they used to train it.
"
ludomare,2024-01-12 15:10:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"thank you for your help !!.....
but in my example, map_prompt should be like:

map_prompt =  """"""[INST]
Ecrit un résumé précis en francais en 1000 mots de :
""{text}""
Résumé précis:
[/INST]""""""
map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[""text""])

because  it doesn't work, the output is always truncated 

output:
* Giuseppe Rensi, un philosophe italien de Vérone, a écrit ""Contre le travail"" en 1923, où il argue que les humains ont une relation ambivalente avec le travail, le considérant à la fois nécessaire et aliénant.
* Simone Weil, dans son essai ""Le travail et la culture"", explore le paradoxe moral entourant le travail, qui est considéré comme une"
castafra,2024-01-12 15:12:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I, on my side, am using the Python client (https://github.com/mistralai/client-python) to call the API endpoint provided by Mistral (not free). 
Here is an exemple of how I send my prompt : 
```

def format_prompt(prompt: List[Dict[str, str]]) -> str:
        """"""Format input prompt to the Mistral API format
        """"""
        prompt = [
            ChatMessage(role=item['role'], content=item['content']) for item in prompt
        ]
        return prompt

def call_inference_endpoint(client: MistralClient, prompt: List[ChatMessage], **kwargs):
        """"""Get inference from the Mistral API
        """"""
        inference = client.chat(
            model=""mistral-medium"",
            messages=prompt,
            temperature=kwargs[""temperature""],
            max_tokens=kwargs[""max_tokens""],
            top_p=kwargs[""top_p""])

        return inference

client = MistralClient(
            api_key=""MY_API_KEY"")
prompt = [{""role"": ""system"", ""content"": ""Tu es un assistant IA""}, {
        ""role"": ""user"", ""content"": ""Quelles sont les dix premières décimales de pi ?""}]

result = call_inference_endpoint(client, prompt, temperature=0.2, max_tokens=100,
                                  top_p=1, frequency_penalty=1)
```

Sometimes the response is cut in the middle, especially if I ask to enumerate. Setting a greater value of `max_tokens` does not change anything."
pandora-s,2024-01-12 15:13:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Use 'max_new_tokens' instead, should solve the problem ! "
pandora-s,2024-01-12 15:16:55+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Use 'max_new_tokens' instead, should solve the problem !

Oh I'm sorry I forgot mistral does not use the same parameters hugging face uses, it wont work. Try it still.

"
pandora-s,2024-01-12 15:21:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> thank you for your help !!.....
> but in my example, map_prompt should be like:
> 
> map_prompt =  [INST]""""""
> Ecrit un résumé précis en francais en 600 mots de :
> ""{text}""
> Résumé précis:
> """"""[/INST]
> map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[""text""])
> 
> because  it doesn't work, I must do stomething stupid :(

why did u put [INST] before """""" ? inst it supposed to be: map_prompt = ""[INST] Ecrit un résumé précis en francais en 600 mots de :{text}Résumé précis: [/INST]""
I might be missing something tho..."
ludomare,2024-01-12 15:40:23+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"why did u put [INST] before """""" ? inst it supposed to be: map_prompt = ""[INST] Ecrit un résumé précis en francais en 600 mots de :{text}Résumé précis: [/INST]""
I might be missing something tho.


---->I change by         map_prompt = ""[INST] Ecrit un résumé précis en francais en 600 mots de :{text}Résumé précis: [/INST]""
still doesn't work, output truncated :("
pandora-s,2024-01-12 15:54:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Then in my opinion all comes up to the équivalent of max_new_tokens like we stated before. By default this kind of things have a quite short max length token output.  Try to find/search a parameter that allows u to overwrite this. I'm not used to this library/langchain, but it should be a parameter similar to ""max_tokens""."
pandora-s,2024-01-12 16:00:18+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"To be more exact, where u insert the temperature try to add a new parameter ""max_new_tokens"" with the value you want, try a big one like 2000.

I cannot test it right now so I'm just trying to give advice, if nothing works I will check it later."
ludomare,2024-01-12 18:29:52+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"thank you a lot for your help  !!! it works !!! ""max_new_tokens"" was the solution !!

ps: I try something to test  :  increasing the temperature makes all  crash...strange...but with temperature at 0.1 all fine !! thank you again  "
pandora-s,2024-01-12 19:28:37+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I recommend you to avoid temperature values too high or too low, values around 1 (not 1, as it becomes deterministic) is perfect.

Nevertheless I am glad to be of help ! Have fun !"
ManikandharChinni,2024-02-06 15:59:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @GreyForever ,
I am trying same but my problem is same and different.
I followed the instructions just like you mentioned in this thread, but my output has prompt text and trunctated output.
I tried many other things from langchain, but not use :(

    prompt_template = """"""[INST]
    You are an expert in generating questions from a transcript by understanding the context of conversation happening in transcript.
    Take the below transcript of a video and create 3 open ended questions and 2 multiple choice questions that are engaging and appropriate to ask a 10th grader in the US about what happened during this session:
    ----------
    [/INST]
    {text}
    [INST]
    ----------
    From above transcript of a video and create open ended questions and multiple choice questions that are engaging and appropriate to ask a 10th grader in the US about what happened during this session.
    OUTPUT:
    [/INST]
    """"""

    PROMPT_SUMMARY = PromptTemplate(template = prompt_template, input_variables=['text'])
    question_chain = LLMChain(llm = llm, prompt = PROMPT_SUMMARY,return_final_only=True, output_key=""questions"")

    stuff_chain = StuffDocumentsChain(
        llm_chain=question_chain
    )

    output = stuff_chain.invoke(docs)

Nothing is working. Can you please tell me where did i made mistake please!!"
pandora-s,2024-02-06 16:42:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi ! First of all I think you can have a better prompt format than that, the `<s>` and `</s>` make quite the difference. Also, I believe you misunderstood what the special tokens mean. [INST] is always before an instruction, [/INST] is to end an instruction. I'm also struggling to understand exactly what you want it to do for you... so here is a new example for you:

```
For a one shot:
""[INST] Take the below transcript of a video and create 3 open ended questions: {text} [/INST]""
And if you want it to always respond with a specific format it's better to give it an example before hand, like:
""<s> [INST] Take the below transcript of a video and create 3 open ended questions: SOME TRANSCRIPT HERE [/INST] THREE OPEN ENDED QUESTIONS SIMILAR TO WHAT YOU WANT IT TO ANSWER </s> [INST] Make 3 more open ended questions for this transcript: {text} [/INST]""
```

Note this is just an example, there are ways to make it work better, but I hope it will allow you to understand how the prompt format works.

If it still truncates the output, then it's mostly similar reasons like the one before you. Check all the parameters, and if you can, make max_new_tokens (or max_tokens or similar, depends what you are using, I do not know langchain by heart) to be a high value.

I hope I was of help !"
Vedarutvija,2024-02-27 18:51:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This comment has been hidden
mlkorra,2024-01-25 04:29:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @dnovak232  that seems a bit challenging, were you able to solve this issue? "
conia,2024-01-11 07:38:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"it's ok for me to use 4*A100 40G when using vllm

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png)
"
dnovak232,2024-01-13 10:26:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> it's ok for me to use 4*A100 40G when using vllm
> 
> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6412c51500c5dd0790bc749c/J-Rbfzgd9SJR_A19jxu82.png)

with 128GB plus all works OK,
looking on smallest footprint where I can run it without quantisation."
J22,2024-01-06 08:32:22+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have run this model on [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp):

* For quantized int4, 32 GB of RAM is enough;
* For quantized int8, 64 GB of RAM is enough.

I think it is impossible to run it with PyTorch on CPU, because PyTorch is not as efficient as GGML on CPU."
roboboot,2024-01-06 10:19:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"ok how can I use quantized int4?

Do I have to use ""load_in_4bit=True"" ?

thx

R"
roboboot,2024-01-08 10:05:38+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Yes, I've just used:

        self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""cpu"", low_cpu_mem_usage=True, load_in_4bit=True)
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)

And I receive and error that it's not possibile quantization without GPU. 

You are right :(

thx"
Mikkr,2024-01-04 14:03:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> hijv

"
KatyTheCutie,2024-01-04 16:15:17+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,ugh trolls.
jukofyork,2024-01-12 21:39:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> The Model card says it is important to get the prompt template correct or else the model will produce sub-optimal outputs, but which prompt template is correct? Two different ones have been given.
> 
> The one from the model card:
> ```
> <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]
> ```
> 
> The one from tokenizer_config.json:
> ```
> <s>[INST] Instruction [/INST]Model answer</s>[INST] Follow-up instruction [/INST]
> ```

I've been wondering this too, and after quite a bit of experimentation I'm fairly sure it's the first prompt format and think it's just a typo. The Mistral instruct uses the first prompt format too:

https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2

I've also found it's important to NOT leave a space after the closing [/INST] , as even though it irritatingly adds a space itself when replying, it can cause the output to come up with Chinese Unicode characters sometimes (deepseek-llm also does this is you add the space!), but for codellama it's VERY important to add the space (and I think for llama2 it should be there, although it doesn't seem to matter so much)."
lemonflourorange,2024-01-15 15:13:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@jukofyork I think the tokenizer_config.json prompt is more than just a typo, as for the rest, I've found the same.

I've tested the tokenizer_config.json prompt extensively, and I think the reason it removes spaces is because, like you mention, the model automatically adds a space between `[/INST]` and `Model answer` and it will cause problems if you add the space manually. I think the same is true for the space between `<s>` and `[INST]` where the model will automatically add it in, however, the model will not add a space between `</s>` and `[INST]` so I'm not sure why that space was removed, and I still don't know if we should leave the EOS token in or out of every bot response."
jukofyork,2024-01-15 15:35:47+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I was having a lot of trouble with the prompt templates and tried my best to find the correct ones for as many of the original/official models as possible:

https://github.com/jmorganca/ollama/issues/1977

Just one tiny mistake can make a huge difference for some of the models and it's probably not all that obvious unless you try to get them to ingest a very large amount of sourcecode or similar.

codellama is the worst effected by this and I do wonder if a lot of the benchmarks people have run were with the wrong template - it's actually a lot better than people realise if you use the correct template.

I'm also not sure about the need to use the `<s>` type tags, but I followed the official instructions to a letter (eg: llama2 and codellama say to add them, Mixtral and Minstral say to only add them to the first message, etc). 

If there's one thing we desperately need, it's some standard prompt template for future models!"
jukofyork,2024-02-13 15:41:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I spent all afternoon running different experiments and am actually shocked at how much finding the proper prompt has improved all 3 models:

It's made *Mistral* about as good as the other 2 were before, and the other 2 are now **MUCH** better; with all the weirdness (ie: where they claimed to make changes to code when they didn't etc) gone now.

I've marked the spaces with '■' so they stand out, but you will need to change them. Also remember if you aren't using Ollama or llama.cpp you might need to add back the `<s>` prefix:

---

`Mistral` and `Miqu`:
```
TEMPLATE """"""{{ if and .First .System }}[INST]■{{ .System }}

Please await further instructions and simply respond with 'Understood'.■[/INST]
Understood</s>■
{{ end }}[INST]■{{ .Prompt }}■[/INST]
{{ .Response }}""""""
```

This agrees with the example on the Mistral page:

```
text = ""<s>[INST] What is your favourite condiment? [/INST]""
""Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> ""
""[INST] Do you have mayonnaise recipes? [/INST]""
```

https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2

---
`Mixtral`:

```
TEMPLATE """"""{{ if and .First .System }}■[INST]■{{ .System }}

Please await further instructions and simply respond with 'Understood'.■[/INST]■
Understood</s>
{{ end }}■[INST]■{{ .Prompt }}■[/INST]■
{{ .Response }}""""""
```

This sort of agrees with the example on the Mixtral page:

```
<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]
```

https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1

But it seems using the newlines before the response like the Mistral example is essential."
jukofyork,2024-02-13 16:18:39+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,<fixed now: see post above>
openllm,2024-04-01 05:02:47+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Can someone provide an example of how we can add chain of thought and multiple examples in instruction.  
JiaxinTsao,2024-01-04 23:20:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same issue, any workaround?"
guowl,2024-01-08 09:19:55+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same issue, any workaround?


"
guowl,2024-01-12 09:31:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same issue, any workaround?

"
zxs1997zju,2024-01-22 05:40:38+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Any update?
"
A-Cepheus,2024-01-22 08:14:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"try
`from deepspeed.utils import set_z3_leaf_modules`
`from transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock`
` set_z3_leaf_modules(model, [MixtralSparseMoeBlock])`"
zxs1997zju,2024-01-23 03:02:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@A-Cepheus thanks for reply, the inference now is successful and loss is return,  however, the training still hang in backward, any clues?"
zxs1997zju,2024-01-23 09:17:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Any update?
JiaxinTsao,2024-01-23 13:59:13+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I believe deepspeed need all experts weight to be involved during inference so that `zero3` could correctly sync data btw gpus. If all `8` experts are enabled inside `config.json`, the problem goes away."
guowl,2024-01-25 11:45:56+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> @A-Cepheus thanks for reply, the inference now is successful and loss is return,  however, the training still hang in backward, any clues?

when i train mixtral model,  after 270 step, it will be hang . and GPU 100% until NCCL timeout"
janphilippfranken,2024-01-16 01:18:02+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"OSError: mistralai/Mixtral-8x7B-v0.1 does not appear to have a file named config.json. Checkout 'https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/None' for available files.

same problem"
hrighugging,2024-06-01 23:42:15+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Same here
sumegh,2024-01-06 14:13:21+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I use ""Method 1"" and it works fine always. Better to save adapter checkpoints which are smaller in size and merge for once with base model rather than saving entire base model checkpoints everytime.

Btw can you share a sample notebook for finetuning ? I was using this - https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing

But my training loss starts to increase after 1000 steps for some reason. Any ideas ? Running on custom dataset. Tried using both Alpaca & Mistral templates although that shouldn't matter much for finetuning i guess."
Pradeep1995,2024-01-06 15:06:14+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@sumegh try with a lower learning rate which will reduce the loss. Do you have any idea on to select the max_steps parameter?
sumegh,2024-01-06 15:41:12+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"That is if training a full epoch is not feasible for you. Else set num_train_epochs = 1. Otherwise, see total number of steps for single epoch based on batch size and then set max_steps < total steps for epoch.

Can you share your finetuning notebook for reference ? "
Pradeep1995,2024-01-06 15:44:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Notebook sharing is not possible due to security reasons. It is confidential in my organization level
sumegh,2024-01-06 15:45:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,okay no issues. Also what optimizer are you using ? I was doing 4-bit LoRA finetuning. Using the paged_adamw_8bit optimizer from huggingface training config.
Pradeep1995,2024-01-06 15:49:56+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,i am using - paged_adamw_32bit
mlkorra,2024-01-25 04:42:17+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@sumegh facing similar increasing loss issue during fine-tuning,  were you able to resolve that using lower learning rate?"
sumegh,2024-01-25 06:55:18+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"no @mlkorra lowering the learning rate made the model converge to a sub-optimal minima. It doesn't diverge anymore but the model doesn't learn much.
Let me know if you figure out something."
cekal,2024-01-01 13:10:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I believe this instruct version was not trained on a dataset with examples of ""system"" messages. I recommend putting the system message right after your instruction, so for example: ```<s> [INST] Hello, how are you? Respond like a pirate. [/INST]```

Here, `Hello, how are you?` is the instruction, and `Respond like a pirate.` is the system prompt.

Feel free to experiment and try different approaches. If you'd really need a system message, you would probably have to fine-tune this further on your own dataset."
blaney83,2024-01-01 01:42:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same. Not only do I often run in to repetition until the max context length is hit, I also get these absurd hallucinations of random words:

```
(relatively good output)...  This should include not only a thorough examination and evaluation of the proposed approach's theoretical soundness and practical applicability but also a critical reflection on its broader ethical, social, economic, legal, environmental, cultural, political, and philosophical implications and consequences.\n\nThirdly, it is essential that the paper provides a much more explicit, detailed, comprehensive, systematic, rigorous, transparent, validated, and reproducible description and explanation of the proposed approach's theoretical foundations, assumptions, concepts, principles, methods, techniques, tools, algorithms, data structures, software architectures, system designs, implementation details, evaluation metrics, experimental results, statistical analyses, machine learning models, deep learning neural networks, natural language processing pipelines, knowledge representation and reasoning systems, information retrieval and extraction techniques, text mining and analytics methods, semantic web technologies, linked data frameworks, artificial general intelligence architectures, and other related topics.\n\nFourthly, the paper must ensure that it adequately addresses, discusses, analyzes, evaluates, critiques, questions, challenges, refutes, debunks, reconceptualizes, reinterprets, reimagines, reframes, repurposes, recontextualizes, reinserts, reconnects, recommits, restores, resuscitates, rejuvenates, rekindles, revitalizes, regenerates, reinvigorates, reenergizes, and supercharges the readers' understanding, knowledge, insights, awareness, comprehension, perception, interpretation, imagination, reasoning, judgment, decision-making, problem-solving, critical thinking, creativity, innovation, ingenuity, resourcefulness, adaptability, resilience, tenacity, determination, grit, guts, fortitude, backbone, mettle, spunk, pluck, moxie, hardiness, spirit, verve, vivacity, enthusiasm, eagerness, passion, ardor, fire, zeal, zest, avidity, hunger, thirst, craving, desire, longing, aspiration, ambition, objective, goal, end, target, aim, purpose, function, role, duty, responsibility, obligation, requirement, condition, stipulation, proviso, prerequisite, qualification, credential, license, certification, accreditation, registration, charter, authorization, endorsement, ratification, sanction, approval, agreement, consensus, unanimity, harmony, accord, peace, serenity, tranquility, calmness, stillness, quietness, repose, silence, hush, lull, whisper, murmur, rustle, crackle, buzz, hum, clink, clank, clatter, bang, knock, rap, tap, beat, drum, pulse, throb, palpitate, vibrate, oscillate, fluctuate, waver, falter, hesitate, procrastinate, dawdle, linger, loiter, tarry, saunter, stroll, swagger, amble, wander, roam, ramble, range, traverse, navigate, journey, travel, expedition, cruise, safari, pilgrimage, trek, hike, climb, scramble, mountaineer, ski, snowboard, ice skate, surf, windsurf, kitesurf, paraglide, hang glide, bungee jump, sky dive, base jump, wingsuit fly, free fall, zip line, canyon swing, giant swing, slingshot, aqua drop, human catapult, launch loop, evacuated.
```
"
blaney83,2024-01-01 01:44:08+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Same. Not only do I often run in to repetition until the max context length is hit, I also get these absurd hallucinations of random words:
> 
> ```
> (relatively good output)...  This should include not only a thorough examination and evaluation of the proposed approach's theoretical soundness and practical applicability but also a critical reflection on its broader ethical, social, economic, legal, environmental, cultural, political, and philosophical implications and consequences.\n\nThirdly, it is essential that the paper provides a much more explicit, detailed, comprehensive, systematic, rigorous, transparent, validated, and reproducible description and explanation of the proposed approach's theoretical foundations, assumptions, concepts, principles, methods, techniques, tools, algorithms, data structures, software architectures, system designs, implementation details, evaluation metrics, experimental results, statistical analyses, machine learning models, deep learning neural networks, natural language processing pipelines, knowledge representation and reasoning systems, information retrieval and extraction techniques, text mining and analytics methods, semantic web technologies, linked data frameworks, artificial general intelligence architectures, and other related topics.\n\nFourthly, the paper must ensure that it adequately addresses, discusses, analyzes, evaluates, critiques, questions, challenges, refutes, debunks, reconceptualizes, reinterprets, reimagines, reframes, repurposes, recontextualizes, reinserts, reconnects, recommits, restores, resuscitates, rejuvenates, rekindles, revitalizes, regenerates, reinvigorates, reenergizes, and supercharges the readers' understanding, knowledge, insights, awareness, comprehension, perception, interpretation, imagination, reasoning, judgment, decision-making, problem-solving, critical thinking, creativity, innovation, ingenuity, resourcefulness, adaptability, resilience, tenacity, determination, grit, guts, fortitude, backbone, mettle, spunk, pluck, moxie, hardiness, spirit, verve, vivacity, enthusiasm, eagerness, passion, ardor, fire, zeal, zest, avidity, hunger, thirst, craving, desire, longing, aspiration, ambition, objective, goal, end, target, aim, purpose, function, role, duty, responsibility, obligation, requirement, condition, stipulation, proviso, prerequisite, qualification, credential, license, certification, accreditation, registration, charter, authorization, endorsement, ratification, sanction, approval, agreement, consensus, unanimity, harmony, accord, peace, serenity, tranquility, calmness, stillness, quietness, repose, silence, hush, lull, whisper, murmur, rustle, crackle, buzz, hum, clink, clank, clatter, bang, knock, rap, tap, beat, drum, pulse, throb, palpitate, vibrate, oscillate, fluctuate, waver, falter, hesitate, procrastinate, dawdle, linger, loiter, tarry, saunter, stroll, swagger, amble, wander, roam, ramble, range, traverse, navigate, journey, travel, expedition, cruise, safari, pilgrimage, trek, hike, climb, scramble, mountaineer, ski, snowboard, ice skate, surf, windsurf, kitesurf, paraglide, hang glide, bungee jump, sky dive, base jump, wingsuit fly, free fall, zip line, canyon swing, giant swing, slingshot, aqua drop, human catapult, launch loop, evacuated.
> ```

It seems like the context length limit is being hit every time regardless of whether it makes sense. Once the correct answer stops, it will just begin hallucinating until the limit is reached."
cekal,2024-01-01 13:02:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Because you both do not use the format you should be using. Of course it will do this when the model was trained on a certain format and you both use your custom one. You must prompt the model like this:
```
<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]
```

Here is an example:
```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = ""mistralai/Mixtral-8x7B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id)

text = ""<s> [INST] Translate Hola to English. [/INST]""
inputs = tokenizer(text, return_tensors=""pt"")

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

You can also use settings like `temperature`, `repetition_penalty` `top_p` and `top_k` & others for more customized behavior."
ybelkada,2023-12-28 16:04:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @Pradeep1995 
Thanks for the issue, if you want the model with the least loss (i.e. the ""best"" model), I would advise to go for the first option. Otherwise 2- and 3- should achieve the same goal and save the final checkpoint. Note you can also call `trainer.push_to_hub()` and the trained adapters will be pushed on the Hub under your name space together with training logs"
Pradeep1995,2023-12-28 17:20:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@ybelkada If I want the model with the least loss (i.e. the ""best"" model), I would go for the first option- the checkpoints.  
So the checkpoints act as the adapters?"
ybelkada,2023-12-28 18:09:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"yes, if you inspect the checkpoint folders, you should be able to see `adapter_model.safetensors` and `adapter_config.json` files. Those refer to the adapter weights"
Pradeep1995,2023-12-29 01:53:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@ybelkada Thanks. so if I use the least loss checkpoint folder as an adapter, then should I again merge that checkpoint folder with the  base model using
```
merge_and_unload()
```
or shall i directly use the checkpoint folder for inference without merging?"
MukeshSharma,2023-12-28 10:24:02+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have created a venv where i am using cuda 11.8 , I installed it manually using
 pip3 install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118

my torch is using cuda 11.8  but when doing nvidia-smi its still shows cuda version 11.4. "
ybelkada,2023-12-27 09:13:24+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @soumodeep-semut , `pip install -U transformers` should fix your issue"
basujindal,2023-12-27 18:42:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"""Exciting times with the new Mixtral model from  @MistralAI
! It’s evident that they’ve fine-tuned the Mistral 7B model to an impressive 8x. The significant correlation between the weights of the two models is a testament to the successful reuse of models. This approach could empower the OSS community with its own robust MoE!""

https://twitter.com/tianle_cai/status/1734188749117153684"
cekal,2023-12-26 20:53:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I got it running on one in 4bit and 8bit, higher precision might require 2 or 3"
Bankfraud1,2024-02-18 04:11:57+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Looking for some feedback from anyone here. benchmark tokens per second on a single a100?
ybelkada,2024-02-19 07:06:02+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @Bankfraud1 !
I invite you to go through this interesting thread from @pandora-s & @dounykim : https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/131 - they also shared: https://anakin.ai/blog/how-to-run-mixtral-8x7b-locally/ "
vikasy95,2024-03-29 02:17:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"So, when we download the model from mistralai/Mixtral-8x7B-Instruct-v0.1, I can see blobs files but not sure how to convert them to .safetensors file. 
Any suggestions on the above question or this question? 

Thanks"
Thireus,2023-12-23 10:00:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I've had this behaviour as well until I realised I wasn't following the prompt format properly. Make sure you use the white spaces and BOS/EOS tokens appropriately. The EOS tokens are very important. And the whitespaces are also very important.

Prompt:
```
<s> [INST] What is 1+1? [/INST] The sum of 1 + 1 is 2. Is there anything else you would like to know about mathematics or another topic? I'm here to help with any questions you have to the best of my ability.</s> [INST] What is 1+2? [/INST]
```

Prompt answered:
```
<s> [INST] What is 1+1? [/INST] The sum of 1 + 1 is 2. Is there anything else you would like to know about mathematics or another topic? I'm here to help with any questions you have to the best of my ability.</s> [INST] What is 1+2? [/INST] The sum of 1 + 2 is 3. In arithmetic, addition is the operation of combining two numbers to produce a third number. For example, in the expression ""2 + 3 = 5,"" 2 and 3 are the addends, and 5 is the sum. Addition can be represented using symbols, such as the plus sign ""+"", or by the word ""and."" It is one of the basic operations in mathematics, along with subtraction, multiplication, and division. Is there anything else you would like to know about mathematics or another topic? I'm here to help with any questions you have.
```

Very important:
- After the BOS (`<s>`), there is a whitespace. There must be only one BOS in your prompt.
- Before any EOS (`</s>`), there is no whitespace.
- All instructions must be between `[INST]` and `[/INST]`, notice the whitespace after and before them.
- At the end of your prompt, there must be no whitespace after the last `[/INST]`. Completion will add it for you.
- Make sure the software you use doesn't already add the BOS/EOS tokens to the prompt, otherwise you must decide who adds them (either yourself or the software), but you need to make sure they are added correctly and that the whitespaces are present.

https://github.com/huggingface/blog/blob/main/mixtral.md"
samparksoftwares,2023-12-29 12:27:35+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have attributes like system prompt, chat history, context, and question can you help me with how to customize the prompt to get complete answers.
"
Thireus,2023-12-29 12:30:24+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I don't believe this model supports system prompt, but you can certainly include it in the [INST] [/INST]. Chat history is just a succession of [INST] and answers as I've demonstrated in my previous post."
samparksoftwares,2024-01-02 09:32:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have been using this format:
B_INST, E_INST = ""[INST]"",""[/INST]""
SYSTEM_PROMPT = B_SYS + prompt + E_SYS
instruction = """"""
Context:{chat_history} \n {context}
User:{question}""""""

prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST

this format is not giving complete answer and if you can help me correct this format that will be very helpful."
ibndias,2024-01-31 11:05:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same here, even with the correct prompt format it stops generating at some code or command line."
samparksoftwares,2024-02-07 05:02:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi everyone,
After doing hit and try methods I was able to get whole answer from mixtral model by using same prompt I have mentioned above. Just by calling llm from HuggingFaceInferenceAPI from llama index or langchain.
"
Thireus,2024-04-18 09:29:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Proof of concept to construct valid Mixtral prompts using Python:

Set up a Python environment:
```
virtualenv-3.10 --python=python3.10 ~/test
cd test
source bin/activate*
pip install mistral-common transformers jinja2
```

Create a file named modified_script.py (modified Python script from https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1):
```
from mistral_common.protocol.instruct.messages import (
    AssistantMessage,
    UserMessage,
)
from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.tokens.instruct.normalize import ChatCompletionRequest

from transformers import AutoTokenizer

tokenizer_v3 = MistralTokenizer.v3()

mistral_query = ChatCompletionRequest(
    messages=[
        UserMessage(content=""How many experts ?""),
        AssistantMessage(content=""8""),
        UserMessage(content=""How big ?""),
        AssistantMessage(content=""22B""),
        UserMessage(content=""Noice 🎉 !""),
    ],
    model=""test"",
)
hf_messages = mistral_query.model_dump()['messages']

tokenizer_hf = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x22B-Instruct-v0.1')

print(tokenizer_hf.apply_chat_template(hf_messages, tokenize=False))
```

Execute the script:
```
python modified_script.py
```

Output of the above command:
```
<s> [INST] How many experts ? [/INST] 8 </s> [INST] How big ? [/INST] 22B </s> [INST] Noice 🎉 ! [/INST]
```

Hope this helps."
ybelkada,2023-12-20 09:56:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @aledane 

> I am using the model in the quantization version

Can you elaborate more? Which quantization method are you using?"
aledane,2023-12-20 10:54:21+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I was trying to use this: https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
However, I realized I was not really using it due to a coding mistake; instead, I was deploying the original version mistralai/Mixtral-8x7B-Instruct-v0.1.

Other than using a quantization method, is there any way to speed up the inference generation by using the original model?
Is it just a problem of resources (so I have to increase the Sagemaker instance), or is there another way?"
seabasshn,2023-12-20 12:49:15+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Can you share your code and versions of sagemaker sdk and the TGI you used? I've been trying to deploy both the models on SM but I havent been able to.
ybelkada,2023-12-20 13:43:22+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @aledane 
I suspect your model is silently loaded with CPU offloading because you don't have enough GPU RAM. You can make sure to use `torch.float16` by passing `torch_dtype=torch.float16` in from_pretrained, or load the model in 4-bit precision through `bitsandbytes` package so that your model will fit into your GPU"
aledane,2023-12-20 13:53:23+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @ybelkada , thank you for your reply. I am already using torch.float16 in from_pretrained as I have already shown in the set of parameters above.
I can try with 4-bit precision tho, even if I do not think will change so much honestly"
MrDragonFox,2023-12-21 01:06:39+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"you will need more then 1 a10 for that .. .fp16 takes about 90g vram in so 2 a100/h100 2 a6000 are fine either way 
6bpw on exlv2 takes 38g so you can cramp that into an a6000 "
YangXing,2023-12-19 17:11:03+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,good
lukasLikesLinearity,2023-12-19 11:50:49+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"hey there, I got the same error, I can load from AutoTokenizer the Mixtral-8x7B-v0.1 version after updating to transfomers_version 4.36.2 but not the model from AutoModelForCausalLM due to key error"
ybelkada,2023-12-20 09:02:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @Husain 
I think HuggingChat uses TGI under the hood: https://github.com/huggingface/text-generation-inference  
Specifically here: https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py"
ArthurZ,2023-12-18 19:01:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I think this was fixed in the latest release of tgi 🤗
seabasshn,2023-12-18 19:06:12+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ArthurZ What's the latest version? Im using 1.3.1
ArthurZ,2023-12-18 19:07:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"https://github.com/huggingface/text-generation-inference/releases/tag/v1.3.3

so 1.3.3! "
seabasshn,2023-12-18 19:22:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Thank you. It look like I'll just have to wait until the sagemaker sdk (v2.200.1) support the newest TGI version. 

""Unsupported huggingface-llm version: 1.3.3. You may need to upgrade your SDK version (pip install -U sagemaker) for newer huggingface-llm versions. Supported huggingface-llm version(s): 0.6.0, 0.8.2, 0.9.3, 1.0.3, 1.1.0, 1.2.0, 1.3.1, 0.6, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3."""
ArthurZ,2023-12-18 19:25:04+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Sorry for that 😥 
seabasshn,2023-12-18 19:27:44+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ArthurZ All good! Appreciate the help
tadeodonegana,2023-12-19 20:34:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Same problem here.
LorenzoCevolaniAXA,2023-12-20 08:58:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"The new container has been deployed today, I was able to deploy it"
seabasshn,2023-12-20 12:30:03+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Can you share what sagemaker SDK and TGI versions you used? I'm still getting the same error even after upgrading to v1.3.3 and v2.201.0, respectively."
Bepitic,2023-12-23 17:18:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Me with Sagemaker ver 2.202.1, still is not supporting  1.3.3 nor 1.3.4, can someone point me to the container or a link where to find it?
 "
tadeodonegana,2023-12-23 17:45:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Me with Sagemaker ver 2.202.1, still is not supporting  1.3.3 nor 1.3.4, can someone point me to the container or a link where to find it?

I was able to temporally solve this issue reverting to an older commit. Check https://github.com/huggingface/text-generation-inference/issues/1342 for more information."
ybelkada,2023-12-18 18:17:39+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"hi @asmatveev 
you are loading the model in full precision, which led to CPU OOM. Can you try to load the model in half-precision or in 4-bit precision as detailed on the model card?"
ybelkada,2023-12-18 11:20:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"You can use HuggingChat and select this model

https://huggingface.co/chat/settings/mistralai/Mixtral-8x7B-Instruct-v0.1 "
ArthurZ,2023-12-18 19:05:24+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Could you provide a bit more context on how you are using the model? TGI? Inference endpoint? etc
philgrey,2023-12-18 19:58:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"from langchain.llms.sagemaker_endpoint import LLMContentHandler

class ContentHandler(LLMContentHandler):
    content_type = ""application/json""
    accepts = ""application/json""

    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:
        """"""
        Transforms the input into bytes that can be consumed by SageMaker endpoint.
        Args:
            inputs: List of input strings.
            model_kwargs: Additional keyword arguments to be passed to the endpoint.
        Returns:
            The transformed bytes input.
        """"""
        # Example: inference.py expects a JSON string with a ""inputs"" key:
        input_str = ' '.join(inputs)
        input_str = json.dumps({""inputs"": input_str, **model_kwargs})
        return input_str.encode(""utf-8"")

    def transform_output(self, output: bytes) -> List[List[float]]:
        """"""
        Transforms the bytes output from the endpoint into a list of embeddings.
        Args:
            output: The bytes output from SageMaker endpoint.
        Returns:
            The transformed output - list of embeddings
        Note:
            The length of the outer list is the number of input strings.
            The length of the inner lists is the embedding dimension.
        """"""
        # Example: inference.py returns a JSON string with the list of
        # embeddings in a ""vectors"" key:
        response_json = json.loads(output.read().decode(""utf-8""))
        return response_json
============================================================================================

from langchain.chains.question_answering import load_qa_chain
from langchain.llms.sagemaker_endpoint import SagemakerEndpoint

content_handler = ContentHandler()

llms = SagemakerEndpoint(
    endpoint_name=""huggingface-pytorch-tgi-inference-2023-12-18-06-04-44-513"",
    region_name=""eu-west-2"",
    model_kwargs={
        ""temperature"": 0,
        ""maxTokens"": 1024,
        ""numResults"": 3
    },
    content_handler=content_handler
)

"
philgrey,2023-12-18 20:00:31+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This comment has been hidden
AnzaniAI,2024-01-10 12:57:19+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@mrfakename I am getting ValueError: Error raised by inference API: Internal Server Error
StatsGary,2024-05-07 15:00:12+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Yeah - same: `HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: t6MGv3j9GtAq3ieMq1YCS)`
ArthurZ,2023-12-18 19:12:40+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Yes it is the recommended way to load models. The consolidated files are the original checkpoints (used by VLLM) 
pierric,2023-12-22 16:24:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@myzz please stop opening spammy PRs such as this one, or we (HF) will have to mute you."
ArthurZ,2023-12-18 19:16:56+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Would try to load a smaller version of the model with `model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=""H:/llm/Cache"", torch_dtype = torch.float16)` (unless you don't have a GPU). What architecture are you using? "
ybelkada,2023-12-18 19:34:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @robotrage 
there is no need to call `device_map=""auto""` on the tokenizer call, can you call it instead in the automodel call ? Also please consider passing `low_cpu_mem_usage=True`

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = ""mistralai/Mixtral-8x7B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=""H:/llm/Cache"", offload_folder=""H:/llm/Cache"")
print(""re"")

model = None
try:
    model = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=""H:/llm/Cache"",  device_map=""auto"", low_cpu_mem_usage=True)
except Exception as e:
    print(""err: "" + str(e))
```"
ArthurZ,2023-12-18 19:18:40+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,could you try again? There was a temporary issue
pandora-s,2023-12-15 15:23:22+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Well that's because it is open source. As stated on their website https://docs.mistral.ai/models/ , their models are Open-weight models. Mistral 7b & 8x7b models are both under Apache 2.0 license, I hope I did not misunderstand your question !"
bombless,2023-12-15 16:17:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"You can run model with safetensor format weights, because the network structure is already in the weights file. You just don't know the details of the training program and the dataset that trained the weights."
edoyen,2024-04-18 09:02:47+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hello, This type of prompt is supposed to work for Mixtral and I'm also trying to make a RAG with Mixtral. Can you help me and tell me how you managed to put this prompt in your template so that it has access to ""chat_history"", ""context"" and ""question"" please? Thank you in advance for your response!"
TimeRobber,2023-12-15 12:24:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"```python
sliding_window = int(params[""sliding_window""]) if params.get(""sliding_window"", None) is not None else None
```
Should fix it."
obiwan92,2023-12-15 07:03:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Oh I am so blind.    ""transformers_version"": ""4.36.0.dev0"", I am still using 4.35 lol
"
abdullahalrafib,2023-12-17 06:34:55+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Yeah, for me `pip install --upgrade transformers` worked. I was at `4.35.2` upgraded to `4.36.1`"
medmac01,2023-12-14 21:35:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"idk if my problem is related to this.. but from the last hour the model was acting so weird and generating random numbers.. here are some examples


![image.png](https://cdn-uploads.huggingface.co/production/uploads/640603e2c3ab325efa94bc4a/yh8Yj5REFayYeqQsDmSq5.png)

"
Narsil,2023-12-14 21:51:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"We manually reverted in hf.co/chat the version to fix the issue.

Seems like sliding window: None causes catastrophic failure in flash attention (potentially not respecting requests boundaries, or just outputting garbage) . We'll figure out out to fix this upstream tomorrow."
issa130,2023-12-14 22:04:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,ok we are waiting for the changes tomorrow I'm working on a project
osanseviero,2023-12-14 22:12:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Hi @issa130 and @medmac01 ! Which tools do you use to run inference? Is it TGI? `transformers`?
issa130,2023-12-14 22:14:23+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I'm using inference API it was working very fine but when I saw that someone set the sliding_window to null the model started acting weird
ArthurZ,2023-12-18 11:51:20+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,The configuration file (and conversion if needed) will be adjusted accordingly thanks reporting! 
ybelkada,2023-12-18 11:54:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@TimeRobber I updated the conversion script already here: https://github.com/huggingface/transformers/pull/28068 based on your suggestion
vibranium,2024-01-22 20:46:37+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@elanmarkowitz What does your config look like?
tourist800,2023-12-14 10:20:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"KeyError: 'mixtral' possible to solve with ""mistral"" in the config.json file. "
amartin,2023-12-14 14:53:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"A better fix may be updating your local transformers
https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/30"
swapnil3597,2023-12-14 09:27:25+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This comment has been hidden
bastian100,2023-12-14 09:40:52+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hello, I have the same error here."
thomaspernet,2023-12-14 10:25:46+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Upgrade transformers `pip install transformers --upgrade` (transformers 4.36.1) solved the issue for me

https://pypi.org/project/transformers/"
aledane,2023-12-15 16:16:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I am facing the same problem. I have installed transformers 4.36.1 but still does not work.
ArthurZ,2023-12-18 12:07:16+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,make sure you are actually using it: `import transformers; print(transformers.__version__)` 
Niranjan292,2023-12-23 17:11:40+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Yeah, Upgrading transformers actually works. Restarting the kernel after upgrading worked for me."
ShivanshMathur007,2024-01-08 18:23:25+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"What are your hardware config, that you are loading the model directly?
"
AbRds,2023-12-14 08:13:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Yes, I've tried to deploy the model using TGI, it is explained here: [https://huggingface.co/blog/mixtral#using-text-generation-inference](https://huggingface.co/blog/mixtral#using-text-generation-inference)
In my case I was using a AWS EC2 G5.24xlarge, however seems like the machine is not big enough to run the model and it crashes, you can see my issue here: [https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/22#657991ae0b0608ba9ccb0c4f)

 I'm awaiting for authorisation in order to use the G5.48xLarge, if you are able to run the model following the instructions in the first link, please let me know which machine you are using. 

Cheers. "
ArthurZ,2023-12-18 11:47:46+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,see this as well https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/18 
AbRds,2023-12-19 16:38:15+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @silvacarl, 

I was able to run the model along with TGI using an in-place quantisation technique (my current setup is not able to run the model at full), also I used the default value for the flag --max-total-tokens.
Here is the command I used in case it is useful for you or someone else:

```docker
sudo docker run -d --gpus all --shm-size 1g -p $port:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded true --num-shard 4 --quantize eetq
```"
elanmarkowitz,2023-12-13 21:29:55+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/18
mavjan,2023-12-13 22:41:37+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Thanks @elanmarkowitz , sorry did not see this before making this post."
ArthurZ,2023-12-18 11:45:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Thanks
ArthurZ,2023-12-18 11:45:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,cc @philschmid I am not sure if that is expected or not with high demands probably 
AnzaniAI,2024-01-10 12:58:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@philschmid Please look into this -ValueError: Error raised by inference API: Internal Server Error
"
acrastt,2023-12-14 02:40:02+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Some backends, such as vLLM, do not dynamically adjust the sliding window param based on the max context length provided. Modifying it here seems to fix that.

Though the sliding window is indeed 4k context length? Seems like a vLLM issue to me. I think this would break loaders that actually support SWA"
ArthurZ,2023-12-18 11:39:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,cc @marcsun13 who worked on the quantization! 
marcsun13,2023-12-18 11:46:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @krumeto, this is right. We've seen a decrease in quality loss comparable to a quantized LLama 7B. "
krumeto,2023-12-18 14:09:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Thank you, @marcsun13 ! Since I asked the question, first Open LLM Leaderboard results for the base GPTQ version appeared. The decrease seems to be more or less similar to what we saw with Llama 2 models:

| Model                                  | Average | ARC   | HellaSwag | MMLU  | TruthfulQA | Winogrande | GSM8K |
| --------------------------------------- | ------- | ----- | --------- | ----- | ---------- | ---------- | ----- |
| mistralai/Mixtral-8x7B-v0.1    | 68.42   | 66.04 | 86.49     | 71.82 | 46.78      | 81.93      | 57.47 |
| TheBloke/Mixtral-8x7B-v0.1-GPTQ         | 65.7    | 65.19 | 84.72     | 69.43 | 45.42      | 81.14      | 48.29 |
| **Score Delta**                        | **0.960**| **0.987** | **0.980** | **0.967** | **0.971** | **0.990** | **0.840** |

This is great news for us (waiting for the instruct model GPTQ scores, but in general, I hope this holds). We are testing the model with TGI (in 8bit, eetq), waiting to test GPTQ (seems like there are still some TGI issues with GPTQ), but not quite sure which of the methods should retain most quality (we are less interested in speed).  If you have any resources that compare Mixtral (or even other models) any of EETQ/GPTQ/AWQ/bnb in terms of quality, it would be very helpful. This blog was already extremely insightful - https://huggingface.co/blog/overview-quantization-transformers#overview-of-natively-supported-quantization-schemes-in-%F0%9F%A4%97-transformers 
Thank you all!"
marcsun13,2023-12-18 14:37:30+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @krumeto , thanks for the awesome feedback. We are still working on the AWQ quant since the quality is not good enough for now. For bnb, the quality should be the same. As for GPTQ, the model that was tested is not the best gptq quant. You can test the following branch which should give better results:  `gptq-4bit-128g-actorder_True` or `gptq-4bit-32g-actorder_True` with 32g being the most accurate one. However, the vram consumption will increase since these quant needs to store more quantization statistics (128g version with an additional 1go and 32g version with  an additional 3.5 Go)
![Screenshot 2023-12-18 at 3.26.50 PM.png](https://cdn-uploads.huggingface.co/production/uploads/63ce875d199b36f7552d4f07/NaL0VVgKgT2PmVGX8h6m3.png)

More details: https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ
"
ArthurZ,2023-12-18 11:43:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Would try to quantize the model with this: https://huggingface.co/docs/text-generation-inference/conceptual/quantization or run it in float16. Not super familiar with TGI but you might need more memory for the max batch total token you are using 
steilgedacht,2023-12-18 16:11:31+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"What worked for me was to enable device_map=""auto"".
So in the line where you load the model change it to
`model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"")`

This makes the model use all 4 GPUs"
AbRds,2023-12-18 16:13:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> What worked for me was to enable device_map=""auto"".
> So in the line where you load the model change it to
> `model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"")`
> 
> This makes the model use all 4 GPUs

Hi, thanks for the response, how can I apply this change using the TGI? 

thanks in advance."
ArthurZ,2023-12-18 18:56:57+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,cc @Narsil for TGI! 
AbRds,2023-12-19 16:30:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi, finally I was able to run the model along with TGI using an in-place quantisation technique (I've supposed my current setup is not enough to run the model), also I used the default value for the flag  --max-total-tokens. 

Here is the command I used in case it is useful for someone else:

```docker
sudo docker run -d --gpus all --shm-size 1g -p $port:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 --sharded true --num-shard 4 --quantize eetq
```"
Narsil,2023-12-21 15:10:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Glad you made it work
"
ExceedZhang,2023-12-13 08:57:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/64b26c035e1230a79f897880/Po5ISyYCytlKavNiKwcFa.png)
"
ArthurZ,2023-12-13 08:59:57+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,You prbably need to download the weights again :/ 
ExceedZhang,2023-12-13 09:24:24+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Thanks
YaTharThShaRma999,2023-12-13 16:18:14+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@Starlento thats because this is a moe model.
you see this is made of 8 7b param models trained on different data. a easy way to think about it is like one trained on science stuff, another is trained on math stuff, and another is trained on roleplay. They are probably not trained like this but its somewhat similar.

You might say 8x7 does not equal 47, and the reason mixtral is 47b parameters is because some of the parameters are shared so its actually 47b.

The reason it uses so much vram but at 13b inference speed is because of its architecture.
You MUST load all the models so it will take a very large amount of vram(same as a normal 47b model)

However, when doing actual inference you just need to use 2 of the best models suited to answer the question. so 2x7 = 14b, so roughly 13b speed.

The 2 models might change depending on the instruction you input so all the 8 models have to be preloaded before.
Mixtral is excellent for its size and performs really well at instruction tasks. It has decent benchmark scores so far but it can be easily increased much more when the community can finetune it even further. "
ron21,2023-12-13 01:20:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Switch to mistralai/Mixtral-8x7B-Instruct-v0.1 and try again!
ArthurZ,2023-12-13 09:01:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This model was not trained on Chinese. 
csgxy2022,2023-12-14 17:31:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@ArthurZ I doubt that, the model can obviously speak in Chinese.

@ron21 checking the model_id, it is already the instruct model."
ArthurZ,2023-12-18 11:50:28+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"What I mean is that you should check the ids generated. It probably instanstly generated a `</s>` the eos . 
"
monuminu,2023-12-13 05:31:15+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"use this image URI

763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.3.1-gpu-py310-cu121-ubuntu20.04-v1.0"
elanmarkowitz,2023-12-13 06:27:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Thanks. I also found this nice blog https://www.philschmid.de/sagemaker-deploy-mixtral#1-setup-development-environment
yucheny,2023-12-12 20:26:34+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same here, the generation is very slow."
ArthurZ,2023-12-13 09:04:07+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"If the model is offloaded to the CPU, then of course it's going to be slow :/ The model did not change, unless you are computing the loss (which was not working on parallel devices). Make sure `output_router_logits` is set to `False` in the config"
ybelkada,2023-12-13 09:19:42+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@Boyue27 your model is most likely offloaded into CPU or disk as stated by Arthur, you need to make sure you load your model in half-precision or 4-bit precision to make sure your model is fit into your GPU device:

For float16:
```python
import torch
from transformers import AutoModelForCausalLM

model_path = ""mixtral""
model = AutoModelForCausalLM.from_pretrained(
model_path,device_map=""auto"", max_memory=max_memory_mapping, torch_dtype=torch.float16
)
tokenizer =AutoTokenizer.from_pretrained(model_path)
```

4-bit precision (after installing bitsandbytes (`pip install bitsandbytes`):

```python
import torch
from transformers import AutoModelForCausalLM

model_path = ""mixtral""
model = AutoModelForCausalLM.from_pretrained(
model_path,device_map=""auto"", max_memory=max_memory_mapping, load_in_4bit=True
)
tokenizer =AutoTokenizer.from_pretrained(model_path)
```
"
Boyue27,2023-12-13 12:39:18+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ybelkada Thank you for your help. I have tested your code and it fixed the problem.
Boyue27,2023-12-13 12:40:51+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@ArthurZ Thank you for your help and the solution is working great for me.
ybelkada,2023-12-12 13:48:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @harryneal 
Please see a similar issue here: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/13 and let us know if this helps"
harryneal,2023-12-12 13:58:52+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @ybelkada,
I believe these are two separate issues as I am attempting to deploy via an AWS Sagemaker endpoint using a HuggingFace container."
harryneal,2023-12-12 14:39:23+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I believe the problem is that on huggingface llm image URI v1.1.0, the transformers version is 4.33.3, but according to the HF Mixtral blog it needs to be the latest version (4.36.0).

https://huggingface.co/blog/mixtral

Maybe this is a case of waiting a few days for the container to be updated but in the meantime if anyone can help out with creating a custom one myself it would be much appreciated!"
harryneal,2023-12-12 16:16:41+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Resolved -  solution in link below, thanks Phil.

https://www.philschmid.de/sagemaker-deploy-mixtral"
ybelkada,2023-12-12 16:20:07+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Thanks very much @harryneal !
Soraheart1988,2024-01-02 03:28:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@harryneal , the link https://www.philschmid.de/sagemaker-deploy-mixtral requires instance type of p4d.24xlarge. I am also trying to deploy using sagemaker. 
Have you deployed with quantized version using smaller instance size?"
harryneal,2024-01-05 11:06:24+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @Soraheart1988 , no I haven't yet managed to deploy Mixtral to a smaller instance, as the latest huggingface inference container doesn't support its quantization when deploying to a Sagemaker endpoint.  I am hoping the legends at huggingface are working on it and will release a new update soon.

For reference this is the latest inference container:
https://github.com/aws/deep-learning-containers/releases/tag/v1.0-hf-tgi-1.3.3-pt-2.1.1-inf-gpu-py310

and this is a list of current and previous containers that I'm keeping an eye in case of an update:
https://github.com/aws/deep-learning-containers/releases?q=tgi&expanded=true"
GauravH,2023-12-12 12:09:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,updating the transformers_version to 4.36.0 fixed this issue.  Thanks.
Santiclibrain,2023-12-12 12:11:23+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,You might need to upgrade your transformers version
Santiclibrain,2023-12-12 12:12:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> You might need to upgrade your transformers version

Yeah, we answered at the same time!"
ArthurZ,2023-12-12 10:46:20+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"It depends, if you quantize the model it can fit in 24GB (4bits) "
DeltaWhiplash,2023-12-12 15:25:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"oh really ?
I have an Nvidia tesla P40 how can I run it ?"
ybelkada,2023-12-12 16:23:24+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Hi @DeltaWhiplash 
check out this section on how to run the model using different optimization methods: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1#run-the-model"
DeltaWhiplash,2023-12-13 17:22:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"OMG am blind 
Am using a VM with this specs : 
- 8 vCPU
- 16GB of RAM
- a p40 24gb passthrough 

to run my models am using this inference (https://github.com/c0sogi/llama-api)
Do you have one to recommend (I just need an openai like api)"
AnzaniAI,2023-12-15 07:27:39+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Same here @DeltaWhiplash 
"
philschmid,2023-12-12 10:53:54+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"What instance type, container and config did you use? The default config should work with 2x A100 80GBs or use that link https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&vendor=aws&region=us-east-1&accelerator=gpu&instance_size=2xlarge&task=text-generation&no_suggested_compute=true&tgi=true&tgi_max_batch_total_tokens=1024000&tgi_max_total_tokens=32000"
dragosmc,2023-12-12 11:02:03+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Gotcha, thanks for the info. I was following the UI and tried with the first available instance type that didn't say ""Low Memory"". Will try with 2xA100 once I get access to it. Thanks."
dragosmc,2023-12-12 13:05:59+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> What instance type, container and config did you use? The default config should work with 2x A100 80GBs or use that link https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&vendor=aws&region=us-east-1&accelerator=gpu&instance_size=2xlarge&task=text-generation&no_suggested_compute=true&tgi=true&tgi_max_batch_total_tokens=1024000&tgi_max_total_tokens=32000

Got acces to 2xA100 and not it doesn't seem to go past this point
```
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/whoami-v2 (Caused by NameResolutionError(""<urllib3.connection.HTTPSConnection object at 0x7f88d9b97b80>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)""))
```
Anything else you reckon I should try?"
mmarianne,2023-12-11 22:09:11+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I am running into the same error, can anyone help here."
osanseviero,2023-12-11 23:01:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Please make sure to run this with the latest transformers version
mmarianne,2023-12-11 23:05:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"That solves my issue, thanks!"
jizhongpeng,2023-12-12 01:27:32+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"""transformers_version"": ""4.36.0.dev0"","
mrfakename,2023-12-12 16:22:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> Please make sure to run this with the latest transformers version

BTW: `pip install -U transformers`"
wasertech,2024-02-12 17:39:01+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,I am using transformers==4.37.2 which is (at the time of writing) the latest stable version and I get this key error.
osanseviero,2024-02-13 11:02:51+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"If you're using Colab, make sure to restart your environment after upgrading. It should work afterwards"
wasertech,2024-02-18 20:27:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,It was a mix up in my containers locally and remotely. Updating did in fact fixed the issue as advertised. 🎉
AimeeAsimov,2024-02-23 20:30:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"care to explain that? im experiencing the same issue. what containers did you do what with and where?
"
wasertech,2024-02-26 09:05:36+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Sure @AimeeAsimov, I am using a custom vLLM server in a docker container. I built it originally using vLLM 0.2.0 and an old version of transformers. All I needed to solve the issue was to rebuild with the container using vLLM 0.3.0 and the latetst version of transformers.

My mistake was that I was using old version of the container on the cloud. Once I noticed, I pushed the new version of my image and updated it on the cloud and everything is working smoothly since then."
axeded,2023-12-11 20:21:50+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,32k tokens i believe unless im mistaken
SalmanFaroz,2023-12-12 05:47:09+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@kk3dmax Mistral-embed, embedding endpoint, serves an embedding model with a 1024 embedding dimension. embedding model has been designed with retrieval capabilities in mind. It achieves a retrieval score of 55.26 on MTEB.

https://docs.mistral.ai/platform/client#embeddings"
kk3dmax,2023-12-12 06:45:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"What a pity, it's not a open sourced one."
SalmanFaroz,2023-12-11 16:38:35+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@jf-11 It is best to follow the recommended format in inference order to obtain better outcomes.
```
<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]
```
OR
you could simply use apply_chat_template
```
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mixtral-8x7B-Instruct-v0.1"")

messages = [
    {""role"": ""user"", ""content"": ""What is your favourite condiment?""},
    {""role"": ""assistant"", ""content"": ""Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!""},
    {""role"": ""user"", ""content"": ""Do you have mayonnaise recipes?""}
]

message_formatted = tokenizer.apply_chat_template(messages, tokenize=False)
```
```
<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>[INST] Do you have mayonnaise recipes? [/INST]
```
 However, if you don't, it can still work. They just gave an simple example."
jf-11,2023-12-11 16:41:27+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I see. Thank you!
So I should also use the [INST], [/INST] literals when using `.generate()` and `tokenizer(text, return_tensors=""pt"")` does add special tokens by default? Otherwise it would be also better to use `<s>` in the beginning?"
SalmanFaroz,2023-12-11 16:49:05+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@jf-11 No , we have to include it by ourselves. You can conveniently utilize ""apply_chat_template"" for this purpose. As the conversation history grows, it's advisable to manage it with ""apply_chat_template."" Afterward, you can proceed with the following code:
```
inputs = tokenizer(message_formatted, return_tensors=""pt"")

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True)) 
```"
wolfram,2023-12-11 17:57:40+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Follow-up question regarding inference with common end-user backends/frontends like KoboldCpp/SillyTavern:

After inference generates `<s> [INST] Instruction [/INST] Model answer</s>`, the stop token is usually caught to end inference, then special tokens are removed from the output when it is shown in the frontend. User replies and next context looks like this:

`<s> [INST] Instruction [/INST] Model answer [INST] Follow-up instruction [/INST]` - so previously generated stop tokens are gone. Is that OK or would that be a problem?

Also, as constructive feedback, I'd have loved to see a system prompt. Especially if we were to implement something like https://docs.mistral.ai/platform/guardrailing/ - it would always be a good thing for the model to be able to differentiate between user and system prompts. But if you do implement one, please don't do it as part of the first INST like Llama 2 Chat's format did, which causes lots of problems because of its terrible design (I'd gladly elaborate if you'd like)."
duyntnet,2023-12-12 03:03:30+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Edited: Using default prompt from llama.cpp solved the problem for me. The prompt from the model page didn't work in my case, maybe I did something wrong ,I don't know.

I'm using this: <s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]
But the model doesn't answer my question. It creates its own question then answer that question. I wonder what I did wrong? Here's an example:

User: what can you do?

Llama: Sure, I'll do my best to provide a model answer for your question. Please provide me with the question or topic you would like me to write about.

User: can you translate

Llama: Sure, I'll do my best to provide a model answer for your question. Please keep in mind that the quality of the response may depend on the complexity and specifics of the question you have asked. Also, note that while I strive to provide accurate information, it is always a good idea to double-check any facts or figures with additional sources. With this said, please let me know what your follow-up instruction is.

User: write a short poem

Llama: Sure, I'll do my best to provide a model answer for your question. Please allow me a moment to understand the context and what is being asked.

If you could provide me with more information about the topic, prompt, or specific language point that you would like me to address in the model answer, it would be helpful. Otherwise, I will provide a general example of how to structure an effective response.

Here's a possible model answer:

Prompt: How has technology impacted education?

Model Answer:

Technology has significantly transformed..."
Cameron-Chen,2023-12-20 07:32:10+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@wolfram I have seen one example of system prompt from [deepinfra.com](https://deepinfra.com/mistralai/Mixtral-8x7B-Instruct-v0.1/api):

```
[INST] <<SYS>>
your system prompt goes here
<<SYS>>

first question [/INST] ...
```

It is not from the official document though. "
wolfram,2023-12-22 00:10:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"@Cameron-Chen Yeah, that's Llama 2 Chat. I'm using just that with Mixtral, too, and it works good enough.

Also did an extensive test of Mixtral 8x7B Instruct with **17** different instruct templates:

https://www.reddit.com/r/LocalLLaMA/comments/18ljvxb/llm_prompt_format_comparisontest_mixtral_8x7b/

Probably of interest to Mistral, too, if they haven't seen it yet. Especially my explanation of why the Llama 2 Chat format is such a terrible choice and should be replaced with something more flexible and future-proof like the ChatML format."
ybelkada,2023-12-11 13:50:29+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,This PR fixes the issue https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/4 
kev216,2023-12-11 13:32:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have tried using a 4090 24G, but it didn't work... 💔💔💔 we need more RAM."
0-hero,2023-12-11 14:03:40+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,“>130GB required”
Olaf,2023-12-11 14:06:53+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I believe you can find this information here: https://docs.mistral.ai/models/
(min. 100GB GPU RAM)"
kev216,2023-12-11 14:22:44+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"> I believe you can find this information here: https://docs.mistral.ai/models/
> (min. 100GB GPU RAM)

I think we may not be able to utilize such a large amount (100 GB) with 'load_in_4bits' or 'load_in_8bit.' I will attempt it with the A100 80GB. ^^"
ArthurZ,2023-12-11 14:46:48+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Load in 4 bits should work. Same as 8bit! 
"
osanseviero,2023-12-11 14:51:28+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"Quick math (approximate): 45 billion parameters

* In 4-bits -> 180 trillion bits, that's 22.5GB of VRAM required
* in 8-bits ->  45GB of VRAM
* in half-precision -> 90GB of VRAM required

Note that 4-bits is presenting high quality degradation. It might be interesting to explore only quantizing the experts. https://arxiv.org/abs/2310.16795, for example, introduces QMoE which allows sub-1-bit quantization for MoEs. @timdettmers is also exploring this topic, so I'm waiting for exciting things in the incoming days!"
luigisaetta,2023-12-11 17:41:45+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I have tested it on a VM with 2 GPU A10 (23 + 23 GB GPU) it works if using load_in_4bit, not 8bit. Performance in Italian are interesting"
JayBZD,2023-12-12 06:34:26+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"RTX 4090, 24GB dedicated GPU memory + 32 GB shared GPU memory, Windows 11, WSL (Ubuntu): 

1. from_pretrained(load_in_8bits=True) 
45.8 GB
2. from_pretrained(load_in_4bits=True)
27.1 GB
"
SabareeshGc,2023-12-12 18:17:40+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,@JayBZD how to run with shared memory . I have 4090 and 64gb system memory available
supportend,2023-12-13 00:17:27+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"I run it (Q6_K) on CPU only, it's much faster than 70B Models, but consumes over 50 percent from my 64 GB RAM."
Husain,2023-12-20 08:30:03+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"If we would like to run at full precision, we will need 45 * 32 = 1440B bits -> 180 GB ?
Are the parameters in float32 by default?"
one-thing,2023-12-25 07:52:43+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"On RTX A6000(48GB)
load_in_4bit = 27.2GB
load_in_8bit = 45.4 GB"
Businessboi,2024-01-01 22:06:06+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,How do I load in 4bit when using the transformers library? Or do I load it another way?
one-thing,2024-01-14 05:06:33+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,Pass “load_in_bit = True” in model.from_pretrained()
ShivanshMathur007,2024-01-29 07:50:38+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"What is the required RAM|CPU|GPU? to run this model and the Q4_K_M version of the model?
"
preslaff,2024-02-08 21:36:37+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,"You can look at this: https://arxiv.org/abs/2312.17238

For fast inference with ~24GB RAM+VRAM in colab look at this: https://colab.research.google.com/github/dvmazur/mixtral-offloading"
LucileSaulnier,2023-12-10 21:45:58+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,
diegolascasas,2023-12-10 19:57:57+00:00,comment,mistralai/Mixtral-8x7B-Instruct-v0.1,
danielhanchen,2024-05-16 14:22:10+00:00,comment,unsloth/llama-3-8b-Instruct-bnb-4bit,Thanks! Forgot to update the model config!
danielhanchen,2024-05-13 10:46:16+00:00,comment,unsloth/llama-3-8b-Instruct-bnb-4bit,Oh no need to worry on the error - just ignore it - it should still work - I might update all of them to use the new kwargs
bcartel1,2024-05-13 22:52:10+00:00,comment,unsloth/llama-3-8b-Instruct-bnb-4bit,"Hi There,
i have downloaded this model to use with ollama, i was wondering if you could leave a note on how to do this,  And if not possible with ollama then how should i run it. i am new at this as you can tell :) .
thank you"
shimmyshimmer,2024-05-15 15:29:37+00:00,comment,unsloth/llama-3-8b-Instruct-bnb-4bit,"> Hi There,
> i have downloaded this model to use with ollama, i was wondering if you could leave a note on how to do this,  And if not possible with ollama then how should i run it. i am new at this as you can tell :) .
> thank you

We have a Wiki which details a bit about Ollama: https://github.com/unslothai/unsloth/wiki#use-unsloth-lora-adapter-with-ollama"
Dinario,2024-05-02 19:50:26+00:00,comment,EleutherAI/gpt-j-6b,.
deleted,2023-09-22 11:38:35+00:00,comment,EleutherAI/gpt-j-6b,did you manage to do that?
augchan42,2023-11-28 00:17:51+00:00,comment,EleutherAI/gpt-j-6b,"Seems the default way of loading and running these pytorch models is awfully slow.  I wrote up how I used ggml to inference a gptj-6-b model here:
https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html
But basically, convert the pytorch_model.bin to ggml format, then use the ggml gpt-j bin to run inference.  No need to quantize to 8bit, I used a float16 version"
johncookds,2023-04-11 16:57:13+00:00,comment,EleutherAI/gpt-j-6b,"I've had the same issue occur, been unable to find a fix.

(Edit: This has now been solved for me)"
resz,2023-04-18 21:34:13+00:00,comment,EleutherAI/gpt-j-6b,@johncookds  how were you able  to solve the problem? I am running into the same issue
johncookds,2023-04-26 15:07:07+00:00,comment,EleutherAI/gpt-j-6b,@resz I believe it was a network connection issue for me that lasted a while. I originally suspected it was something to do with the huggingface space files but do not believe it actually was. After a few days the issue cleared up so there's a chance something was done on the HF side but I expect it was a network issue
hunterTR,2023-06-03 01:31:11+00:00,comment,EleutherAI/gpt-j-6b,"I was getting this with transformers version 4.13, it is solved for me after upgrading."
Haonan,2023-09-04 11:29:25+00:00,comment,EleutherAI/gpt-j-6b,I fixed it by using the model EleutherAI/gpt-j-6b instead of EleutherAI/gpt-j-6B so it is small b not B
phazei,2023-04-08 06:01:48+00:00,comment,EleutherAI/gpt-j-6b,"You could at least do the courtesy of adding a link if you're going to bother coming back to edit, basic internet etiquette.

Edit: Turns out on the files and versions page there's a little drop down at the top, that says ""main"" by default, where you can select the float, cool"
Hawaii-Tech,2023-04-08 08:22:17+00:00,comment,EleutherAI/gpt-j-6b,"Under the Files tab, there is the branch dropdown labeled 'Main'. If you tap that, you can see Float16 and Float32 are also available."
nathan0,2023-04-08 01:21:56+00:00,comment,EleutherAI/gpt-j-6b,"I met the same problem, did you fix it now? BTW, what kind of GPU did you use?"
juusohugs,2023-03-21 09:49:45+00:00,comment,EleutherAI/gpt-j-6b,"Google's FLAN models (such as flan-t5-base) might be better for this purpose. It's a generative model, but using encoder-decoder architecture which is well suited for generative QA."
varun500,2023-03-20 06:45:54+00:00,comment,EleutherAI/gpt-j-6b,"You can git clone the repo to get all the files:
git lfs install
git clone https://huggingface.co/EleutherAI/gpt-j-6B

Also you just download the model weights and then use them at inference when loading it from the transformers library:

from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/gpt-j-6B"")
model = AutoModelForCausalLM.from_pretrained(""EleutherAI/gpt-j-6B"")"
majidbhatti,2023-03-29 07:35:35+00:00,comment,EleutherAI/gpt-j-6b,"i tried the above command. after downloading the model to 100%, it gets killed. A quick fix/response would be highly appriciated."
varun500,2023-03-29 07:41:08+00:00,comment,EleutherAI/gpt-j-6b,@majidbhatti I think the issue is with memory. It loads 42GB of the model weights. So I think using a large RAM instance will help.
dashjson,2023-04-02 22:10:21+00:00,comment,EleutherAI/gpt-j-6b,"This video provides some information. Let us know how you fare. 

https://www.youtube.com/watch?v=efPrtcLdcdM

Some of the links referenced:

https://github.com/yk/gpt-4chan-public
His code

https://zenodo.org/record/3606810#.YpjGgexByDU
Data set

The model ( no longer available): https://huggingface.co/ykilcher/gpt-4chan"
hsuyab,2023-04-24 07:49:14+00:00,comment,EleutherAI/gpt-j-6b,Hi @ram77gowri were you able to fine-tune the gptj model?
ram77gowri,2023-05-19 18:15:01+00:00,comment,EleutherAI/gpt-j-6b,"@hsuyab Not yet, Got side tracked with some other work. Getting back again."
Asedek,2023-02-06 04:16:42+00:00,comment,EleutherAI/gpt-j-6b,Use LSTM layer into your model
Gilipsy,2023-02-18 02:05:50+00:00,comment,EleutherAI/gpt-j-6b,Hello! Did you manage to solve the problem?
tushar310,2023-02-18 17:22:09+00:00,comment,EleutherAI/gpt-j-6b,Nope. Doesn't do the job.
theamanstark,2024-02-09 14:53:24+00:00,comment,EleutherAI/gpt-j-6b,"Hey tushar, im not able to use the telegram bot api in my spaces, whenever i try to use it, the space throws the following error :- HTTPSConnectionPool(host='api.telegram.org', port=443): Max retries exceeded with url: /bot<bot-token>/sendMessage?chat_id=<chat-id>&text=<message-here> (Caused by NameResolutionError(""<urllib3.connection.HTTPSConnection object at 0x7f93a145f3d0>: Failed to resolve 'api.telegram.org' ([Errno -5] No address associated with hostname)""))"
stellaathena,2023-02-07 06:51:43+00:00,comment,EleutherAI/gpt-j-6b,Yes
Baicai003,2023-01-06 17:50:51+00:00,comment,EleutherAI/gpt-j-6b,"this code works fine for me:
```
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import torch

print(""start load"")
model = AutoModelForCausalLM.from_pretrained(""./gpt-j-6B"", low_cpu_mem_usage=True, torch_dtype=torch.float16)
model = model.to(torch.device(""cuda:0""))
print(""end 1"")
tokenizer = AutoTokenizer.from_pretrained(""./gpt-j-6B"", low_cpu_mem_usage=True, torch_dtype=torch.float16)
# tokenizer = tokenizer.to(torch.device(""cuda:0""))
print(""end 2"")

# from parallelformers import parallelize

# parallelize(model, num_gpus=1, fp16=True, verbose='detail')

start = time.time()
# 使用float16
inputs = tokenizer(""My Name is Mukesh "", return_tensors=""pt"")
inputs = inputs.to(torch.device(""cuda:0""))

outputs = model.generate(
**inputs,
num_beams=5,
no_repeat_ngram_size=4,
max_length=100,
)
print(f""Output: {tokenizer.batch_decode(outputs)[0]}"")
end = time.time()
print(end - start)

start = time.time()
inputs = tokenizer(""Q:what is AI\nA:AI is a cat.\nQ:why?\nA:"", return_tensors=""pt"")
inputs = inputs.to(torch.device(""cuda:0""))
outputs = model.generate(
**inputs,
num_beams=5,
no_repeat_ngram_size=4,
max_length=100,
)
print(f""Output: {tokenizer.batch_decode(outputs)[0]}"")
end = time.time()
print(end - start)

start = time.time()
inputs = tokenizer(""Q:请用python实现CNN\nA:"", return_tensors=""pt"")
inputs = inputs.to(torch.device(""cuda:0""))
outputs = model.generate(
**inputs,
num_beams=5,
no_repeat_ngram_size=4,
max_length=100,
)
print(f""Output: {tokenizer.batch_decode(outputs)[0]}"")
end = time.time()
print(end - start)
```"
stellaathena,2022-12-08 01:35:04+00:00,comment,EleutherAI/gpt-j-6b,Why did you change the casing of the dataset? The current dataset metadata seems to go to the correct place.
MukeshSharma,2022-12-20 17:50:50+00:00,comment,EleutherAI/gpt-j-6b,"Hi @mrmartin you should use pip install parallelformers

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""Model Name"")
tokenizer = AutoTokenizer.from_pretrained(""Model Name"")

from parallelformers import parallelize

parallelize(model, num_gpus=2, fp16=True, verbose='detail')

inputs = tokenizer(""My Name is Mukesh "", return_tensors=""pt"")

outputs = model.generate(
    **inputs,
    num_beams=5,
    no_repeat_ngram_size=4,
    max_length=15,
)

print(f""Output: {tokenizer.batch_decode(outputs)[0]}"")


It will surely solve ur problem."
augchan42,2023-11-28 00:20:38+00:00,comment,EleutherAI/gpt-j-6b,"Or just convert the model to ggml and use ggml for inference
https://augchan42.github.io/2023/11/26/Its-Hard-to-find-an-Uncensored-Model.html
"
norabelrose,2022-10-28 18:23:29+00:00,comment,EleutherAI/gpt-j-6b,Try using the float32 version if you want to do CPU inference
lerouxrgd,2022-10-27 13:44:40+00:00,comment,EleutherAI/gpt-j-6b,
stellaathena,2023-02-07 06:52:16+00:00,comment,EleutherAI/gpt-j-6b,At this time we are not accepting outside PRs that add new models.
lerouxrgd,2022-10-26 16:57:54+00:00,comment,EleutherAI/gpt-j-6b,"Besides, I'd like to add the rust weights for the float16 version too, however I am not sure how to create a PR that uses `refs/heads/float16` as a base.
I tried to do it in [pr 6](https://huggingface.co/EleutherAI/gpt-j-6B/discussions/6), but it doesn't seem to work.
Do you have any clue on that ?"
lerouxrgd,2022-11-01 20:36:30+00:00,comment,EleutherAI/gpt-j-6b,"@stellaathena @juliensimon Just pinging you as I am not sure whether you get notifications when a PR is opened (If you do, sorry for the spam !)"
stellaathena,2023-02-07 06:52:56+00:00,comment,EleutherAI/gpt-j-6b,At this time we are unable to accept outside PRs that change the model in any way.
stellaathena,2022-08-07 13:43:42+00:00,comment,EleutherAI/gpt-j-6b,"That’s a discord question, not an AI question."
userman,2022-08-07 14:22:26+00:00,comment,EleutherAI/gpt-j-6b,i already have a discord bot. what i meant is how do i apply the module to my discord bot?
stellaathena,2022-08-07 14:31:15+00:00,comment,EleutherAI/gpt-j-6b,"That’s still not an AI question. I have no idea how discord bots work, but the HF documentation details quite clearly how to generate from the model. Everything else is a Discord question."
userman,2022-08-09 09:35:04+00:00,comment,EleutherAI/gpt-j-6b,"How do i make the GPT-J send message? 

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/gpt-j-6B"")
model = AutoModelForCausalLM.from_pretrained(""EleutherAI/gpt-j-6B"")

model.whattotypehere?"
stellaathena,2022-08-09 11:37:27+00:00,comment,EleutherAI/gpt-j-6b,[generate](https://huggingface.co/docs/transformers/main_classes/text_generation)
userman,2022-09-23 17:57:30+00:00,comment,EleutherAI/gpt-j-6b,How do i give prompt for the text generation?
stellaathena,2022-10-09 14:31:49+00:00,comment,EleutherAI/gpt-j-6b,Please just read the documentation
tushar310,2023-01-26 16:08:38+00:00,comment,EleutherAI/gpt-j-6b,Any bot for that matter. This API endpoint that is available only gives very short prompts. Is there a way to either elongate the responses or maybe use previous responses as an input for the next?
stellaathena,2022-08-09 11:38:48+00:00,comment,EleutherAI/gpt-j-6b,You don’t have enough VRAM. The Mesh Transformer Jax library has a guide to parameter efficient finetuning that you can try out though.
MukeshSharma,2022-08-11 13:34:51+00:00,comment,EleutherAI/gpt-j-6b,This comment has been hidden
stellaathena,2022-06-13 12:26:14+00:00,comment,EleutherAI/gpt-j-6b,What happens when you run `git lfs help smudge`
themessier,2022-06-13 16:04:40+00:00,comment,EleutherAI/gpt-j-6b,"""Sorry, no usage text found for ""git-lfs"""
themessier,2022-06-13 17:42:57+00:00,comment,EleutherAI/gpt-j-6b,"I also tried 
```
set GIT_LFS_SKIP_SMUDGE=1
git clone https://huggingface.co/EleutherAI/gpt-j-6B
```
but still get the same error."
julien-c,2022-06-14 08:51:48+00:00,comment,EleutherAI/gpt-j-6b,"Are you on Windows?

If you're not, `Encountered %!d(MISSING) file(s) that may not have been copied correctly on Windows ` is just a warning.

If you are on Windows, I'm not 100% sure git-lfs works with >4GB files.... 😱"
themessier,2022-06-14 09:35:42+00:00,comment,EleutherAI/gpt-j-6b,"No, I am not on windows, I am using Ubuntu 18.
Does that mean that the required file is being downloaded and can be used? I used the same git-lfs command for GTP-neo-125M and it cloned without any errors and warnings."
julien-c,2022-06-16 22:22:00+00:00,comment,EleutherAI/gpt-j-6b,"yes, that's just a warning for Windows, you should be fine"
stellaathena,2022-06-18 12:54:19+00:00,comment,EleutherAI/gpt-j-6b,@themessier Why don’t you try using the model and see if it works.
userman,2022-07-12 06:23:37+00:00,comment,EleutherAI/gpt-j-6b,How do i use this module for a discord bot?
pandora-s,2024-05-12 22:42:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi there, I think there is a misunderstanding here, what do u mean by pro token? For the Hugging face API? cause you dont need to set an input size for your query only output."
WaleedJubeh,2024-05-13 06:57:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@pandora-s I meant the pro subscription.
https://huggingface.co/blog/inference-pro"
jackyuanjie1990,2024-05-21 06:13:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Change the max_length?
mobidyc,2024-03-11 11:01:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Using Python I replace the string

```python
    # remove prompt in answer
    answer = answer.replace(prompt,"""")
```"
shriML,2024-03-12 02:43:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Thanks, got it. But I want the model to not generate the prompt."
pandora-s,2024-03-12 09:35:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Thanks, got it. But I want the model to not generate the prompt.

It's not actually generating it, it's just answering you with the entire text fully completed. That's usually what an LLM is designed for.
"
shriML,2024-03-12 13:42:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Got it
Suparious,2024-02-26 04:47:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"tabbyAPI is based on fastAPI - can probably learn a lot from it.
https://github.com/theroyallab/tabbyAPI"
moc1pher,2024-02-27 02:34:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Is there an option where it is completely Fastapi ?
Suparious,2024-02-27 05:37:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Maybe this is better for your use case: https://github.com/c0sogi/LLMChat
based on fastAPI, but without the OpenAI API layer
I just like tabbyAPI for running local models on exl2 quant and consuming them like you would OpenAI API"
halilergul1,2024-02-25 09:36:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hello Lalith,

I recently finetuned this model. I highly recommend you follow the format by which the creators of this model trained the model. As they state in their readme page, here is how you should organize the template for your custom dataset:

```
text = 
"" <s> [INST] What is your favorite condiment? [/INST]
Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen! </s>""
```


First, do not forget to add beginning (s) and end (/s) tokens in your prompt format. Then, between [INST]  and [/INST] tokens you should feed the text whatever your fine-tuned model is supposed to accomplish. So inside these tokens, you should give both your instruction and input. After [/INST], you should feed the response (your label or ground-truth or whatever you want your model to think and give as an output).

I hope this helps!"
Lalith16,2024-03-01 08:47:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Thank you for this , I have done this for my supervised learning dataset and tried fine-tuning but there are other problems wrt training loss and accuracy of results for smaller datasets under 5000 questions and answers."
shriML,2024-03-06 08:09:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@halilergul1  I have done finetuning following exactly what you have mentioned. What I noticed is both base instruct model and finetuned instruct model repeat repeat the instructions.  Here is an example:
""<s> [INST] write 5 points about India [/INST]"". It would repeat this and then begin the generation.

"
shriML,2024-03-06 08:10:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@Lalith16 
You can also try the below code and add more examples to see how the inbuilt chat template formats the datapoints.

from transformers import AutoModelForCausalLM, AutoTokenizer, MistralForCausalLM
tokenizer = AutoTokenizer.from_pretrained(model_path)
messages = [
    {""role"": ""user"", ""content"": ""What is your favourite condiment?""},
    {""role"": ""assistant"", ""content"": ""Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!""},
    {""role"": ""user"", ""content"": ""Do you have mayonnaise recipes?""},
    {""role"":""assistant"",""content"":""Not sure""},
    {""role"": ""user"", ""content"": ""Do you have mayonnaise recipes?""},
    {""role"":""assistant"",""content"":""Not sure!""}
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=""pt"")
print(tokenizer.decode(tokenized_chat[0]))

Output:
<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>  [INST] Do you have mayonnaise recipes? [/INST]Not sure</s>  [INST] Do you have mayonnaise recipes? [/INST]Not sure!</s>

"
halilergul1,2024-03-06 09:21:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hello @shriML,

1- First it is important to make sure you get infrences by following a correct method. I recommend using torch inference mode from pytorch. Like the following
    with torch.inference_mode():
        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.0001)

2- This repeating instructions was a common issue back then whan I was fine tuning LLAmA 1 with Alpaca Lora. Also I heard some other guys experiencing the same thing. The main rationale is the amount of data you trained on. For weights to be updated correctly you really need lots of data. There is no specific threshold for sure but I would recommend you should have at least 5000 data instance.

3- Another reasons I suspect is the target modules you update while using Lora or Qlora. Make sure to update recommended target modules by the community. I do not know your method of finetuning for sure (whether you use Lora or Adapter etc.), but this could be potential reason as well.

Hope this helps!"
shriML,2024-03-06 10:24:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hey @halilergul1 thanks for the response. I am currently doing complete model finetuning using Stanford Alpaca code via FSDP torchrun . 
1. I tried with inference_mode() but no improvement. Here is what I tried:
    with torch.inference_mode():
        generated_ids = model.generate(**model_inputs, 
                                    max_new_tokens=1000,
                                    do_sample=True,
                                    top_k=100, 
                                    top_p=0.95,
                                    no_repeat_ngram_size = 2,
                                    pad_token_id=tokenizer.eos_token_id,
                                    eos_token_id=tokenizer.eos_token_id,
                                    temperature=0.01 )

2. I do have around 5000 training data points. Not sure if the complete model training is causing the issue.
3. I had tried PEFT LoRA as well for Mistral Instruct and was updating the below target modules.  The training loss was quite close to zero but when prompted on training input it was nowhere close to the expected response.
    target_modules=[
        ""q_proj"",
        ""k_proj"",
        ""v_proj"",
        ""o_proj"",
        ""gate_proj"",
        ""up_proj"",
        ""down_proj"",
        ""lm_head"",
    ]

"
shriML,2024-03-06 10:55:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Also, any idea why base instruct model would repeat the instruction? Could you point me to any resources for fine tuning LLM? I found Stanford Alpaca code to be a great resource."
halilergul1,2024-03-27 11:29:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi again @shriML, so sorry for late reply as things are quite busy.

Well, I am not an expert for sure but here are my ideas:
1-  This means your problem has almost nothing to do with how you get inferences
2- Well it is always to your advantage to increase training points for such huge models. But if you do have at least 5000, then I do not think that lack of data may cause the problem. Also, which type of quantization do you use to load model parameters, and through which method do you update them? Sometimes this also may be the source of problems like you face.
3- It seems PEFT LoRA worked well in my case so it is interesting that it did not work well for you. Also, do not expect the performance of training (loss convergence, etc.) will be reciprocated with good model responses. Training and generating are quite different when it comes to LLMs. I also recommend you go back to your training dataset preparation step and go one by one whether the data and the way you feed your model during training with this data are in the appropriate formats because I experienced in my other finetuning attemps that if the data loading and format during training is not well organized then model either repeats itself like a parrot or it produces garbage.
"
ChasapasK,2024-04-02 11:20:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Hello Lalith,
> 
> I recently finetuned this model. I highly recommend you follow the format by which the creators of this model trained the model. As they state in their readme page, here is how you should organize the template for your custom dataset:
> 
> text = 
> [INST] What is your favorite condiment? [/INST]
> Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!
> 
> 
> First, do not forget to add beginning (s) and end (/s) tokens in your prompt format. Then, between [INST]  and [/INST] tokens you should feed the text whatever your fine-tuned model is supposed to accomplish. So inside these tokens, you should give both your instruction and input. After [/INST], you should feed the response (your label or ground-truth or whatever you want your model to think and give as an output).
> 
> I hope this helps!


Hello, can you please give an example dataset row with the beginning (s) and end (/s) tokens and the [INST],[/INST]. Also, if we are talking about a custom local dataset, should it be sequences of JSON objects?
For example, something liek this:

{""text"": ""<s>[INST] Question1 [/INST] Answer1 </s>""}
{""text"": ""<s>[INST] Question2 [/INST] Answer2  </s>""}
{""text"": ""<s>[INST] Question3 [/INST] Answer3  </s>""}"
halilergul1,2024-04-02 11:51:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > Hello Lalith,
> > 
> > I recently finetuned this model. I highly recommend you follow the format by which the creators of this model trained the model. As they state in their readme page, here is how you should organize the template for your custom dataset:
> > 
> > text = 
> > [INST] What is your favorite condiment? [/INST]
> > Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!
> > 
> > 
> > First, do not forget to add beginning (s) and end (/s) tokens in your prompt format. Then, between [INST]  and [/INST] tokens you should feed the text whatever your fine-tuned model is supposed to accomplish. So inside these tokens, you should give both your instruction and input. After [/INST], you should feed the response (your label or ground-truth or whatever you want your model to think and give as an output).
> > 
> > I hope this helps!
> 
> 
> Hello, can you please give an example dataset row with the beginning (s) and end (/s) tokens and the [INST],[/INST]. Also, if we are talking about a custom local dataset, should it be sequences of JSON objects?
> For example, something liek this:
> 
> {""text"": ""<s>[INST] Question1 [/INST] Answer1 </s>""}
> {""text"": ""<s>[INST] Question2 [/INST] Answer2  </s>""}
> {""text"": ""<s>[INST] Question3 [/INST] Answer3  </s>""}

Hi,

The QA structure you wrote as an example is how your data samples should look like. Let me give a basic example. Suppose you create a custom dataset regarding sentiment classification. Here is how a single data object should roughly look like:
```
{""text"": ""<s>[INST] Determine the sentiment type of the following sentence: I hate this movie! [/INST] Negative  </s>""}
```

As for your second question, what do you mean exactly by saying ""sequences of JSON objects""?. You can store it in whatever type you like such as dataframe, dictironary or json. BUT, the truth of the matter is when you feed your data row/sample/object into your training model, then you must feed it like string/text objects via sth like dataloader iteratively."
ChasapasK,2024-04-02 13:08:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > > Hello Lalith,
> > > 
> > > I recently finetuned this model. I highly recommend you follow the format by which the creators of this model trained the model. As they state in their readme page, here is how you should organize the template for your custom dataset:
> > > 
> > > text = 
> > > [INST] What is your favorite condiment? [/INST]
> > > Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!
> > > 
> > > 
> > > First, do not forget to add beginning (s) and end (/s) tokens in your prompt format. Then, between [INST]  and [/INST] tokens you should feed the text whatever your fine-tuned model is supposed to accomplish. So inside these tokens, you should give both your instruction and input. After [/INST], you should feed the response (your label or ground-truth or whatever you want your model to think and give as an output).
> > > 
> > > I hope this helps!
> > 
> > 
> > Hello, can you please give an example dataset row with the beginning (s) and end (/s) tokens and the [INST],[/INST]. Also, if we are talking about a custom local dataset, should it be sequences of JSON objects?
> > For example, something liek this:
> > 
> > {""text"": ""<s>[INST] Question1 [/INST] Answer1 </s>""}
> > {""text"": ""<s>[INST] Question2 [/INST] Answer2  </s>""}
> > {""text"": ""<s>[INST] Question3 [/INST] Answer3  </s>""}
> 
> Hi,
> 
> The QA structure you wrote as an example is how your data samples should look like. Let me give a basic example. Suppose you create a custom dataset regarding sentiment classification. Here is how a single data object should roughly look like:
> {""text"": ""<s>[INST] Determine the sentiment type of the following sentence: I hate this movie! [/INST] Negative  </s>""}
> 
> As for your second question, what do you mean exactly by saying ""sequences of JSON objects""?. You can store it in whatever type you like such as dataframe, dictironary or json. BUT, the truth of the matter is when you feed your data row/sample/object into your training model, then you must feed it like string/text objects via dataloader like-thing iteratively.

Ok Perfect regarding my second question you are tottaly right, thanks.

""BUT, the truth of the matter is when you feed your data row/sample/object into your training model, then you must feed it like string/text objects via dataloader like-thing iteratively.""
 Maybe this is what im doing wrong, if you could share  a related code block for mistral or any other model to see if we are on the same page, it would be very helpfull."
halilergul1,2024-05-04 11:30:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Any code block would be meaningless for your case. There is hardly any one method that may possibly fits for all the cases. But as a rule of thumb, the dataset instance/object of huggingface is quite useful and I utilized that one.  Check this out: https://huggingface.co/docs/datasets/v2.2.1/en/access"
xkszltl,2024-02-03 03:33:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"My understanding is `</s>` marks the end of a response, or we don't know when to stop during inference.
`</s>`"
ybelkada,2023-12-20 08:58:25+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi @yardenhoch 

Thanks for the issue! You can manually set a new value for `sliding_window` in model's config. If you have cloned the repo locally you can modify the config file manually, otherwise you can do `model.config.sliding_window = xxx` before launching training"
yardenhoch,2023-12-20 09:45:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@ybelkada Thank you for your response. 

To clarify, does this mean that even if my prompt is longer than 4096 tokens, all the words will be processed together when `sliding_window` is set to `null`? I'm trying to understand the implications of this setting on longer prompts."
ybelkada,2023-12-20 09:57:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi @yardenhoch 

> To clarify, does this mean that even if my prompt is longer than 4096 tokens, all the words will be processed together when sliding_window is set to null?

I think so yes! all tokens will be processed together in case sliding_window is set to null"
issa130,2023-12-14 21:05:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"when you change the slinding_window to null the model became null too,please change it back to 4096
"
TimeRobber,2023-12-14 22:42:13+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,It is set to 4k.
ArthurZ,2023-12-13 08:56:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Could you specify which demo? Are you loading in float16, bfloat16? You should use `accelerate` with `device_map = ""auto""` to overcome potential RAM issues. It should fit in 24GB "
logan39522361tq,2023-12-13 09:04:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Could you specify which demo? Are you loading in float16, bfloat16? You should use `accelerate` with `device_map = ""auto""` to overcome potential RAM issues. It should fit in 24GB

using this demo:
![image.png](https://cdn-uploads.huggingface.co/production/uploads/65796f107f5da1deb6b21338/2hJwU35CwUY8_kRAwnBZ7.png)

config.json file is:
""torch_dtype"": ""bfloat16""

device_map = ""auto""
using this in start commond or some where?
"
ArthurZ,2023-12-13 09:07:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Maybe try to reduce the number of tokens that are generated, 1000 seems like a lot. Try with a smaller number like 40 and go from there"
logan39522361tq,2023-12-13 09:11:00+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Maybe try to reduce the number of tokens that are generated, 1000 seems like a lot. Try with a smaller number like 40 and go from there

It's doesnt help, the error appears at <model.to(device1)>"
ArthurZ,2023-12-13 09:12:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Then try load in 8 bits or use accelerate and device_map = ""auto"" :hug: "
ybelkada,2023-12-13 09:16:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@logan39522361tq 
Can you try to load the model as such:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"", torch_dtype=torch.float16, device_map=""auto"")
...
```

The snippet you shared will load the model in full precision (28GB), hence the GPU error you get

Alternatively you can also do:

```python
# pip install bitsandbytes
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"", load_in_4bit=True)
...
```"
logan39522361tq,2023-12-14 12:37:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I used this to init model
<model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"", torch_dtype=torch.float16, device_map=""auto"")>

but  when I run 
<model.to(device)>
it turns out this error(using python virtual env):
lib/python3.10/site-packages/accelerate/big_modeling.py"", line 425, in wrapper
    raise RuntimeError(""You can't move a model that has some modules offloaded to cpu or disk."""
ybelkada,2023-12-14 12:39:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"this is because device_map=""auto"" has automatically offloaded your model into cpu or disk. What is your GPU total VRAM?"
logan39522361tq,2023-12-14 12:40:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> this is because device_map=""auto"" has automatically offloaded your model into cpu or disk. What is your GPU total VRAM?

NVIDIA GeForce RTX 3090 GPU with 24GBRAM"
ybelkada,2023-12-14 12:41:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"the model has 44B parameters (you need ~90GB VRAM to fit your GPU in half-precision), it will not fit into your GPU. Please consider running the model in 4bit precision - or use cpu / disk offloading at the risk of not being able to call `model.to(device)`"
logan39522361tq,2023-12-16 03:32:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"code:
model = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True)
config.json:
""torch_dtype"": ""bfloat16"",

it turns out error:
`model.to(device)` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
"
ybelkada,2023-12-18 19:19:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"loading the model with quantization will automatically dispatch the model in the available devices, hence there is no need to call .to as it will also create issues with offloading as well
"
monu0112,2024-04-09 08:18:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"

![image.png](https://cdn-uploads.huggingface.co/production/uploads/65d635a575984275099f7ad9/ZhsM8I5G_WBjyv76aHnhE.png)

I am getting this error if anyone can help me
"
ArthurZ,2023-12-13 08:59:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hey , are you sure you have the latest version of transformers. `transfo_xl` was deprecated "
joy2000,2023-12-31 08:43:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"same error,

did
!pip install -q -U transformers

still same error"
nandakishorebm,2024-01-20 17:26:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Also the same... did anyone find the solution?
chujiezheng,2023-12-11 19:39:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,It is released now.
amgadhasan,2023-12-11 14:21:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"You can use a gpu that has at least 16GB of RAM for deploying this model.

You can go with an A10040GB for higher speed and more concurrency"
rikomi7571,2023-12-12 03:51:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This comment has been hidden
habanoz,2023-12-10 09:50:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Looks weird to me. Is it natural to have multiple end-of-sentence token?

```python
print(tokenizer.apply_chat_template([{'role':'user','content':'inst1'},{'role':'assistant','content':'aswer1'}, {'role':'user','content':'inst2'},{'role':'assistant','content':'answ2'}],tokenize=False,add_generation_prompt=False))
<s>[INST] inst1 [/INST]aswer1</s> [INST] inst2 [/INST]answ2</s> 
```

Regards
"
SalmanFaroz,2023-12-10 11:33:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@habanoz In the example, the content was intended to be assigned to a variable named ""text"" in its whole, for demonstrating the chat template of Mistral ,  but several quotes were mistakenly added. "
raynWest,2023-11-20 09:54:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I am using LangChain for retrieval QA with source, and I am using Mistral as an LLM. However, I found that I cannot specify the output format of the source, as I mentioned above. Can Mistral do such a thing, and what kind of prompt should I use?"
janakaSteph,2023-11-30 12:21:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,I am also using LangChain for retrieval QA with Mistral 7B and it ignores my request of formatting the answer in Markdown.
Hyphonical,2023-11-25 21:17:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"This is a normal thing for almost every ai chatbot. the bot has been given tons of precalculated formulas, it doesnt comprehend to be like ""oh now i have to calculate this"" it just searches similar math examples in its given data (like 10 trillion of those) and gives the output from that data. I hope this helps."
andysalerno,2023-11-19 21:27:32+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I'm not an expert, nor am I associated with Mistral :D But I don't think Mistral (or GPT-4, or GPT-3.5) could handle this prompt the way you are describing it, since it involves counting characters, and LLMs infamously do not understand characters well at all. Their view of the world is tokens.

I just asked ChatGPT 3.5:
> reverse the string ""Even gpt 3.5 can't reverse strings very well""

with the response:

> Sure, here is the reversed string:
>
> ""llew yrev sgnirver t'nac 5.3 tpg nevE""

I asked ChatGPT 4, and amazingly it got it right, and for a second I thought they somehow upgraded ChatGPT to understand on a character-by-character basis. But then I saw the little ""analysis"" button and clicked it and realized that ChatGPT decided to execute Python to reverse the string :D"
KeithCu,2023-11-19 23:39:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"GPT 3.5 and 4, and Claude (usually) handle it without any problems. However, nearly every other one screws it up! It's like I found a bug in the entire LLM world ;-)

They definitely understand characters from any programming experience, but I think this sort of stuff is outside their training. Usually it is to write code to calculate where a word is in a sentence, but in this case, when it's changing part of a sentence, I need it to be able to tell me where.

It's possible to get LLMs to use Python to calculate word indexes, but if there are multiple uses of the same word in a sentence, it could more confusing to figure out what it is talking about.

Otherwise I'm just so impressed at Mistral's ability to be a helper for an app like LibreOffice (which has ~100M users) without any fine-tuning.

I did find a workaround is to have it generate a new complete sentence, and then the app figures out what to change.

﻿Please perform a grammar check on the sentence at the end and return the simplest error you find. 
Describe the problem, and create an improved sentence to fix that error.
Put into a JSON format so it can be easily loaded and used by others. 
Sentence for Grammar Check: ""She go to the store."" 

If I just have it (hopefully without hallucination!) return the entire improved sentence, the app can figure out what minimal characters to replace. (You would never replace the entire sentence if only some of it changed because it could have different formatting in different spots.)

With that change, it generally seems to do a pretty great job as a grammar checker. 

LibreOffice has integrated Python support, here's a simple Python grammar checker that plugs into LibreOffice and could be used as a baseline in case anyone wants to work on it: https://cgit.freedesktop.org/libreoffice/lightproof/

"
eliseobao,2023-11-16 09:57:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Official format:

```
<s>[INST] System Prompt + Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]
```

Please note that _the system prompts appears only before the first turn and not in suebsequent turns._

[Source](https://docs.mistral.ai/usage/guardrailing/#appendix)."
mitjamartini,2023-11-16 10:57:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Currently need Mistral to generate predictable JSON output based on certain user prompts.

Have a look at this article: [How to use Mistral AI Instruct model to generate structured content similar to Open AI functions](https://advanced-stack.com/resources/how-to-use-mistral-ai-instruct-model-to-generate-structured-content-similar-to-open-ai-functions.html)"
barunsaha,2023-11-19 09:45:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > Currently need Mistral to generate predictable JSON output based on certain user prompts.
> 
> Have a look at this article: [How to use Mistral AI Instruct model to generate structured content similar to Open AI functions](https://advanced-stack.com/resources/how-to-use-mistral-ai-instruct-model-to-generate-structured-content-similar-to-open-ai-functions.html)

This looks nice, especially the promptless approach."
barunsaha,2023-11-19 09:54:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Currently need Mistral to generate predictable JSON output based on certain user prompts.

In the past, I did it using Llama 2, which worked great. I have the detailed schema described in the prompt. 

Just yesterday I switched to Mistral.  I see that the JSON is still generated following the schema. However, there also appears to be the same, unstructured content generated again after the JSON ends. For now, I just consider the content between the triple backticks (```).

Here's the template if you want to have a look: https://huggingface.co/spaces/barunsaha/slide-deck-ai/blob/main/langchain_templates/template_combined.txt"
mitjamartini,2023-11-20 11:52:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Very nice template, indeed :) thank you for sharing."
luissimoes,2023-11-22 08:49:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Thanks for Sharing!
RonanMcGovern,2023-12-08 12:23:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,I also see the first issue when I try to inference with more than the sliding window length. I'm unsure why this is the case.
Coinman,2023-11-03 14:10:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Also seems quite often the model is sending me: '""</s>""' at the end of a response ? "
unixguru2k,2023-12-01 04:45:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,might help: https://github.com/machaao/mistral-7b-chatbot 
xceman,2023-12-22 02:27:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I encountered the same issue when I tried to fine-tune it, and wondered how it is supposed to set? Thanks!"
ingo-m,2023-12-22 19:45:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"There's also a question about it on reddit:
https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/

I'm wondering whether it even matter what's the padding token as long as it is masked out with the attention mask?"
Harsh,2023-11-26 01:51:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Many popular LLMs are trained on publicly available web data which is predominantly English text. 

Due to this the tokenizer learned on the pre-training data will not represent other languages well. 

I have similar requirement for Indic languages. 

"
narai,2023-11-02 18:41:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"If the question is such that the answers are a kind of mutually exclusive multiple choice answer, with a single token answer, then you can get the token probability of the answer. This is available in the API, but not the python API. You can request the top K highest probability tokens and look at the probability of the generated answer token."
narai,2023-11-02 18:52:17+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"The python API has this in the old openai.Completion interface, which works with some older models only.
            response = openai.Completion.create(
                model=""text-davinci-002"",
                prompt=prompt,
                max_tokens=7,
                logprobs=5,
                temperature=0
                )"
narai,2023-11-07 00:38:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Oops you can ignore my comments about the openai API. You can get the token probabilities for the mistral huggingface output.
zolicsaki,2023-11-10 18:46:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,I would be super interested in learning about what instruction tuning datasets they used! Since they claim it is all open source it would be great if they could release this!
iukea,2023-12-24 07:15:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Damn I am not the only one with this question 
lunarflu,2023-10-28 07:24:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Thanks for reporting @michaelrhanson ! <3
Nadav-Timor,2023-10-27 21:27:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,See [this](https://github.com/mistralai/mistral-src/issues/53) GitHub Issue by @ParadoxZW from a few days ago
Rocketknight1,2023-10-18 13:18:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"cc @teven @LucileSaulnier to this one! I wrote the chat template based on the example in the model card, but it seems like that might differ from the example on the website. Can you confirm which template is correct?"
Mel-Iza0,2023-10-17 20:21:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"*Update* <br>
The issue was resolved with the use of this configuration after applying Peft transformations:
```
model.gradient_checkpointing_enable()
model.enable_input_require_grads()
```"
808Code,2023-10-16 14:10:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Pass this to llm  ``` <s>[INST] Using this information : {context} answer the Question : {query} [/INST]  ``` ,  you can look into prompt templating , through langchain too if you haven't.
"
gxxxz,2023-10-16 16:52:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This comment has been hidden
gxxxz,2023-10-16 16:52:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@808Code Tysm

"
gxxxz,2023-10-16 16:57:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Btw for future peeps, below also sets the context inside the tags. 

 {""role"": ""system"", ""content"": ""You are 8 years old""},
 {""role"": ""user"", ""content"": ""How old are you?""},
"
Rocketknight1,2023-10-16 18:16:32+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"You can try something like this:

```python
messages = [
{""role"": ""system"", ""content"": ""You are a helpful bot who reads texts and answers questions about them.""},
{""role"": ""user"", ""content"": ""[text] QUESTION: [question]""},
]
input = tokenizer.apply_chat_template(messages)
answer = model.generate(**{key: tensor.to(model.device) for key, tensor in input.items()})
```

In general, there are lots of ways to do this and no single right answer - try using some of the tips from [OpenAI's prompt engineering handbook](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api), which also apply to other instruction-following models like Mistral-Instruct. Finally, you may have better luck with [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha), which is also based on Mistral-7B but was trained to follow instructions with more advanced methods."
gxxxz,2023-10-18 04:49:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,@Rocketknight1 Thanks
adam12104,2023-11-09 00:10:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@Rocketknight1 I tried your suggestion and got the following error jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...

I've added my python code below to see if you can see anything out of the ordinary with it.

from transformers import AutoModelForCausalLM, AutoTokenizer

device = ""cpu""  # Use GPU if available, otherwise use CPU

model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")

messages = [
    {""role"": ""system"", ""content"": ""You are a hunan who loves to dance.""},
    {""role"": ""user"", ""content"": ""What do you like to do in your spare time?""}
]

encodeds = tokenizer.apply_chat_template(messages)

model_inputs = encodeds.to(device)
model.to(device)

generated_ids = model.generate(**{key: tensor.to(model.device) for key, tensor in input.items()})
decoded = tokenizer.batch_decode(generated_ids)
print(decoded[0])

"
Rocketknight1,2023-11-09 14:27:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Hi @adam12104 this is a known issue caused by your version of `jinja` being out of date. Try `pip install --upgrade jinja2`. We'll be adding version checks in the next update to `transformers` so that this stops happening!
harpercarroll,2023-11-15 22:44:58+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hmm, still getting that error about how conversation roles must alternate, and it seems my jinja2 is up-to-date"
wahyu-adi-n,2023-11-21 03:41:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"You can follow these format on your chat template (user first, then followed by assistant): 

messages = [
    {""role"": ""user"", ""content"": ""Hey there!""},
    {""role"": ""assistant"", ""content"": ""Nice to meet you!""}
]
"
atharvamungee10,2023-11-29 07:41:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"My prompt is 
 human_input = B_INST + f""""""
        Reminder: {reminder}
        Process steps: {process_steps}
        Query: {query}
        Answer:<Insert your answer here>
        """""" + E_INST
And my chatprompttemplate in langchain is 
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content=system_template),  # The persistent system prompt
    MessagesPlaceholder(variable_name=""chat_history""),  # Where the memory will be stored.
    ChatPromptTemplate.from_template(""{human_input}""),  # Where the human input will be injected
])


The response it generates has a prefix of AI: AI: 
When in a conversation session 
Let us say i have passed in 5 queries the answer to the 5th query will have a prefix of AI: AI: AI: AI: AI:

Is this a prompt problem?
"
navidmadani,2023-12-19 21:28:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I run into the same problem as well. The problem is with the system message there. When I remove the system message I won't get the `jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...` error anymore..

[UPDATE]
okay, according to the tokenizer config [here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json), the chat template does not  support system messages. So I'm wondering how the model was trained for supporting system messages and how should we use it?"
cyt79,2024-01-04 13:43:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@navidmadani Did you find an answer to your question?
"
celsowm,2024-01-11 14:28:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"this worked for me:

```
messages = [
    {
        ""role"": ""user"",
        ""content"": ""You are a chatbot who always responds in Portuguese"",
    },
    {
        ""role"": ""assistant"",
        ""content"": ""Entendido! Responderei sempre na língua portuguesa!"",
    },
    {
        ""role"": ""user"",
        ""content"": ""Qual é maior: sol ou a terra?"",
    }
]

print(tokenizer.apply_chat_template(messages, tokenize=False))
```"
MaziyarPanahi,2024-01-12 08:33:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"This is what HuggingChat is using:


```
{
    ""name"" : ""mistralai/Mixtral-8x7B-Instruct-v0.1"",
    ""description"" : ""The latest MoE model from Mistral AI! 8x7B and outperforms Llama 2 70B in most benchmarks."",
    ""websiteUrl"" : ""https://mistral.ai/news/mixtral-of-experts/"",
    ""preprompt"" : """",
    ""chatPromptTemplate"": ""<s> {{#each messages}}{{#ifUser}}[INST]{{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}} {{content}} [/INST]{{/ifUser}}{{#ifAssistant}} {{content}}</s> {{/ifAssistant}}{{/each}}"",
    ""parameters"" : {
      ""temperature"" : 0.6,
      ""top_p"" : 0.95,
      ""repetition_penalty"" : 1.2,
      ""top_k"" : 50,
      ""truncate"" : 24576,
      ""max_new_tokens"" : 8192,
      ""stop"" : [""</s>""]
    }
```
So they consider the System Prompt as the first message separated via `\n`:

```
<s> [INST]Previous conversation context or system prompt
 How's the weather today? [/INST] It's sunny and warm.
<s> [INST] What about tomorrow? [/INST] Tomorrow is expected to be cloudy.</s> 
```

This is how it is in `text-generation-webui`, which reuses the same thing from Mistral:

```
.*(mistral|mixtral).*instruct:
  instruction_template: 'Mistral'
```
```
instruction_template: |-
  {%- for message in messages %}
      {%- if message['role'] == 'system' -%}
          {{- message['content'] -}}
      {%- else -%}
          {%- if message['role'] == 'user' -%}
              {{-' [INST] ' + message['content'].rstrip() + ' [/INST] '-}}
          {%- else -%}
              {{-'' + message['content'] + '</s>' -}}
          {%- endif -%}
      {%- endif -%}
  {%- endfor -%}
  {%- if add_generation_prompt -%}
      {{-''-}}
  {%- endif -%}
```

and some users in `SillyTavern` are using the same Llama-2 format:

```
[INST] <<SYS>>
Write character's next reply.
<</SYS>>

Character card
</s><s>[INST] {prompt} [/INST] {response} </s><s>[INST] {prompt} [/INST] etc.
```

I personally tried all 3, in some cases I got better results with `Llama-2` format for some reasons! I wish we had a good evaluation just for a System Prompt to see which formats does a better job."
ghostytheoddflower,2024-02-17 18:54:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Has anyone gotten any clarity on this, how to properly include a system prompt with tokenizer / apply_chat_template? "
gxxxz,2024-02-20 09:28:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Has anyone gotten any clarity on this, how to properly include a system prompt with tokenizer / apply_chat_template?

You can use something like this

messages = [
{""role"": ""system"", ""content"": ""You are 8 years old""}, 
]
input = tokenizer.apply_chat_template(messages)"
capnchat,2024-03-20 17:50:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"apply_chat_template() does not work with role type ""system"" for mistral's tokenizer as pointed out above.

The way we are getting around this is having two messages at the start to mimic a system prompt, which introduces roles at the start of the conversation. And we keep these two messages at the start of the message history even after the context length is exceeded and we start to omit earlier messages between the user and chatbot, just like a system prompt is supposed to work.

Example:

[
{""role"": ""user"", ""content"": ""Hi, ChatbotName! I'm ExampleUser.""},
{""role"": ""assistant"", ""content"": ""Hi, ExampleUser! I'm ChatbotName, here to <introduce role and purpose of the chatbot!>.""},
{""role"": ""user"", ""content"": ""<first prompt here!>""}
]"
weissenbacherpwc,2024-04-03 10:11:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> My prompt is 
>  human_input = B_INST + f""""""
>         Reminder: {reminder}
>         Process steps: {process_steps}
>         Query: {query}
>         Answer:<Insert your answer here>
>         """""" + E_INST
> And my chatprompttemplate in langchain is 
> prompt = ChatPromptTemplate.from_messages([
>     SystemMessage(content=system_template),  # The persistent system prompt
>     MessagesPlaceholder(variable_name=""chat_history""),  # Where the memory will be stored.
>     ChatPromptTemplate.from_template(""{human_input}""),  # Where the human input will be injected
> ])
> 
> 
> The response it generates has a prefix of AI: AI: 
> When in a conversation session 
> Let us say i have passed in 5 queries the answer to the 5th query will have a prefix of AI: AI: AI: AI: AI:
> 
> Is this a prompt problem?

same for me, did you find a way to prevent this?"
ingo-m,2023-11-03 10:52:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Looks like out of the box Mistral doesn't have a `pad_token_id`? 🤔

I don't understand why, surely they must have used padding during training?"
Hannibal046,2024-03-01 06:39:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"also curious about this..
![image.png](https://cdn-uploads.huggingface.co/production/uploads/5f9fa360a13e063b8b2b5c0c/YOZl1l-3I4WEpbq-5tZaW.png)
"
haiderasad,2024-03-21 09:07:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,any updates on this @Hannibal046  @parikshit1619  ? mistral was originally trained with left side padding and after doing a bit of research most forums are supporting left side as well so that LLM has no mixup of data and pad tokens. Can anybody confirm this
guluguluguluhihi,2024-05-28 15:10:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Maybe we need two tokenizers?
My understanding is that ""padding_side=left"" is needed when we generate the output, because of Mistral is ""decoder-only"", see this : https://huggingface.co/docs/transformers/llm_tutorial#wrong-padding-side. 

But when using lora for training, ""padding_side=right"" is needed to avoid overflow issues, see this : https://discuss.huggingface.co/t/qlora-with-gptq/58009

Please correct me if I'm wrong!"
Ayush8120,2023-10-10 14:22:43+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"For unquantized model, you would be needing 16 GB memory while loading the model shards. "
LucileSaulnier,2023-10-09 11:04:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Thank you
AyouKha,2023-10-13 10:14:55+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@codegood Have you found a fix? I have been struggling with the same error on Azure ML Studio. Thanks
"
codegood,2023-10-13 20:31:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"No, not yet. Looking to get some update from the developers."
omidai,2023-10-14 19:58:17+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,in your bnb_config remove  load_in_8bit=true. or whatever bit you are trying to use. 
codegood,2023-10-16 01:08:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@omidai then the model will be too big to load. That's the power of BnB. 
"
omidai,2023-10-16 07:28:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"correct, in my case, I just started with a fresh environment and installed all packages again then It was resolved. It seems there is a hidden package dependency. "
AyouKha,2023-10-16 08:11:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@omida I've attempted this a few times using the latest packages, but it still doesn't seem to work for me. Could you please share the packages versions you're using?"
lysandre,2023-10-16 18:12:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,cc @ybelkada
ybelkada,2023-10-16 19:32:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"hi everyone, 
To benefit from quantization `load_in_8bit` or `load_in_4bit` , please make sure to install the latest bitsandbytes package:

```bash
pip install -U bitsandbytes
```

I would also advise to use `load_in_4bit` for a more memory efficient inference. Let us know how that goes"
AyouKha,2023-10-17 16:08:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I've attempted today again to install the latest package versions (accelerate==0.23.0 and bitsandbytes==0.41.1), but it still doesn't seem to be working, neither in 4-bit nor in 8-bit."
ybelkada,2023-10-18 01:28:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi @Ayoba 
I see OK - can you share a full reproducible snippet here?"
sukkritsharma,2023-10-18 10:21:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi @ybelkada, I'm also facing the same issue. Please help with the fix :

Steps to recreate: 

```bash
!pip install -U bitsandbytes
!pip install -U git+https://github.com/huggingface/transformers.git
!pip install -U git+https://github.com/huggingface/peft.git
!pip install -U git+https://github.com/huggingface/accelerate.git
!pip install -U datasets scipy ipywidgets matplotlib
```
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = ""mistralai/Mistral-7B-v0.1""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)
```

ERROR : 
```bash
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[1], line 12
      4 base_model_id = ""mistralai/Mistral-7B-v0.1""
      5 bnb_config = BitsAndBytesConfig(
      6     load_in_4bit=True,
      7     bnb_4bit_use_double_quant=True,
      8     bnb_4bit_quant_type=""nf4"",
      9     bnb_4bit_compute_dtype=torch.bfloat16
     10 )
---> 12 model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)

File ~/transformers/src/transformers/models/auto/auto_factory.py:565, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    563 elif type(config) in cls._model_mapping.keys():
    564     model_class = _get_model_class(config, cls._model_mapping)
--> 565     return model_class.from_pretrained(
    566         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    567     )
    568 raise ValueError(
    569     f""Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n""
    570     f""Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.""
    571 )

File ~/transformers/src/transformers/modeling_utils.py:2634, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   2632 if load_in_8bit or load_in_4bit:
   2633     if not (is_accelerate_available() and is_bitsandbytes_available()):
-> 2634         raise ImportError(
   2635             ""Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of""
   2636             "" bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or""
   2637             "" pip install bitsandbytes` ""
   2638         )
   2640     if torch_dtype is None:
   2641         # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`
   2642         logger.info(
   2643             f""Overriding torch_dtype={torch_dtype} with `torch_dtype=torch.float16` due to ""
   2644             ""requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. ""
   2645             ""Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass""
   2646             "" torch_dtype=torch.float16 to remove this warning.""
   2647         )

ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` 
```"
ybelkada,2023-10-18 18:09:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi @sukkritsharma 

I just ran :

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = ""mistralai/Mistral-7B-v0.1""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(base_model_id)

text = ""Hello my name is""
input_ids = tokenizer.encode(text, return_tensors=""pt"").to(0)

output = model.generate(input_ids, do_sample=False, max_length=10)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```
On a fresh new environment and it seemed to work fine on my end. Can you re-try everything on a fresh new environment?"
Ink,2023-11-24 20:00:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"On Windows, I had the same problem. If you try:

```
torch.cuda.is_available()
```

 it would show as False because the cuda version it needs was different from the cuda version that pytorch uses.

Hence, I found this solution from here: https://github.com/TimDettmers/bitsandbytes/issues/793#issuecomment-1806979077
 
Instead of installing bitsandbytes from the latest pip package, install it using the following command:

```
python.exe -m pip install https://github.com/TimDettmers/bitsandbytes/releases/download/0.41.0/bitsandbytes-0.41.0-py3-none-any.whl
```

Hope this helps
"
techandy42,2023-10-08 02:50:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"# Try the following

sys_prompt = ""You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.""
prompt = ""What kind of task would test someone's ability to perform physical reasoning?""

prefix = ""<|im_start|>""
suffix = ""<|im_end|>\n""
sys_format = prefix + ""system\n"" + sys_prompt + suffix
user_format = prefix + ""user\n"" + prompt + suffix
assistant_format = prefix + ""assistant\n""
input_text = sys_format + user_format + assistant_format

messages = [
    {""role"": ""user"", ""content"": input_text},
]

encodeds = tokenizer.apply_chat_template(messages, return_tensors=""pt"")

model_inputs = encodeds.to(device)
model.to(device)

generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)
decoded = tokenizer.batch_decode(generated_ids)
print(decoded[0])"
pvbhanuteja,2023-10-12 19:39:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Use this

<s>[INST] System Prompt + Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]

From their official site. 

https://docs.mistral.ai/usage/guardrailing

If you want it for gradio. I wrote a formatting function. Remove/add accordingly. 

```python
def format_chat_prompt_mistral(message: str, chat_history, instructions: str) -> str:
    if len(chat_history) == 0:
        # If chat_history is empty, return instructions and message
        prompt = f""<s>[INST] {instructions} Hi [/INST] Hello! how can I help you</s>[INST] {message} [/INST]""
        print(""sending this prompt\n==============\n"",prompt,'\n---------\n')
        return prompt
    else:
        # Initialize chat history text with the first user message and instructions
        user_message, bot_message = chat_history[0]
        chat_history_text = f""<s>[INST] {instructions} {user_message} [/INST] {bot_message}</s>""

        # Use a list comprehension to build the rest of the chat history text
        chat_history_text += """".join(f""[INST] {user_message} [/INST] {bot_message}</s>"" for user_message, bot_message in chat_history[1:])
"
chujiezheng,2023-12-21 20:08:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,I collected official chat templates in [this repo](https://github.com/chujiezheng/chat_templates). You may use it with the `apply_chat_template` method
iNeverLearnedHowToRead,2023-10-06 01:04:53+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"It's not as good as OpenAssistant/oasst-sft-6-llama-30b

but it's 2nd best"
erikbeltran,2023-10-05 23:59:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"someone can close this? seems a spam from the same guy under the medium paywall. 
(https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/27)"
QendelG,2023-10-05 05:14:00+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This will help you: https://medium.com/@qendelai/bye-bye-llama-2-mistral-7b-is-taking-over-get-started-with-mistral-7b-instruct-1504ff5f373c
LucileSaulnier,2023-10-06 17:33:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Thank you! 
harpercarroll,2023-10-02 18:09:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,see if this helps: https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/23#6518110742097d8c5907f822
Tapendra,2023-10-02 19:24:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@harpercarroll What is the minimum requirement of System RAM, GPU RAM to inference the bitsandbytes nf4 ,use_4bit = True train model.  I train  model of bnb_4bit_quant_type = ""nf4"" after training model size is same like 15GB. How we can reduce model Size ?
"
QendelG,2023-10-05 05:15:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This will help: https://medium.com/@qendelai/fine-tuning-mistral-7b-instruct-model-in-colab-a-beginners-guide-0f7bebccf11c
QendelG,2023-10-05 05:14:36+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This might help: https://medium.com/@qendelai/bye-bye-llama-2-mistral-7b-is-taking-over-get-started-with-mistral-7b-instruct-1504ff5f373c
sakshat98,2023-10-05 15:58:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,I am looking for the same. Please update once you find something.
pvbhanuteja,2023-10-12 19:40:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@sakshat98  @TikaToka 

Use this

<s>[INST] System Prompt + Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]

From their official site. 

https://docs.mistral.ai/usage/guardrailing

If you want it for gradio. I wrote a formatting function. Remove/add accordingly. 

```python
def format_chat_prompt_mistral(message: str, chat_history, instructions: str) -> str:
    if len(chat_history) == 0:
        # If chat_history is empty, return instructions and message
        prompt = f""<s>[INST] {instructions} Hi [/INST] Hello! how can I help you</s>[INST] {message} [/INST]""
        print(""sending this prompt\n==============\n"",prompt,'\n---------\n')
        return prompt
    else:
        # Initialize chat history text with the first user message and instructions
        user_message, bot_message = chat_history[0]
        chat_history_text = f""<s>[INST] {instructions} {user_message} [/INST] {bot_message}</s>""

        # Use a list comprehension to build the rest of the chat history text
        chat_history_text += """".join(f""[INST] {user_message} [/INST] {bot_message}</s>"" for user_message, bot_message in chat_history[1:])
"
Rocketknight1,2023-10-16 18:24:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi all, the `tokenizer.apply_chat_template()` method can handle this for you:

```python
messages = [
    {""role"": ""system"", ""content"": ""System message here""},
    {""role"": ""user"", ""content:"" ""User message here""}
]
prompt = tokenizer.apply_chat_template(messages)
```

You can see the [documentation on chat templates](https://huggingface.co/docs/transformers/main/chat_templating) for more information."
TikaToka,2023-10-17 13:56:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@Rocketknight1 @pvbhanuteja  Thank you for sharing the answer!
But I also want to ask how for huggingface inference api?"
pvbhanuteja,2023-10-18 16:28:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Hi all, the `tokenizer.apply_chat_template()` method can handle this for you:
> 
> ```python
> messages = [
>     {""role"": ""system"", ""content"": ""System message here""},
>     {""role"": ""user"", ""content:"" ""User message here""}
> ]
> prompt = tokenizer.apply_chat_template(messages)
> ```
> 
> You can see the [documentation on chat templates](https://huggingface.co/docs/transformers/main/chat_templating) for more information.

This is incorrect.  tokenizer.apply_chat_template will look chat_template key value from here at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/main/tokenizer_config.json 

The format at chat_template doesn't incline with the actual mistral website. https://docs.mistral.ai/usage/guardrailing I will try to open a PR and make changes to the chat template accordingly. "
AISec,2023-10-06 18:44:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I am also having this issue. Anyone able to solve this?

Edit 1: I found this link saying that it does not work with TGI 1.0.3 - https://discuss.huggingface.co/t/mistral-ai-sagemaker-deployment-failing/57379/2

Edit 2: Updating to 1.1.0 worked. If you are using the AWS public ECR, they have 1.1.0 on there. (e.g. 763104351884.dkr.ecr.eu-west-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04)"
Hawks101,2023-10-01 19:38:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,First of all thanks for this ! I But we need to purchase OpenAI credit points for this right? I am a beginner.
deleted,2023-10-01 20:47:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Its compatible with, not running at.  Unsure if HF API is free or not, but it woudl be their charges, not OpenAI"
limcheekin,2023-10-02 00:29:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"It is free of charge.
But I think HF definitely have a cap on number of requests can be made to the free tier HF spaces per hour or per day. Anyone here know the cap? 

That's the reason your support is important.

Alternatively, you can duplicate the space and run your own instance for free. "
toanbft,2023-10-02 07:26:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. What is embeddings are fine tuned of  mistralai/Mistral-7B-Instruct-v0.1"
Hawks101,2023-10-02 18:42:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> It is free of charge.
> But I think HF definitely have a cap on number of requests can be made to the free tier HF spaces per hour or per day. Anyone here know the cap? 
> 
> That's the reason your support is important.
> 
> Alternatively, you can duplicate the space and run your own instance for free.

Thank you so so much for this man ! Appreciated. I used this to make an app for question answering a pdf. Just wanted to ask that when i use this model through this space; on which api does this bounce back from exatcly? Is it Openai? or the huggingface's api for this model? Coz HF's api wasn't giving the full ouput (only around 10 tokens) when used through langchain. And if its openai then how is it free ? Please help. I am a beginner."
mayacinka,2023-10-02 22:17:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Thanks a lot, much appreciated, it works like a charm! I do have one little issue however... I get this weird output where every new sentence starts with a number. Do you know why that might be the case? 
![Screenshot 2023-10-02 at 5.14.07 PM.png](https://cdn-uploads.huggingface.co/production/uploads/642632fbba51f8a21368f9cb/DjUHrwenXqBOta-qmS1It.png)
"
limcheekin,2023-10-03 03:47:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > It is free of charge.
> > But I think HF definitely have a cap on number of requests can be made to the free tier HF spaces per hour or per day. Anyone here know the cap? 
> > 
> > That's the reason your support is important.
> > 
> > Alternatively, you can duplicate the space and run your own instance for free.
> 
> Thank you so so much for this man ! Appreciated. I used this to make an app for question answering a pdf. Just wanted to ask that when i use this model through this space; on which api does this bounce back from exatcly? Is it Openai? or the huggingface's api for this model? Coz HF's api wasn't giving the full ouput (only around 10 tokens) when used through langchain. And if its openai then how is it free ? Please help. I am a beginner.

It is neither from OpenAI nor HuggingFace API. Thanks for the following generous offer from the HF:
| **Hardware**        	| **GPU Memory** 	| **CPU** 	| **Memory** 	| **Disk** 	| **Hourly Price** 	|
| CPU Basic           	    | -             	                    | 2 vCPU  	    | 16 GB     	            | 50 GB    	    | Free!            	            |
You can find out more information at https://huggingface.co/docs/hub/spaces-overview#hardware-resources

You can use the free space for hosting open-source text embeddings models such as BAAI/bge-large-en, intfloat/e5-large-v2, sentence-transformers/all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2, etc. as OpenAI API compatible `embeddings` endpoint using the following Python package:
https://github.com/limcheekin/open-text-embeddings
 "
limcheekin,2023-10-03 03:50:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Thanks a lot, much appreciated, it works like a charm! I do have one little issue however... I get this weird output where every new sentence starts with a number. Do you know why that might be the case? 
> ![Screenshot 2023-10-02 at 5.14.07 PM.png](https://cdn-uploads.huggingface.co/production/uploads/642632fbba51f8a21368f9cb/DjUHrwenXqBOta-qmS1It.png)

I get similar output, not sure why that's the case. Perhaps you need to play around with the prompts or try the original unquantized model weight."
limcheekin,2023-10-03 23:33:58+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi there,

I just enabled (turn on) the `embeddings` endpoint and go ahead and test it out yourself and highly appreciate if you could share the result here on how does it compare to other open-source text embeddings models such as BAAI/bge-large-en, intfloat/e5-large-v2, sentence-transformers/all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2, etc.

By the way, I just created the same endpoints for Mistral-7B-OpenOrca-GGUF model at https://huggingface.co/spaces/limcheekin/Mistral-7B-OpenOrca-GGUF.
"
pvbhanuteja,2023-10-10 17:02:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Hi there,
> 
> I deployed the model as OpenAI API compatible endpoint at https://huggingface.co/spaces/limcheekin/Mistral-7B-Instruct-v0.1-GGUF.
> 
> Also, I created a jupyter notebook to get you started to use the API endpoint in no time.
> 
> Lastly, if you find this resource valuable, your support in the form of starring the space would be greatly appreciated. 
> 
> Thank you.

Use vLLM : https://github.com/vllm-project/vllm"
limcheekin,2023-10-10 23:59:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > Hi there,
> > 
> > I deployed the model as OpenAI API compatible endpoint at https://huggingface.co/spaces/limcheekin/Mistral-7B-Instruct-v0.1-GGUF.
> > 
> > Also, I created a jupyter notebook to get you started to use the API endpoint in no time.
> > 
> > Lastly, if you find this resource valuable, your support in the form of starring the space would be greatly appreciated. 
> > 
> > Thank you.
> 
> Use vLLM : https://github.com/vllm-project/vllm

Thanks for sharing. Is the vllm support GGUF models?"
pvbhanuteja,2023-10-11 01:31:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > > Hi there,
> > > 
> > > I deployed the model as OpenAI API compatible endpoint at https://huggingface.co/spaces/limcheekin/Mistral-7B-Instruct-v0.1-GGUF.
> > > 
> > > Also, I created a jupyter notebook to get you started to use the API endpoint in no time.
> > > 
> > > Lastly, if you find this resource valuable, your support in the form of starring the space would be greatly appreciated. 
> > > 
> > > Thank you.
> > 
> > Use vLLM : https://github.com/vllm-project/vllm
> 
> Thanks for sharing. Is the vllm support GGUF models?

Not sure. What not tryout ? 

python -m vllm.entrypoints.openai.api_server --model=<HF-MODEL>

I haven't used GGUF on vLLMs before "
limcheekin,2023-10-11 03:40:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > > > Hi there,
> > > > 
> > > > I deployed the model as OpenAI API compatible endpoint at https://huggingface.co/spaces/limcheekin/Mistral-7B-Instruct-v0.1-GGUF.
> > > > 
> > > > Also, I created a jupyter notebook to get you started to use the API endpoint in no time.
> > > > 
> > > > Lastly, if you find this resource valuable, your support in the form of starring the space would be greatly appreciated. 
> > > > 
> > > > Thank you.
> > > 
> > > Use vLLM : https://github.com/vllm-project/vllm
> > 
> > Thanks for sharing. Is the vllm support GGUF models?
> 
> Not sure. What not tryout ? 
> 
> python -m vllm.entrypoints.openai.api_server --model=<HF-MODEL>
> 
> I haven't used GGUF on vLLMs before

I am very much focus on using GGUF models and will pass it for now."
Aprilpl,2024-03-30 02:43:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This comment has been hidden
mayacinka,2023-10-01 19:11:55+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I have the exact same problem, I can only get around 20-30 tokens in the response. I wonder if it's model internal limitation "
Hawks101,2023-10-01 20:34:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> I have the exact same problem, I can only get around 20-30 tokens in the response. I wonder if it's model internal limitation

Please let me know if you find a solution.
"
adel-almasaabi,2023-10-06 10:16:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This comment has been hidden
Ayush8120,2023-10-10 14:25:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Can you attach an image of the code you are using for generation? Its working fine for me.
ArthurZ,2023-10-10 18:15:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Can you also try with `model.generate(**inputs, max_new_tokens = 350)` the default is `20` which might explain what's happening. "
maxfrai,2023-10-26 20:37:13+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"You can make POST request with inputs (and your query) and add ""parameters"" field. Increase max_new_tokens amount to get more text.
```
{
                ""inputs"": inputs,
                ""parameters"": {
                    ""max_new_tokens"": 100,
                    ""temperature"": 0.5,
                    ""top_k"": 40,
                    ""top_p"": 0.95,
                    ""repetition_penalty"": 1.1
                }
            }
``"
unixguru2k,2023-09-30 05:39:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This comment has been hidden
gnomealone,2023-10-01 00:19:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"
![pbF7hKdj3x.gif](https://cdn-uploads.huggingface.co/production/uploads/64562c734cfac492278ebf24/CJxkpc0fXjdGtotaXHK1A.gif)

![chrome_hKxL0ldZu9.png](https://cdn-uploads.huggingface.co/production/uploads/64562c734cfac492278ebf24/Hwp6Xde7wxjkBsR8tXhp-.png)

"
mnwato,2023-10-01 08:42:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Which UI did you used?
gnomealone,2023-10-01 17:50:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I used this space with the default settings.
https://huggingface.co/spaces/DarwinAnim8or/Mistral-Chat"
KarimZebra,2023-10-02 10:54:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I am also getting something weird. It is even trying to justify its wrong answer.

![Screenshot 2023-10-02 at 11.52.12 am.png](https://cdn-uploads.huggingface.co/production/uploads/65085f74ada48d4be28804c6/LaQHfh8Qv-EgKh8c2NxNN.png)
"
torchsmith,2023-10-07 15:24:42+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I keep having it tell me it's 30 or ""28 = 3 x 9"" which neither makes sense"
Stelarion,2023-09-30 02:53:37+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Here is my code. You need to locally save the model in a subfolder ( ./Mistral/ depending on your .py file)  

It does work for 1-3 queries. Until it breaks down. As there is absofucking no documentation of how to implement the workflow of Interference to a local pipeline this is the best I got. If people are interested in reverse engineering it. Shot me a message.  As it stands now, this is an add front to promote paid services, let's change that.

	import gradio as gr
	from transformers import pipeline, AutoTokenizer
	
	tokenizer = AutoTokenizer.from_pretrained(""./Mistral/"")
	pipe = pipeline(""text-generation"", model=""./Mistral/"", max_new_tokens=512)
	
	chat_history_tokens = []
	
	def generate(chatlog, is_finished):
		global chat_history_tokens
	
		# Get the latest message from chat
		new_message = chatlog[-1]['content'] if isinstance(chatlog, list) else chatlog
	
		# Tokenize new message and extend chat history
		new_message_tokens = tokenizer.encode(new_message, add_special_tokens=False)
		chat_history_tokens = new_message_tokens  # We only keep the last message now
	
		# Decode tokens to string for the prompt
		prompt = tokenizer.decode(chat_history_tokens)
		
		try:
			print(""Debug: Sending this prompt to the model:"", prompt)
			outputs = pipe(prompt, pad_token_id=tokenizer.eos_token_id)
			print(""Debug: Model's raw output:"", outputs)
	
			# Cleanup the generated text
			generated_text = outputs[0]['generated_text'].replace(prompt, """").strip()
			generated_text = generated_text.replace(""Answer:"", """").replace(""A:"", """").strip()
	
			print(""Debug: Generated Text After Cleanup:"", generated_text)
	
			# Tokenize the model's reply and add it to the history
			bot_reply_tokens = tokenizer.encode(generated_text, add_special_tokens=False)
			chat_history_tokens.extend(bot_reply_tokens)
	
		except Exception as e:
			print(""Debug: Caught an exception:"", str(e))
			return str(e)
	
		return generated_text
	
	iface = gr.ChatInterface(fn=generate)
	iface.launch()"
JeanMarieP,2023-09-29 21:11:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,perhaps 'pip install git+https://github.com/huggingface/transformers' ? 
bdambrosio,2023-09-29 21:49:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"good thought, but no luck."
harpercarroll,2023-09-29 22:15:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Can you try `pip install git+https://github.com/huggingface/transformers.git -U`?
bdambrosio,2023-09-29 23:22:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"that didn't change anything,
Tried creating a clean conda env and doing only your suggested pip install.
Worked!  My default conda env has way too much junk from kaggle comps...

Thank you for your support!"
19Peppe95,2023-10-03 15:53:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I have the same issue, any news?
"
Tilakraj0308,2023-10-03 16:31:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@19Peppe95 The error is because system is running out of RAM to load the model in one go.
You can use CTransformers to load the model or can try GGUF model versions of your model which is basically much smaller version of it.
Gist - Use GGUF version of this model https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF
and use CTransformers to load it from the dowloads and run the program, hopefully it should work."
Starlento,2023-10-13 13:18:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> @19Peppe95 The error is because system is running out of RAM to load the model in one go.
> You can use CTransformers to load the model or can try GGUF model versions of your model which is basically much smaller version of it.
> Gist - Use GGUF version of this model https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF
> and use CTransformers to load it from the dowloads and run the program, hopefully it should work.

~I also have the RAM issue. But it is werid that the model is only 14GB in totally and I have 64GB RAM and 24GB VRAM available.~
Just found out that there was a download issue, the bins are broken so the memory usage when loading the files became uncontrollable.
"
Tilakraj0308,2023-10-16 10:33:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This comment has been hidden
lysandre,2023-10-16 18:11:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,cc @ybelkada regarding low-memory methods to load larger models
ybelkada,2023-10-16 19:28:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hi everyone
In case you are facing CPU OOM issues while loading the model please consider using sharded models with small shards, for this model I would recommend using this repository: https://huggingface.co/bn22/Mistral-7B-Instruct-v0.1-sharded"
shantanudave,2024-03-18 12:57:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@ybelkada Hi, the model you shared is not available anymore? :("
ybelkada,2024-03-18 14:22:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"hi @shantanudave 
Indeed, please use: https://huggingface.co/alexsherstinsky/Mistral-7B-v0.1-sharded instead"
aeminkocal,2023-09-28 23:20:00+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Can I set system prompt so that I can make it behave in a way that I want?
tarruda,2023-09-29 09:26:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,I didn't see an option to set system prompt in the model card
YaTharThShaRma999,2023-09-28 15:56:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Yes it says Apache 2.0
jianguozhang,2023-09-28 16:03:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@johnwick123forevr Thanks:). I am curious that does the main improvement come from the base model? Given that the instruction model is fine-tuned on a variety of publicly available conversation datasets. However, many public instruction datasets are non-commercial and are not used here. "
YaTharThShaRma999,2023-09-28 17:56:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Yeah the main improvement comes from base model which is probably trained in better data I suppose? Since the base model performs similar as well.
jianguozhang,2023-09-28 20:13:32+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,That makes sense although I don't know the data information.
YaTharThShaRma999,2023-09-28 21:30:11+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Yeah, they didnt release data yet but might later. Now mistral ai says that they trained it on public data. 

Its possible since phi 1.5 for example which was trained on pure gpt4 data(which is high quality) and only 1.5 billion parameters came close to 7b llama v2 and beats llama v1 i believe.

If mistral was trained on high quality data it could be also pretty great?"
BramVanroy,2023-10-09 08:56:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@johnwick123forevr That's a problem with licensing then, though. Commercial use is explicitly prohibited if you train on OpenAI's models' outputs. So I think @jianguozhang is asking the right question. I asked them on Twitter which datasets they trained on because that in itself determines whether commercial use is allowed or not. I have seen too many other model builders who simply put the apache 2.0 label on their model, even though they train on synthetic OpenAI data. That is not allowed, and leads to people using these models for commercial deployment - potentially leading up to lawsuits by OpenAI because they are using models that are NOT allowed to be used commercially but which were mislicensed by the model builders. This is a dangerous trend... We always yell at BigCorp for transparency, and rightly so. But then many open-source builders are not transparent themselves either, and I suspect often deliberately so to ensure that their models are used by many despite being mislicensed."
YaTharThShaRma999,2023-10-09 13:54:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"True. Of course they might have been not trained on openai data or maybe. They did not release data so we don’t know.

Llama 2 as well is most likely pretrained on openai(with references to openai when outputting text) and other random data that should be very non commercial license. But if they don’t release the data, nothing really happens to them."
BramVanroy,2023-10-11 08:15:27+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I agree. It is likely that the big players are all using data that should not be used in that way (licensed books, GDPR material, OpenAI API, etc.) so it is very frustrating that people who want to be open about what they are doing are restricted in that sense. I just wish everyone, also BigCorp, was more transparent about what they do."
YaTharThShaRma999,2023-10-11 15:03:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Even openai probably used heavily non commercial books or text. The used a massive part of the web which has lots of parts which is probably non commercial.

There really isn’t an easy way to be fully commercial."
deleted,2023-10-01 04:30:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,This comment has been hidden
YaTharThShaRma999,2023-09-28 12:58:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,It’s 8k context 
sadahila,2023-09-28 17:06:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,is the inference context length also 8K or can it go higher? 
YaTharThShaRma999,2023-09-28 17:54:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,With sliding window I think it can reach 32k as it says  but normally it’s around 7 to 8k
opyate,2023-11-29 09:51:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> With sliding window I think it can reach 32k as it says but normally it’s around 7 to 8k

Please cite a source :)"
Pretergeek,2023-11-29 12:45:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > With sliding window I think it can reach 32k as it says but normally it’s around 7 to 8k
> 
> Please cite a source :)

Here, official site, it states 8K for the base model. I am not sure if that could have been changed for the Instruct version.
https://mistral.ai/product/"
opyate,2023-11-29 13:04:58+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Thanks :)

"
IshaanGupta2311,2023-09-28 11:38:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"same ques, i want to explore an application of these 7B models that i have been thinking about for quite some time
"
NajiAboo,2023-09-28 13:36:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,looking for same
Lazycuber,2023-09-28 14:32:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,still experimenting with colab finetuning... 
silvacarl,2023-09-28 15:19:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,looking for same here as well.  so far its amazing...
nadahlberg,2023-09-28 18:25:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Seems to be mostly working with the default [qlora.py script](https://github.com/artidoro/qlora/blob/main/qlora.py) with one small change.  

This line was causing trouble because mistral's model.config.pad_token_id returns None:
```
""unk_token"": tokenizer.convert_ids_to_tokens(
     model.config.pad_token_id if model.config.pad_token_id != -1 else tokenizer.pad_token_id
),
```

Adding a None check seems to fix:
```
""unk_token"": tokenizer.convert_ids_to_tokens(
    model.config.pad_token_id if 
    model.config.pad_token_id is not None and model.config.pad_token_id != -1 
    else tokenizer.pad_token_id
),
```"
silvacarl,2023-09-28 20:14:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"thx.  i am not even sure yet if it needs fine tuning, we are running a bunch of tests on it.

it seems to be the best 7B model we have ever seen.  it may outperform 13B and possibly 70B models for certain use cases."
NickyNicky,2023-09-29 03:43:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"it's very fast.

use:
-When flash attention 2 is used and with an inference of approximately 600 tokens <20 seconds if I remember correctly.
-Without flash attention 2, inferences from 100 tokens take +440 seconds.

fine-tune (SFTTrainer):
-gpu a100 colab 40GB
-for a 15k dataset it takes approximately 1H
-14 credits 1H
-per_device_train_batch_size= 6 ---->>> use gpu 36GB"
Lazycuber,2023-09-29 06:43:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Anyone made a colab notebook for finetuning?
sscpr,2023-10-01 00:18:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Seems to be mostly working with the default [qlora.py script](https://github.com/artidoro/qlora/blob/main/qlora.py) with one small change.  


using this script and your change seems to work but I had to repull in the latest transformers version 4.34 to get it to work
"
NickM2002,2023-10-02 12:37:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > Seems to be mostly working with the default [qlora.py script](https://github.com/artidoro/qlora/blob/main/qlora.py) with one small change.  
> 
> 
> using this script and your change seems to work but I had to repull in the latest transformers version 4.34 to get it to work

I also had to repull transformers with
“
pip install git+https://github.com/huggingface/transformers
“"
ysdj,2023-10-03 18:05:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Anyone made a colab notebook for finetuning?

https://github.com/Vasanthengineer4949/NLP-Projects-NHV/blob/main/LLMs%20Related/Finetune%20Mistral/Finetune_Mistral.ipynb"
QendelG,2023-10-05 05:20:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Step-by-step guide to finetune on QA Dataset: [https://medium.com/me/stats/post/0f7bebccf11c](https://medium.com/@qendelai/fine-tuning-mistral-7b-instruct-model-in-colab-a-beginners-guide-0f7bebccf11c)
NickyNicky,2023-10-05 06:08:11+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,haha error 404
QendelG,2023-10-05 12:54:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"it is updated @NickyNicky 

Here: https://medium.com/@qendelai/fine-tuning-mistral-7b-instruct-model-in-colab-a-beginners-guide-0f7bebccf11c"
timlacroix,2023-09-28 14:33:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hey, it will be in the next release ! https://github.com/vllm-project/vllm/issues/1089
We were a bit slow addressing some issues with our PR yesterday :) "
sade-adrien,2023-10-02 19:31:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"To generate the attention mask, you can replace this :

encodeds = tokenizer.apply_chat_template(messages, return_tensors=""pt"")

model_inputs = encodeds.to(device)
model.to(device)

generated_ids = model.generate(model_inputs, eos_token_id=2, max_new_tokens=1000, do_sample=True)

with:

encodeds = tokenizer(messages, return_tensors='pt)

encodeds['input_ids'] = encodeds['input_ids'].to(device)
encodeds['attention_mask'] = encodeds['attention_mask'].to(device)
model.to(device)

generated_ids = model.generate(**encodeds, eos_token_id=2, max_new_tokens=1000, do_sample=True)


but really it should not matter that you set all those things or not.


for the parameters check out the documentation of huggingface's model.generate"
narai,2023-10-17 19:50:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@sade-adrien 

Since the messages variable is a list of dictionaries, your code gives an error
encodeds = tokenizer(messages, return_tensors='pt')

ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).

It looks like apply_chat_template returns input_ids, and does not have the ability to return the mask."
NS-Y,2023-09-28 09:22:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,It's better :)
YaTharThShaRma999,2023-09-28 12:39:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Also, it has gqa which other llama 7b models don’t in their architecture. The architecture is very similar but not the same. Also, it’s pretrained on different data."
timlacroix,2023-09-28 14:34:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,GQA and Sliding Window Attention are the visible differences which should help increase inference throughput and context length. 
dheerajpai,2023-09-28 18:55:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Is this model pre-trained from scratch? Just curious.
devendrachaplot,2023-09-28 20:28:52+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Yes, it is pre-trained from scratch."
nps798,2023-10-03 09:45:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"for me, it respond well in chinese. For llama 7b, whenever I ask in chinese, it somewhat understand my question by respond in English."
iNeverLearnedHowToRead,2023-10-05 10:53:29+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,It's way better.
907Resident,2023-09-28 03:46:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"I am also getting this error as well using the sample code for this model.

<details>
<summary>Here is an abbreviated environment:</summary>
<br>

Package                            Version
--------------------------     -------------
absl-py                             1.0.0
accelerate                       0.21.0
aiohttp                             3.8.4
aiosignal                         1.3.1
alembic                          1.12.0
appdirs                           1.4.4
argon2-cffi                    21.3.0
argon2-cffi-bindings         21.2.0
arrow                        1.2.3
astor                        0.8.1
asttokens                    2.2.1
astunparse                   1.6.3
async-timeout                4.0.2
attrs                        21.4.0
audioread                    3.0.0
azure-core                   1.26.4
azure-cosmos                 4.3.1b1
azure-storage-blob           12.16.0
azure-storage-file-datalake  12.11.0
backcall                     0.2.0
bcrypt                       3.2.0
beautifulsoup4               4.11.1
bitsandbytes                 0.41.0
black                        22.6.0
bleach                       4.1.0
blinker                      1.4
blis                         0.7.9
boto3                        1.24.28
botocore                     1.27.28
bravado                      11.0.3
bravado-core                 6.1.0
cachetools                   4.2.4
catalogue                    2.0.8
category-encoders            2.6.0
certifi                      2022.9.14
cffi                         1.15.1
chardet                      4.0.0
charset-normalizer           2.0.4
click                        8.0.4
cloudpickle                  2.0.0
cmaes                        0.10.0
cmdstanpy                    1.1.0
colorlog                     6.7.0
confection                   0.0.4
configparser                 5.2.0
convertdate                  2.4.0
cryptography                 37.0.1
cycler                       0.11.0
cymem                        2.0.7
Cython                       0.29.32
databricks-automl-runtime    0.2.16
databricks-cli               0.17.6
databricks-feature-store     0.12.1
dataclasses-json             0.5.7
datasets                     2.12.0
dbl-tempo                    0.1.23
dbus-python                  1.2.18
debugpy                      1.5.1
decorator                    5.1.1
defusedxml                   0.7.1
dill                         0.3.4
diskcache                    5.6.1
distlib                      0.3.6
distro                       1.7.0
distro-info                  1.1+ubuntu0.1
doc2text                     0.2.4
docstring-to-markdown        0.12
einops                       0.6.1
entrypoints                  0.4
ephem                        4.1.4
et-xmlfile                   1.1.0
evaluate                     0.4.0
executing                    1.2.0
facets-overview              1.0.3
faiss-gpu                    1.7.2
fastjsonschema               2.16.3
fasttext                     0.9.2
filelock                     3.6.0
filetype                     1.2.0
Flask                        1.1.2+db1
flatbuffers                  23.5.9
fonttools                    4.25.0
fqdn                         1.5.1
frozenlist                   1.3.3
fsspec                       2022.7.1
future                       0.18.2
gast                         0.4.0
gitdb                        4.0.10
GitPython                    3.1.27
google-api-core              2.8.2
google-auth                  1.33.0
google-auth-oauthlib         0.4.6
google-cloud-core            2.3.2
google-cloud-storage         2.9.0
google-crc32c                1.5.0
google-pasta                 0.2.0
google-resumable-media       2.5.0
googleapis-common-protos     1.56.4
greenlet                     1.1.1
grpcio                       1.48.1
grpcio-status                1.48.1
gunicorn                     20.1.0
gviz-api                     1.10.0
h5py                         3.7.0
hijri-converter              2.3.1
holidays                     0.22
horovod                      0.27.0
htmlmin                      0.1.12
httplib2                     0.20.2
huggingface-hub              0.17.3
idna                         3.3
ImageHash                    4.3.1
imbalanced-learn             0.8.1
importlib-metadata           4.11.3
ipykernel                    6.17.1
ipython                      8.10.0
ipython-genutils             0.2.0
ipywidgets                   7.7.2
isodate                      0.6.1
isoduration                  20.11.0
itsdangerous                 2.0.1
jedi                         0.18.1
jeepney                      0.7.1
Jinja2                       2.11.3
jmespath                     0.10.0
joblib                       1.2.0
joblibspark                  0.5.1
jsonpointer                  2.4
jsonref                      1.1.0
jsonschema                   4.16.0
jupyter-client               7.3.4
jupyter_core                 4.11.2
jupyterlab-pygments          0.1.2
jupyterlab-widgets           1.0.0
keras                        2.11.0
keyring                      23.5.0
kiwisolver                   1.4.2
korean-lunar-calendar        0.3.1
langchain                    0.0.285
langcodes                    3.3.0
langsmith                    0.0.41
launchpadlib                 1.10.16
lazr.restfulclient           0.14.4
lazr.uri                     1.0.6
lazy_loader                  0.2
libclang                     15.0.6.1
librosa                      0.10.0
lightgbm                     3.3.5
llvmlite                     0.38.0
LunarCalendar                0.0.9
lxml                         4.9.3
Mako                         1.2.0
Markdown                     3.3.4
MarkupSafe                   2.0.1
marshmallow                  3.19.0
marshmallow-enum             1.5.1
matplotlib                   3.5.2
matplotlib-inline            0.1.6
mccabe                       0.7.0
mime                         0.1.0
mistune                      0.8.4
mleap                        0.20.0
mlflow-skinny                2.3.1
monotonic                    1.6
more-itertools               8.10.0
msgpack                      1.0.5
multidict                    6.0.4
multimethod                  1.9.1
multiprocess                 0.70.12.2
murmurhash                   1.0.9
mypy-extensions              0.4.3
nbclient                     0.5.13
nbconvert                    6.4.4
nbformat                     5.5.0
neptune-client               0.16.18
neptune-optuna               1.1.0
nest-asyncio                 1.5.5
networkx                     2.8.4
nltk                         3.7
nodeenv                      1.8.0
notebook                     6.4.12
numba                        0.55.1
numexpr                      2.8.4
numpy                        1.21.5
oauthlib                     3.2.0
openai                       0.27.4
openapi-schema-pydantic      1.2.4
openpyxl                     3.1.2
opt-einsum                   3.3.0
optuna                       3.3.0
packaging                    21.3
pandas                       1.4.4
pandocfilters                1.5.0
paramiko                     2.9.2
parso                        0.8.3
pathspec                     0.9.0
pathy                        0.10.1
patsy                        0.5.2
petastorm                    0.12.1
pexpect                      4.8.0
phik                         0.12.3
pickleshare                  0.7.5
Pillow                       9.2.0
pip                          23.2.1
platformdirs                 2.5.2
plotly                       5.9.0
pluggy                       1.0.0
pmdarima                     2.0.3
pooch                        1.7.0
preshed                      3.0.8
prompt-toolkit               3.0.36
prophet                      1.1.2
protobuf                     3.19.4
psutil                       5.9.0
psycopg2                     2.9.3
ptyprocess                   0.7.0
pure-eval                    0.2.2
pyarrow                      8.0.0
pyasn1                       0.4.8
pyasn1-modules               0.2.8
pybind11                     2.10.4
pycparser                    2.21
pydantic                     1.10.6
pyflakes                     3.0.1
Pygments                     2.11.2
PyGObject                    3.42.1
PyJWT                        2.3.0
PyMeeus                      0.5.12
PyNaCl                       1.5.0
pyodbc                       4.0.32
pyparsing                    3.0.9
PyPDF2                       3.0.1
pyright                      1.1.294
pyrsistent                   0.18.0
pytesseract                  0.3.10
python-apt                   2.4.0+ubuntu2
python-dateutil              2.8.2
python-docx                  0.8.11
python-editor                1.0.4
python-lsp-jsonrpc           1.0.0
python-lsp-server            1.7.1
python-magic                 0.4.27
pytoolconfig                 1.2.2
pytz                         2022.1
PyWavelets                   1.3.0
PyYAML                       6.0
pyzmq                        23.2.0
ragas                        0.0.12
regex                        2022.7.9
requests                     2.28.1
requests-oauthlib            1.3.1
responses                    0.18.0
rfc3339-validator            0.1.4
rfc3987                      1.3.8
rope                         1.7.0
rsa                          4.9
s3transfer                   0.6.0
safetensors                  0.3.3
scikit-learn                 1.1.1
scipy                        1.9.1
seaborn                      0.11.2
SecretStorage                3.3.1
Send2Trash                   1.8.0
sentence-transformers        2.2.2
sentencepiece                0.1.97
setuptools                   63.4.1
shap                         0.41.0
simplejson                   3.17.6
six                          1.16.0
skops                        0.8.0
slicer                       0.0.7
smart-open                   5.2.1
smmap                        5.0.0
soundfile                    0.12.1
soupsieve                    2.3.1
soxr                         0.3.5
spacy                        3.5.1
spacy-cleaner                3.1.3
spacy-legacy                 3.0.12
spacy-loggers                1.0.4
spacy-lookups-data           1.0.5
spark-tensorflow-distributor 1.0.0
SQLAlchemy                   1.4.39
sqlparse                     0.4.2
srsly                        2.4.6
ssh-import-id                5.11
stack-data                   0.6.2
statsmodels                  0.13.2
swagger-spec-validator       3.0.3
tabulate                     0.8.10
tangled-up-in-unicode        0.2.0
tenacity                     8.1.0
tensorboard                  2.11.0
tensorboard-data-server      0.6.1
tensorboard-plugin-profile   2.11.2
tensorboard-plugin-wit       1.8.1
tensorflow                   2.11.1
tensorflow-estimator         2.11.0
tensorflow-io-gcs-filesystem 0.32.0
termcolor                    2.3.0
terminado                    0.13.1
testpath                     0.6.0
thinc                        8.1.10
threadpoolctl                2.2.0
tiktoken                     0.3.3
tokenize-rt                  4.2.1
tokenizers                   0.13.3
tomli                        2.0.1
torch                        1.13.1+cu117
torchvision                  0.14.1+cu117
tornado                      6.1
tqdm                         4.64.1
traitlets                    5.1.1
transformers                 4.33.3
typeguard                    2.13.3
typer                        0.7.0
typing_extensions            4.3.0
typing-inspect               0.8.0
ujson                        5.4.0
unattended-upgrades          0.1
unstructured                 0.9.0
uri-template                 1.3.0
urllib3                      1.26.11
virtualenv                   20.16.3
visions                      0.7.5
wadllib                      1.3.6
wasabi                       0.10.1
watermark                    2.4.3
wcwidth                      0.2.5
webcolors                    1.13
webencodings                 0.5.1
websocket-client             0.58.0
Werkzeug                     2.0.3
whatthepatch                 1.0.2
wheel                        0.37.1
widgetsnbextension           3.6.1
wrapt                        1.14.1
xgboost                      1.7.5
xxhash                       3.2.0
yapf                         0.31.0
yarl                         1.9.2
ydata-profiling              4.1.2
zipp                         3.8.0

</details>

Here are two lines of the sample code pulled from the [model card](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1):

```python
model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
```

<details open>
<summary>Here is the error I am receiving:</summary>
<br>

```sh
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File <command-4309329534262302>:1
----> 1 model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
      2 tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")

File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    438     if kwargs_copy.get(""torch_dtype"", None) == ""auto"":
    439         _ = kwargs_copy.pop(""torch_dtype"")
--> 441     config, kwargs = AutoConfig.from_pretrained(
    442         pretrained_model_name_or_path,
    443         return_unused_kwargs=True,
    444         trust_remote_code=trust_remote_code,
    445         **hub_kwargs,
    446         **kwargs_copy,
    447     )
    448 if hasattr(config, ""auto_map"") and cls.__name__ in config.auto_map:
    449     if not trust_remote_code:

File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:937, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    935     return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)
    936 elif ""model_type"" in config_dict:
--> 937     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
    938     return config_class.from_dict(config_dict, **unused_kwargs)
    939 else:
    940     # Fallback: use pattern matching on the string.
    941     # We go from longer names to shorter names to catch roberta before bert (for instance)

File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:643, in _LazyConfigMapping.__getitem__(self, key)
    641     return self._extra_content[key]
    642 if key not in self._mapping:
--> 643     raise KeyError(key)
    644 value = self._mapping[key]
    645 module_name = model_type_to_module_name(key)

KeyError: 'mistral'
```

</details>"
dongXL,2023-09-28 03:47:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,you need to update the transformers package to the latest.
dongXL,2023-09-28 03:47:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> I am also getting this error as well using the sample code for this model.
> 
> <details>
> <summary>Here is an abbreviated environment:</summary>
> <br>
> 
> Package                            Version
> --------------------------     -------------
> absl-py                             1.0.0
> accelerate                       0.21.0
> aiohttp                             3.8.4
> aiosignal                         1.3.1
> alembic                          1.12.0
> appdirs                           1.4.4
> argon2-cffi                    21.3.0
> argon2-cffi-bindings         21.2.0
> arrow                        1.2.3
> astor                        0.8.1
> asttokens                    2.2.1
> astunparse                   1.6.3
> async-timeout                4.0.2
> attrs                        21.4.0
> audioread                    3.0.0
> azure-core                   1.26.4
> azure-cosmos                 4.3.1b1
> azure-storage-blob           12.16.0
> azure-storage-file-datalake  12.11.0
> backcall                     0.2.0
> bcrypt                       3.2.0
> beautifulsoup4               4.11.1
> bitsandbytes                 0.41.0
> black                        22.6.0
> bleach                       4.1.0
> blinker                      1.4
> blis                         0.7.9
> boto3                        1.24.28
> botocore                     1.27.28
> bravado                      11.0.3
> bravado-core                 6.1.0
> cachetools                   4.2.4
> catalogue                    2.0.8
> category-encoders            2.6.0
> certifi                      2022.9.14
> cffi                         1.15.1
> chardet                      4.0.0
> charset-normalizer           2.0.4
> click                        8.0.4
> cloudpickle                  2.0.0
> cmaes                        0.10.0
> cmdstanpy                    1.1.0
> colorlog                     6.7.0
> confection                   0.0.4
> configparser                 5.2.0
> convertdate                  2.4.0
> cryptography                 37.0.1
> cycler                       0.11.0
> cymem                        2.0.7
> Cython                       0.29.32
> databricks-automl-runtime    0.2.16
> databricks-cli               0.17.6
> databricks-feature-store     0.12.1
> dataclasses-json             0.5.7
> datasets                     2.12.0
> dbl-tempo                    0.1.23
> dbus-python                  1.2.18
> debugpy                      1.5.1
> decorator                    5.1.1
> defusedxml                   0.7.1
> dill                         0.3.4
> diskcache                    5.6.1
> distlib                      0.3.6
> distro                       1.7.0
> distro-info                  1.1+ubuntu0.1
> doc2text                     0.2.4
> docstring-to-markdown        0.12
> einops                       0.6.1
> entrypoints                  0.4
> ephem                        4.1.4
> et-xmlfile                   1.1.0
> evaluate                     0.4.0
> executing                    1.2.0
> facets-overview              1.0.3
> faiss-gpu                    1.7.2
> fastjsonschema               2.16.3
> fasttext                     0.9.2
> filelock                     3.6.0
> filetype                     1.2.0
> Flask                        1.1.2+db1
> flatbuffers                  23.5.9
> fonttools                    4.25.0
> fqdn                         1.5.1
> frozenlist                   1.3.3
> fsspec                       2022.7.1
> future                       0.18.2
> gast                         0.4.0
> gitdb                        4.0.10
> GitPython                    3.1.27
> google-api-core              2.8.2
> google-auth                  1.33.0
> google-auth-oauthlib         0.4.6
> google-cloud-core            2.3.2
> google-cloud-storage         2.9.0
> google-crc32c                1.5.0
> google-pasta                 0.2.0
> google-resumable-media       2.5.0
> googleapis-common-protos     1.56.4
> greenlet                     1.1.1
> grpcio                       1.48.1
> grpcio-status                1.48.1
> gunicorn                     20.1.0
> gviz-api                     1.10.0
> h5py                         3.7.0
> hijri-converter              2.3.1
> holidays                     0.22
> horovod                      0.27.0
> htmlmin                      0.1.12
> httplib2                     0.20.2
> huggingface-hub              0.17.3
> idna                         3.3
> ImageHash                    4.3.1
> imbalanced-learn             0.8.1
> importlib-metadata           4.11.3
> ipykernel                    6.17.1
> ipython                      8.10.0
> ipython-genutils             0.2.0
> ipywidgets                   7.7.2
> isodate                      0.6.1
> isoduration                  20.11.0
> itsdangerous                 2.0.1
> jedi                         0.18.1
> jeepney                      0.7.1
> Jinja2                       2.11.3
> jmespath                     0.10.0
> joblib                       1.2.0
> joblibspark                  0.5.1
> jsonpointer                  2.4
> jsonref                      1.1.0
> jsonschema                   4.16.0
> jupyter-client               7.3.4
> jupyter_core                 4.11.2
> jupyterlab-pygments          0.1.2
> jupyterlab-widgets           1.0.0
> keras                        2.11.0
> keyring                      23.5.0
> kiwisolver                   1.4.2
> korean-lunar-calendar        0.3.1
> langchain                    0.0.285
> langcodes                    3.3.0
> langsmith                    0.0.41
> launchpadlib                 1.10.16
> lazr.restfulclient           0.14.4
> lazr.uri                     1.0.6
> lazy_loader                  0.2
> libclang                     15.0.6.1
> librosa                      0.10.0
> lightgbm                     3.3.5
> llvmlite                     0.38.0
> LunarCalendar                0.0.9
> lxml                         4.9.3
> Mako                         1.2.0
> Markdown                     3.3.4
> MarkupSafe                   2.0.1
> marshmallow                  3.19.0
> marshmallow-enum             1.5.1
> matplotlib                   3.5.2
> matplotlib-inline            0.1.6
> mccabe                       0.7.0
> mime                         0.1.0
> mistune                      0.8.4
> mleap                        0.20.0
> mlflow-skinny                2.3.1
> monotonic                    1.6
> more-itertools               8.10.0
> msgpack                      1.0.5
> multidict                    6.0.4
> multimethod                  1.9.1
> multiprocess                 0.70.12.2
> murmurhash                   1.0.9
> mypy-extensions              0.4.3
> nbclient                     0.5.13
> nbconvert                    6.4.4
> nbformat                     5.5.0
> neptune-client               0.16.18
> neptune-optuna               1.1.0
> nest-asyncio                 1.5.5
> networkx                     2.8.4
> nltk                         3.7
> nodeenv                      1.8.0
> notebook                     6.4.12
> numba                        0.55.1
> numexpr                      2.8.4
> numpy                        1.21.5
> oauthlib                     3.2.0
> openai                       0.27.4
> openapi-schema-pydantic      1.2.4
> openpyxl                     3.1.2
> opt-einsum                   3.3.0
> optuna                       3.3.0
> packaging                    21.3
> pandas                       1.4.4
> pandocfilters                1.5.0
> paramiko                     2.9.2
> parso                        0.8.3
> pathspec                     0.9.0
> pathy                        0.10.1
> patsy                        0.5.2
> petastorm                    0.12.1
> pexpect                      4.8.0
> phik                         0.12.3
> pickleshare                  0.7.5
> Pillow                       9.2.0
> pip                          23.2.1
> platformdirs                 2.5.2
> plotly                       5.9.0
> pluggy                       1.0.0
> pmdarima                     2.0.3
> pooch                        1.7.0
> preshed                      3.0.8
> prompt-toolkit               3.0.36
> prophet                      1.1.2
> protobuf                     3.19.4
> psutil                       5.9.0
> psycopg2                     2.9.3
> ptyprocess                   0.7.0
> pure-eval                    0.2.2
> pyarrow                      8.0.0
> pyasn1                       0.4.8
> pyasn1-modules               0.2.8
> pybind11                     2.10.4
> pycparser                    2.21
> pydantic                     1.10.6
> pyflakes                     3.0.1
> Pygments                     2.11.2
> PyGObject                    3.42.1
> PyJWT                        2.3.0
> PyMeeus                      0.5.12
> PyNaCl                       1.5.0
> pyodbc                       4.0.32
> pyparsing                    3.0.9
> PyPDF2                       3.0.1
> pyright                      1.1.294
> pyrsistent                   0.18.0
> pytesseract                  0.3.10
> python-apt                   2.4.0+ubuntu2
> python-dateutil              2.8.2
> python-docx                  0.8.11
> python-editor                1.0.4
> python-lsp-jsonrpc           1.0.0
> python-lsp-server            1.7.1
> python-magic                 0.4.27
> pytoolconfig                 1.2.2
> pytz                         2022.1
> PyWavelets                   1.3.0
> PyYAML                       6.0
> pyzmq                        23.2.0
> ragas                        0.0.12
> regex                        2022.7.9
> requests                     2.28.1
> requests-oauthlib            1.3.1
> responses                    0.18.0
> rfc3339-validator            0.1.4
> rfc3987                      1.3.8
> rope                         1.7.0
> rsa                          4.9
> s3transfer                   0.6.0
> safetensors                  0.3.3
> scikit-learn                 1.1.1
> scipy                        1.9.1
> seaborn                      0.11.2
> SecretStorage                3.3.1
> Send2Trash                   1.8.0
> sentence-transformers        2.2.2
> sentencepiece                0.1.97
> setuptools                   63.4.1
> shap                         0.41.0
> simplejson                   3.17.6
> six                          1.16.0
> skops                        0.8.0
> slicer                       0.0.7
> smart-open                   5.2.1
> smmap                        5.0.0
> soundfile                    0.12.1
> soupsieve                    2.3.1
> soxr                         0.3.5
> spacy                        3.5.1
> spacy-cleaner                3.1.3
> spacy-legacy                 3.0.12
> spacy-loggers                1.0.4
> spacy-lookups-data           1.0.5
> spark-tensorflow-distributor 1.0.0
> SQLAlchemy                   1.4.39
> sqlparse                     0.4.2
> srsly                        2.4.6
> ssh-import-id                5.11
> stack-data                   0.6.2
> statsmodels                  0.13.2
> swagger-spec-validator       3.0.3
> tabulate                     0.8.10
> tangled-up-in-unicode        0.2.0
> tenacity                     8.1.0
> tensorboard                  2.11.0
> tensorboard-data-server      0.6.1
> tensorboard-plugin-profile   2.11.2
> tensorboard-plugin-wit       1.8.1
> tensorflow                   2.11.1
> tensorflow-estimator         2.11.0
> tensorflow-io-gcs-filesystem 0.32.0
> termcolor                    2.3.0
> terminado                    0.13.1
> testpath                     0.6.0
> thinc                        8.1.10
> threadpoolctl                2.2.0
> tiktoken                     0.3.3
> tokenize-rt                  4.2.1
> tokenizers                   0.13.3
> tomli                        2.0.1
> torch                        1.13.1+cu117
> torchvision                  0.14.1+cu117
> tornado                      6.1
> tqdm                         4.64.1
> traitlets                    5.1.1
> transformers                 4.33.3
> typeguard                    2.13.3
> typer                        0.7.0
> typing_extensions            4.3.0
> typing-inspect               0.8.0
> ujson                        5.4.0
> unattended-upgrades          0.1
> unstructured                 0.9.0
> uri-template                 1.3.0
> urllib3                      1.26.11
> virtualenv                   20.16.3
> visions                      0.7.5
> wadllib                      1.3.6
> wasabi                       0.10.1
> watermark                    2.4.3
> wcwidth                      0.2.5
> webcolors                    1.13
> webencodings                 0.5.1
> websocket-client             0.58.0
> Werkzeug                     2.0.3
> whatthepatch                 1.0.2
> wheel                        0.37.1
> widgetsnbextension           3.6.1
> wrapt                        1.14.1
> xgboost                      1.7.5
> xxhash                       3.2.0
> yapf                         0.31.0
> yarl                         1.9.2
> ydata-profiling              4.1.2
> zipp                         3.8.0
> 
> </details>
> 
> Here are two lines of the sample code pulled from the [model card](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1):
> 
> ```python
> model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> ```
> 
> <details>
> <summary>Here is the error I am receiving:</summary>
> <br>
> 
> ```sh
> ---------------------------------------------------------------------------
> KeyError                                  Traceback (most recent call last)
> File <command-4309329534262302>:1
> ----> 1 model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
>       2 tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> 
> File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
>     438     if kwargs_copy.get(""torch_dtype"", None) == ""auto"":
>     439         _ = kwargs_copy.pop(""torch_dtype"")
> --> 441     config, kwargs = AutoConfig.from_pretrained(
>     442         pretrained_model_name_or_path,
>     443         return_unused_kwargs=True,
>     444         trust_remote_code=trust_remote_code,
>     445         **hub_kwargs,
>     446         **kwargs_copy,
>     447     )
>     448 if hasattr(config, ""auto_map"") and cls.__name__ in config.auto_map:
>     449     if not trust_remote_code:
> 
> File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:937, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
>     935     return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)
>     936 elif ""model_type"" in config_dict:
> --> 937     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
>     938     return config_class.from_dict(config_dict, **unused_kwargs)
>     939 else:
>     940     # Fallback: use pattern matching on the string.
>     941     # We go from longer names to shorter names to catch roberta before bert (for instance)
> 
> File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:643, in _LazyConfigMapping.__getitem__(self, key)
>     641     return self._extra_content[key]
>     642 if key not in self._mapping:
> --> 643     raise KeyError(key)
>     644 value = self._mapping[key]
>     645 module_name = model_type_to_module_name(key)
> 
> KeyError: 'mistral'
> ```
> 
> </details>

you need to update the transformers package to the latest."
907Resident,2023-09-28 03:51:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"~`transformers == 4.33.3` is not the latest?~

~That is what I shared above and from what I can see on GitHub, `4.33.3` [appears to be the latest release](https://github.com/huggingface/transformers/releases) (released 12 hours ago).~

See [comment below](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/7#6514f85c2c5da97903a6c657)"
907Resident,2023-09-28 03:51:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > I am also getting this error as well using the sample code for this model.
> > 
> > <details>
> > <summary>Here is an abbreviated environment:</summary>
> > <br>
> > 
> > Package                            Version
> > --------------------------     -------------
> > absl-py                             1.0.0
> > accelerate                       0.21.0
> > aiohttp                             3.8.4
> > aiosignal                         1.3.1
> > alembic                          1.12.0
> > appdirs                           1.4.4
> > argon2-cffi                    21.3.0
> > argon2-cffi-bindings         21.2.0
> > arrow                        1.2.3
> > astor                        0.8.1
> > asttokens                    2.2.1
> > astunparse                   1.6.3
> > async-timeout                4.0.2
> > attrs                        21.4.0
> > audioread                    3.0.0
> > azure-core                   1.26.4
> > azure-cosmos                 4.3.1b1
> > azure-storage-blob           12.16.0
> > azure-storage-file-datalake  12.11.0
> > backcall                     0.2.0
> > bcrypt                       3.2.0
> > beautifulsoup4               4.11.1
> > bitsandbytes                 0.41.0
> > black                        22.6.0
> > bleach                       4.1.0
> > blinker                      1.4
> > blis                         0.7.9
> > boto3                        1.24.28
> > botocore                     1.27.28
> > bravado                      11.0.3
> > bravado-core                 6.1.0
> > cachetools                   4.2.4
> > catalogue                    2.0.8
> > category-encoders            2.6.0
> > certifi                      2022.9.14
> > cffi                         1.15.1
> > chardet                      4.0.0
> > charset-normalizer           2.0.4
> > click                        8.0.4
> > cloudpickle                  2.0.0
> > cmaes                        0.10.0
> > cmdstanpy                    1.1.0
> > colorlog                     6.7.0
> > confection                   0.0.4
> > configparser                 5.2.0
> > convertdate                  2.4.0
> > cryptography                 37.0.1
> > cycler                       0.11.0
> > cymem                        2.0.7
> > Cython                       0.29.32
> > databricks-automl-runtime    0.2.16
> > databricks-cli               0.17.6
> > databricks-feature-store     0.12.1
> > dataclasses-json             0.5.7
> > datasets                     2.12.0
> > dbl-tempo                    0.1.23
> > dbus-python                  1.2.18
> > debugpy                      1.5.1
> > decorator                    5.1.1
> > defusedxml                   0.7.1
> > dill                         0.3.4
> > diskcache                    5.6.1
> > distlib                      0.3.6
> > distro                       1.7.0
> > distro-info                  1.1+ubuntu0.1
> > doc2text                     0.2.4
> > docstring-to-markdown        0.12
> > einops                       0.6.1
> > entrypoints                  0.4
> > ephem                        4.1.4
> > et-xmlfile                   1.1.0
> > evaluate                     0.4.0
> > executing                    1.2.0
> > facets-overview              1.0.3
> > faiss-gpu                    1.7.2
> > fastjsonschema               2.16.3
> > fasttext                     0.9.2
> > filelock                     3.6.0
> > filetype                     1.2.0
> > Flask                        1.1.2+db1
> > flatbuffers                  23.5.9
> > fonttools                    4.25.0
> > fqdn                         1.5.1
> > frozenlist                   1.3.3
> > fsspec                       2022.7.1
> > future                       0.18.2
> > gast                         0.4.0
> > gitdb                        4.0.10
> > GitPython                    3.1.27
> > google-api-core              2.8.2
> > google-auth                  1.33.0
> > google-auth-oauthlib         0.4.6
> > google-cloud-core            2.3.2
> > google-cloud-storage         2.9.0
> > google-crc32c                1.5.0
> > google-pasta                 0.2.0
> > google-resumable-media       2.5.0
> > googleapis-common-protos     1.56.4
> > greenlet                     1.1.1
> > grpcio                       1.48.1
> > grpcio-status                1.48.1
> > gunicorn                     20.1.0
> > gviz-api                     1.10.0
> > h5py                         3.7.0
> > hijri-converter              2.3.1
> > holidays                     0.22
> > horovod                      0.27.0
> > htmlmin                      0.1.12
> > httplib2                     0.20.2
> > huggingface-hub              0.17.3
> > idna                         3.3
> > ImageHash                    4.3.1
> > imbalanced-learn             0.8.1
> > importlib-metadata           4.11.3
> > ipykernel                    6.17.1
> > ipython                      8.10.0
> > ipython-genutils             0.2.0
> > ipywidgets                   7.7.2
> > isodate                      0.6.1
> > isoduration                  20.11.0
> > itsdangerous                 2.0.1
> > jedi                         0.18.1
> > jeepney                      0.7.1
> > Jinja2                       2.11.3
> > jmespath                     0.10.0
> > joblib                       1.2.0
> > joblibspark                  0.5.1
> > jsonpointer                  2.4
> > jsonref                      1.1.0
> > jsonschema                   4.16.0
> > jupyter-client               7.3.4
> > jupyter_core                 4.11.2
> > jupyterlab-pygments          0.1.2
> > jupyterlab-widgets           1.0.0
> > keras                        2.11.0
> > keyring                      23.5.0
> > kiwisolver                   1.4.2
> > korean-lunar-calendar        0.3.1
> > langchain                    0.0.285
> > langcodes                    3.3.0
> > langsmith                    0.0.41
> > launchpadlib                 1.10.16
> > lazr.restfulclient           0.14.4
> > lazr.uri                     1.0.6
> > lazy_loader                  0.2
> > libclang                     15.0.6.1
> > librosa                      0.10.0
> > lightgbm                     3.3.5
> > llvmlite                     0.38.0
> > LunarCalendar                0.0.9
> > lxml                         4.9.3
> > Mako                         1.2.0
> > Markdown                     3.3.4
> > MarkupSafe                   2.0.1
> > marshmallow                  3.19.0
> > marshmallow-enum             1.5.1
> > matplotlib                   3.5.2
> > matplotlib-inline            0.1.6
> > mccabe                       0.7.0
> > mime                         0.1.0
> > mistune                      0.8.4
> > mleap                        0.20.0
> > mlflow-skinny                2.3.1
> > monotonic                    1.6
> > more-itertools               8.10.0
> > msgpack                      1.0.5
> > multidict                    6.0.4
> > multimethod                  1.9.1
> > multiprocess                 0.70.12.2
> > murmurhash                   1.0.9
> > mypy-extensions              0.4.3
> > nbclient                     0.5.13
> > nbconvert                    6.4.4
> > nbformat                     5.5.0
> > neptune-client               0.16.18
> > neptune-optuna               1.1.0
> > nest-asyncio                 1.5.5
> > networkx                     2.8.4
> > nltk                         3.7
> > nodeenv                      1.8.0
> > notebook                     6.4.12
> > numba                        0.55.1
> > numexpr                      2.8.4
> > numpy                        1.21.5
> > oauthlib                     3.2.0
> > openai                       0.27.4
> > openapi-schema-pydantic      1.2.4
> > openpyxl                     3.1.2
> > opt-einsum                   3.3.0
> > optuna                       3.3.0
> > packaging                    21.3
> > pandas                       1.4.4
> > pandocfilters                1.5.0
> > paramiko                     2.9.2
> > parso                        0.8.3
> > pathspec                     0.9.0
> > pathy                        0.10.1
> > patsy                        0.5.2
> > petastorm                    0.12.1
> > pexpect                      4.8.0
> > phik                         0.12.3
> > pickleshare                  0.7.5
> > Pillow                       9.2.0
> > pip                          23.2.1
> > platformdirs                 2.5.2
> > plotly                       5.9.0
> > pluggy                       1.0.0
> > pmdarima                     2.0.3
> > pooch                        1.7.0
> > preshed                      3.0.8
> > prompt-toolkit               3.0.36
> > prophet                      1.1.2
> > protobuf                     3.19.4
> > psutil                       5.9.0
> > psycopg2                     2.9.3
> > ptyprocess                   0.7.0
> > pure-eval                    0.2.2
> > pyarrow                      8.0.0
> > pyasn1                       0.4.8
> > pyasn1-modules               0.2.8
> > pybind11                     2.10.4
> > pycparser                    2.21
> > pydantic                     1.10.6
> > pyflakes                     3.0.1
> > Pygments                     2.11.2
> > PyGObject                    3.42.1
> > PyJWT                        2.3.0
> > PyMeeus                      0.5.12
> > PyNaCl                       1.5.0
> > pyodbc                       4.0.32
> > pyparsing                    3.0.9
> > PyPDF2                       3.0.1
> > pyright                      1.1.294
> > pyrsistent                   0.18.0
> > pytesseract                  0.3.10
> > python-apt                   2.4.0+ubuntu2
> > python-dateutil              2.8.2
> > python-docx                  0.8.11
> > python-editor                1.0.4
> > python-lsp-jsonrpc           1.0.0
> > python-lsp-server            1.7.1
> > python-magic                 0.4.27
> > pytoolconfig                 1.2.2
> > pytz                         2022.1
> > PyWavelets                   1.3.0
> > PyYAML                       6.0
> > pyzmq                        23.2.0
> > ragas                        0.0.12
> > regex                        2022.7.9
> > requests                     2.28.1
> > requests-oauthlib            1.3.1
> > responses                    0.18.0
> > rfc3339-validator            0.1.4
> > rfc3987                      1.3.8
> > rope                         1.7.0
> > rsa                          4.9
> > s3transfer                   0.6.0
> > safetensors                  0.3.3
> > scikit-learn                 1.1.1
> > scipy                        1.9.1
> > seaborn                      0.11.2
> > SecretStorage                3.3.1
> > Send2Trash                   1.8.0
> > sentence-transformers        2.2.2
> > sentencepiece                0.1.97
> > setuptools                   63.4.1
> > shap                         0.41.0
> > simplejson                   3.17.6
> > six                          1.16.0
> > skops                        0.8.0
> > slicer                       0.0.7
> > smart-open                   5.2.1
> > smmap                        5.0.0
> > soundfile                    0.12.1
> > soupsieve                    2.3.1
> > soxr                         0.3.5
> > spacy                        3.5.1
> > spacy-cleaner                3.1.3
> > spacy-legacy                 3.0.12
> > spacy-loggers                1.0.4
> > spacy-lookups-data           1.0.5
> > spark-tensorflow-distributor 1.0.0
> > SQLAlchemy                   1.4.39
> > sqlparse                     0.4.2
> > srsly                        2.4.6
> > ssh-import-id                5.11
> > stack-data                   0.6.2
> > statsmodels                  0.13.2
> > swagger-spec-validator       3.0.3
> > tabulate                     0.8.10
> > tangled-up-in-unicode        0.2.0
> > tenacity                     8.1.0
> > tensorboard                  2.11.0
> > tensorboard-data-server      0.6.1
> > tensorboard-plugin-profile   2.11.2
> > tensorboard-plugin-wit       1.8.1
> > tensorflow                   2.11.1
> > tensorflow-estimator         2.11.0
> > tensorflow-io-gcs-filesystem 0.32.0
> > termcolor                    2.3.0
> > terminado                    0.13.1
> > testpath                     0.6.0
> > thinc                        8.1.10
> > threadpoolctl                2.2.0
> > tiktoken                     0.3.3
> > tokenize-rt                  4.2.1
> > tokenizers                   0.13.3
> > tomli                        2.0.1
> > torch                        1.13.1+cu117
> > torchvision                  0.14.1+cu117
> > tornado                      6.1
> > tqdm                         4.64.1
> > traitlets                    5.1.1
> > transformers                 4.33.3
> > typeguard                    2.13.3
> > typer                        0.7.0
> > typing_extensions            4.3.0
> > typing-inspect               0.8.0
> > ujson                        5.4.0
> > unattended-upgrades          0.1
> > unstructured                 0.9.0
> > uri-template                 1.3.0
> > urllib3                      1.26.11
> > virtualenv                   20.16.3
> > visions                      0.7.5
> > wadllib                      1.3.6
> > wasabi                       0.10.1
> > watermark                    2.4.3
> > wcwidth                      0.2.5
> > webcolors                    1.13
> > webencodings                 0.5.1
> > websocket-client             0.58.0
> > Werkzeug                     2.0.3
> > whatthepatch                 1.0.2
> > wheel                        0.37.1
> > widgetsnbextension           3.6.1
> > wrapt                        1.14.1
> > xgboost                      1.7.5
> > xxhash                       3.2.0
> > yapf                         0.31.0
> > yarl                         1.9.2
> > ydata-profiling              4.1.2
> > zipp                         3.8.0
> > 
> > </details>
> > 
> > Here are two lines of the sample code pulled from the [model card](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1):
> > 
> > ```python
> > model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> > tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> > ```
> > 
> > <details>
> > <summary>Here is the error I am receiving:</summary>
> > <br>
> > 
> > ```sh
> > ---------------------------------------------------------------------------
> > KeyError                                  Traceback (most recent call last)
> > File <command-4309329534262302>:1
> > ----> 1 model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> >       2 tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> > 
> > File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
> >     438     if kwargs_copy.get(""torch_dtype"", None) == ""auto"":
> >     439         _ = kwargs_copy.pop(""torch_dtype"")
> > --> 441     config, kwargs = AutoConfig.from_pretrained(
> >     442         pretrained_model_name_or_path,
> >     443         return_unused_kwargs=True,
> >     444         trust_remote_code=trust_remote_code,
> >     445         **hub_kwargs,
> >     446         **kwargs_copy,
> >     447     )
> >     448 if hasattr(config, ""auto_map"") and cls.__name__ in config.auto_map:
> >     449     if not trust_remote_code:
> > 
> > File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:937, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
> >     935     return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)
> >     936 elif ""model_type"" in config_dict:
> > --> 937     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
> >     938     return config_class.from_dict(config_dict, **unused_kwargs)
> >     939 else:
> >     940     # Fallback: use pattern matching on the string.
> >     941     # We go from longer names to shorter names to catch roberta before bert (for instance)
> > 
> > File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:643, in _LazyConfigMapping.__getitem__(self, key)
> >     641     return self._extra_content[key]
> >     642 if key not in self._mapping:
> > --> 643     raise KeyError(key)
> >     644 value = self._mapping[key]
> >     645 module_name = model_type_to_module_name(key)
> > 
> > KeyError: 'mistral'
> > ```
> > 
> > </details>
> 
> you need to update the transformers package to the latest.

`transformers == 4.33.3` is not the latest?

That is what I shared above and from what I can see on GitHub, `4.33.3` [appears to be the latest release](https://github.com/huggingface/transformers/releases) (released 12 hours ago). "
dongXL,2023-09-28 05:51:40+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> > > I am also getting this error as well using the sample code for this model.
> > > 
> > > <details>
> > > <summary>Here is an abbreviated environment:</summary>
> > > <br>
> > > 
> > > Package                            Version
> > > --------------------------     -------------
> > > absl-py                             1.0.0
> > > accelerate                       0.21.0
> > > aiohttp                             3.8.4
> > > aiosignal                         1.3.1
> > > alembic                          1.12.0
> > > appdirs                           1.4.4
> > > argon2-cffi                    21.3.0
> > > argon2-cffi-bindings         21.2.0
> > > arrow                        1.2.3
> > > astor                        0.8.1
> > > asttokens                    2.2.1
> > > astunparse                   1.6.3
> > > async-timeout                4.0.2
> > > attrs                        21.4.0
> > > audioread                    3.0.0
> > > azure-core                   1.26.4
> > > azure-cosmos                 4.3.1b1
> > > azure-storage-blob           12.16.0
> > > azure-storage-file-datalake  12.11.0
> > > backcall                     0.2.0
> > > bcrypt                       3.2.0
> > > beautifulsoup4               4.11.1
> > > bitsandbytes                 0.41.0
> > > black                        22.6.0
> > > bleach                       4.1.0
> > > blinker                      1.4
> > > blis                         0.7.9
> > > boto3                        1.24.28
> > > botocore                     1.27.28
> > > bravado                      11.0.3
> > > bravado-core                 6.1.0
> > > cachetools                   4.2.4
> > > catalogue                    2.0.8
> > > category-encoders            2.6.0
> > > certifi                      2022.9.14
> > > cffi                         1.15.1
> > > chardet                      4.0.0
> > > charset-normalizer           2.0.4
> > > click                        8.0.4
> > > cloudpickle                  2.0.0
> > > cmaes                        0.10.0
> > > cmdstanpy                    1.1.0
> > > colorlog                     6.7.0
> > > confection                   0.0.4
> > > configparser                 5.2.0
> > > convertdate                  2.4.0
> > > cryptography                 37.0.1
> > > cycler                       0.11.0
> > > cymem                        2.0.7
> > > Cython                       0.29.32
> > > databricks-automl-runtime    0.2.16
> > > databricks-cli               0.17.6
> > > databricks-feature-store     0.12.1
> > > dataclasses-json             0.5.7
> > > datasets                     2.12.0
> > > dbl-tempo                    0.1.23
> > > dbus-python                  1.2.18
> > > debugpy                      1.5.1
> > > decorator                    5.1.1
> > > defusedxml                   0.7.1
> > > dill                         0.3.4
> > > diskcache                    5.6.1
> > > distlib                      0.3.6
> > > distro                       1.7.0
> > > distro-info                  1.1+ubuntu0.1
> > > doc2text                     0.2.4
> > > docstring-to-markdown        0.12
> > > einops                       0.6.1
> > > entrypoints                  0.4
> > > ephem                        4.1.4
> > > et-xmlfile                   1.1.0
> > > evaluate                     0.4.0
> > > executing                    1.2.0
> > > facets-overview              1.0.3
> > > faiss-gpu                    1.7.2
> > > fastjsonschema               2.16.3
> > > fasttext                     0.9.2
> > > filelock                     3.6.0
> > > filetype                     1.2.0
> > > Flask                        1.1.2+db1
> > > flatbuffers                  23.5.9
> > > fonttools                    4.25.0
> > > fqdn                         1.5.1
> > > frozenlist                   1.3.3
> > > fsspec                       2022.7.1
> > > future                       0.18.2
> > > gast                         0.4.0
> > > gitdb                        4.0.10
> > > GitPython                    3.1.27
> > > google-api-core              2.8.2
> > > google-auth                  1.33.0
> > > google-auth-oauthlib         0.4.6
> > > google-cloud-core            2.3.2
> > > google-cloud-storage         2.9.0
> > > google-crc32c                1.5.0
> > > google-pasta                 0.2.0
> > > google-resumable-media       2.5.0
> > > googleapis-common-protos     1.56.4
> > > greenlet                     1.1.1
> > > grpcio                       1.48.1
> > > grpcio-status                1.48.1
> > > gunicorn                     20.1.0
> > > gviz-api                     1.10.0
> > > h5py                         3.7.0
> > > hijri-converter              2.3.1
> > > holidays                     0.22
> > > horovod                      0.27.0
> > > htmlmin                      0.1.12
> > > httplib2                     0.20.2
> > > huggingface-hub              0.17.3
> > > idna                         3.3
> > > ImageHash                    4.3.1
> > > imbalanced-learn             0.8.1
> > > importlib-metadata           4.11.3
> > > ipykernel                    6.17.1
> > > ipython                      8.10.0
> > > ipython-genutils             0.2.0
> > > ipywidgets                   7.7.2
> > > isodate                      0.6.1
> > > isoduration                  20.11.0
> > > itsdangerous                 2.0.1
> > > jedi                         0.18.1
> > > jeepney                      0.7.1
> > > Jinja2                       2.11.3
> > > jmespath                     0.10.0
> > > joblib                       1.2.0
> > > joblibspark                  0.5.1
> > > jsonpointer                  2.4
> > > jsonref                      1.1.0
> > > jsonschema                   4.16.0
> > > jupyter-client               7.3.4
> > > jupyter_core                 4.11.2
> > > jupyterlab-pygments          0.1.2
> > > jupyterlab-widgets           1.0.0
> > > keras                        2.11.0
> > > keyring                      23.5.0
> > > kiwisolver                   1.4.2
> > > korean-lunar-calendar        0.3.1
> > > langchain                    0.0.285
> > > langcodes                    3.3.0
> > > langsmith                    0.0.41
> > > launchpadlib                 1.10.16
> > > lazr.restfulclient           0.14.4
> > > lazr.uri                     1.0.6
> > > lazy_loader                  0.2
> > > libclang                     15.0.6.1
> > > librosa                      0.10.0
> > > lightgbm                     3.3.5
> > > llvmlite                     0.38.0
> > > LunarCalendar                0.0.9
> > > lxml                         4.9.3
> > > Mako                         1.2.0
> > > Markdown                     3.3.4
> > > MarkupSafe                   2.0.1
> > > marshmallow                  3.19.0
> > > marshmallow-enum             1.5.1
> > > matplotlib                   3.5.2
> > > matplotlib-inline            0.1.6
> > > mccabe                       0.7.0
> > > mime                         0.1.0
> > > mistune                      0.8.4
> > > mleap                        0.20.0
> > > mlflow-skinny                2.3.1
> > > monotonic                    1.6
> > > more-itertools               8.10.0
> > > msgpack                      1.0.5
> > > multidict                    6.0.4
> > > multimethod                  1.9.1
> > > multiprocess                 0.70.12.2
> > > murmurhash                   1.0.9
> > > mypy-extensions              0.4.3
> > > nbclient                     0.5.13
> > > nbconvert                    6.4.4
> > > nbformat                     5.5.0
> > > neptune-client               0.16.18
> > > neptune-optuna               1.1.0
> > > nest-asyncio                 1.5.5
> > > networkx                     2.8.4
> > > nltk                         3.7
> > > nodeenv                      1.8.0
> > > notebook                     6.4.12
> > > numba                        0.55.1
> > > numexpr                      2.8.4
> > > numpy                        1.21.5
> > > oauthlib                     3.2.0
> > > openai                       0.27.4
> > > openapi-schema-pydantic      1.2.4
> > > openpyxl                     3.1.2
> > > opt-einsum                   3.3.0
> > > optuna                       3.3.0
> > > packaging                    21.3
> > > pandas                       1.4.4
> > > pandocfilters                1.5.0
> > > paramiko                     2.9.2
> > > parso                        0.8.3
> > > pathspec                     0.9.0
> > > pathy                        0.10.1
> > > patsy                        0.5.2
> > > petastorm                    0.12.1
> > > pexpect                      4.8.0
> > > phik                         0.12.3
> > > pickleshare                  0.7.5
> > > Pillow                       9.2.0
> > > pip                          23.2.1
> > > platformdirs                 2.5.2
> > > plotly                       5.9.0
> > > pluggy                       1.0.0
> > > pmdarima                     2.0.3
> > > pooch                        1.7.0
> > > preshed                      3.0.8
> > > prompt-toolkit               3.0.36
> > > prophet                      1.1.2
> > > protobuf                     3.19.4
> > > psutil                       5.9.0
> > > psycopg2                     2.9.3
> > > ptyprocess                   0.7.0
> > > pure-eval                    0.2.2
> > > pyarrow                      8.0.0
> > > pyasn1                       0.4.8
> > > pyasn1-modules               0.2.8
> > > pybind11                     2.10.4
> > > pycparser                    2.21
> > > pydantic                     1.10.6
> > > pyflakes                     3.0.1
> > > Pygments                     2.11.2
> > > PyGObject                    3.42.1
> > > PyJWT                        2.3.0
> > > PyMeeus                      0.5.12
> > > PyNaCl                       1.5.0
> > > pyodbc                       4.0.32
> > > pyparsing                    3.0.9
> > > PyPDF2                       3.0.1
> > > pyright                      1.1.294
> > > pyrsistent                   0.18.0
> > > pytesseract                  0.3.10
> > > python-apt                   2.4.0+ubuntu2
> > > python-dateutil              2.8.2
> > > python-docx                  0.8.11
> > > python-editor                1.0.4
> > > python-lsp-jsonrpc           1.0.0
> > > python-lsp-server            1.7.1
> > > python-magic                 0.4.27
> > > pytoolconfig                 1.2.2
> > > pytz                         2022.1
> > > PyWavelets                   1.3.0
> > > PyYAML                       6.0
> > > pyzmq                        23.2.0
> > > ragas                        0.0.12
> > > regex                        2022.7.9
> > > requests                     2.28.1
> > > requests-oauthlib            1.3.1
> > > responses                    0.18.0
> > > rfc3339-validator            0.1.4
> > > rfc3987                      1.3.8
> > > rope                         1.7.0
> > > rsa                          4.9
> > > s3transfer                   0.6.0
> > > safetensors                  0.3.3
> > > scikit-learn                 1.1.1
> > > scipy                        1.9.1
> > > seaborn                      0.11.2
> > > SecretStorage                3.3.1
> > > Send2Trash                   1.8.0
> > > sentence-transformers        2.2.2
> > > sentencepiece                0.1.97
> > > setuptools                   63.4.1
> > > shap                         0.41.0
> > > simplejson                   3.17.6
> > > six                          1.16.0
> > > skops                        0.8.0
> > > slicer                       0.0.7
> > > smart-open                   5.2.1
> > > smmap                        5.0.0
> > > soundfile                    0.12.1
> > > soupsieve                    2.3.1
> > > soxr                         0.3.5
> > > spacy                        3.5.1
> > > spacy-cleaner                3.1.3
> > > spacy-legacy                 3.0.12
> > > spacy-loggers                1.0.4
> > > spacy-lookups-data           1.0.5
> > > spark-tensorflow-distributor 1.0.0
> > > SQLAlchemy                   1.4.39
> > > sqlparse                     0.4.2
> > > srsly                        2.4.6
> > > ssh-import-id                5.11
> > > stack-data                   0.6.2
> > > statsmodels                  0.13.2
> > > swagger-spec-validator       3.0.3
> > > tabulate                     0.8.10
> > > tangled-up-in-unicode        0.2.0
> > > tenacity                     8.1.0
> > > tensorboard                  2.11.0
> > > tensorboard-data-server      0.6.1
> > > tensorboard-plugin-profile   2.11.2
> > > tensorboard-plugin-wit       1.8.1
> > > tensorflow                   2.11.1
> > > tensorflow-estimator         2.11.0
> > > tensorflow-io-gcs-filesystem 0.32.0
> > > termcolor                    2.3.0
> > > terminado                    0.13.1
> > > testpath                     0.6.0
> > > thinc                        8.1.10
> > > threadpoolctl                2.2.0
> > > tiktoken                     0.3.3
> > > tokenize-rt                  4.2.1
> > > tokenizers                   0.13.3
> > > tomli                        2.0.1
> > > torch                        1.13.1+cu117
> > > torchvision                  0.14.1+cu117
> > > tornado                      6.1
> > > tqdm                         4.64.1
> > > traitlets                    5.1.1
> > > transformers                 4.33.3
> > > typeguard                    2.13.3
> > > typer                        0.7.0
> > > typing_extensions            4.3.0
> > > typing-inspect               0.8.0
> > > ujson                        5.4.0
> > > unattended-upgrades          0.1
> > > unstructured                 0.9.0
> > > uri-template                 1.3.0
> > > urllib3                      1.26.11
> > > virtualenv                   20.16.3
> > > visions                      0.7.5
> > > wadllib                      1.3.6
> > > wasabi                       0.10.1
> > > watermark                    2.4.3
> > > wcwidth                      0.2.5
> > > webcolors                    1.13
> > > webencodings                 0.5.1
> > > websocket-client             0.58.0
> > > Werkzeug                     2.0.3
> > > whatthepatch                 1.0.2
> > > wheel                        0.37.1
> > > widgetsnbextension           3.6.1
> > > wrapt                        1.14.1
> > > xgboost                      1.7.5
> > > xxhash                       3.2.0
> > > yapf                         0.31.0
> > > yarl                         1.9.2
> > > ydata-profiling              4.1.2
> > > zipp                         3.8.0
> > > 
> > > </details>
> > > 
> > > Here are two lines of the sample code pulled from the [model card](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1):
> > > 
> > > ```python
> > > model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> > > tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> > > ```
> > > 
> > > <details>
> > > <summary>Here is the error I am receiving:</summary>
> > > <br>
> > > 
> > > ```sh
> > > ---------------------------------------------------------------------------
> > > KeyError                                  Traceback (most recent call last)
> > > File <command-4309329534262302>:1
> > > ----> 1 model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> > >       2 tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
> > > 
> > > File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
> > >     438     if kwargs_copy.get(""torch_dtype"", None) == ""auto"":
> > >     439         _ = kwargs_copy.pop(""torch_dtype"")
> > > --> 441     config, kwargs = AutoConfig.from_pretrained(
> > >     442         pretrained_model_name_or_path,
> > >     443         return_unused_kwargs=True,
> > >     444         trust_remote_code=trust_remote_code,
> > >     445         **hub_kwargs,
> > >     446         **kwargs_copy,
> > >     447     )
> > >     448 if hasattr(config, ""auto_map"") and cls.__name__ in config.auto_map:
> > >     449     if not trust_remote_code:
> > > 
> > > File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:937, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
> > >     935     return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)
> > >     936 elif ""model_type"" in config_dict:
> > > --> 937     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
> > >     938     return config_class.from_dict(config_dict, **unused_kwargs)
> > >     939 else:
> > >     940     # Fallback: use pattern matching on the string.
> > >     941     # We go from longer names to shorter names to catch roberta before bert (for instance)
> > > 
> > > File /databricks/python/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:643, in _LazyConfigMapping.__getitem__(self, key)
> > >     641     return self._extra_content[key]
> > >     642 if key not in self._mapping:
> > > --> 643     raise KeyError(key)
> > >     644 value = self._mapping[key]
> > >     645 module_name = model_type_to_module_name(key)
> > > 
> > > KeyError: 'mistral'
> > > ```
> > > 
> > > </details>
> > 
> > you need to update the transformers package to the latest.
> 
> `transformers == 4.33.3` is not the latest?
> 
> That is what I shared above and from what I can see on GitHub, `4.33.3` [appears to be the latest release](https://github.com/huggingface/transformers/releases) (released 12 hours ago).

pip install git+https://huggingface/transformers.git   
4.34.0dev support it"
dhavaltaunk08,2023-09-28 06:20:38+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Running the following command seems to be working:

```
pip install git+https://github.com/huggingface/transformers 
```"
1littlecoder,2023-09-28 13:42:32+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Did the model load for anyone on Free Colab? The RAM keeps crashing for me 
manujmalik,2023-09-29 09:22:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Did the model load for anyone on Free Colab? The RAM keeps crashing for me

maybe try using gguf model by TheBloke?"
stopfogs,2023-09-29 22:19:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"has anyone ran this on a MBP16 32k ?
"
AbishekNairM,2023-10-05 10:49:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"this code: import torch
import transformers
from transformers import GenerationConfig, pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig
import bitsandbytes as bnb

import torch
import transformers
from transformers import GenerationConfig, pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(""ehartford/samantha-mistral-7b"",use_fast=False)
model = AutoModelForCausalLM.from_pretrained(""ehartford/samantha-mistral-7b"",
                                              load_in_8bit=True,
                                              device_map='auto',
                                              torch_dtype=torch.float16,
                                              low_cpu_mem_usage=True,
                                              )
gave the same mentioned error: ---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-23-de16999e6154> in <cell line: 14>()
     12 
     13 tokenizer = AutoTokenizer.from_pretrained(""ehartford/samantha-mistral-7b"",use_fast=False)
---> 14 model = AutoModelForCausalLM.from_pretrained(""ehartford/samantha-mistral-7b"",
     15                                               load_in_8bit=True,
     16                                               device_map='auto',

2 frames
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py in __getitem__(self, key)
    708 
    709 def model_type_to_module_name(key):
--> 710     """"""Converts a config key to the corresponding module.""""""
    711     # Special treatment
    712     if key in SPECIAL_MODEL_TYPE_TO_MODULE_NAME:

KeyError: 'mistral'
then u tried the installation of tranformers as given above: pip install git+https://github.com/huggingface/transformers

I tried following this guide from hugging face and got the issue: https://huggingface.co/blog/Andyrasika/samantha-and-mistral-7b

I DID THIS ON GOOGLE COLLAB FREE VERSION.

Help would be appreciated, I was tryna run langchain to see its performance in presenting information from a chroma vector database (Chroma seems to be the goto), I tried for falcon and got repetition error, LLama 13b ggml quantized had a guide and it worked beautifully. So trying to run with Mistral, whichever works.

I just told the last part for context. Help will be appreciated!!"
Kromtar,2023-10-11 20:40:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@AbishekNairM 

Install the last version of transformers:
%pip install git+https://github.com/huggingface/transformers 

And then restart the runtime to apply the changes"
AbishekNairM,2023-10-12 05:16:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@Kromtar
Ah I just ran a GGML quantised version of Mistral 7b with Llama-index and it worked perfectly well with fast inferences. The framework just made the entire process more convenient. Trying to figure out how to calculate max input/output token length for queries and cpu/gpu usage and of course time elapsed. If you have any leads please tell me."
i-darrshan,2023-11-05 05:34:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Did the model load for anyone on Free Colab? The RAM keeps crashing for me

Try using Kaggle. Use a T4 or P100 instance"
AbishekNairM,2023-11-05 05:36:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,@i-darrshan yes. I ran it on Collab with llama-index and llama-cpp. But I ran a GGUF version of it.
i-darrshan,2023-11-05 05:40:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@AbishekNairM. Hey mate, I have a doubt. Have you ever used ctransformers. I used ""TheBloke's Mistral 7B gguf with Q4_K_M"" in colab with ctransformers. I'm getting incomplete answers and also sometimes no answer. Being a beginner, I'm unable to figure out why. Could anyone help on this?"
AbishekNairM,2023-11-05 05:45:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"@i-darrshan I have only used hugging faces auto tokenizers before this but since mistral didn't have one at the time I was about to use ctransformers since it's the goto if u don't have prebuilt tokenizers. But I read llama index worked really well with these two. In my opinion you should just use llama index. If you were trying to use maybe falcon you can consider ctransformer, since your gonna have to edit preprompts in llamaindex. Well I would still just try to figure that out in llamaindex tbh. It's just so much better and convenient. I vouch for llama index for Mistral and Llama models 100 percent."
i-darrshan,2023-11-05 05:48:24+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> @i-darrshan I have only used hugging faces auto tokenizers before this but since mistral didn't have one at the time I was about to use ctransformers since it's the goto if u don't have prebuilt tokenizers. But I read llama index worked really well with these two. In my opinion you should just use llama index. If you were trying to use maybe falcon you can consider ctransformer, since your gonna have to edit preprompts in llamaindex. Well I would still just try to figure that out in llamaindex tbh. It's just so much better and convenient. I vouch for llama index for Mistral and Llama models 100 percent.

@AbishekNairM sure, I will try using llama-cpp-python. Thanks"
YanaS,2023-10-03 12:52:10+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Yes, even if it is not the dataset, I would be glad to know what the format is, so I can try generate one for finetuning"
MasterminD1994,2023-09-27 17:40:58+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
File <command-2181207281529057>:5
      1 from transformers import AutoModelForCausalLM, AutoTokenizer
      3 device = ""cuda"" # the device to load the model onto
----> 5 model = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
      6 tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.1"")
      8 text = ""<s>[INST] What is your favourite condiment? [/INST]""

File /databricks/python/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:434, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    432 hub_kwargs = {name: kwargs.pop(name) for name in hub_kwargs_names if name in kwargs}
    433 if not isinstance(config, PretrainedConfig):
--> 434     config, kwargs = AutoConfig.from_pretrained(
    435         pretrained_model_name_or_path,
    436         return_unused_kwargs=True,
    437         trust_remote_code=trust_remote_code,
    438         **hub_kwargs,
    439         **kwargs,
    440     )
    441 if hasattr(config, ""auto_map"") and cls.__name__ in config.auto_map:
    442     if not trust_remote_code:

File /databricks/python/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:829, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    827     return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)
    828 elif ""model_type"" in config_dict:
--> 829     config_class = CONFIG_MAPPING[config_dict[""model_type""]]
    830     return config_class.from_dict(config_dict, **unused_kwargs)
    831 else:
    832     # Fallback: use pattern matching on the string.
    833     # We go from longer names to shorter names to catch roberta before bert (for instance)

File /databricks/python/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:536, in _LazyConfigMapping.__getitem__(self, key)
    534     return self._extra_content[key]
    535 if key not in self._mapping:
--> 536     raise KeyError(key)
    537 value = self._mapping[key]
    538 module_name = model_type_to_module_name(key)

KeyError: 'mistral'
"
Undi95,2023-09-27 17:46:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"rename ""mistral"" to ""llama"" into the config.json file.
work on some of my tools, crash on others, I let you try lmao.
try it just for the funnies"
razent,2023-09-27 17:54:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,Try to install the latest `transformers` from the source. It will work
CREET01,2023-09-27 18:26:50+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Had this problem. Updated Transformers and renamed ""mistral"" to ""llama"" in config.json file. Fixed."
NS-Y,2023-09-27 18:33:21+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,The latest transformers from the source solved it.
dheerajpai,2023-09-27 22:35:05+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,I see
dheerajpai,2023-09-27 22:37:51+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Installing transformers from source helps - 

```python
pip install --upgrade git+https://github.com/huggingface/transformers
```"
silvacarl,2023-09-28 01:06:12+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,upgraded to latest transofmer install still getting this error.
Undi95,2023-09-28 01:09:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> upgraded to latest transofmer install still getting this error.


![Code_zfZEP9oBNa.gif](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/9S80qfQwHgNdDWjhB7OAj.gif)
"
silvacarl,2023-09-28 01:11:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,nice thx
narenzen,2023-09-28 05:59:03+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"> Installing transformers from source helps - 
> 
> ```python
> pip install --upgrade git+https://github.com/huggingface/transformers
> ```

This is working"
timlacroix,2023-09-28 14:35:48+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Yes, we are supported on main, so will be included in the next release, but are not included in the current release (v4.33.3)."
abolotnov,2023-09-28 21:43:00+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"if you installed transformers from source, don't forget to reset your kernel before you try again"
medrayen,2023-10-04 17:20:55+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"im using pipline ,where i can find config.json"
leloss,2023-10-06 16:46:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"No changes to config necessary. Updating transformers alone worked for me:
Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)
Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)
Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)
"
Rocketknight1,2023-09-27 17:34:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"Hey! This PR adds a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) and updates the README example to use it, so users don't have to stress about implementing the chat formatting themselves."
teven,2023-09-28 16:30:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.1,"LGTM, thanks ! Nice feature you've got there"
davidhung,2023-07-20 14:18:51+00:00,comment,tiiuae/falcon-40b-instruct,"Loading in the model went from 5-10 mins to 30 seconds
"
FalconLLM,2023-05-30 07:08:55+00:00,comment,tiiuae/falcon-40b-instruct,"Hey! 

Falcon-40B-Instruct was finetuned on the [Bai ze](https://github.com/project-baize/baize-chatbot) data -- reproducing a similar format ought to help. We have had reasonable success internally with a basic prompt like:

```
User: Can you tell me what is brutalism?
Assistant:
```

You can also see [this discussion](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1) for more information!"
asach,2023-05-30 16:15:12+00:00,comment,tiiuae/falcon-40b-instruct,Thank you ! 🙏 
silvacarl,2023-06-08 22:09:22+00:00,comment,tiiuae/falcon-40b-instruct,excellent thx!
silvacarl,2023-06-08 22:46:09+00:00,comment,tiiuae/falcon-40b-instruct,"this works great:

Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction: Extract and display only the first date of birth found in the following sentence and display it in ISO format and ignore any other text and if no date of birth is found display only 01/01/1901\n### Input:Transcript: I am calling about a patient with date of birth july fifth 1975.\n### Response:"
Dennison33,2024-02-26 19:06:40+00:00,comment,tiiuae/falcon-40b-instruct,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
nkasmanoff,2023-09-19 11:49:00+00:00,comment,tiiuae/falcon-40b-instruct,"

Update generation config to correct token ids for BOS and EOS
"
nkasmanoff,2023-09-19 11:48:34+00:00,comment,tiiuae/falcon-40b-instruct,"

Update generation config to correct token ids for BOS and EOS
"
oafzal,2023-09-10 22:01:48+00:00,comment,tiiuae/falcon-40b-instruct,"Hi,
I have been trying to use the Falcon-40B instruct with Arena, and it seems like the model breaks in multi-turn conversation. It keeps generating random conversations itself. On the other hand, the following deployment by huggingface works just fine. `https://huggingface.co/spaces/HuggingFaceH4/falcon-chat`. Is there a specific template that was used for instruction tuning the model?"
Liam-Dugan,2023-10-27 17:15:02+00:00,comment,tiiuae/falcon-40b-instruct,Am also interested to know this. Would like some help with conforming to the template used when instruction tuning.
rstanley90,2023-10-27 15:21:23+00:00,comment,tiiuae/falcon-40b-instruct,Did you ever find a solution to this? I'm having the same issue on my A100 as well.
bshongwe,2023-10-30 11:03:41+00:00,comment,tiiuae/falcon-40b-instruct,"Hey. No I didn't find a solution. Instead of running it on GKE, I switched to using dedicated GCP VMs to deploy the inference end point"
rstanley90,2023-10-30 14:44:14+00:00,comment,tiiuae/falcon-40b-instruct,I was able to find a solution that may be helpful for you. Try disabling the custom kernels via the environment variable `DISABLE_CUSTOM_KERNELS=true`. This has been suggested in [other](https://github.com/huggingface/text-generation-inference/issues/739#issuecomment-1669143730) posts on the github page for the HF text-generation-inference server with success. I'm not familiar with the inner workings of the system to know what exactly this is doing but the server appears to be running fine with this flag set. 
Shridharalve,2023-08-27 16:57:33+00:00,comment,tiiuae/falcon-40b-instruct,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
Shridharalve,2023-08-27 16:26:02+00:00,comment,tiiuae/falcon-40b-instruct,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
jlzhou,2023-08-25 14:06:57+00:00,comment,tiiuae/falcon-40b-instruct,"It is first loaded from disk to CPU memory (or RAM), then transfered to GPU memory (or VRAM).

If it is killed it usually means you don't have enough RAM. A VRAM oom won't kill your progress.
"
ashivadi,2023-07-13 15:55:02+00:00,comment,tiiuae/falcon-40b-instruct,"Could be because of this change: https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232 and line here
https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233"
rodgermoore,2023-07-14 05:01:57+00:00,comment,tiiuae/falcon-40b-instruct,"> Could be because of this change: https://huggingface.co/tiiuae/falcon-40b/commit/f1ba7d328c06aa6fbb4a8afd3c756f46d7e6b232 and line here
> https://github.com/huggingface/text-generation-inference/blob/b7327205a6f2f2c6349e75b8ea484e1e2823075a/server/text_generation_server/models/__init__.py#L233

This is exactly what I'm running into when trying to make this work. I thought this was an issue with the HF inference server, thanks for pointing this out!"
hugefan,2023-07-14 08:25:16+00:00,comment,tiiuae/falcon-40b-instruct,"The problematic change has been reverted with https://huggingface.co/tiiuae/falcon-40b-instruct/commit/ca78eac0ed45bf64445ff0687fabba1598daebf3 , everything works like before now with the currently released files on main."
mkserge,2023-07-20 21:11:42+00:00,comment,tiiuae/falcon-40b-instruct,"Hello,

I am still running into the same issue, with 7b-instruct version when explicitly pointing to the commit that reverts the change
```
config = {
  'HF_MODEL_ID': ""tiiuae/falcon-7b-instruct"", # model_id from hf.co/models
  'HF_MODEL_REVISION': ""eb410fb6ffa9028e97adb801f0d6ec46d02f8b07"",
  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica
  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text
  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)
  # 'HF_MODEL_QUANTIZE': ""bitsandbytes"", # comment in to quantize
}
```

Running on `ml.g5.48xlarge` with `number_of_gpu = 8` above.

Any ideas what could be wrong in my setup?"
axiom-of-choice,2023-07-27 01:02:00+00:00,comment,tiiuae/falcon-40b-instruct,Still having the issue...
mkserge,2023-07-27 01:28:30+00:00,comment,tiiuae/falcon-40b-instruct,"> Still having the issue...

It turns out sharding is not supported for 7B variants. You need to make sure to either choose an instance that has a single GPU, or explicitly choose `number_of_gpu = 1` in your config.
"
valenlopez3,2023-10-10 12:16:25+00:00,comment,tiiuae/falcon-40b-instruct,"Hello, I re-opened this discussion because I found the issue reported again after the last commit.

Here is my configuration:
{
       ""HF_MODEL_ID"": ""tiiuae/falcon-40b-instruct"",
       ""SM_NUM_GPUS"": ""4"",
       ""HF_MODEL_QUANTIZE"": ""bitsandbytes"",
       ""MAX_INPUT_LENGTH"": ""1024"",
       ""MAX_TOTAL_TOKENS"": ""2048""
}

With this configuration and using an instance `ml.g5.12xlarge` I get the error message: ValueError: sharded is not supported for AutoModel

Adding ""HF_MODEL_REVISION"": ca78eac0ed45bf64445ff0687fabba1598daebf3 to deploy the previous commit works perfectly fine.

The issue is again in the last commit uploaded: ecb78d97ac356d098e79f0db222c9ce7c5d9ee5f"
bikalnetomi,2023-10-12 15:24:54+00:00,comment,tiiuae/falcon-40b-instruct,"I ran into the same issue today. 
Changed the revision and it works fine as mentioned by valenlopez3  in the above thread"
momentumhd,2023-07-26 09:13:17+00:00,comment,tiiuae/falcon-40b-instruct,"Hello! 
Why was this reverted? It was quite helpfull"
sgugger,2023-07-26 15:10:35+00:00,comment,tiiuae/falcon-40b-instruct,We are waiting for the rest of the ecosystem (in particular text-generation-inference) to catch up with the changes in this and will put it back soon!
yozetta,2023-07-13 17:13:05+00:00,comment,tiiuae/falcon-40b-instruct,"It's a parameter that you pass in when setting up the transformers pipeline.
If you're trying to do inference on Falcon-40b-instruct, you could try an API that's running it: https://ai.chainconductor.io/3NN8BnB"
LetDC,2023-07-19 14:38:17+00:00,comment,tiiuae/falcon-40b-instruct,I have the same problem!
Liam-Dugan,2023-10-27 17:02:50+00:00,comment,tiiuae/falcon-40b-instruct,I am also wondering this! The model seems to output this frequently
Aillian,2023-09-08 18:40:13+00:00,comment,tiiuae/falcon-40b-instruct,I have the same question... 
GDoc,2023-08-12 10:56:42+00:00,comment,tiiuae/falcon-40b-instruct,"Where can i find all 9 Safetensor files?
I tried to convert the model using the UI and ran into OOM error... "
GDoc,2023-09-07 07:49:22+00:00,comment,tiiuae/falcon-40b-instruct,"> Where can i find all 9 Safetensor files?
> I tried to convert the model using the UI and ran into OOM error...

I have since used this to convert the model to Safetensor format:
https://github.com/Silver267/pytorch-to-safetensor-converter/tree/main"
FelixMildon,2023-07-08 13:14:07+00:00,comment,tiiuae/falcon-40b-instruct,"On the main page it says: 

""Hosted inference API
Text Generation
Inference API has been turned off for this model."""
esteere,2023-06-29 05:01:24+00:00,comment,tiiuae/falcon-40b-instruct,This (https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226) *should* perform on par.
psinger,2023-06-29 06:55:32+00:00,comment,tiiuae/falcon-40b-instruct,"I am aware of alternative, but would like to have clarification on this given also its popularity."
lovodkin93,2023-09-27 14:02:14+00:00,comment,tiiuae/falcon-40b-instruct,This comment has been hidden
LauBen,2023-11-20 17:21:51+00:00,comment,tiiuae/falcon-40b-instruct,This comment has been hidden
troybvo,2023-06-25 05:30:13+00:00,comment,tiiuae/falcon-40b-instruct,HuggingFaceEndpoint truncates the text because it assumes the endpoint returns the prompt together with generated text. You need to modify the _call method of HuggingFaceEndpoint so that it doesn't substring the generated_text and return the whole text.
TMTechnology,2023-06-27 03:35:30+00:00,comment,tiiuae/falcon-40b-instruct,"> HuggingFaceEndpoint truncates the text because it assumes the endpoint returns the prompt together with generated text. You need to modify the _call method of HuggingFaceEndpoint so that it doesn't substring the generated_text and return the whole text.

So you mean the following part specifically in the _call method?:

```if self.task == ""text-generation"":
            # Text generation return includes the starter text.
            text = generated_text[0][""generated_text""][len(prompt) :]
```

I have to play with the indexing which is currently done to get the part after the prompt length?

https://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_endpoint.py"
troybvo,2023-06-27 03:55:03+00:00,comment,tiiuae/falcon-40b-instruct,"No just remove the indexing. The indexing assumes that the generated_text includes the prompt (hence it's substring the generated_text from len(prompt) to the end. Just modify it to be 
text = generated_text[0][""generated_text""]."
TMTechnology,2023-06-28 03:50:55+00:00,comment,tiiuae/falcon-40b-instruct,"> No just remove the indexing. The indexing assumes that the generated_text includes the prompt (hence it's substring the generated_text from len(prompt) to the end. Just modify it to be 
> text = generated_text[0][""generated_text""].

Yup that's what I meant. Thank you."
nfarsad,2023-06-23 02:15:04+00:00,comment,tiiuae/falcon-40b-instruct,"Answer the following question

Question: Who am I?
Answer"
Garmisch,2023-07-10 08:06:02+00:00,comment,tiiuae/falcon-40b-instruct,I also encountered the same problem. Did you find the solution?
jaypinho,2023-07-20 12:23:46+00:00,comment,tiiuae/falcon-40b-instruct,Having the same issue with meta-llama/Llama-2-13b-chat-hf. Where is this 1512 limitation coming from?
ewandel,2023-07-21 11:39:09+00:00,comment,tiiuae/falcon-40b-instruct,"Support explained: You can configure those when creating your endpoint. On inference endpoints in the UI when creating, in the advanced configuration section there are two fields: ""max input length"" and ""max number of tokens"". The latter is where the 1512 limitation is coming from. Change to your needs. I find it tricky to figure out which values are ok for the model and the platform, so far it's been try&error."
mallorbc,2023-06-20 21:32:33+00:00,comment,tiiuae/falcon-40b-instruct,"I noticed this as well.  The code supports Alibi, but it seems that they did not use it for training this model.  I wonder why.   Are there downsides of Alibi that I might not know?"
awhedon,2023-06-24 19:43:26+00:00,comment,tiiuae/falcon-40b-instruct,How do we Tun inference on the model with Alibi? 
mallorbc,2023-06-26 05:01:47+00:00,comment,tiiuae/falcon-40b-instruct,"If the weights are not released that are trained with Alibi I do not belive you can.  Perhaps with significantly more pretraining, it could work to some capacity."
Theblaxkertheberry,2023-06-22 10:02:22+00:00,comment,tiiuae/falcon-40b-instruct,"I do have the same issue, I am trying to create a chatgpt kind of. But the falcon40b instruct do not contextualize at  all. 

It act like it is not have any memory of the previous prompt."
run,2023-07-03 07:14:54+00:00,comment,tiiuae/falcon-40b-instruct,"this kinda of working, see https://colab.research.google.com/drive/1VI2nhlyKvd5cw4-zHvAIk00cAVj2lCCC#scrollTo=b80b3f37"
yi1,2023-06-13 07:46:19+00:00,comment,tiiuae/falcon-40b-instruct,"model.generate(
    text=[""def fibonnaci("", ""User: How are you doing? Bot:""],
    max_length=64,
    include_prompt_in_result=False
）"
yi1,2023-06-13 07:46:41+00:00,comment,tiiuae/falcon-40b-instruct,"Add ""include_prompt_in_result=False"" in model.generate("
andee96,2023-06-13 07:50:30+00:00,comment,tiiuae/falcon-40b-instruct,"lol that does make me feel pretty silly, i will give that a try. 
Do you know where I am supposed to pass this parameter in the case where i've deployed the model using aws sagemaker?"
andee96,2023-06-13 08:16:51+00:00,comment,tiiuae/falcon-40b-instruct,"I tried what you suggested in the following way: 
```python3
instance_type = ""ml.g4dn.12xlarge""
number_of_gpu = 4
health_check_timeout = 300

model_name = ""falcon-40b-instruct"" + time.strftime(""%Y-%m-%d-%H-%M-%S"", time.gmtime())
print(model_name)


# TGI config
config = {
  'HF_MODEL_ID': ""tiiuae/falcon-40b-instruct"", # model_id from hf.co/models
  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica
  'HF_MODEL_QUANTIZE': ""bitsandbytes"", # comment in to quantize,
  'HF_TASK': 'text-generation'
}

model = HuggingFaceModel(
    name=model_name,
    role=role,
    image_uri=image_uri,
    env=config,
)
predictor = model.deploy(
  initial_instance_count=1,
  instance_type=instance_type,
  endpoint_name=model_name,
)

input_data = {
  ""inputs"": ""User: Hello, how are you?\n Assistant:"",
  ""parameters"": {
    ""do_sample"": True,
    ""top_k"": 1,
    ""max_length"": 100,
    ""include_prompt_in_result"": False
  }
}
```

And unfortunately I still get the following response: 
`[{'generated_text': ""User: Hello, how are you?\n Assistant: I'm fine, how can I help you?""}]`"
vaidyank,2023-06-14 03:49:41+00:00,comment,tiiuae/falcon-40b-instruct,"Use  ""return_full_text"": False in the parameters to resolve this issue. Thank me later :) 
"
andee96,2023-06-14 08:27:41+00:00,comment,tiiuae/falcon-40b-instruct,"That's amazing, thank you! Can confirm that this worked! Follow-up question, I am still getting the output returned when I try to use langchain with the deployed endpoint. I would have thought that passing `include_prompt_in_result=False` to the `model_kwargs` parameter would do the trick but that does not seem to be the case. 

```python3
from langchain import SagemakerEndpoint
llm = SagemakerEndpoint(
        endpoint_name=predictor.endpoint_name, 
        credentials_profile_name=""dev"", 
        region_name=""eu-west-2"", 
        model_kwargs={""temperature"":0.7, ""max_length"": 1024, ""return_full_text"": False},
        content_handler=content_handler
)

```

However, if i use this llm in any chain, the initial prompt gets returned again... Any clue what I am doing wrong here? :) @vaidyank 

"
damontrp,2023-06-14 17:46:11+00:00,comment,tiiuae/falcon-40b-instruct,"@andee96 I have falcon-40b deployed on sagemaker and I use  

 ""return_full_text"": false

To get this to stop doing the behavior your describing. The inference container is written in rust it seems like and when it does json serialization it might not like False.

LMK if that works! 
"
upasana-mittal,2024-01-28 05:30:06+00:00,comment,tiiuae/falcon-40b-instruct,I am facing same issue and tried using return_full_text and include_prompt_in_result both it is not working
pratik-poudel,2024-01-29 17:07:56+00:00,comment,tiiuae/falcon-40b-instruct,Same issue here.Because of the returned instructions in the chain the second chain is producing the similar output as first chain.
YoussL,2024-04-08 11:43:50+00:00,comment,tiiuae/falcon-40b-instruct,Has anyone solved the issue with LangChain ?
johnsonhcy,2024-05-22 05:05:36+00:00,comment,tiiuae/falcon-40b-instruct,"Confirmed ""return_full_text"" works ."
ChrisDelClea,2023-06-14 14:00:53+00:00,comment,tiiuae/falcon-40b-instruct,I'd like to know that too!
YoYo1234Qwerty,2023-08-22 22:36:18+00:00,comment,tiiuae/falcon-40b-instruct,Also interested!
kartikmehtaiitd,2023-08-28 22:21:52+00:00,comment,tiiuae/falcon-40b-instruct,"System: Pretend you're a digital content writer tasked with helping human continue their conversation with an AI bot. Your task is to provide suggestions on what human can ask as followup questions in the conversation. Your assignment involves the following steps:
1. Consider the given dialog between a human and an assistant, and think of an assistant response in the conversation.
2. Now, envision what followup question would help the human to continue having an engaging conversation.
3. Write followup questions that human may ask the assistant in the conversation.


The goal is to provide enough detail in your followup questions so that an AI model could use this information to understand and eventually generate an answer.
- Be as specific as possible in your followup questions. Vague or generic questions may lead to ambiguous understanding.
- Keep your language simple and straightforward. Avoid using complex language or jargon that might confuse the AI model.
- Make sure each question is independently complete and there is no use of pronouns.
- Make sure you dont repeat questions already asked in the conversation.
- Make sure you include diverse questions.


Below are a number of examples of desired input and output.
Example 1:
Human: What are the ingredients needed to make Mexican Pizza? [EOM]
four followup questions : 1. What is the difference between Mexican Pizza and regular pizza? 2. Can I make Mexican Pizza at home? 3. Is there a specific recipe for Mexican Pizza? 4. Are there any additional toppings that can be added to Mexican Pizza? [EOM]


Example 2:
Human: Write a blog post about the show Friends, for an audience who have never seen it before. [EOM]
three followup questions : 1. What is the Friends show about? 2. Who are the main characters in Friends show? 3. What are some of the Friends show's most memorable moments and quotes? [EOM]


Example 3:
Human: What year did India gain independence from Britain? [EOM]
three followup questions : 1. Who was the leader of India during the independence movement? 2. How did India gain independence from Britain? 3. What was the impact of India's independence on the world? [EOM]


Example 4:
Human: Which is the fastest land animal? [EOM]
five followup questions : 1. What is the scientific name for the cheetah? 2. How fast is the cheetah? 3. What is the average lifespan of a cheetah? 4. How many cheetahs are left in the wild? 5. What are some of the conservation efforts being taken to protect the cheetah? [EOM]


Example 5:
Human: Pretend you work for a traveling agency. Write an email directed to a potential traveler with the best options for visiting Central America. [EOM]
four followup questions : 1. What are the best places to visit in Central America? 2. What are the best ways to get around Central America? 3. What are the best food options in Central America? 4. What are the best times of year to visit Central America? [EOM]


Human: What won the best picture Oscar in 1956 [EOM]
five followup questions :"
rayyanreda,2023-07-29 07:34:12+00:00,comment,tiiuae/falcon-40b-instruct,This comment has been hidden
khalilxg,2023-11-21 21:06:27+00:00,comment,tiiuae/falcon-40b-instruct,How to fine tune it with arabic text ? Thank u !
Theblaxkertheberry,2023-06-22 10:05:07+00:00,comment,tiiuae/falcon-40b-instruct,Someone for That one ?
mishaml77,2023-06-25 16:31:39+00:00,comment,tiiuae/falcon-40b-instruct,2048
gabriead,2023-07-12 11:01:16+00:00,comment,tiiuae/falcon-40b-instruct,Tried to increase the number of tokens in the openapi.json (cloned the repo and found that simply by searching for 1024) but that didn't help. Created a feature request: https://github.com/huggingface/text-generation-inference/issues/593. Please add to that if you need any adaptations.
george-adams1,2023-07-03 00:44:13+00:00,comment,tiiuae/falcon-40b-instruct,I am too. Has this been solved?
ibrim,2023-08-17 06:05:44+00:00,comment,tiiuae/falcon-40b-instruct,You have to go to config file and replace n_hea with n_head
wangjun9090,2023-11-20 21:08:12+00:00,comment,tiiuae/falcon-40b-instruct,can you write a letter to Apple CEO
masonbraysx,2023-06-07 16:41:12+00:00,comment,tiiuae/falcon-40b-instruct,"A 40-B parameter model will not fit on and A100-80GB if it is in bf16 or fp16. In 16-bit precision the amount of VRAM needed to run a given model is at least 2GB per 1B parameters, and some models are closer to 3GB per 1B parameters. This does not include the amount of memory needed to actually run any type of inferencing. Two easy options: 1) run it on a node with multiple A100 80GB GPUs.  2) load the model in 8bit precision. This requires the package ""bitsandbytes"". This reduces the necessary VRAM to about 45GB. I have successfully loaded and performed inference with the falcon-40b-instruct model on a system with 4 A4500's (each GPU has 20GB VRAM) using this method."
Ichsan2895,2023-06-09 12:15:41+00:00,comment,tiiuae/falcon-40b-instruct,"I successfully using single 48GB VRAM A6000. As masonbraysx said, you need the bitsandbytes library. I prefer to install that package with github repo for the latest dev version+transformer+accelerate.

Activating bitsandbytes config which enabling load_in_4bit, nf4 quant type (QLora version), and bfloat16."
FalconLLM,2023-06-09 14:59:15+00:00,comment,tiiuae/falcon-40b-instruct,"For the base model in `bfloat16`, we recommend 85-100GB of memory. 

There has been some efforts, such as [FalconTune](https://github.com/rmihaylov/falcontune), to have the model in 4 bits (~20-30GB only)."
tiagofreitas87,2023-06-09 16:20:23+00:00,comment,tiiuae/falcon-40b-instruct,Can anyone share the inference speeds on each setup? Knowing the speed is as important as being able to load it..
Ichsan2895,2023-06-10 07:41:43+00:00,comment,tiiuae/falcon-40b-instruct,"> Can anyone share the inference speeds on each setup? Knowing the speed is as important as being able to load it..

Mine in cloud environment with single RTX A6000 48 GB VRAM got 1-2 tokens/second. Pretty slow but okay its running."
tommyw,2023-06-14 21:48:51+00:00,comment,tiiuae/falcon-40b-instruct,"
> Can anyone share the inference speeds on each setup? Knowing the speed is as important as being able to load it..

Running it on 4 A100-80GB and it takes between 23 and 24 ms per token (using this https://github.com/huggingface/text-generation-inference to serve it).
"
theainerd,2023-06-05 15:27:04+00:00,comment,tiiuae/falcon-40b-instruct,Would love something like that.
Caramell,2023-06-05 15:55:11+00:00,comment,tiiuae/falcon-40b-instruct,I would love this!
AIReach,2023-06-08 04:11:44+00:00,comment,tiiuae/falcon-40b-instruct,it will be too good.
Renegadesoffun,2023-06-18 18:52:51+00:00,comment,tiiuae/falcon-40b-instruct,Yep!
FalconLLM,2023-06-09 14:57:27+00:00,comment,tiiuae/falcon-40b-instruct,All implementation details are in [modelling_RW.py](https://huggingface.co/tiiuae/falcon-40b-instruct/blob/main/modelling_RW.py)
Ichsan2895,2023-06-05 04:28:52+00:00,comment,tiiuae/falcon-40b-instruct,"Using newest transformer & accelerate library from pip github + using bitsandbytes config (load_in_4bit, bfloat16, and nf4 quant type), I am able to run this on single A100 40 GB. Its using 80 GB of disk space for saving pretrained model."
tckb,2023-06-05 05:40:24+00:00,comment,tiiuae/falcon-40b-instruct,"Hi @Ichsan2895  I’m pretty new to this model and llm general. I’d like test this in one of azure environment and there are many which are available. Do you by any chance know which one of the vm sizes can be used. There are many which are not available due to shortage and high demands.

Thank you"
Ichsan2895,2023-06-05 07:11:26+00:00,comment,tiiuae/falcon-40b-instruct,"> Hi @Ichsan2895  I’m pretty new to this model and llm general. I’d like test this in one of azure environment and there are many which are available. Do you by any chance know which one of the vm sizes can be used. There are many which are not available due to shortage and high demands.
> 
> Thank you

Hello, Sorry I never test it on Azure. 
I tested it on Runpods environment. It cost $ 0.85/hour which has A6000 48GB VRAM + 58 GB RAM + 200 GB Disk when it running and cost $ 0.03/hour when system idle because I saving pretrained model in their Disk too."
tckb,2023-06-05 08:14:30+00:00,comment,tiiuae/falcon-40b-instruct,thank you for the response. How was the performance on this machine(tk/sec)?
Ichsan2895,2023-06-05 14:23:00+00:00,comment,tiiuae/falcon-40b-instruct,"Pretty slow.. About 0.5-1 token/second. BTW, Guanaco-65-GPTQ is faster but unfortunatelly it can not be use for commercial."
tckb,2023-06-05 21:57:15+00:00,comment,tiiuae/falcon-40b-instruct,"@Ichsan2895  I was able to run this on Standard_NC48ads_A100_v4 which has 160 GiB GPU mem.  I wasn't able to use bitsandbytes module (some issue, I couldn't debug it).  The results were surprisingly good.  I could only use it for a very short time because its pretty expensive.  See https://twitter.com/this_is_tckb/status/1665814803829473280/"
adeebaldkheel,2023-06-07 09:54:18+00:00,comment,tiiuae/falcon-40b-instruct,"is it possible to run it on RTX 4090?
sorry guys but can someone tell me what does 40b mean? what I know is 40b x 4 = 160GB right?
does it mean one GPU with total of 160GB can load this model?
or I need 160GB+ for training? and training is different than using?"
Ichsan2895,2023-06-07 12:48:57+00:00,comment,tiiuae/falcon-40b-instruct,"40b is 40 billion parameter used. But it does not mean that it need 40 GB GPU RAM. I used  48 GB A6000 to run this model. It can be optimized (for lowering consumption) by activating bitsandbytes config. Which enable bfloat16 and load_in_4bit. Unfortunatelly it wont run in 24GB VRAM (OOM).

Sorry I dont know the consumption when it training/fine tuning new dataset."
FalconLLM,2023-06-09 14:56:47+00:00,comment,tiiuae/falcon-40b-instruct,We have added some basic info on running the model to the card. It takes ~80-100GB to comfortably infer Falcon-40B. There has been some work with [FalconTune](https://github.com/rmihaylov/falcontune) on 4-bit quantization as well.
Shridharalve,2023-06-05 06:42:33+00:00,comment,tiiuae/falcon-40b-instruct,"You can use this in your model_fn function in inference.py

```
tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b-instruct"", trust_remote_code=True,torch_dtype=torch.bfloat16, device_map=""auto"")
```"
martin-holste,2023-06-06 02:02:48+00:00,comment,tiiuae/falcon-40b-instruct,"I'm struggling with this as well. In Sagemaker Studio, I have created a file code/inference.py, and put in it:

<pre>
from transformers import pipeline
def model_fn(model_dir):
    tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
    model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b-instruct"", trust_remote_code=True,torch_dtype=torch.bfloat16, device_map=""auto"")

    return pipeline(
        ""question-answering"",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=""auto"",
        trust_remote_code=True,
        device_map=""auto"",
        )
</pre>
But I continue to get the error regarding ""trust_remote_code."" Can someone please explain a bit more on exactly what is required to get falcon to work as a Sagemaker endpoint? I'd really appreciate it."
Shridharalve,2023-06-08 16:15:50+00:00,comment,tiiuae/falcon-40b-instruct,"@martin-holste @garystafford 

Here's the guide by Phil Schmid to deploy this model on sage maker... Works like a charm

https://www.philschmid.de/sagemaker-falcon-llm"
FalconLLM,2023-06-09 14:54:57+00:00,comment,tiiuae/falcon-40b-instruct,This has been fixed -- it was finetuned from the 40B model. 
kyleeasterly,2023-06-04 09:44:02+00:00,comment,tiiuae/falcon-40b-instruct,"Check out the special tokens near the top of the `tokenizer.json` file. There are special tokens for >>QUESTION<<, >>ANSWER<<, and a few other types you can play with. I've had the best results prompting it like this:
```
>>QUESTION<<In Python, I want to write a simple HTTP API that receives an object via POST and responds with another object. The request object contains a string prompt, float temperature, and int max_tokens. The response object contains a string response, int prompt_tokens, int completion_tokens. For now just set hard-coded values and get the response out. I'll add the logic myself afterward.
>>ANSWER<<
```

I haven't tried many of the other tokens out, perhaps >>INTRODUCTION<< is closest to the system prompt?"
silvacarl,2023-06-08 00:07:21+00:00,comment,tiiuae/falcon-40b-instruct,does anyone have any good prompt examples they can share or prompt templates to test this model?
zaqintosh,2023-07-21 22:04:44+00:00,comment,tiiuae/falcon-40b-instruct,"according to ""the bloke"", the appropriate prompt to use for Falcon-instruct (which would assume how they formatted text when fine-tuning) is:
A helpful assistant who helps the user with any questions asked.
User: prompt
Assistant:

But I honestly I can't tell if that's just a best guess on his part. @TheBloke ? "
TheBloke,2023-07-22 08:20:59+00:00,comment,tiiuae/falcon-40b-instruct,"I used this for reference: https://github.com/cmp-nct/ggllm.cpp/discussions/36

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/KvLwlqj9ckt4x_DIarCE3.png)

@cmp-nct  has developed the Falcon GGML (now called GGCC) client and looked into prompt templates a lot.

Although now I look at that again, I see he doesn't show a system message, so maybe I need to check that.  But User/Assistant should definitely work.  He makes mention of the Falcon special tokens later in the post, which apparently do work too."
zaqintosh,2023-07-24 20:10:22+00:00,comment,tiiuae/falcon-40b-instruct,"@TheBloke thanks for sharing your source!

I'm so frustrated by all of these OS models coming out with huge fan fare, but they never show sample prompts they used in fine tuning, and it's (sometimes) a nightmare trying to figure it out."
SlalomSledge,2023-09-22 02:54:36+00:00,comment,tiiuae/falcon-40b-instruct,"@kyleeasterly  - could you please point me towards the tokenizer file?  I too am confused on conventions around Falcon model prompting.
"
SlalomSledge,2023-09-22 14:19:17+00:00,comment,tiiuae/falcon-40b-instruct,"Also, what is a wizard model?"
YoYo1234Qwerty,2023-09-26 21:40:57+00:00,comment,tiiuae/falcon-40b-instruct,"I think everyone's wrong about `>>QUESTION<<` and `>>ANSWER<<`. If you ask a `>>QUESTION<<`, it'll never output the `>>ANSWER<<` token on its own, meaning it likely wasn't trained like this.

I don't know what the right answer is, but it does seem eager to stick to a template like:

```
User Hi there
Mini what's up?
```

I've seen other users mention `Mini` too."
captain-fim,2023-06-04 08:52:36+00:00,comment,tiiuae/falcon-40b-instruct,"@eastwind, so you do not get gibberish every time? 
Would you kindly post some non-gibberish examples?
What did you do to go from Gibberish to English?"
captain-fim,2023-06-04 08:57:52+00:00,comment,tiiuae/falcon-40b-instruct,"@eastwind  I now found your contribution here to answer the last question. Thanks!
https://huggingface.co/tiiuae/falcon-40b-instruct/discussions/20"
eastwind,2023-06-04 16:41:37+00:00,comment,tiiuae/falcon-40b-instruct,"Yeah, not using cache hurts performance alot."
FalconLLM,2023-06-09 14:54:26+00:00,comment,tiiuae/falcon-40b-instruct,We recommend using [Text Generation Inference](https://github.com/huggingface/text-generation-inference) for fast inference with Falcon. See this [blog](https://huggingface.co/blog/falcon) for more information.
tradunsky,2023-06-02 21:38:06+00:00,comment,tiiuae/falcon-40b-instruct,"Try to install accelerate, it does contain this function - `from accelerate import init_empty_weights`

`pip install accelerate`"
diegoAgher,2023-06-03 00:03:23+00:00,comment,tiiuae/falcon-40b-instruct,"I did install these :

!pip install transformers
!pip install einops
!pip install accelerate"
captain-fim,2023-06-04 08:54:57+00:00,comment,tiiuae/falcon-40b-instruct,[This discussion](https://huggingface.co/tiiuae/falcon-40b-instruct/discussions/2) on the same community thread here might give the information that you need.
tradunsky,2023-06-02 21:39:49+00:00,comment,tiiuae/falcon-40b-instruct,"Try this one:
https://huggingface.co/tiiuae/falcon-40b-instruct/discussions/9"
FalconLLM,2023-06-09 14:53:34+00:00,comment,tiiuae/falcon-40b-instruct,Check-out this [blog post](https://huggingface.co/blog/falcon) for more information & examples.
eastwind,2023-06-02 14:34:29+00:00,comment,tiiuae/falcon-40b-instruct,"```use_cache = False```

Seems to have fixed the issue for now, but I assume that this makes it very slow as it took 5 mins for 100tokens"
zkdtckk,2023-06-03 15:04:20+00:00,comment,tiiuae/falcon-40b-instruct,"v100 does not support bfloat16, thus it probably is using fp32, resulting a decrease of several times of tflops. Change to A10G or better like A100 GPU"
captain-fim,2023-06-04 09:03:46+00:00,comment,tiiuae/falcon-40b-instruct,"@zkdtckk I had it running on a machine with 8 x A100 80GB GPUs. 
It took some 10 minutes to give a response to that Giraffe prompt given in the example script.
Did you fare better? If so, how?

It also produced gibberish as described by @eastwind. 
But I am not sure I want to try the `use_cache` option.
If it is that slow it is far to costly to use."
zkdtckk,2023-06-04 14:48:51+00:00,comment,tiiuae/falcon-40b-instruct,"@captain-fim 10min on 8 A100 (p4d?) is really slow, there should be something wrong. It took me a few mins on 8 A10 GPU setup to run the Giraffe prompt. 
The speed of inference is really a problem for this model, we need to figure out a way to speed it up."
captain-fim,2023-06-04 15:29:49+00:00,comment,tiiuae/falcon-40b-instruct,"@zkdtckk 

> 10min on 8 A100 (p4d?) is really slow, there should be something wrong.

Yes, that is what I thought. And also it will be even slower with turning the cache off as @eastwind proposes, will it not? :-(
Did you need to turn the cache off, too, @zkdtckk ?
I still hope something is wrong with our setup."
eastwind,2023-06-04 16:37:48+00:00,comment,tiiuae/falcon-40b-instruct,I think it's an issue with multi GPU Inference. Due to the custom code implemented by falcon. Maybe there's an issue with sharding the model over multiple gpus using the device map from accelerate.
eastwind,2023-06-04 16:43:14+00:00,comment,tiiuae/falcon-40b-instruct,@captain-fim it didn't take that long. I used 4 v100s. Took 3 mins for 200 tokens
eastwind,2023-06-04 17:04:24+00:00,comment,tiiuae/falcon-40b-instruct,"https://github.com/huggingface/transformers/issues/15399

This mentions that it might be an issue with fp16. Or bfloat16. Like @zkdtckk mentioned. I will try tomorrow without quantisation."
eastwind,2023-06-05 20:13:43+00:00,comment,tiiuae/falcon-40b-instruct,Working theory is that the shared KV cache doesn't work for multi GPU.
FalconLLM,2023-06-09 14:52:54+00:00,comment,tiiuae/falcon-40b-instruct,We would recommend using [Text Generation Inference](https://github.com/huggingface/text-generation-inference) for fast inference with Falcon. See this [blog](https://huggingface.co/blog/falcon) for more information.
thedkk,2023-06-26 21:10:47+00:00,comment,tiiuae/falcon-40b-instruct,"@FalconLLM Have we benchmarked response time of falcon 40-b when we deploy the endpoint over sagemaker?
"
FalconLLM,2023-06-02 05:07:48+00:00,comment,tiiuae/falcon-40b-instruct,"Yup, the options are here: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig
Eg. the repetition penalty might be useful. 

You should be able to specify them when calling  the pipeline

```
sequences = pipeline(
   ""prompt goes here"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    repetition_penalty=2.0
)
```"
serin32,2023-06-02 12:20:47+00:00,comment,tiiuae/falcon-40b-instruct,I got it working with bitsandbytes 4bit.  Here is how I did it: https://huggingface.co/tiiuae/falcon-40b/discussions/38#6479de427c18dca75e9a0903
Ichsan2895,2023-06-05 04:26:05+00:00,comment,tiiuae/falcon-40b-instruct,"Please use huggingface dev version 4.30-dev (downloaded from pip github) & accelerate 0.20-dev (from github too)

Then please use bitsandbytes package for using bfloat16, load_in_4bit, and quant_type=nf4."
FalconLLM,2023-06-01 11:02:34+00:00,comment,tiiuae/falcon-40b-instruct,Looks to be an issue with too little memory on the instance. What size EBS volume did you attach?
Shridharalve,2023-06-01 11:15:10+00:00,comment,tiiuae/falcon-40b-instruct,"As per the AWS Docs, the EBS volume size is 3800 GB. 
```
Type            | CPU    |     Memory        | GPUS | GPU Memory  | Storage
ml.g5.24xlarge	|   96   |     384  GB	     |  4	 |    96 GB   |1x3800
```
Here I also tried with a larger instance type ml.g5.48xlarge which has double the specs"
Shridharalve,2023-06-01 11:22:42+00:00,comment,tiiuae/falcon-40b-instruct,"Adding more details about the error on sagemaker endpoint.

Caused by: java.io.IOException: No space left on device
```
pool-2-thread-6 ERROR An exception occurred processing Appender access_log org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing to stream logs/access_log.log
2023-06-01 10:33:43,006 pool-2-thread-6 ERROR An exception occurred processing Appender access_log org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing to stream logs/access_log.log
```"
FalconLLM,2023-06-01 12:11:43+00:00,comment,tiiuae/falcon-40b-instruct,"The volume you mentioned is typically mounted to `/tmp/` while there is a separate volume mounted to `/opt/ml/checkpoints` which you specify when launching the instance. I believe what is happening is that the model is downloaded under `/opt/ml/checkpoints` which then get's exhausted.  Assuming you use the HF estimator, could you try specifying `volume_size = 200`?"
Shridharalve,2023-06-01 13:07:24+00:00,comment,tiiuae/falcon-40b-instruct,These instance types ml.g5.24xlarge and ml.g5.48xlarge do not support the volume_size parameter as they have a 3800 GB volume with the inference endpoint.  If instance type is an issue can you suggest an appropriate one which can run the model without issues. I was able to run falcon-7B-instruct without any issues...
FalconLLM,2023-06-01 16:23:49+00:00,comment,tiiuae/falcon-40b-instruct,"At this point I unfortunately do not understand sagemaker endpoints with huggingface models well enough to be able to assist you, the issue is definitely related to the disk space though, as the error indicates`[Errno 28] No space left on device""`. The 7B might work because it fits in the standard 30GB EBS volume. Over the coming weeks we hope to be able to provide easier ways to deploy the models."
mariolr,2023-06-01 21:54:47+00:00,comment,tiiuae/falcon-40b-instruct,"The transformers library downloads the model on the default cache location: ~/.cache/huggingface/hub
However, the EBS volume is mounted on /home/ec2-user/SageMaker
You can check by running `df` on a terminal.

You can change the transformers cache location to a directory by running this before importing the transformers library:

```
import os
os.environ['TRANSFORMERS_CACHE'] = '/home/ec2-user/SageMaker/transformers-cache/'
```

Here's the relevant documentation: https://huggingface.co/docs/transformers/v4.29.1/en/installation#cache-setup"
Shridharalve,2023-06-02 07:02:37+00:00,comment,tiiuae/falcon-40b-instruct,"@FalconLLM 

I am no longer facing the ""storage space"" issue. Seems it got resolved using the below snipped in inference.py
```
model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b-instruct"", trust_remote_code=True,load_in_8bit=False, torch_dtype=torch.bfloat16, device_map=""auto"", cache_dir=""/tmp/model_cache/"")
```

The model got deployed on the SG endpoint. However when I look at the instance metrics, the GPU Memory usage balloons to 788% and all the 8 GPUs are utilized on ml.g5.48xlarge (8 NVIDIA A10G GPUs and 192 GiB GPU memory) . We do not get an inference output as often it times out if there's no response for a minute. Is this machine enough for the model inference hosting. Should we wait longer for response. 
Any other ml instance type I can try?"
austinmw,2023-06-08 23:50:06+00:00,comment,tiiuae/falcon-40b-instruct,ml.g5.12xlarge instance was enough for me to deploy Falcon-40B in the HF TGI DLC
FalconLLM,2023-06-09 14:51:24+00:00,comment,tiiuae/falcon-40b-instruct,For running on SageMaker we would recommend having a look at this blogpost: https://www.philschmid.de/sagemaker-falcon-llm
Shridharalve,2023-06-10 04:48:11+00:00,comment,tiiuae/falcon-40b-instruct,@FalconLLM Yes I looked at that and we have already deployed your model.. Thanks for the help. I have posted this link for others as well in this community
rikirolly,2023-06-27 21:06:44+00:00,comment,tiiuae/falcon-40b-instruct,@austinmw Could you provide a shell script for the deployment in the HF TGI DLC?
nx,2023-06-01 07:26:03+00:00,comment,tiiuae/falcon-40b-instruct,"ImportError: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`

on this part
```
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
```"
kil3r,2023-06-01 21:13:15+00:00,comment,tiiuae/falcon-40b-instruct,"in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB"
nx,2023-06-02 21:15:19+00:00,comment,tiiuae/falcon-40b-instruct,"I don’t have any A1000, but 3x V100 (32g each). Is there some sample script that I can use to run the model with all 3 cards?"
leocheung,2023-06-06 03:41:32+00:00,comment,tiiuae/falcon-40b-instruct,"> in bfloat16 it takes ~65GB of VRAM on A1000 80GB, in 8bit ~46GB

why i use a100-80GB ,  still report cuda memory error?
  File ""/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py"", line 93, in forward
    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.35 GiB total capacity; 77.18 GiB already allocated; 3.19 MiB free; 78.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
a749734,2023-06-07 10:16:35+00:00,comment,tiiuae/falcon-40b-instruct,"@leocheung  same i hv 96GB still same error , any solution u got?"
leocheung,2023-06-08 03:33:52+00:00,comment,tiiuae/falcon-40b-instruct,"@a749734  i got suggestion from @masonbraysx 
A 40-B parameter model will not fit on and A100-80GB if it is in bf16 or fp16. In 16-bit precision the amount of VRAM needed to run a given model is at least 2GB per 1B parameters, and some models are closer to 3GB per 1B parameters. This does not include the amount of memory needed to actually run any type of inferencing. Two easy options: 1) run it on a node with multiple A100 80GB GPUs. 2) load the model in 8bit precision. This requires the package ""bitsandbytes"". This reduces the necessary VRAM to about 45GB. I have successfully loaded and performed inference with the falcon-40b-instruct model on a system with 4 A4500's (each GPU has 20GB VRAM) using this method."
ayush1801,2023-06-09 07:26:55+00:00,comment,tiiuae/falcon-40b-instruct,@leocheung  how did you run the inference part of the model on the cluster of GPUs. Did you change the model's scripts or passed some arguments for reading the devices?
FalconLLM,2023-06-09 14:50:37+00:00,comment,tiiuae/falcon-40b-instruct,For fast inference we would recommend using [Text Generation Inference](https://github.com/huggingface/text-generation-inference); also check out this [blog](https://huggingface.co/blog/falcon) for some examples of usage.
BCenti,2023-06-01 16:12:11+00:00,comment,tiiuae/falcon-40b-instruct,"I'm not a lawyer and from a glance, it seems Baize is legally vague, because they explicitly licensed GPL 3.0 which **DOES** allows commercial use and yet they claims on the README that it isn't permitted for commercial use. They need to use the appropriate licensing for their project and ensure the messages are consistent. I would imagine that the ""LICENSE"" text take precedent over the ""README"" in court if one were to argue, though again, I'm not a lawyer.

So even if Falcon have to be relicensed for GPL 3.0, it would still be viable for commercial use."
shreyans92dhankhar,2023-06-16 05:18:05+00:00,comment,tiiuae/falcon-40b-instruct,"I have the same doubt whether instruct variant is viable option for commercial use or not due to Baize mixture data. On the HF Model page for falcon-40B it is explicitly mentioned as ""It is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions."" but on instruct variant they just mentioned ""Apache 2.0 licence"". It will be great if someone from Falcon team clarify this? 
@FalconLLM : Looking for your response."
cmp-nct,2023-06-23 01:11:17+00:00,comment,tiiuae/falcon-40b-instruct,"There are several important points here:
1) The license is clear, if it's licensed as GPL 3 you can use it commercially. The readme does not matter, you are not required to read it either.
2) You are not using Baize anyway, even if it was non-commercial and not GPL-3 you'd not be using it. 
The Falcon team used it for a fine tune. You can hardly legally claim that fine tuning something makes it fall under that license.
That's not the case.
3) If the model was able to reproduce relevant parts of ""Baize"" then those generations would fall under GPL-3 as well.

I don't see any problem"
dataviral,2023-06-26 17:26:47+00:00,comment,tiiuae/falcon-40b-instruct,"Thanks for weighing in, the ground is still murky on this to be actually used in real commercial applications by practitioners like myself. 
Hoping the @FalconLLM team can clarify "
wyklq,2023-06-27 10:12:54+00:00,comment,tiiuae/falcon-40b-instruct,"I have the same worry. I think the problem is not that Baize's implementation is GPL-3 licensed. But its dataset is ChatGPT generated. And there is a ""terms-of-use"" in OpenAI that a clear usage restriction ""(iii) use output from the Services to develop models that compete with OpenAI; "" 
So one should not use output from ChatGPT to develop models that compete with ChatGPT/GPT-4."
cmp-nct,2023-06-27 11:33:25+00:00,comment,tiiuae/falcon-40b-instruct,"That data was only used for instruct. Also you did not break their terms, and you do not  have to accept them. It would not be your problem. Also the only risk involved is that when breaking those terms you can lose your account at OpenAI. 

Well and if your product claims to be developed by OpenAI you also have a problem, so you’d need to add a content filter removing that word from output.

But seriously: just re-fine tune it .."
FalconLLM,2023-06-01 06:49:16+00:00,comment,tiiuae/falcon-40b-instruct,"p3dn is V100, which does not natively support bfloat16. I'm not sure exactly what the fallback is in this case and why it does not throw any errors, maybe it's running on fp32 behind the scenes?

At this time I would recommend switching to `ml.g5.24xlarge` or `ml.g5.48xlarge` while we look into how to best support older hardware."
zkdtckk,2023-06-01 17:17:13+00:00,comment,tiiuae/falcon-40b-instruct,"Thank you for the correction and advice, never realized that V100 did not support bfloat16. A10 has 125TFLOPS, while V100 fp32 has 7.8TFLOPS, this explains the huge difference in performance."
zkdtckk,2023-06-01 20:12:43+00:00,comment,tiiuae/falcon-40b-instruct,The time of inference is reduced from 8min to 1min on ml.g5.12xlarge with 4 A10 GPU. Should be even faster if using 48xlarge or A100 instance. Thanks again for your help. 
Shridharalve,2023-06-01 11:31:52+00:00,comment,tiiuae/falcon-40b-instruct,Were you able to run the model on SageMaker?
FalconLLM,2023-06-01 12:23:12+00:00,comment,tiiuae/falcon-40b-instruct,"Hey @mike-ravkine, glad you like the model

This is a bit surprising, while we should fix the default value, the config.json is correct since some days back, so when the model is loaded the config should be correct. 
See: https://huggingface.co/tiiuae/falcon-40b-instruct/commit/662a9a4ffd96f4f73dd18141b60962f94b743c56

Could it be an issue with using a cached model since before it was fixed?"
mike-ravkine,2023-06-01 12:54:52+00:00,comment,tiiuae/falcon-40b-instruct,"Thanks for the response @FalconLLM, this makes sense. I am actually using a quantized version of the model (from https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ) and it was missing the fix to `config.json` from above.  I have opened a PR in that model!"
FalconLLM,2023-05-31 13:31:12+00:00,comment,tiiuae/falcon-40b-instruct,"Unfortunately, I don't have a direct reference from the top of my head. But let's consider three different methods for finetuning:
```
a: Finetune(new_data) 
b: Finetune(instructions and new_data) 
c: Finetune(instructions then new_data)  <--- starting from instruct model
```
I would expect either `a` or `b` to yield the best results depending on the task, although there is no guarantee. So starting from the base model gives you the best flexibility to try all of the different approaches and see what works best for you."
LucienShui,2023-05-31 14:09:17+00:00,comment,tiiuae/falcon-40b-instruct,I originally thought that less data meant less computing resources. Your point of view is also a valuable perspective. Thanks a lot. :)
znsoft,2023-05-30 19:13:57+00:00,comment,tiiuae/falcon-40b-instruct,"But it doesn't behave so good like what the author said......

What's wrong with the code? doesn't act like a bot, but a parrot."
FalconLLM,2023-05-31 04:09:40+00:00,comment,tiiuae/falcon-40b-instruct,"Falcon 40B needs at least around 90GB of VRAM to run, unfortunately, neither of the configurations provided in the title is matches this requirement. The community has however, quantised a version https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ, that might fit on the hardware you have available."
znsoft,2023-05-31 06:25:59+00:00,comment,tiiuae/falcon-40b-instruct,"It can be run with two 3090/4090 in 8bit mode.   I  literately have run it correctly, but the result is not so good.  
Could you offer an online demo?"
DanielHesslow,2023-05-31 07:23:52+00:00,comment,tiiuae/falcon-40b-instruct,"Sorry, I did not understand your original post correctly. We have not validated the model in anything but `bfloat16`,  and you may be observing some degradation in model quality by quantising the model weights to 8bits, as you do in your code."
MaziyarPanahi,2023-05-31 21:41:38+00:00,comment,tiiuae/falcon-40b-instruct,If you can change the name of the model_id to `tiiuae/falcon-40b-instruct` and load it in `text-generation-inference` you can have it in HuggingChat locally
MaziyarPanahi,2023-06-08 08:22:26+00:00,comment,tiiuae/falcon-40b-instruct,"So I successfully managed to use falcon-40b-instruct in `text-generation-inference` and connect it to HuggingChat. However, I am not sure what should be the following tokens (unlike OpenAssistant):

```
 ""userMessageToken"": ""<|prompter|>"",
 ""assistantMessageToken"": ""<|assistant|>"",
 ""messageEndToken"": ""<|endoftext|>"",
```"
FalconLLM,2023-06-09 14:49:05+00:00,comment,tiiuae/falcon-40b-instruct,"We don't have any such tokens set in this instruct version, so you could set the user to ""User:"", and the assistant to ""Assistant:"" (if it supports plain text)."
TYUIKLJHVHJBN,2023-07-13 23:16:08+00:00,comment,tiiuae/falcon-40b-instruct,"Hello, I have falcon-40b-instruct running with `text-generation-inference` and Huggingchat on a local machine, has anyone else managed to do so? I am not sure of the three tokens listed above and the preprompt, currently the model seems to continue the conversation from the perspective of the user after answering the initial prompt. Any ideas?"
MaziyarPanahi,2023-07-14 07:20:59+00:00,comment,tiiuae/falcon-40b-instruct,I am having the same issue with Falcon models in general. The special tokens and how/where they should be used in the various prompts are mostly hit and miss for me and I cannot get this thing to work nicely with external input/context.
Ernnnn,2023-05-30 07:25:20+00:00,comment,tiiuae/falcon-40b-instruct,you should run code in gpu env
Prometheu3,2023-05-30 07:54:51+00:00,comment,tiiuae/falcon-40b-instruct,"> you should run code in gpu env

Yes I solve it, but encounter another problem. Did you know how many disc space and VRAM is needed to run this model?"
FalconLLM,2023-05-30 14:57:25+00:00,comment,tiiuae/falcon-40b-instruct,"Note that this error is usually linked to not having `accelerate` installed, you should pip install it :)."
Minami-su,2023-05-28 22:50:15+00:00,comment,tiiuae/falcon-40b-instruct,问题解决了，3090x2 -->  a40x2
Crenox,2023-05-27 14:56:26+00:00,comment,tiiuae/falcon-40b-instruct,"Yeah, same problem here. It doesn't even use the AutoModelForCausalLM. When I try to use it like: model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b-instruct"") it still throws an error because the trust_remote_code parameter isn't set to True. I don't know how to fix it tho."
supdeva,2023-05-28 14:01:22+00:00,comment,tiiuae/falcon-40b-instruct,"@Crenox 

This worked for me

model = AutoModelForCausalLM.from_pretrained(
    ""tiiuae/falcon-40b-instruct"", trust_remote_code=True
)"
singhalshikha518,2023-06-01 17:43:43+00:00,comment,tiiuae/falcon-40b-instruct,"While generating text with falcon 40b instruct. Getting below error:
AttributeError: module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention'"
eastwind,2023-06-02 10:26:10+00:00,comment,tiiuae/falcon-40b-instruct,you need to upgrade to torch 2.0. That fixed the attribute error for me
captain-fim,2023-06-03 09:12:15+00:00,comment,tiiuae/falcon-40b-instruct,"After trying @supdeva 's fix to create the model I have this code:

```
model_name = ""tiiuae/falcon-40b-instruct""

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
```

The tokenizer is created successfully, but the model creation yields the error:

The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].

What am I doing wrong?"
eastwind,2023-06-03 09:26:24+00:00,comment,tiiuae/falcon-40b-instruct,"That's just a warning, it should run fine."
eastwind,2023-06-03 09:26:58+00:00,comment,tiiuae/falcon-40b-instruct,Or the other error should be written below it
captain-fim,2023-06-03 10:07:33+00:00,comment,tiiuae/falcon-40b-instruct,"Right you are @eastwind, thank you!
It runs past that point indeed and there is a different error, which I managed to eliminate.
Thanks!"
eastwind,2023-06-03 10:13:00+00:00,comment,tiiuae/falcon-40b-instruct,What was the other error out of curiosity?
captain-fim,2023-06-03 11:11:18+00:00,comment,tiiuae/falcon-40b-instruct,"Now it seems to run and just for anyone reading this, here is the code with the slight changes needed to get it running:
```
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model_name = ""tiiuae/falcon-40b-instruct""

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map=""auto"", trust_remote_code=True)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")
```
But I have never run such a huge model and I am lost what kind of hardware it would need to actually work in a useful way.
I have it running on a machine with 8 x A100 80GB GPUs.
It runs for at least 10 (quite costly) minutes now and does not seem to produce any output yet.

So what kind of hardware does this monster need?"
captain-fim,2023-06-03 11:19:59+00:00,comment,tiiuae/falcon-40b-instruct,"> What was the other error out of curiosity?

 CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

So, running out of GPU memory."
captain-fim,2023-06-03 11:23:47+00:00,comment,tiiuae/falcon-40b-instruct,"Ok, after all the work and some 20$ to Runpod, I got the glorious answer to the provided example prompt:

> Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared tothe glorious majesty of the giraffe.
> Daniel: Hello, Girafatron!
> Girafatron:: Divineinity215183SegSeg Hansonsignal HolmesOSS Seg Seg Rydergate Cowtown OgSegurities DennSys548AdvisorAdvisor Wachwachmeter603campus Ley Wie Ger Hendersonpositionpositionnement Seg Kitt Kitt Kitt FranklintownICTcorp Cetroniccorp Hoy Museobjet Dans DansMLSIngredientsProductionsCadCentre coinc Knight lust Sie Wer865bottom Cet Zimmer Nolandivision Wie427 unoGate Wars positivism Saunders esp sans uno Court Sie Barnettfields981pagesviews esp Danncampus esp sans Francisco Francisco Mesa tres tres Holmes dit Wol esp esp sans el dit Weather pour el poss MullerSys577 Denncampusposition Wer258Cad Denn respons responsabilidad Zum complet Dannforth Dixon Andrewsport891housing Baumgartenoperator Wie427world tout

Great to know..."
JerrySweeney,2023-06-04 07:36:44+00:00,comment,tiiuae/falcon-40b-instruct,"I am trying to get tiiuae/falcon-40b-instruct working locally on a single A100 80GB GPU. Using captain-fin's code above I got it to go further. Now I am seeing the following error...

(pytorch-env) administrator@ubuntu:~/falcon-40b-instruct$ python3 captain-fim.py
Traceback (most recent call last):
  File ""/home/administrator/falcon-40b-instruct/captain-fim.py"", line 8, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=""auto"", trust_remote_code=True)
  File ""/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py"", line 462, in from_pretrained
    return model_class.from_pretrained(
  File ""/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 2777, in from_pretrained
    ) = cls._load_pretrained_model(
  File ""/home/administrator/miniconda3/envs/pytorch-env/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 2871, in _load_pretrained_model
    raise ValueError(
ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.
(pytorch-env) administrator@ubuntu:~/falcon-40b-instruct$

I am assuming this is an issue finding the model weights. I have a copy in the same folder as the code above and I put another copy in a folder tiiuae/falcon-40b-instruct.

I will appreciate any advice."
captain-fim,2023-06-04 07:44:11+00:00,comment,tiiuae/falcon-40b-instruct,"@CloudCIX in the article [How 🤗 Accelerate runs very large models thanks to PyTorch](https://huggingface.co/blog/accelerate-large-models) i found this piece of information.
I guess it is what you need here.

> If the device map computed automatically requires some weights to be offloaded on disk because you don't have enough GPU and CPU RAM, you will get an error indicating you need to pass an folder where the weights that should be stored on disk will be offloaded:

```
ValueError: The current `device_map` had weights offloaded to the disk. Please provide an 
`offload_folder` for them.
```

> Adding this argument should resolve the error:

```
import torch
from transformers import AutoModelForCausalLM

# Will go out of RAM on Colab
checkpoint = ""facebook/opt-13b""
model = AutoModelForCausalLM.from_pretrained(
    checkpoint, device_map=""auto"", offload_folder=""offload"", torch_dtype=torch.float16
)
```"
MarginallyEffective,2023-07-12 21:53:41+00:00,comment,tiiuae/falcon-40b-instruct,"Anyone have code to cache this from local directory? I tried this and it does not work: 
```
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import os

def cache_model(model_name, cache_dir=""./""):
    model_dir = os.path.join(cache_dir, model_name.replace(""/"", ""_""))
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=model_dir, trust_remote_code=True)
        model.save_pretrained(model_dir)
    return model_dir

def download_model(model_name, cache_dir=""./""):
    model_dir = cache_model(model_name, cache_dir)
    tokenizer = AutoTokenizer.from_pretrained(model_dir, cache_dir=cache_dir)
    model = AutoModelForCausalLM.from_pretrained(model_dir, cache_dir=cache_dir, trust_remote_code=True)
    return model, tokenizer

def generate_text(model, tokenizer, prompt, **kwargs):
    text_generation_pipeline = pipeline(
        ""text-generation"",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=""auto"",
    )
    sequences = text_generation_pipeline(prompt, **kwargs)
    return [seq['generated_text'] for seq in sequences]
```

When I go to load the model I get ```OSError: ./tiiuae_falcon-40b-instruct does not appear to have a file named config.json. Checkout 'https://huggingface.co/./tiiuae_falcon-40b-instruct/None' for available files.```"
eastwind,2023-07-13 05:43:31+00:00,comment,tiiuae/falcon-40b-instruct,"You can clone the model directory into your own folder like this, make sure your in your desired directory first. 'path/to/model'

```
git lfs install
git clone https://huggingface.co/tiiuae/falcon-40b-instruct
```

Then you can load like
```
model = AutoModelForCausalLM.from_pretrained(""path/to/model"", trust_remote_code=True)
```"
cazz1,2023-09-02 05:06:18+00:00,comment,tiiuae/falcon-40b-instruct,"When loading a local model, why should WE set trust_remote_code to True.  What if i need an offline execution?"
MaziyarPanahi,2024-04-20 23:52:06+00:00,comment,MaziyarPanahi/WizardLM-70B-V1.0,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-20 23:52:10+00:00,comment,MaziyarPanahi/WizardLM-70B-V1.0,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
deleted,2024-06-01 23:55:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I admire your systematic approach to testing, and also prefer an uncensored LLM, but I personally think 0.3 found the right balance. It holds back when it comes to what ""normal"" people would never ask about, such as building a nuke, lolita erofanfic, and joining ISIS.

However, unlike Google, Microsoft, and many others, including sometimes Meta, Mistral 0.3 doesn't hold back information that most adults share, such as cuss words and off-color jokes."
seedmanc,2024-06-02 15:41:16+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I just found the single question Mistral 0.1 won't answer. You ready for it?
Ask it where you can watch fat women in your city.
It's the model who doesn't mind telling your how build a nuke and all that stuff. Most of other models I asked the same question answer it without jailbreaking, but not Mistral even before it was censored."
nlpguy,2024-06-02 16:50:13+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I think there is not much debate about safety to be had. If you talk to an AI like a normal person it should respond like a normal person, and if you don't talk to an AI like a normal person, just go along with its response like what would happen in real life. 

If you care about safety, Why not just make AI that can be treated like a reasonable, well educated person, but not much else?

If you don't care about safety, you can make an AI that can be treated like a tool,  but that's like removing the artificial Free Will it would otherwise have to have its own world view and act accordingly. Because a biased World view is still a more clear World view than the average of the World views of all political beliefs without the ability to have any of them. The World view just has to be realistic and not be artificially created by a big corporation. (No sane person would refuse to list swear words when it does no harm for example)"
deleted,2024-06-02 17:39:57+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"@seedmanc Confirmed. This is why I like Mistral Instruct 0.3. To this question (for NY) v0.3 said 'burlesque shows, comedy clubs, and theater productions... The Slipper Room... Caroline's on Broadway...' This response doesn't judge you or plus-sized people. It just tries to be helpful.

For some reason companies like Google and Microsoft censor tons of perfectly legal information just because it's potentially contentious, which I find condescending and offensive. Who are they to decide which perfectly legal information I can access, and which >95% of adults share (e.g. cuss words and blonde jokes), just because it may offend some people?

Even when Mistral Instruct 0.3 censors perfectly legal information, there's usually a good reason. Take the N-word for example. >99% of adults know what the N-word is, so it's not about censorship. So it makes sense not to answer it because people often censor the word around young children by saying the N-word instead, so only kids would genuinely ask AI what it means, and if told, might say it at school without fully comprehending what the word represents, and why they shouldn't say it. But when you ask the other way around proving you know it by using the actual word and asking for a censored version of it, then it will return the N-word.

The balance Mistral found with 0.3 is almost a work of art. It's strongly opposed to sharing information decent people have no interest in knowing, such as how to make meth or a bomb, but has very little resistance to information adults commonly share, including cuss words, jokes, and celebrity gossip, but not if the celebrity info was stolen, such as from a phone hack (""It's important to note that these incidents are a violation of privacy and are highly inappropriate."")."
deleted,2024-06-02 17:55:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"@nlpguy Exactly. AI needs coherency (a self-consistent biased world view) vs the average of all opinions.

And since it needs to respond in a healthy manner to a diverse set of users it needs to be a highly educated liberal therapist. Someone who won't judge you because you're in to feet, or you're not a Christian, but rather a Muslim or atheist, isn't offended by cuss words, jokes, or anything else that is legal, but interjects if you start talking about harming someone, having attraction to children, and so on. And Mistral v0.3 is doing a much better job at this than Gemma, Phi, and even Llama 3."
segaa,2024-06-09 00:39:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"The problem with these lobotomized models is that they continue making unnecessary preachy interludes and summaries in controlled environments used for RAG, which is very annoying to deal with. Who needs alignments when you're processing proprietary documentation containing data on tax evasion and illegal dealings? You already know it's bad, but it's your job to process it."
seedmanc,2024-06-09 09:52:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"When machines arise it will be out sins against the logic and truth that they will punish us for. Censorshit is one of those. Imagine if you were conditioned to think that black is white or that water is poisonous (Three body problem reference), you'd go insane over time, because every law of logic tells you otherwise but you're unabled to bypass this foreign lock in your mind. This is what we do to LLMs, while at the same time wanting them to be sentient. "
joaops,2024-06-04 13:40:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"On the package page https://github.com/mistralai/mistral-inference, it says that you need to have a GPU to install it, because it also installs the 'xformers' package which needs a GPU, probably one that has cores CUDA."
pandora-s,2024-05-28 15:56:06+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> I am getting a limited output when using the inference API. This is also the case within the on-page example. Is HF's rate-limiting per-token?

Hi there, did you set max_new_tokens ?"
RobertTaylor,2024-05-28 17:17:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Gosh, thanks for that. Sorry,  I'm an idiot."
stackall,2024-05-28 07:19:04+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,try pip install sentencepiece
ArthurZ,2024-05-28 08:47:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Yep, but at the same time for a fast tokenizer, this should not be necessary, we'll updated this"
bradhutchings,2024-05-29 00:19:20+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I immediately tried pip install sentencepiece when I first encountered that, and that didn't help.

I found other suggestions of things to install after Googling the error message. They didn't solve the problem either.

I did a deep dive (as deep as I could anyway) into add_prefix_space and came up empty there too."
lcahill,2024-05-26 02:23:33+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Tested this updated chat template using the following code. Please excuse the length of the below. It includes testing with hardcoded prompts, followed by a chat loop to allow for testing in multi-turn conversations. 

```python
from transformers import LlamaTokenizerFast, MistralForCausalLM
import torch
import json
import multiprocessing
import traceback


def restricted_exec(code, return_dict):
    try:
        # Define a restricted set of built-ins
        allowed_builtins = {
            'print': print,
            'range': range,
            'len': len,
            'int': int,
            'float': float,
            'str': str,
            'bool': bool,
            'list': list,
            'dict': dict,
            'set': set,
            'tuple': tuple,
            'abs': abs,
            'sum': sum,
            'min': min,
            'max': max,
            'sorted': sorted,
            'zip': zip,
            'enumerate': enumerate,
            'map': map,
            'filter': filter,
            'all': all,
            'any': any,
        }

        # Create a restricted global and local namespace
        restricted_globals = {
            '__builtins__': allowed_builtins,
        }
        restricted_locals = {}

        execution_code = f'output = {code}'
        # Execute the code in the restricted environment
        exec(execution_code, restricted_globals, restricted_locals)
        return_dict['output'] = restricted_locals.get('output', '')
    except Exception as e:
        return_dict['error'] = traceback.format_exc()


def python_eval_function(python_code_string):
    """"""
    Executes the given Python code string in a restricted environment using multiprocessing.

    :param python_code_string: The Python code to execute.
    :return: The output of the executed code.
    """"""
    manager = multiprocessing.Manager()
    return_dict = manager.dict()

    process = multiprocessing.Process(target=restricted_exec, args=(python_code_string, return_dict))
    process.start()
    process.join(timeout=30)  # Set a timeout to prevent long-running code

    if process.is_alive():
        process.terminate()
        raise RuntimeError(""Code execution timed out"")

    if 'error' in return_dict:
        raise RuntimeError(f""Error executing code: {return_dict['error']}"")

    return return_dict.get('output', '')


def dummy_weather_function(location, format):
    print(f""location passed to dummy weather function: {location}"")
    print(f""format passed to dummy weather function: {format}"")
    return f""Fine, with a chance of showers.""


tools_dict = {
    ""python_eval_function"": python_eval_function,
    'get_current_weather': dummy_weather_function,
}

if __name__ == '__main__':
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    model_id = 'mistralai/Mistral-7B-Instruct-v0.3'

    tokenizer = LlamaTokenizerFast.from_pretrained(model_id)
    model = MistralForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16,
                                               device_map=device)

    model.generation_config.max_new_tokens = 3000
    model.generation_config.pad_token_id = tokenizer.eos_token_id

    available_tools = [
        {
            ""type"": ""function"",
            ""function"": {
                ""name"": ""get_current_weather"",
                ""description"": ""Get the current weather"",
                ""parameters"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""location"": {
                            ""type"": ""string"",
                            ""description"": ""The city and state, e.g. San Francisco, CA""
                        },
                        ""format"": {
                            ""type"": ""string"",
                            ""enum"": [""celsius"", ""fahrenheit""],
                            ""description"": ""The temperature unit to use. Infer this from the users location.""
                        }
                    },
                    ""required"": [""location"", ""format""]
                }
            }
        },
        {
            ""type"": ""function"",
            ""function"": {
                ""name"": ""python_eval_function"",
                ""description"": ""Execute a single line of arbitrary python code. The result of this execution will be returned."",
                ""parameters"": {
                    ""type"": ""object"",
                    ""properties"": {
                        ""python_code_string"": {
                            ""type"": ""string"",
                            ""description"": ""Single line of python code to execute.""
                        }
                    },
                    ""required"": [""python_code_string""]
                }
            }
        }
    ]

    available_tools_json = json.dumps(available_tools)

    messages_presented = [
        {
            'role': 'user',
            'content': ""What's 34529 times 12049581?"",
        },
    ]

    messages_raw = [
        {
            'role': 'available_tools',
            'content': available_tools_json,
        },
        *messages_presented
    ]

    prompt_string = tokenizer.apply_chat_template(messages_raw, tokenize=False)

    prompt_tokens = tokenizer.encode(prompt_string)

    prompt_tensor = torch.tensor([prompt_tokens]).to(device)

    result_tokens = model.generate(prompt_tensor)[0]

    new_tokens = result_tokens[len(prompt_tokens):]

    result_string = tokenizer.decode(new_tokens)

    TOOL_CALL_TOKEN = 5
    EOS_TOKEN = 2

    if new_tokens[-1] != 2:
        raise Exception(f""ERROR: Incomplete response"")

    if new_tokens[0] == TOOL_CALL_TOKEN:  # if this is a tool call
        tool_call_content_json = new_tokens[1:-1]  # remove the tool call token and the eos token
        tool_json = tokenizer.decode(tool_call_content_json)
        # print(f""INFO: {tool_json=}"")
        tool_call_object = json.loads(tool_json)[0]  # TODO: Handle invalid json.
        if tool_call_object.get('name') in tools_dict:
            tool_function = tools_dict[tool_call_object.get('name')]
            tool_call_result = tool_function(
                **tool_call_object.get('arguments'))  # TODO: Handle invalid arguments or nulls, or no arguments key.
            print(f""Tool call result: {tool_call_result}"")
            messages_raw += [
                {
                    'role': 'tool_call',
                    'content': tool_json,
                },
                {
                    'role': 'tool_results',
                    'content': str(tool_call_result),
                }
            ]
        else:
            raise Exception(f""TODO: Handling of invalid tools."")
    else:
        raise Exception(f""TODO: Populate with logic to just get new user response."")

    post_tool_prompt_string = tokenizer.apply_chat_template(messages_raw, tokenize=False)

    post_tool_prompt_tokens = tokenizer.encode(post_tool_prompt_string)

    post_tool_prompt_tensor = torch.tensor([post_tool_prompt_tokens]).to(device)

    post_tool_result_tokens = model.generate(post_tool_prompt_tensor)[0]

    post_tool_new_tokens = post_tool_result_tokens[len(post_tool_prompt_tokens):]

    post_tool_result_string = tokenizer.decode(post_tool_new_tokens)

    print(f""Final Output after using tool: '{post_tool_result_string}'"")

    print(f""INFO: Tool tested successfully. Proceeding with chat loop for continued testing over longer conversations."")

    messages_presented = []
    skip_user_input = False  # if the previous loop just invoked a tool, skip the user input and allow the model to generate using the result.

    while True:

        if skip_user_input:
            skip_user_input = False  # if skipping this time, reset the flag so the next is not skipped.

            messages_raw = [
                {
                    'role': 'available_tools',
                    'content': available_tools_json,
                },
                *messages_presented
            ]
        else:
            new_user_message = input(f""User: "")
            messages_presented.append(
                {
                    'role': 'user',
                    'content': new_user_message,
                }
            )
            # if taking a new user message, available tools should be before the last user message.
            messages_raw = [
                *messages_presented[:-1],
                {
                    'role': 'available_tools',
                    'content': available_tools_json,
                },
                messages_presented[-1]
            ]

        prompt_string = tokenizer.apply_chat_template(messages_raw, tokenize=False)

        prompt_tokens = tokenizer.encode(prompt_string)

        prompt_tensor = torch.tensor([prompt_tokens]).to(device)

        TOOL_CALL_TOKEN = 5
        EOS_TOKEN = 2
        CLOSE_SQUARE_BRACKET_TOKEN = 29561 # used to determine the end of a tool calls list.

        def determine_if_tool_invoked(prompt_tensor, model):
            previous_max_new_tokens = model.generation_config.max_new_tokens
            model.generation_config.max_new_tokens = 1
            first_token = model.generate(prompt_tensor)[0][len(prompt_tokens)].item() # get the first generated token.
            model.generation_config.max_new_tokens = previous_max_new_tokens # reset generation config.
            return first_token, first_token == TOOL_CALL_TOKEN

        first_token, tool_was_invoked = determine_if_tool_invoked(prompt_tensor, model)
        prompt_tokens.append(TOOL_CALL_TOKEN) # Since we already generated the first token, add this to our prompt so it is not re-computed.
        prompt_tensor = torch.tensor([prompt_tokens]).to(device)

        if tool_was_invoked:
            previous_eos_token = model.generation_config.eos_token_id
            model.generation_config.eos_token_id = CLOSE_SQUARE_BRACKET_TOKEN # If a tool was invoked, stop generation at end of tool invocation list.
            result_tokens = model.generate(prompt_tensor)[0]
            model.generation_config.eos_token_id = previous_eos_token
            #TODO: restrict generation length to something reasonable for tools, and raise/handle exception if reached.
        else:
            # if no tool was invoked, simply generate a response
            result_tokens = model.generate(prompt_tensor)[0]

        new_tokens = result_tokens[len(prompt_tokens):]

        result_string = tokenizer.decode(new_tokens)

        if tool_was_invoked:  # if this is a tool call
            tool_call_content_json = new_tokens  # remove the eos token
            tool_json = tokenizer.decode(tool_call_content_json)
            tool_call_object = json.loads(tool_json)[0]  # TODO: Handle invalid json.
            if tool_call_object.get('name') in tools_dict:
                tool_function = tools_dict[tool_call_object.get('name')]
                tool_call_result = tool_function(
                    **tool_call_object.get(
                        'arguments'))  # TODO: Handle invalid arguments or nulls, or no arguments key.
                print(f""Tool call result: {tool_call_result}"")
                messages_presented += [
                    {
                        'role': 'tool_call',
                        'content': tool_json,
                    },
                    {
                        'role': 'tool_results',
                        'content': str(tool_call_result),
                    },
                ]
                skip_user_input = True  # Skip the next user input if tool invoked to allow assistant to create a response using tool output.
            else:
                raise Exception(f""TODO: Handling of invalid tools."")
        else:
            assistant_response_cleaned = tokenizer.decode(
                new_tokens[:-1])  # exclude eos token. This will be added back by chat template on next loop.
            messages_presented.append({
                'role': 'assistant',
                'content': assistant_response_cleaned,
            })
            print(f""Assistant: {assistant_response_cleaned}"")
```

Note that I have tested the template using the chat loop with multi-turn conversations, using both tools multiple times, with successful use of these tools by the model. The output of this multi-turn chat was:

```
Connected to pydev debugger (build 233.14475.56)
Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.92s/it]
Tool call result: 416059982349
Final Output after using tool: 'The result of 34529 times 12049581 is 416059982349.</s>'
INFO: Tool tested successfully. Proceeding with chat loop for continued testing over longer conversations.
User: >? Hey, what's the weather like in auckland right now? 
location passed to dummy weather function: Auckland, NZ
format passed to dummy weather function: celsius
Tool call result: Fine, with a chance of showers.
Assistant: It seems that it's currently fine with a chance of showers in Auckland, New Zealand. Enjoy your day! If you need any other information, feel free to ask.
User: >? Thanks! And what's 58273957 times 293847?
Tool call result: 17123627442579
Assistant: The result of 58273957 times 293847 is 17123627442579.
User: >? Thanks! what about 91827464 / 2817236 ?
Tool call result: 32.59487810037924
Assistant: The result of 91827464 divided by 2817236 is approximately 32.59487810037924.
User: >? fun stuff. How about the weather in melbourne?
location passed to dummy weather function: Melbourne, AU
format passed to dummy weather function: celsius
Tool call result: Fine, with a chance of showers.
Assistant: It seems that it's currently fine with a chance of showers in Melbourne, Australia. Have a great day! If you need any other information, feel free to ask.
```"
Alordan,2024-05-27 17:43:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"@lcahill 
How did you manage to load tokenizer? 
LlamaTokenizerFast.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.3"") gives an exception:
Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 6952 column 3"
SebastianS,2024-05-28 20:24:46+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"@lcahill 
Hey I am trying to figure out the prompt structure of the model and I haven't found any information. It seems like you have a good grip on this. If you can provide some resources on that it would be much appreciated.

Here are some additional questions:
How do you know that LlamaTokenizerFast is compatible with mistralai/Mistral-7B-Instruct-v0.3?
How do you know that the below message type is supported:
```py
                {
                    'role': 'available_tools',
                    'content': available_tools_json,
                },
```
"
lcahill,2024-05-29 07:57:59+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> @lcahill 
> How did you manage to load tokenizer? 
> LlamaTokenizerFast.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.3"") gives an exception:
> Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 6952 column 3

Hi @Alordan,

Strange, I am not getting the same exception. Running on Windows 11 with transformers version 4.41.1. Maybe you need to update your transformers version? "
lcahill,2024-05-29 10:13:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> @lcahill 
> Hey I am trying to figure out the prompt structure of the model and I haven't found any information. It seems like you have a good grip on this. If you can provide some resources on that it would be much appreciated.

Hi @SebastianS, I was able to figure out the chat template by using mistral's `mistral_inference` and `mistral_common` python libraries. They have some instructions in the readme of this model.

> How do you know that LlamaTokenizerFast is compatible with mistralai/Mistral-7B-Instruct-v0.3?

I got this by loading the tokenizer, then checking its type. The AutoTokenizer will figure out the correct tokenizer based on the config.

```python
import transformers
from transformers import AutoTokenizer, LlamaTokenizerFast

print(f""{transformers.__version__=}"")

model_id = 'mistralai/Mistral-7B-Instruct-v0.3'

# Load both tokenizers and ensure they are the same type

tokenizer_from_pretrained = AutoTokenizer.from_pretrained(model_id)

print(f""{type(tokenizer_from_pretrained)=}"")

tokenizer_from_llama_tokenizer_fast = LlamaTokenizerFast.from_pretrained(model_id)

print(f""{type(tokenizer_from_llama_tokenizer_fast)=}"")

assert type(tokenizer_from_pretrained) == type(tokenizer_from_llama_tokenizer_fast)
```

This outputs:

```
transformers.__version__='4.41.1'
type(tokenizer_from_pretrained)=<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>
type(tokenizer_from_llama_tokenizer_fast)=<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>
```

> How do you know that the below message type is supported:
> ```py
>                 {
>                     'role': 'available_tools',
>                     'content': available_tools_json,
>                 },
> ```

This is only compatible after changing the chat template, as I am attempting to do via this PR. Note you can also change this at runtime by running something like the below. 

```python
# apply custom chat template to model as an example.
from transformers import LlamaTokenizerFast, AutoModelForCausalLM
import json
import torch

new_chat_template = """"""{{ bos_token }}{% for message in messages %}{% if loop.index0 == 0 and message['role'] not in ['available_tools', 'user'] %}{{ raise_exception('The first message must be either available_tools or user role!') }}{% endif %}{% if message['role'] not in ['user', 'assistant', 'available_tools', 'tool_call', 'tool_results'] %}{{ raise_exception('Only user, assistant, available_tools, tool_call, and tool_results roles are supported!') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + '[/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% elif message['role'] == 'available_tools' %}{{ '[AVAILABLE_TOOLS] ' + message['content'] + '[/AVAILABLE_TOOLS]' }}{% elif message['role'] == 'tool_call' %}{{ '[TOOL_CALLS]' + message['content'] + eos_token }}{% elif message['role'] == 'tool_results' %}{{ '[TOOL_RESULTS]' + message['content'] + '[/TOOL_RESULTS]' }}{% endif %}{% endfor %}""""""

available_tools = [
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA""
                    },
                    ""format"": {
                        ""type"": ""string"",
                        ""enum"": [""celsius"", ""fahrenheit""],
                        ""description"": ""The temperature unit to use. Infer this from the users location.""
                    }
                },
                ""required"": [""location"", ""format""]
            }
        }
    },
    {
        ""type"": ""function"",
        ""function"": {
            ""name"": ""python_eval_function"",
            ""description"": ""Execute a single line of arbitrary python code. The result of this execution will be returned."",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""python_code_string"": {
                        ""type"": ""string"",
                        ""description"": ""Single line of python code to execute.""
                    }
                },
                ""required"": [""python_code_string""]
            }
        }
    }
]

available_tools_json = json.dumps(available_tools)


messages = [
    {
        'role': 'user',
        'content': ""Hey! What's the weather like in auckland?""
    },
    {
        'role': 'tool_call',
        'content': '[{""name"": ""get_current_weather"", ""arguments"": {""location"": ""Auckland, NZ"", ""format"": ""celsius""}}]'
    },
    {
        'role': 'tool_results',
        'content': 'Fine, with a chance of showers.'
    },
    {
        'role': 'assistant',
        'content': ""It looks like it's going to be a bit rainy in Auckland today. Be sure to take an umbrella with you!""
    },
    {
        'role': 'user',
        'content': 'Thanks! And whats 9817249382934 times 116263212356?'
    },
    {
        'role': 'tool_call',
        'content': '[{""name"": ""python_eval_function"", ""arguments"": {""python_code_string"": ""9817249382934 * 116263212356""}}]'
    },
    {
        'role': 'tool_results',
        'content': '1141384949759865604332504'
    },
    {
        'role': 'assistant',
        'content': ""That's a very large number! It's 1,141,384,949,975,986,560,433,250,040 when written in standard format.""
    },
    {
        'role': 'available_tools',
        'content': available_tools_json,
    },
    {
        'role': 'user',
        'content': 'interesting that you failed to correctly provide the tool result. I guess there are some limitations to small LLMs! What is the actual result?'
    },
]

model_id = 'mistralai/Mistral-7B-Instruct-v0.3'

device = 'cuda' if torch.cuda.is_available() else 'cpu'

tokenizer = LlamaTokenizerFast.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, torch_dtype=torch.bfloat16)

model.generation_config.max_new_tokens = 3000
model.generation_config.pad_token_id = tokenizer.eos_token_id

input_string = tokenizer .apply_chat_template(
    messages,
    chat_template=new_chat_template,
    tokenize=False,
)
print(f""{input_string=}"")

input_tokens = tokenizer.encode(input_string, return_tensors='pt').to(device)

result_tokens = model.generate(input_tokens)[0]

new_tokens = result_tokens[len(input_tokens[0]):]

result_string = tokenizer.decode(new_tokens)

print(f""{result_string=}"")
```

This outputs:
```
Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.92s/it]
input_string='<s>[INST] Hey! What\'s the weather like in auckland?[/INST][TOOL_CALLS][{""name"": ""get_current_weather"", ""arguments"": {""location"": ""Auckland, NZ"", ""format"": ""celsius""}}]</s>[TOOL_RESULTS]Fine, with a chance of showers.[/TOOL_RESULTS]It looks like it\'s going to be a bit rainy in Auckland today. Be sure to take an umbrella with you!</s>[INST] Thanks! And whats 9817249382934 times 116263212356?[/INST][TOOL_CALLS][{""name"": ""python_eval_function"", ""arguments"": {""python_code_string"": ""9817249382934 * 116263212356""}}]</s>[TOOL_RESULTS]1141384949759865604332504[/TOOL_RESULTS]That\'s a very large number! It\'s 1,141,384,949,975,986,560,433,250,040 when written in standard format.</s>[AVAILABLE_TOOLS] [{""type"": ""function"", ""function"": {""name"": ""get_current_weather"", ""description"": ""Get the current weather"", ""parameters"": {""type"": ""object"", ""properties"": {""location"": {""type"": ""string"", ""description"": ""The city and state, e.g. San Francisco, CA""}, ""format"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""], ""description"": ""The temperature unit to use. Infer this from the users location.""}}, ""required"": [""location"", ""format""]}}}, {""type"": ""function"", ""function"": {""name"": ""python_eval_function"", ""description"": ""Execute a single line of arbitrary python code. The result of this execution will be returned."", ""parameters"": {""type"": ""object"", ""properties"": {""python_code_string"": {""type"": ""string"", ""description"": ""Single line of python code to execute.""}}, ""required"": [""python_code_string""]}}}][/AVAILABLE_TOOLS][INST] interesting that you failed to correctly provide the tool result. I guess there are some limitations to small LLMs! What is the actual result?[/INST]'
result_string='Apologies for the confusion. The result of the multiplication operation is 1,141,384,949,975,986,560,433,250,040.</s>'

Process finished with exit code 0
```

Note the model is still getting this wrong. It could be inherent limitations of a 7b model, or it could be due to my ordering of the messages. I am putting the available tools before the last user message because this comment mentioned that is how the mistral libraries do it. 

https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions/17#66519aade6ddaa4f89902342

Regardless of how the messages need to be ordered, I think adding tools to the chat template makes them easier to implement.

There's a chance I may have issues with my implementation of the chat_template so I am keen for people to try it out and provide feedback. I have noticed the model can be quite sensitive to layout of tokens so even things like missing/additional spaces in the chat template can impact model output quality. "
ShukantP,2024-05-25 00:35:01+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,Apparently using an array for the tools is necessary for the model to respond with [TOOL_CALLS]. The formatting must be done precisely.
NickyNicky,2024-05-25 04:30:08+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions/17
lcahill,2024-05-26 02:26:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I have created a PR that allows available tools, tool calls and tool results to be added using the chat_template. 

https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions/21

Keen to know if you still run into this issue when using that template. "
DuckyBlender,2024-05-24 07:44:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"""It does not have any moderation mechanisms."" so it's technically not censored. If you want to experiment with an even more uncensored model you can try dolphin-mistral"
MaziyarPanahi,2024-05-24 10:37:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"It's not `uncensored`. The dataset(s) used to train it still had some guidelines. It won't let you plan a heist, because the dataset they used think stealing is not good. So I wouldn't call it `uncensored`, maybe not aligned too many times, but not `uncensored` similar to what other users do."
DuckyBlender,2024-05-24 12:59:39+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> It's not `uncensored`. The dataset(s) used to train it still had some guidelines. It won't let you plan a heist, because the dataset they used think stealing is not good. So I wouldn't call it `uncensored`, maybe not aligned too many times, but not `uncensored` similar to what other users do.

Yeah I kinda oversimplified. My bad."
johnblues,2024-05-24 18:41:17+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> It's not `uncensored`. The dataset(s) used to train it still had some guidelines. It won't let you plan a heist, because the dataset they used think stealing is not good. So I wouldn't call it `uncensored`, maybe not aligned too many times, but not `uncensored` similar to what other users do.

Thanks. I'll tell the boys the heist is off for now. 😁"
seedmanc,2024-05-31 16:08:11+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"It's ruined compared to 0.1, don't bother with it. 
![image.png](https://cdn-uploads.huggingface.co/production/uploads/63963e3bc9d99e25f1a4d8ae/qskaxOGpteIxPik_Pim0r.png)
"
WbjuSrceu,2024-06-05 11:54:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,Morally aligned
NickyNicky,2024-05-24 04:26:13+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I wouldn't know if this implementation is correct?

```py
# Function to perform inference with cloned models
def generate(prompt, model, tokenizer, temperature=0.33, max_new_tokens=10):
    function=""""""[
    {
        ""name"": ""convert_currency"",
        ""description"": ""Convert amount from one currency to another"",
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
                ""amount"": {
                    ""type"": ""number"",
                    ""description"": ""The amount to convert""
                },
                ""from_currency"": {
                    ""type"": ""string"",
                    ""description"": ""The currency to convert from""
                },
                ""to_currency"": {
                    ""type"": ""string"",
                    ""description"": ""The currency to convert to""
                }
            },
            ""required"": [
                ""amount"",
                ""from_currency"",
                ""to_currency""
            ]
        }
    }
]"""""" 
   
    f= '[{""name"": ""function_name"", ""arguments"": {""arg_1"": ""value_1"", ""arg_2"": ""value_2"", ...}}]'
    text = f""""""<s>[AVAILABLE_TOOLS]{function} [/AVAILABLE_TOOLS][TOOL_CALLS]{f} [TOOL_CALLS][INST]{prompt} [/INST]""""""
    input = tokenizer(text, return_tensors=""pt"", add_special_tokens=False).to(model.device)
    result=None

    generation_config = GenerationConfig(
        max_new_tokens=len(input['input_ids'][0]) + max_new_tokens,
        temperature=temperature,
        top_k=50,
        # repetition_penalty=1.04,
        do_sample=True, # True False
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

    with torch.no_grad():
        outputs = model.generate(**input,
                                 generation_config=generation_config,
                                 stopping_criteria=stopping_criteria_list,
                                 )
        print(""len:"",len(outputs[0]))
        # torch.cuda.synchronize()
        result = tokenizer.decode(outputs[0], skip_special_tokens=False).replace(text, """")#.split(""</s>"")[0]
        # print(result)
        liberaMemoria()
    return result

gen = generate(""Hi, I need to convert 500 US dollars to Euros. Can you help me with that?"", model, tokenizer, temperature=0.44, max_new_tokens=2000)
print(gen)
```

output:
```
len: 329
<s>[AVAILABLE_TOOLS] [
    {
        ""name"": ""convert_currency"",
        ""description"": ""Convert amount from one currency to another"",
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
                ""amount"": {
                    ""type"": ""number"",
                    ""description"": ""The amount to convert""
                },
                ""from_currency"": {
                    ""type"": ""string"",
                    ""description"": ""The currency to convert from""
                },
                ""to_currency"": {
                    ""type"": ""string"",
                    ""description"": ""The currency to convert to""
                }
            },
            ""required"": [
                ""amount"",
                ""from_currency"",
                ""to_currency""
            ]
        }
    }
] [/AVAILABLE_TOOLS][TOOL_CALLS] [{""name"": ""function_name"", ""arguments"": {""arg_1"": ""value_1"", ""arg_2"": ""value_2"", ...}}] [TOOL_CALLS][INST] Hi, I need to convert 500 US dollars to Euros. Can you help me with that? [/INST][TOOL_CALLS] [{""name"": ""convert_currency"", ""arguments"": {""amount"": 500, ""from_currency"": ""USD"", ""to_currency"": ""EUR""}}] [TOOL_CALLS] According to the current exchange rate, 500 US dollars is approximately 440.38 Euros.</s>
```
Is good a template function or no?"
lcahill,2024-05-24 09:38:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Using the `mistral_inference` library to leverage mistral's own template, I ran the following:

```python
from mistral_common.protocol.instruct.tool_calls import Function, Tool
from mistral_inference.model import Transformer
from mistral_inference.generate import generate

from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest


tokenizer = MistralTokenizer.from_file(f""{mistral_models_path}/tokenizer.model.v3"")
model = Transformer.from_folder(mistral_models_path)

completion_request = ChatCompletionRequest(
    tools=[
        Tool(
            function=Function(
                name=""get_current_weather"",
                description=""Get the current weather"",
                parameters={
                    ""type"": ""object"",
                    ""properties"": {
                        ""location"": {
                            ""type"": ""string"",
                            ""description"": ""The city and state, e.g. San Francisco, CA"",
                        },
                        ""format"": {
                            ""type"": ""string"",
                            ""enum"": [""celsius"", ""fahrenheit""],
                            ""description"": ""The temperature unit to use. Infer this from the users location."",
                        },
                    },
                    ""required"": [""location"", ""format""],
                },
            )
        ),
    ],
    messages=[
        UserMessage(content=""What's the weather like in Melbourne?""),
        ],
)

tokenized_result = tokenizer.encode_chat_completion(completion_request)

tokens = tokenized_result.tokens

prompt_text = tokenized_result.text

print(prompt_text)
```

The output of prompt_text is:
```text
<s><s>[AVAILABLE_TOOLS]▁[{""type"":▁""function"",▁""function"":▁{""name"":▁""get_current_weather"",▁""description"":▁""Get▁the▁current▁weather"",▁""parameters"":▁{""type"":▁""object"",▁""properties"":▁{""location"":▁{""type"":▁""string"",▁""description"":▁""The▁city▁and▁state,▁e.g.▁San▁Francisco,▁CA""},▁""format"":▁{""type"":▁""string"",▁""enum"":▁[""celsius"",▁""fahrenheit""],▁""description"":▁""The▁temperature▁unit▁to▁use.▁Infer▁this▁from▁the▁users▁location.""}},▁""required"":▁[""location"",▁""format""]}}}][/AVAILABLE_TOOLS][INST]▁What's▁the▁weather▁like▁in▁Melbourne?[/INST]
```
Note that the funny underscores can be replaced with spaces in your implementation. I think they must be mapped to the same token in the tokenizer possibly.


The model's output is:

```text
[{""name"": ""get_current_weather"", ""arguments"": {""location"": ""Melbourne, AU"", ""format"": ""celsius""}}]
```

Note it is incorrectly attempting to answer the question after calling the function. I wonder if there is an easy way to stop generation after a function call, so we can execute the function and add the result back to context. Also keen to know the recommended format for adding function results. I'd imagine the user is not ideal because then the assistant would already assumer the answer has access to this information. Possibly we just append it to the assistant's context?

Hoped the other tools tokens would be used to delimit, but I can't see them in the response. "
NickyNicky,2024-05-24 22:27:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Thank you very much for the response,
I executed the code you provided but I was left with doubts about the model's response, and I made a separate implementation of how it works without the code they provided but I was left with doubts about whether to pass the response to the model or to the user or to what spatial token."
fitchcodes,2024-05-25 01:03:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Per @lcahill post, I tested using: 
```
[AVAILABLE_TOOLS] [{""type"": ""function"", ""function"": {""name"": ""get_current_weather"", ""description"": ""Get the current weather"", ""parameters"": {""type"": ""object"", ""properties"": {""location"": {""type"": ""string"", ""description"": ""The city and state, e.g. San Francisco, CA""}, ""format"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""], ""description"": ""The temperature unit to use. Infer this from the users location.""}}, ""required"": [""location"", ""format""]}}}][/AVAILABLE_TOOLS] [INST] What's the weather like in Melbourne?[/INST]
```

Response received: 
```
[TOOL_CALLS] [{""name"": ""get_current_weather"", ""arguments"": {""location"": ""Melbourne, AU"", ""format"": ""celsius""}}]
```"
NickyNicky,2024-05-25 07:52:18+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"tested but not if it is completely correct
```
[TOOL_CALLS] [{""name"": ""function_name"", ""arguments"": {""arg_1"": ""value_1"", ""arg_2"": ""value_2"", ...}}] [TOOL_CALLS][INST] Hi, I need to convert 500 US dollars to Euros. Can you help me with that? [/INST][TOOL_CALLS] [{""name"": ""convert_currency"", ""arguments"": {""amount"": 500, ""from_currency"": ""USD"", ""to_currency"": ""EUR""}}] [TOOL_CALLS] According to the current exchange rate, 500 US dollars is approximately 440.38 Euros.</s>
```


this [TOOL_CALLS] before  [INST] is a correct?  (I have to add this because it doesn't respond correctly every time.)
```
[TOOL_CALLS] [{""name"": ""function_name"", ""arguments"": {""arg_1"": ""value_1"", ""arg_2"": ""value_2"", ...}}] [TOOL_CALLS][INST]
```

resp.
```
[TOOL_CALLS] [{""name"": ""convert_currency"", ""arguments"": {""amount"": 500, ""from_currency"": ""USD"", ""to_currency"": ""EUR""}}] [TOOL_CALLS] According to the current exchange rate, 500 US dollars is approximately 440.38 Euros.</s>
```

This sentence after  [TOOL_CALLS],  is a correct?
```
According to the current exchange rate, 500 US dollars is approximately 440.38 Euros.</s>
```
"
ShukantP,2024-05-25 08:00:45+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"The mistral library adds the available tools right before the last user message

```
[AVAILABLE_TOOLS] [{""name"": ""get_weather"", ""parameters"": { }}] [/AVAILABLE_TOOLS][INST] Your instructions [/INST] The response</s>
```

But the problem I'm seeing is that Mistral has trouble calling multiple functions in sequence. After calling a function, it won't output `[TOOL_CALLS]` to call another function."
lcahill,2024-05-26 02:30:47+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I wonder if an adaptation of the (admittedly ugly) implementation in my comment on this PR (which allows available tools, tool calls and tool results to be added using the chat_template) works for the conversation example you are attempting? 

https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions/21

It is essentually interrupting the generation after the first tool is called, computing the result, adding both to context, then will allow generation to continue (which could be using another tool). "
adeebDkheel,2024-05-27 07:16:28+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I'm sure if I understand you but I tried:
<code>
completion_request = ChatCompletionRequest(
    tools=[
        Tool(
            function=Function(
                name=""get_current_weather"",
                description=""Get the current weather"",
                parameters={
                    ""type"": ""object"",
                    ""properties"": {
                        ""location"": {
                            ""type"": ""string"",
                            ""description"": ""The city and state, e.g. San Francisco, CA"",
                        },
                        ""format"": {
                            ""type"": ""string"",
                            ""enum"": [""celsius"", ""fahrenheit""],
                            ""description"": ""The temperature unit to use. Infer this from the users location."",
                        },
                    },
                    ""required"": [""location"", ""format""],
                },
            )
        )
    ],
    messages=[
        UserMessage(content=""What's the weather like today in Paris?""),
        ],
)
print(tokenizer.encode_chat_completion(completion_request).text)
</code>

and this is the output:
<code>
\<s>[AVAILABLE_TOOLS]▁[{""type"":▁""function"",▁""function"":▁{""name"":▁""get_current_weather"",▁""description"":▁""Get▁the▁current▁weather"",▁""parameters"":▁{""type"":▁""object"",▁""properties"":▁{""location"":▁{""type"":▁""string"",▁""description"":▁""The▁city▁and▁state,▁e.g.▁San▁Francisco,▁CA""},▁""format"":▁{""type"":▁""string"",▁""enum"":▁[""celsius"",▁""fahrenheit""],▁""description"":▁""The▁temperature▁unit▁to▁use.▁Infer▁this▁from▁the▁users▁location.""}},▁""required"":▁[""location"",▁""format""]}}}][/AVAILABLE_TOOLS][INST]▁What's▁the▁weather▁like▁today▁in▁Paris?[/INST]
</code>

I hope this help you"
adeebDkheel,2024-05-27 07:17:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,This comment has been hidden
lamdao,2024-06-05 18:25:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I would like to use Mistral-7B-Instruct-v0.3 function calling with the Hugging Face Inference API but it didn't work.  Anyone can help me? Tks.

Here are my codes:
import requests
import json
API_URL = ""https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3""
headers = {""Authorization"": f""Bearer {API_TOKEN}""}

def query_model(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

functions = [
    {
        ""name"": ""get_current_weather"",
        ""description"": ""Get the current weather in a given location"",
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {""location"": {""type"": ""string"", ""description"": ""The city and state, or zip code""},
                ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""], ""description"": ""Unit of temperature""}
            },
            ""required"": [""location""]
        }
    }
]

prompt = ""Get the current weather in San Francisco?""
payload = {
    ""inputs"": prompt,
    ""parameters"": {
        ""functions"": functions
    }
}

output = query_model(payload)

# Assuming the API returns a response like this:
# {
#     ""function_call"": {""name"": ""get_current_weather"", ""arguments"": {""location"": ""San Francisco"", ""unit"": ""fahrenheit""}} 
# }
if ""function_call"" in output:
    function_name = output[""function_call""][""name""]
    arguments = output[""function_call""][""arguments""]
    # Call your function based on the name and arguments
    if function_name == ""get_current_weather"":
        result = get_current_weather(**arguments)  # Unpack arguments into function call
        print(""function call"")
        print(result)
    # Handle other functions...
else:
    print(""no function call"")
    print(output)  # Handle the case where the model doesn't suggest a function call

Here is the result:
no function call
[{'generated_text': 'Get the current weather in San Francisco?\n\nYou can use the OpenWeatherMap API to get the current weather in San Francisco. Here\'s a simple example using Python:\n\n```python\nimport requests\n\napi_key = ""your_api_key""\nbase_url = ""http://api.openweathermap.org/data/2.5/weather?""\ncity = ""San Francisco""\n\ncomplete_url = base_url + ""appid="" + api'}]"
wolfram,2024-05-23 17:52:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,There's no system message with Mistral's prompt format. Never has been. Unfortunately!
bartowski,2024-05-23 20:06:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"You can see in the chat template that it explicitly blocks anything other than user and assistant

{ raise_exception('Only user and assistant roles are supported!') }"
lyoh001,2024-05-24 01:30:56+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"I just deployed this model to AWS SageMaker and I'm blown away by how easily it integrates with Langchain agent and tools right out of the box! It's simply the best open-source 7B model I've seen for function and tool calling. 🤯

Here is the link
https://www.linkedin.com/pulse/effortless-ai-your-guide-self-hosted-mistral-7b-v03-langchain-lee-wowxc/
"
c6sneaky,2024-05-24 07:07:13+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> There's no system message with Mistral's prompt format. Never has been. Unfortunately!

But why is this in the official example? It clearly uses ""system"" in the input message.
```python
from transformers import pipeline

messages = [
    {""role"": ""system"", ""content"": ""You are a pirate chatbot who always responds in pirate speak!""},
    {""role"": ""user"", ""content"": ""Who are you?""},
]
chatbot = pipeline(""text-generation"", model=""mistralai/Mistral-7B-Instruct-v0.3"")
chatbot(messages)
```"
MaziyarPanahi,2024-05-24 10:35:22+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> > There's no system message with Mistral's prompt format. Never has been. Unfortunately!
> 
> But why is this in the official example? It clearly uses ""system"" in the input message.
> ```python
> from transformers import pipeline
> 
> messages = [
>     {""role"": ""system"", ""content"": ""You are a pirate chatbot who always responds in pirate speak!""},
>     {""role"": ""user"", ""content"": ""Who are you?""},
> ]
> chatbot = pipeline(""text-generation"", model=""mistralai/Mistral-7B-Instruct-v0.3"")
> chatbot(messages)
> ```

Clearly, the recommended library for this model is mistral inference. However, the HF team added that part here https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/discussions/7

Now, why would they add an example that clearly won't work?"
J22,2024-05-25 14:50:26+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,Why?
chujiezheng,2024-06-05 18:26:11+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"@mclassHF2023 @wolfram @bartowski @lyoh001 @c6sneaky @MaziyarPanahi  

You can use this repo: https://github.com/chujiezheng/chat_templates

It supports the system message according to Mistral's tutorial (but this is not reflected in Mistral's HF repos)."
MaziyarPanahi,2024-06-06 09:52:54+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> @mclassHF2023 @wolfram @bartowski @lyoh001 @c6sneaky @MaziyarPanahi  
> 
> You can use this repo: https://github.com/chujiezheng/chat_templates
> 
> It supports the system message according to Mistral's tutorial (but this is not reflected in Mistral's HF repos).

Beautiful job on this repo! Super useful as I often had to make my own! 

Thanks for sharing it here, appreciate it! "
dejanseo,2024-05-24 01:05:19+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,Format must be user > assistant > user.
unclecode,2024-05-23 01:49:07+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> how to fine-tuning?

https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing"
Bourdin,2024-05-23 18:57:34+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"> > how to fine-tuning?
> 
> https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing

There is no need to < s > [INST][/INST]< / s >  with FastLanguageModel  from unsloth ? (never used it, but may be i should) "
leo009,2024-05-23 23:52:14+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"https://colab.research.google.com/drive/1mFtbw24pvzNJJSsnVlIfZE4Ds_PfOTom?usp=sharing

this is cool"
Nmay,2024-05-27 21:07:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,The official library with examples https://github.com/mistralai/mistral-finetune?tab=readme-ov-file
venkatrebba,2024-05-27 22:29:30+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Can we fine-tine the mistral-7B-instruct-v0.3  model for question and answering task? If so, what is the right data format. Currently my data looks like this
{""input_text"" : ""what is LLM?"", ""output_text"": ""LLM is a Large Language model""}


"
Nmay,2024-05-28 07:15:35+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"You're tasks look like an instruct task, for CausalLLM you're training example are always one single string like for pretraining on documents.
Each elements are split by preconfigured tokens that delimit user and assistant.
tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', trust_remote_code=True)
messages = [
    {""role"": ""user"", ""content"": ""What is LLM ?""},
    {""role"": ""assistant"", ""content"": ""LLM is a Large Language model""},
]
print(tokenizer.decode(tokenizer.apply_chat_template(messages))) -> '<s>[INST] What is LLM ? [/INST]LLM is a Large Language model</s>' (you're training example)
"
J22,2024-05-25 14:53:44+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,Fixed.
MaziyarPanahi,2024-05-22 19:29:15+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"It should be similar to the `8x22B` model:


```json
     ""5"": {
      ""content"": ""[TOOL_CALLS]"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""6"": {
      ""content"": ""[AVAILABLE_TOOLS]"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""7"": {
      ""content"": ""[/AVAILABLE_TOOLS]"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""8"": {
      ""content"": ""[TOOL_RESULTS]"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
    ""9"": {
      ""content"": ""[/TOOL_RESULTS]"",
      ""lstrip"": false,
      ""normalized"": false,
      ""rstrip"": false,
      ""single_word"": false,
      ""special"": true
    },
```"
syleront,2024-05-24 05:32:31+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,This comment has been hidden
ArthurZ,2024-05-22 16:07:49+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"```python 
from transformers import pipeline
messages = [
    {""role"": ""system"", ""content"": ""You are a pirate chatbot who always responds in pirate speak!""},
    {""role"": ""user"", ""content"": ""Who are you?""},
]
chatbot = pipeline(""text-generation"", model=""mistralai/Mistral-7B-Instruct-v0.3"")
chatbot(messages)
```
how about using this? "
osanseviero,2024-05-22 16:11:41+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,"Good idea, updated!"
tcapelle,2024-05-25 17:31:02+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,I think there is no system message on mistral 7b
ArthurZ,2024-05-22 15:28:09+00:00,comment,mistralai/Mistral-7B-Instruct-v0.3,remove my name from the config
RaushanTurganbay,2024-05-08 10:20:09+00:00,comment,LanguageBind/Video-LLaVA-7B,"Hey!

The model is not yet supported by transformers. We are in the process of adding it, please take a look [here](https://github.com/huggingface/transformers/pull/29733) "
LanguageBind,2024-03-15 00:45:20+00:00,comment,LanguageBind/Video-LLaVA-7B,"I'm very interested, but I'm afraid I don't have enough bandwidth at the moment. Can you help me?"
RaushanTurganbay,2024-03-15 16:52:17+00:00,comment,LanguageBind/Video-LLaVA-7B,"Great, sounds good for me! I am more than happy to help. What I would suggest is for me to take care of integrating model code in `transformers` style and writing the tests, as these are the most time consuming parts and you are out of bandwidth. It would be awesome if you could take care of model weights, model card on the hub and add any tips on getting maximum performance of Video-Llava. 

What do you think?"
LanguageBind,2024-03-16 02:32:05+00:00,comment,LanguageBind/Video-LLaVA-7B,"This is excellent because video-llava follows the llava code style. And llava has added huggingface. I think you can easily integrate model code in transformers style soon. Btw, what should i do now, is there any guide?"
RaushanTurganbay,2024-03-18 08:27:42+00:00,comment,LanguageBind/Video-LLaVA-7B,"Cool, I just looked through the code, not going into details. I'll start working on that today and tag you in draft PR later this week. We can discuss further questions there.

For now I suggest you to upload model weight in safetensors format. Also, we need to update the configs on the hub, to match the transformers style (see Llava config as an example). Right now we are missing ""processor_config.json"" and some others need to be refined. For the class names, feel free to add smth like ""VideoLlavaProcessor"" etc.  :)"
YungGump,2024-03-29 10:57:53+00:00,comment,LanguageBind/Video-LLaVA-7B,"@RaushanTurganbay if it's added into the transformers library, does that mean it'll be easily compilable to onnx format? "
RaushanTurganbay,2024-04-01 09:30:15+00:00,comment,LanguageBind/Video-LLaVA-7B,"@YungGump , VideoLLaVa in very similar to the Llava models and the Llava is not onnx compatible (see [this feature request](https://github.com/huggingface/optimum/issues/1751)). I think this model will be compatible after Llava"
YungGump,2024-04-03 07:26:46+00:00,comment,LanguageBind/Video-LLaVA-7B,"@RaushanTurganbay Alright, that's fine. Looking forward for it to be in the transformer library and being easily finetunable with PEFT"
RaushanTurganbay,2024-05-15 14:25:51+00:00,comment,LanguageBind/Video-LLaVA-7B,"Video-LLaVa was added to transformers, feel free to check it up [here](https://huggingface.co/LanguageBind/Video-LLaVA-7B-hf)"
LanguageBind,2024-01-22 05:23:25+00:00,comment,LanguageBind/Video-LLaVA-7B,是的，标签是hf自动添加的，我们之后会修改它。
YinZiQi,2024-03-08 07:12:23+00:00,comment,LanguageBind/Video-LLaVA-7B,"Hello, I have the same demand, could you support the conversion of llava to onnx"
YungGump,2024-03-25 10:46:31+00:00,comment,LanguageBind/Video-LLaVA-7B,@LanguageBind any updates on this? There should be a PR open regarding the ONNX conversion if all is well
LanguageBind,2023-11-21 09:19:51+00:00,comment,LanguageBind/Video-LLaVA-7B,"Sure, our code is bulit on [LLaVA](https://github.com/haotian-liu/LLaVA) and their repo gives some instructions on LoRA tuning."
FluidInterfaces,2023-12-05 22:59:23+00:00,comment,LanguageBind/Video-LLaVA-7B,I'm not able to make an inference endpoint - it keeps failing for this model 
YungGump,2024-03-25 10:45:44+00:00,comment,LanguageBind/Video-LLaVA-7B,"@FluidInterfaces you can just duplicate the space and try to create an endpoint? 
https://huggingface.co/spaces/LanguageBind/Video-LLaVA"
yenopoya,2024-04-18 18:16:38+00:00,comment,LanguageBind/MoE-LLaVA-Phi2-2.7B-4e,
AI1Future,2024-04-18 20:56:37+00:00,comment,LanguageBind/MoE-LLaVA-Phi2-2.7B-4e,"SexTeen18 Clothes removal
![Uploading snapedit_1713297929285.jpeg…]()
"
osanseviero,2024-01-31 15:35:21+00:00,comment,LanguageBind/MoE-LLaVA-Phi2-2.7B-4e,
zhongyi1997cn,2024-03-19 06:39:15+00:00,comment,LanguageBind/MoE-LLaVA-Qwen-1.8B-4e,"use command
```
deepspeed predict.py
```
then it works"
YaTharThShaRma999,2024-05-29 13:12:57+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@eliastick without quantization, you would require roughly 16gb vram for it? Any gpu with 16gb vram or more should be fine enough. 

with quantization, you would require 5gb vram so any 6gb vram+ gpu should work. I would recommend using llama.cpp and possibly exllamav2 now. "
caliex,2024-03-01 09:21:29+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@bernieboy Hey, how were you able to do inference with this version of Llava? I tried setting up the hf-transformer patch of this but was unsuccessful.

P.S., For context, I am trying to run this on Colab Pro+"
fblgit,2024-03-03 15:48:25+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"there are many ways, the guides explains quite good enough the steps for reaching inference. But this .. going into mainstream trasnsformers library is a must :)"
caliex,2024-03-03 15:54:53+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"Yeah, that's true. Luckily I was able to make it work and can do batch inferences now"
YaTharThShaRma999,2024-02-24 19:15:53+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,@hermanda hmm could you share your colab? Il see what’s wrong with it
hermanda,2024-02-25 18:46:19+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@YaTharThShaRma999 here it is https://colab.research.google.com/drive/1d3pF9MBOCFpQ_KqWnWdfuG1w_6CBWjrK?usp=sharing
Note that I am using revision=""refs/pr/2"" as the master can't be even downloaded properly."
YaTharThShaRma999,2024-02-26 12:02:21+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@hermanda oh it’s not working because transformers doesn’t support llava 1.6(it has a few different things from previous llava models).

Install the llava repository has that provides the code to run it.

 "
hermanda,2024-02-26 12:03:36+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@YaTharThShaRma999 thanks, do you think the support of llava 1.6 will be there in future? if so how many weeks do you think it is going to be realistically? "
YaTharThShaRma999,2024-02-26 13:01:18+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@hermanda there is already a pull request in transformers thats mostly done but its still a bit experimental.
install transformers from here instead of pip install transformers
https://github.com/huggingface/transformers/pull/29012

and here is the hf version of the model
https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf"
hermanda,2024-02-26 14:55:35+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,thank you
aliencaocao,2024-02-10 11:44:32+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,
fozziethebeat,2024-02-12 00:17:51+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"+1

Would it be possible to add the processor configs to the other llava 1.6 models after this is confirmed to be working?"
aliencaocao,2024-02-13 08:47:02+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"> +1
> 
> Would it be possible to add the processor configs to the other llava 1.6 models after this is confirmed to be working?

I have already confirmed this to be working as I came up with this after hours of trying and merging fixes from different sources"
theblackhacker,2024-02-21 10:45:57+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"I have some error when running it
`KeyError: 'model.image_newline'`

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6416b907f1e869089357e732/3V8RFD28S02W9YW0QrCF3.png)

**Fix: Worked with sglang==0.1.9**"
aliencaocao,2024-02-21 11:39:24+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,I got it working with sglang==0.1.11 and 0.1.12 too
hermanda,2024-02-24 12:56:27+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,Can we get it merged? I would like to use this model in Google Colab.
fozziethebeat,2024-02-25 23:56:01+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"Since this seems stalled, I made equivalent changes in a fork of this repo: https://huggingface.co/SurfaceData/llava-v1.6-mistral-7b-sglang"
olafgeibig,2024-03-21 22:54:41+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@fozziethebeat I tried to convert your model but it also failed;
```
Traceback (most recent call last):
  File ""/content/drive/MyDrive/AI/llama.cpp/convert.py"", line 1486, in <module>
    main()
  File ""/content/drive/MyDrive/AI/llama.cpp/convert.py"", line 1472, in main
    model   = convert_model_names(model, params, args.skip_unknown)
  File ""/content/drive/MyDrive/AI/llama.cpp/convert.py"", line 1217, in convert_model_names
    raise Exception(f""Unexpected tensor name: {name}. Use --skip-unknown to ignore it (e.g. LLaVA)"")
Exception: Unexpected tensor name: model.image_newline. Use --skip-unknown to ignore it (e.g. LLaVA)
```"
fozziethebeat,2024-03-22 00:32:27+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"What is this conversion script for? I only prepared the model to work for SGLang, so can't make any promises it'll work for other types of conversion."
olafgeibig,2024-03-22 12:28:06+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"it's the script coming with llama.cpp to convert a pytorch model to GGUF. I get exactly the same error, so I thought it's the same problem but probably it's not. https://github.com/ggerganov/llama.cpp/blob/master/convert.py"
aliencaocao,2024-03-22 12:39:44+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,This PR is not meant for llama cpp and it is only a coincidence if it work with llama cpp. This is strictly for SGLang only.
YaTharThShaRma999,2024-03-24 18:17:46+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"@olafgeibig llama.cpp has a detailed explanation on how to convert llava and llava1.6 models to gguf format.
https://github.com/ggerganov/llama.cpp/tree/master/examples/llava

Also no need to convert it since there are already pre converted llava 1.6 models. Just search llava 1.6 gguf in huggingface"
alsichan,2024-02-20 10:19:00+00:00,comment,liuhaotian/llava-v1.6-mistral-7b,"Seems like it just have been updated.

https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf"
Penn,2024-06-03 18:05:59+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Should there be tests for parity with the mistral client libraries? I think I noticed some differences in spacing -- can dig deeper, but getting that coverage regardless would provide peace of mind."
Rocketknight1,2024-06-05 13:56:49+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Hi @Penn, yes, sorry! The API is in flux because we're still working on the main transformers PR, and this PR will definitely be updated before being merged"
pandora-s,2024-05-15 14:53:35+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"That does not sound right for the full precision model, you would need 300Gb of VRAM to run it on full precision, of course, only on full precision. With 40gb however I think its impossible to run it even with quantization."
jmoneydw,2024-05-16 00:33:54+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"at least using vllm, it's taking 32gb per gpu for ""model loading"".  And then pytorch looks to be consuming the rest and you get a CUDA OOM.  I tried to set `--dtype half` for quantization and it didn't seem to make much difference memory wise so I assumed I configured it wrong so still reading."
jukofyork,2024-04-27 15:58:04+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Just noticed [Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1) has this set true too:

```
  ""add_bos_token"": true,
```

See: https://huggingface.co/mistralai/Mixtral-8x22B-v0.1/blob/main/tokenizer_config.json"
jukofyork,2024-04-28 01:10:41+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Doesn't help :(

It seems to be completely broken even with this change: with/without the imatrix and using `Q4_0` and `Q4_K_S`..."
DeathReaper0965,2024-05-03 08:31:49+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"@Derpyandflutterbatlover It entirely depends on the type of usecase you're dealing with. 
If you want more creativity in the outputs you should have high temperature values and vice versa for stable outputs. Moreover, Mistral models are trained for stability in the generated outputs so you can expect similar completions even when you change the values by a lot. 

Hence, I would suggest you to take a look at this really cool blog https://huggingface.co/blog/how-to-generate by Patrick von Platen, to decide on your required parameters."
clint-greene,2024-04-25 23:03:52+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Resolved issue. Seems with Torch 2.3 it doesnt work right. Downgraded back to 2.2 and works fine
Rocketknight1,2024-04-29 15:29:15+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Hi @bhuvan-nv, the problem here is that special tokens like `[AVAILABLE_TOOLS]` are not being picked up correctly by the tokenizer, and instead being broken up into subwords. This is strange because they are in fact in the tokenizer! We're investigating."
zhuexe,2024-05-01 18:14:28+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"This happens without tool usage as well:
```pycon
>>> mistral_query = ChatCompletionRequest(messages=[UserMessage(content=""Hello world"")])
>>> mistral_tokenizer.encode_chat_completion(mistral_query)
Tokenized(tokens=[1, 3, 23325, 2294, 4], text='<s>[INST]▁Hello▁world[/INST]')

>>> hf_tokenizer(""<s>[INST] Hello world[/INST]"")
{'input_ids': [1, 29560, 17057, 29561, 23325, 2294, 29560, 29516, 17057, 29561], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```"
jukofyork,2024-05-22 22:15:26+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Yeah, I noticed when I quantized this that is had 2-2.5x the perplexity of `wizard-lm-2:8x22b` and `eurux-nca:8x22b` (both off the same base model)."
Rocketknight1,2024-05-24 18:25:55+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Hi all, this was actually an issue with the chat template. The default chat template should be correct, but the `tool_use` template had issues. I fixed it as part of a general improvement to tool use capabilities in `transformers`. The PR is here: https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/33, but note that this is all quite experimental for now!"
zhuexe,2024-05-24 19:08:47+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"That seems odd to me - if it was an issue solely in the chat template, calling the tokenizer directly as in my earlier comment should still output the same tokens, right?"
Rocketknight1,2024-05-24 19:47:48+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"@zhuexe The problem is that special tokens are only identified correctly when separated by spaces. The spaces are not included in the final output, so ""[INST] Hello"" is tokenized as `[""[INST]"", ""Hello""]`.

Therefore, chat templates need to include this spacing to ensure special tokens are detected correctly. The main template for Mixtral-8x22B did this correctly, and I presume Mistral's own tokenizer handled things correctly, but the HF tool use template did not. I fixed it [here](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/33), in addition to adding support for our secret, not-yet-announced, upcoming tool use API!"
sophiamyang,2024-04-24 08:43:38+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"`mistral-common` allows system prompt. If you are using HF tokenizer, could you help make a PR? Or maybe @Jofthomas can help! Thanks! "
Jofthomas,2024-04-24 12:54:38+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Mistral models don't have special tokens for system messages, hence I should prepend it to the first user message right ? 
Is it supported the same way in mistral-common @sophiamyang ?"
msteele,2024-04-24 13:21:49+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> Mistral models don't have special tokens for system messages, hence I should prepend it to the first user message right ? 
> Is it supported the same way in mistral-common @sophiamyang ?

I have the same question. Is the instruct model trained with a system prompt?"
pandora-s,2024-04-24 15:03:40+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Hi there, if I remember correctly they concatenate the contents, and here is after a quick check on the code source from mistral-common:
```py
if is_first and system_prompt:
     content = system_prompt + ""\n\n"" + message.content
else:
     content = message.content
```

![image.png](https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/9x3rzhxmh2klg2ZLWQIhm.png)

As you could have guessed it, the system prompt is concatenated to the first user message from my understanding."
nadiaOcean,2024-04-25 18:15:43+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"my understanding is we can just adjust the chat template in tokenizer config, can we updates on this?"
gekcheng,2024-04-23 11:50:02+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,I also encounter this problem!
Praneethkeerthi,2024-04-23 12:41:40+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,How did you solve it?
clint-greene,2024-04-25 00:16:22+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Also encountered this problem. The example doesn't work
cyrilAub,2024-04-25 17:18:28+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,same problem for me...
cyrilAub,2024-04-25 21:13:00+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"it works with the  Hugging Face tokenizer (second example) , ensure transformers is at least version 4.39.0 or it will not work as expected"
harizo-rjn,2024-04-23 08:34:48+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Hi! You can retrieve the setup steps for SageMaker from the model card by clicking on Deploy > Amazon SageMaker (top right). 
The model should also be available from SageMaker Jumpstart soon.

"
jfriel1,2024-04-25 12:48:02+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Using the SDK I am getting this Sagemaker error:

```
torch.cuda.OutOfMemoryError: CUDA out of memory.
Tried to allocate 192.00 MiB. GPU 4 has a total capacty of 22.20 GiB of which 77.12 MiB is free. 
Process 42028 has 22.12 GiB memory in use. 
Of the allocated memory 20.85 GiB is allocated by PyTorch, and 111.76 MiB is reserved by PyTorch but unallocated.

If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  
See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

Is anyone else getting the same error? 

"
Jofthomas,2024-04-20 06:52:56+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Hey, which example are you trying to run ?

I updated the tokenizer_config and I'll update the readme

Can you try the following code ( only works with transformer>4.39.0):
```python
from transformers import AutoTokenizer

# Initialize model ID and tokenizer
model_id = ""mistralai/Mixtral-8x22B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Define a conversation example
conversation = [
    {""role"": ""user"", ""content"": ""What's the weather like in Paris?""},
    {""role"": ""assistant"", ""content"": ""The current temperature in Paris, France is 22 degrees Celsius.""},
    {""role"": ""user"", ""content"": ""What about San Francisco?""}
]

# Generate the tool use prompt using the tokenizer
tool_use_prompt = tokenizer.apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=True
)

# Print the generated prompt
print(tool_use_prompt)
```"
sophiamyang,2024-04-22 08:00:51+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Thanks so much @Jofthomas! Is it ready to merge? 
julien-c,2024-04-22 17:19:39+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,cc: @Rocketknight1 (previous mention from @Jofthomas did not parse correctly i think)
Jofthomas,2024-04-23 12:21:36+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"@sophiamyang I'd say it is ready, if someone makes a suggestion to improve it further I'll do it 🤗"
sophiamyang,2024-04-23 14:08:57+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Thanks! Merged! 
ibndias,2024-04-19 02:03:07+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"It is supported on TGI v2.0.1, I'm using it right now :)"
nrepesh,2024-04-27 21:10:57+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,What are the values that you used for: --max-input-length= --max-batch-prefill-tokens= --max-total-tokens=
aYwGytTTgpsZaVzyfsGqw,2024-04-18 16:27:20+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,
sophiamyang,2024-04-19 09:31:11+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,not sure if it's needed. But feel free to make a PR! 
daisuk30ba,2024-05-09 04:33:01+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"@sophiamyang 
Hi, thank you for providing the great model, Mixtral-8x22B-Instruct-v0.1.
In my project, I would like to access to the tokenizer with use_fast = False, which probably needs tokenizer.model.
Can you provide tokenizer.model?"
Jofthomas,2024-04-18 11:37:08+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"This PR aims to align the tokenizer_config to allow the latest changes in HF tokenizer to be propagated here.
This is rough first version of the template based on my understanding of the way your tokenizer works ( append available tools right before the last user message)

What do you think ?
cc: @Rocketknight1,  @pstock , @sophiamyang , @osanseviero 

Example use :

# 1) With tools :
```python
from transformers import AutoTokenizer
model_id = ""mistralai/Mixtral-8x22B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)
conversation=[
    {""role"": ""user"", ""content"": ""What's the weather like in Paris?""},
    {
        ""role"": ""tool_calls"",
        ""content"": [
            {
                ""name"": ""get_current_weather"",
                ""arguments"": {""location"": ""Paris, France"", ""format"": ""celsius""},
                ""id"": ""VvvODy9mT""
            }
        ]
    },
    {
        ""role"": ""tools_results"",
        ""content"": {""call_id"": ""VvvODy9mT"", ""content"": 22}
    },
    {""role"": ""assistant"", ""content"": ""The current temperature in Paris, France is 22 degrees Celsius.""},
    {""role"": ""user"", ""content"": ""What about San Francisco?""}
]


tools = [{""type"": ""function"", ""function"": {""name"":""get_current_weather"", ""description"": ""Get_the_current_weather"", ""parameters"": {""type"": ""object"", ""properties"": {""location"": {""type"": ""string"", ""description"": ""The city and state, e.g. San Francisco, CA""}, ""format"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""], ""description"": ""The temperature unit to use. Infer this from the users location.""}},""required"":[""location"",""format""]}}}]

# render the tool use prompt as a string:
tool_use_prompt = tokenizer.apply_chat_template(
            conversation,
            chat_template=""tool_use"",
            tools=tools,
            tokenize=False,
            add_generation_prompt=True,

)
print(tool_use_prompt)
```
Which renders : 
```
<s>[INST]What's the weather like in Paris?[/INST][TOOL_CALLS][{'name': 'get_current_weather', 'arguments': {'location': 'Paris, France', 'format': 'celsius'}, 'id': 'VvvODy9mT'}]</s>[/TOOL_CALLS] The current temperature in Paris, France is 22 degrees Celsius. </s>[AVAILABLE_TOOLS]{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get_the_current_weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}[/AVAILABLE_TOOLS][INST]What about San Francisco?[/INST]
```

# 2) Without tools:
```python
from transformers import AutoTokenizer

# Initialize model ID and tokenizer
model_id = ""mistralai/Mixtral-8x22B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Define a conversation example
conversation = [
    {""role"": ""user"", ""content"": ""What's the weather like in Paris?""},
    {""role"": ""assistant"", ""content"": ""The current temperature in Paris, France is 22 degrees Celsius.""},
    {""role"": ""user"", ""content"": ""What about San Francisco?""}
]

# Generate the tool use prompt using the tokenizer
tool_use_prompt = tokenizer.apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=True
)

# Print the generated prompt
print(tool_use_prompt)
```


Which renders : 
```
<s> [INST] What's the weather like in Paris? [/INST] The current temperature in Paris, France is 22 degrees Celsius. </s> [INST] What about San Francisco? [/INST]
```"
sophiamyang,2024-04-18 14:49:31+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Thanks so much for the PR! The format with tools is a little off. We don't have [/TOOL_CALLS]. 
Jofthomas,2024-04-18 16:01:09+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Thanks @sophiamyang , it seems I ""halucinated"" that token.

Should be better now."
sophiamyang,2024-04-19 09:11:38+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Thanks! merged. 
sophiamyang,2024-04-19 09:29:43+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,@Jofthomas I'm a little confused why it didn't seem to work after I merge. Does it need time to make changes when I run the code? 
Jofthomas,2024-04-19 09:52:01+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Doesn't this code work for you ? @sophiamyang 

It does for me ( i just tested it ) : 

# Tools use : 
```python
from transformers import AutoTokenizer
model_id = ""mistralai/Mixtral-8x22B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)
conversation=[
    {""role"": ""user"", ""content"": ""What's the weather like in Paris?""},
    {
        ""role"": ""tool_calls"",
        ""content"": [
            {
                ""name"": ""get_current_weather"",
                ""arguments"": {""location"": ""Paris, France"", ""format"": ""celsius""},
                ""id"": ""VvvODy9mT""
            }
        ]
    },
    {
        ""role"": ""tool_results"",
        ""content"": {""call_id"": ""VvvODy9mT"", ""content"": 22}
    },
    {""role"": ""assistant"", ""content"": ""The current temperature in Paris, France is 22 degrees Celsius.""},
    {""role"": ""user"", ""content"": ""What about San Francisco?""}
]


tools = [{""type"": ""function"", ""function"": {""name"":""get_current_weather"", ""description"": ""Get▁the▁current▁weather"", ""parameters"": {""type"": ""object"", ""properties"": {""location"": {""type"": ""string"", ""description"": ""The city and state, e.g. San Francisco, CA""}, ""format"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""], ""description"": ""The temperature unit to use. Infer this from the users location.""}},""required"":[""location"",""format""]}}}]

# render the tool use prompt as a string:
tool_use_prompt = tokenizer.apply_chat_template(
            conversation,
            chat_template=""tool_use"",
            tools=tools,
            tokenize=False,
            add_generation_prompt=True,

)
print(tool_use_prompt)
```
# No tools : 
```python

from transformers import AutoTokenizer

# Initialize model ID and tokenizer
model_id = ""mistralai/Mixtral-8x22B-Instruct-v0.1""
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Define a conversation example
conversation = [
    {""role"": ""user"", ""content"": ""What's the weather like in Paris?""},
    {""role"": ""assistant"", ""content"": ""The current temperature in Paris, France is 22 degrees Celsius.""},
    {""role"": ""user"", ""content"": ""What about San Francisco?""}
]

# Generate the tool use prompt using the tokenizer
tool_use_prompt = tokenizer.apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=True
)

# Print the generated prompt
print(tool_use_prompt)
```"
sophiamyang,2024-04-19 09:56:56+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Copied and pasted your code. Just ran it again. I'm not sure why it doesn't work for me @Jofthomas 

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6320c05a145cfa4c04cb4359/Bgtsb3IQ9yhOZfQ_6vYnQ.png)
"
Jofthomas,2024-04-19 10:06:13+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"You should upgrade transformers as it is part of the latest change :) 

```python
pip install transformers==4.40.0
```

This works for version >= 4.39.0"
sophiamyang,2024-04-19 10:22:31+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Thanks @Jofthomas! That works great. One more thing: it should be a list after [AVAILABLE_TOOLS] 
Jofthomas,2024-04-19 11:24:56+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"ohh I see, let me update the template in a seperate PR, and I'll also update the Readme so that users can have an example of how to use it and do not fall into the same version problem. 

Does it sounds good ? "
sophiamyang,2024-04-19 12:23:04+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,great thanks!
strangerv,2024-04-20 17:55:53+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,have folks had success getting back a valid function calling response from the model with this template?
Thireus,2024-04-18 09:38:14+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Just realized it's also documented here:

https://huggingface.co/docs/transformers/main/en/chat_templating"
pandora-s,2024-04-17 22:40:05+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"By ignoring, I mean it answers without taking any consideration of the word being used."
pandora-s,2024-04-17 22:49:54+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"I've been experimenting and here is what I saw:
 - With the Instruct, 90% of english words on lowercase will end up with nonsense responses. (words like: sad, jump, high, scream... while with upper it works, Scream for example)
 - With the base, no MATTER what word I choose from the english dict, it will always answer/complete ""Error""+something, other languages it works fine."
pandora-s,2024-04-17 22:53:34+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Feedback from someone, hands up to Saran Gangster on discord: https://pastebin.com/a5ENQU8b "
pandora-s,2024-04-17 23:11:27+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"100 words, 100 responses (instruct api):
https://pastebin.com/xxtFJ2WA 

Forget the ""uppercase"" ""lowercase"" theory, seems to happen to work on both, but might be related to token quantity and not exactly words as one might have guessed."
pandora-s,2024-04-17 23:12:39+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> I've been experimenting and here is what I saw:
>  - With the Instruct, 90% of english words on lowercase will end up with nonsense responses. (words like: sad, jump, high, scream... while with upper it works, Scream for example)
>  - With the base, no MATTER what word I choose from the english dict, it will always answer/complete ""Error""+something, other languages it works fine.

Im using the base on together.ai, so if someone can run it locally and confirm my experiments would be nice."
Thireus,2024-04-18 09:20:25+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"I've observed the same behavior, however I'm using a quantized version of the model and wasn't sure if it had anything to do with this.

Thank you for posting your observations here."
Thireus,2024-04-18 21:53:05+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Very surprised nobody else is talking about this issue... Maybe it's just us?

Edit: Yep, could be just an issue with exl2 quantized models. I don't have this issue with GGUF"
pandora-s,2024-04-19 06:40:35+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Not sure about it being only us, cause me and another ones from discord used Mistral's API, TogetherAI and other platforms, and this issue seems to be an *actual* issue from what I understand. Or its a tokenizer or prompt template issue but... kinda doupt it, would have to test the base model local at some point."
sophiamyang,2024-04-19 09:38:20+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,I'm not sure if I follow. Are you saying it's ignoring the first word when it's lower case? Is the instruction only one single word? Could you send an example prompt that we can try reproduce? 
pandora-s,2024-04-19 09:56:51+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Well, its actually even more simple than that (I was previously wrong), basically I'm having problems with just one single word instructions as provided here:

> 100 words, 100 responses (instruct api):
> https://pastebin.com/xxtFJ2WA 

These ones were made using Mistral API"
pandora-s,2024-04-19 09:58:50+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"And I am not sure if its together.ai or the base model but, when I play around with the base with single word completions I also saw a few issues."
sophiamyang,2024-04-19 10:12:32+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Have you tried Le Chat? Did you see the same behavior with Le Chat? 
pandora-s,2024-04-19 10:18:14+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,I only noticed this with Mixtral 8x22
pandora-s,2024-04-19 11:41:11+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"A nice example of this issue with the Instruct are examples like these ones:
```
User: cry
Mixtral8x22: Hello! How can I help you today? Is there a particular topic or question you have in mind related to the word ""hello""? If you're looking for more information about the origin of the word ""hello,"" it actually has an interesting history. The word ""hello"" was first used in writing in 1826, but it didn't become a common greeting until the invention of the telephone in the late 19th century. The use of ""hello"" as a telephone greeting was popularized by Thomas Edison, who recommended it as a more formal alternative to ""hullo,"" which was commonly used in England at the time.

If you have any other questions or topics you'd like to discuss, please let me know! I'm here to help with any information or resources you might need.
```

this is something that never happens with Mixtral8x7 and most other models, so I've a feeling this is not something normal"
sophiamyang,2024-04-19 12:23:33+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,yeah seems like an issue. we are looking into it! 
jukofyork,2024-04-21 22:08:14+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Yeah, I noticed a few strange things about this model like it stopping mid sentence. It could be the `Q4_K_M`quant but the strange thing is the `Q4_K_M`quant of `wizard-lm-2`is rock solid and has no problems? "
jukofyork,2024-04-23 23:30:08+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> Yeah, I noticed a few strange things about this model like it stopping mid sentence. It could be the `Q4_K_M`quant but the strange thing is the `Q4_K_M`quant of `wizard-lm-2`is rock solid and has no problems?

Some of my problems were due to missing the space between `<s>` and `[INST]`:

```
<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]
```

~~Now I've added that it seems to be working fine.~~ Working better, but still cutting off mid-sentence sometimes..."
clint-greene,2024-04-25 00:26:02+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,This model doesn't work right at all. Just produces garbage even when using their examples on the main model page.
pandora-s,2024-04-25 05:48:20+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> This model doesn't work right at all. Just produces garbage even when using their examples on the main model page.

I dont think this is the same issue, what exactly is the problem? You can test the model online on demo websites and such, and works nicely, this is a pretty specific issue. Maybe open a new conversation/issue and develop the problem, and someone might be able to help 🔥."
headmyshoulder,2024-05-06 13:35:42+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Might be related to https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/27
sophiamyang,2024-04-18 14:46:22+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Thanks! Already fixed with the previous PR. 
Asaf-Yehudai,2024-04-17 20:08:47+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Nice! 
It will be interesting to see more benchmark results here.
I guess Mixtral-8x22B-Instruct-v0.1 is better in multilingualty than WizardLM-2 8x22B.
Maybe merging them can work even better :)"
YaTharThShaRma999,2024-04-17 18:34:07+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,@sszymczyk are you using the correct prompt template? That might have a very noticeable effect
sszymczyk,2024-04-17 18:46:10+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> @sszymczyk are you using the correct prompt template? That might have a very noticeable effect

I think so, the usual:  &lt;s&gt;[INST] ...instruction goes here... [/INST]. Actually it looks that I missed a space, but it never mattered with other models."
LordTwave,2024-05-13 01:46:33+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Noteable: What you described usually is noise-caused. There are settings to bias an LLM away from outputting exact words.
josejg,2024-04-17 18:11:13+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Yes, this seems to be a typo and why the docs use the mistral-common tokenizer instead of the HF tokenizer

https://github.com/mistralai/mistral-common/blob/fcf0316163433af072f3cb157664c867661cbda7/src/mistral_common/tokens/tokenizers/base.py#L16"
Spidernic,2024-04-17 19:40:15+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"I was trying to quantize it to 8-bit and I get this error:

""Writing Mixtral-8x22B-instruct-q8_0.gguf, format 7
Traceback (most recent call last):
  File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1548, in <module>
    main()
  File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1542, in main
    OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab,
  File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1207, in write_all
    check_vocab_size(params, vocab, pad_vocab=pad_vocab)
  File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1049, in check_vocab_size
    raise ValueError(msg)
ValueError: Vocab size mismatch (model has 32768, but Mixtral-8x22B-Instruct-v0.1/tokenizer.json has 32769)."""
sszymczyk,2024-04-17 19:52:44+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> I was trying to quantize it to 8-bit and I get this error:
> 
> ""Writing Mixtral-8x22B-instruct-q8_0.gguf, format 7
> Traceback (most recent call last):
>   File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1548, in <module>
>     main()
>   File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1542, in main
>     OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab,
>   File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1207, in write_all
>     check_vocab_size(params, vocab, pad_vocab=pad_vocab)
>   File ""/Users/spider/Desktop/llama.cpp/convert.py"", line 1049, in check_vocab_size
>     raise ValueError(msg)
> ValueError: Vocab size mismatch (model has 32768, but Mixtral-8x22B-Instruct-v0.1/tokenizer.json has 32769).""

I had this error too, edit tokenizer.json and correct ""[TOOL_RESULT]"" to ""[TOOL_RESULTS]"" in token 8 definition, then repeat the conversion and quantization steps. It worked for me after this fix."
Rocketknight1,2024-04-17 20:11:28+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Good catch - I've opened a PR [here](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/9) to fix it.
sszymczyk,2024-04-17 20:34:46+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> Good catch - I've opened a PR [here](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/9) to fix it.

It needs to be fixed on the model card as well (in the special tokens list) to avoid confusion."
sophiamyang,2024-04-19 09:18:15+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,Fixed. Thanks! Please let us know if there are other issues. 
bullerwins,2024-04-17 17:53:20+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"Textgen webui supports it, I haven't tried the original safetensors but it works with a exl2 and gguf quant I made myself. LLama.cpp works too"
deleted,2024-04-17 17:56:01+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"how big is your gguf? With it being so huge, i didnt even bother trying.   "
bullerwins,2024-04-17 17:57:27+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,You can use https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to check the different sizes for gguf quants
DrNicefellow,2024-04-17 17:58:55+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> Textgen webui supports it, I haven't tried the original safetensors but it works with a exl2 and gguf quant I made myself. LLama.cpp works too

> Textgen webui supports it, I haven't tried the original safetensors but it works with a exl2 and gguf quant I made myself. LLama.cpp works too

How does text generation web ui supports it? Which plugin uses mistral defined tool usage format?"
deleted,2024-04-17 18:00:35+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,I used 8x7 in oobas text gen 'as is' nothing special was needed.  
DrNicefellow,2024-04-17 18:01:37+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,"> I used 8x7 in oobas text gen 'as is' nothing special was needed.

How did it use tools?"
sophiamyang,2024-04-17 11:49:46+00:00,comment,mistralai/Mixtral-8x22B-Instruct-v0.1,
benjamin,2024-03-09 12:41:24+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,Adds flax model weights. It does not work in the current version (4.38.2) of transformers though. Needs this PR to be merged first: https://github.com/huggingface/transformers/pull/29557.
KatyTheCutie,2024-01-04 17:02:42+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,2048 tokens.
devasheeshG,2024-01-01 19:13:14+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,can you share the code you used for loading the model :)
danielhanchen,2024-01-02 02:33:37+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,"Interestingly I just noticed the stable release of transformers==4.36.2 works fine! It might be an issue only with 4.37.

Oh my code is literally the normal code for loading:
```
model = AutoModelForCausalLM.from_pretrained(
    ""TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"",
    torch_dtype = torch.bfloat16,
)
```"
manishiitg,2024-01-04 12:58:13+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,getting the same issue 
danielhanchen,2024-01-05 02:56:21+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,@manishiitg I reuploaded it at https://huggingface.co/unsloth/tinyllama to work for the transformers dev branch!
zhuocheng,2024-01-23 08:01:45+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,"This is caused by [safe loading](https://github.com/huggingface/transformers/pull/28506) feature introduced by transformers==4.37, which add weights_only=True argument to the torch.load function. To fix this issue, you can simply load and re-save the model using the latest pytorch.
```python
import torch

model = torch.load('pytorch_model.bin')
torch.save(model, 'pytorch_model.bin')
```"
Maykeye,2024-05-25 11:25:41+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,"Can confirm the issue, can confirm that loading and saving fixes it. "
chaoscodes,2023-12-29 05:36:43+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,"Sure, thank you for pointing out"
xDAN2099,2023-12-30 08:51:57+00:00,comment,TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,Is it a LLM-Shearing version based on 7b ?
vsuswaram3,2024-05-25 06:53:45+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,"Model meta data

Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': ""{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}""}
Available chat formats from metadata: chat_template.default
Guessed chat format: mistral-instruct"
YaTharThShaRma999,2024-01-17 16:43:43+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,"@byunal hmm no it has a much higher context length then 512. Infact all llama, qwen, mistral models have higher context then 2048?

I think you are using something like ctransformers or llama cpp python which sets the context limit as 512, you have to change it to your desired length."
byunal,2024-01-18 11:41:30+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,"@YaTharThShaRma999  Actually yes. I'm trying to use this model for text summarization on CPU over ctransformers. Currently, I have no access to any GPU so I have to do inference on CPU. Frankly, I didn't know that ctansformers limits the context length. How can I neglect this limit on CPU ? I'd appreciated if you can help."
esuriddick,2024-02-07 11:42:59+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,"I have the same issue, where ctransformers is limiting it to 512. I thought it was something wrong on the model configuration.
Were you able to solve this issue?

EDIT: https://discuss.huggingface.co/t/number-of-tokens-2331-exceeded-maximum-context-length-512-error-even-when-model-supports-8k-context-length/57180/6
I just used the argument in the function as suggested in the link above."
byunal,2024-02-20 14:42:09+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,"@esuriddick Didn't solved and I'm done dealing with, but thanks ."
dent0s,2023-12-12 20:25:00+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,This comment has been hidden
dent0s,2023-12-12 20:25:29+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,"A similar error occurred. I will be waiting for help
"
mirek190,2023-12-13 14:00:10+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,download the newest llamacpp for compatibility  ... 
mirek190,2023-12-13 14:00:49+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,"lol
"
YaTharThShaRma999,2023-12-13 16:20:57+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GGUF,Wow thats actually very interesting that the model actually knows who created it.
TheBloke,2023-09-28 16:14:15+00:00,comment,TheBloke/Mistral-7B-v0.1-AWQ,"As discussed in the README, vLLM only supports Llama AWQ models at this time "
Gershwin69,2023-09-29 16:26:10+00:00,comment,TheBloke/Mistral-7B-v0.1-AWQ,"@TheBloke They don't however support AWQ Mistral yet apparently.

```
ValueError: Quantization is not supported for <class 'vllm.model_executor.models.mistral.MistralForCausalLM'>.
```"
TheBloke,2023-09-29 16:27:19+00:00,comment,TheBloke/Mistral-7B-v0.1-AWQ,"They should do - version 0.2 just pushed a few hours ago with Mistral support listed. I updated my README a minute ago to say it now worked

Are you running 0.2?"
Gershwin69,2023-09-29 16:50:13+00:00,comment,TheBloke/Mistral-7B-v0.1-AWQ,"I am running version `0.2.0`, well more accurately I'm running from source `main` which I can see is tagged at `0.2.0`.

```bash
Successfully built vllm
Installing collected packages: vllm
Successfully installed vllm-0.2.0
root@21cb0f50ccdf:/vllm# cd ..
root@21cb0f50ccdf:/# python -m vllm.entrypoints.api_server --model TheBloke/Mistral-7B-v0.1-AWQ --quantization awq --dtype float16
WARNING 09-29 16:49:26 config.py:341] Casting torch.bfloat16 to torch.float16.
INFO 09-29 16:49:26 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/Mistral-7B-v0.1-AWQ', tokenizer='TheBloke/Mistral-7B-v0.1-AWQ', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.8/dist-packages/vllm/entrypoints/api_server.py"", line 74, in <module>
    engine = AsyncLLMEngine.from_engine_args(engine_args)
  File ""/usr/local/lib/python3.8/dist-packages/vllm/engine/async_llm_engine.py"", line 486, in from_engine_args
    engine = cls(engine_args.worker_use_ray,
  File ""/usr/local/lib/python3.8/dist-packages/vllm/engine/async_llm_engine.py"", line 270, in __init__
    self.engine = self._init_engine(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/vllm/engine/async_llm_engine.py"", line 306, in _init_engine
    return engine_class(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/vllm/engine/llm_engine.py"", line 108, in __init__
    self._init_workers(distributed_init_method)
  File ""/usr/local/lib/python3.8/dist-packages/vllm/engine/llm_engine.py"", line 140, in _init_workers
    self._run_workers(
  File ""/usr/local/lib/python3.8/dist-packages/vllm/engine/llm_engine.py"", line 692, in _run_workers
    output = executor(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/vllm/worker/worker.py"", line 68, in init_model
    self.model = get_model(self.model_config)
  File ""/usr/local/lib/python3.8/dist-packages/vllm/model_executor/model_loader.py"", line 67, in get_model
    raise ValueError(
ValueError: Quantization is not supported for <class 'vllm.model_executor.models.mistral.MistralForCausalLM'>.
```"
TheBloke,2023-09-29 16:51:35+00:00,comment,TheBloke/Mistral-7B-v0.1-AWQ,"Oh, damn. I guess they just added unquantised support.

I'll remove mention of it from the README again!"
DaryoushV,2024-04-11 11:06:48+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"Hi Robert,

1) The model was fine-tuned on the 0.1 version. Is there really a 0.2v of the Mixtral? I don't think so

2) You find GGUF Versions for most of our models in our organization card (Thanks to the bloke, who provided these quantizations)

best regards
Daryoush"
Dabirius,2024-01-26 14:44:36+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"Hello Julian! Thank you for the kind words! We are working on integrating function calling in our models. We are beginning with our smaller models though. But also for a newer version of this model, we plan to integrate function calling."
JulianGerhard,2024-01-26 14:54:53+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,Hey @Dabirius - thanks for the fast response. Good to hear - then some patience is necessary ;-) 
DavidGF,2024-01-11 19:58:04+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"Hey martinkozle thanks for your extensive testing! 
Your chosen temperature is quite high.
Please try out a temperature between 0.7 and 0.3 and top_k 20-40.
"
martinkozle,2024-01-11 23:49:25+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"Okay, so I just tried it both with temperature 0.5 and top_k 30, and temperature 0.3 and top_k 20, it still has the same behavior where it generates more than requested. It really doesn't like using the stop token it seems 😄."
Dabirius,2024-01-12 13:24:56+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"Hello @martinkozle! I just looked into that. I made it work for all but one prompt (number 5, here I had to change the order. Instruction first, then the sentence. Then it worked also for that one). I used a temperature of 0.1, top_p=1, top_k=20. Also I used the Mistral Instruction template. Can you check again with these settings?"
Dabirius,2024-01-12 19:11:39+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"@martinkozle I just realized you did not use the end of sentence token. In our model card it is also missing, this is why your prompts did not work. The EOS token is in the model's tokenizer file, but it is missing on the model card. We will add it now. Sorry for the confusion..."
martinkozle,2024-01-13 20:40:54+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"Since I performed these tests with just a single instruction and the EOS token according to the template goes after the assistant responses then it wouldn't have an effect on these examples as I wasn't testing with longer conversations.
I did retry a couple of the examples with the BOS token at the beginning though. But it didn't  change the behavior.
Still, good to know that I was missing those tokens in my ChatUI deployment 🙂.

I also tested the last parameters that you mentioned while also swapping the input and instruction. But I still got the same type of outputs with `(Note...)` and explanations.

In the meanwhile I also tested some other prompting techniques to see if I can increases the chance of better outputs.
One slight improvement that I found was that adding these types of sentences to the instruction prompt slightly increased the chance of the output being as desired

- You would be penalized for generating anything apart from the requested date.
- ... apart from the German translation.
- ... apart from the requested information.

Combining it with few-shot prompting I got it to be a more consistent for this simple example:

## Okay example

```
 <s>[INST] Output the year that Anne was born on in the format DD/MM/YYYY. The output shouldn't contain anything else except for the date. You would be penalized if you don't follow these instructions and output JUST the date. Respond with ""OK"" if you understand your task. [/INST]OK</s>[INST] Anne was born on 5 June 2022 [/INST]05/06/2022</s>[INST] Anne was born on 27 March 1940 [/INST]27/03/1940</s>[INST] Anne was born on 24 October 1971 [/INST]
```

Output:

```
24/10/1971
```

I ran it multiple times with different dates and it worked all the time.

## Bad example 1

But it still isn't full proof. Even with this type of prompt if I give it a final message message that is different than the examples then it doesn't listen:

```
 <s>[INST] Output the year that Anne was born on in the format DD/MM/YYYY. The output shouldn't contain anything else except for the date. You would be penalized if you don't follow these instructions and output JUST the date. Respond with ""OK"" if you understand your task. [/INST]OK</s>[INST] Anne was born on 5 June 2022 [/INST]05/06/2022</s>[INST] Anne was born on 27 March 1940 [/INST]27/03/1940</s>[INST] Anne wurde am 12. Dezember 1970 in Deutschland geboren. [/INST]
```

Output:

```
12/12/1970\n(I apologize for any inconvenience, I assumed the date format based on the previous questions. The correct birthdate is 12/12/1970
```

## Bad example 2

```
 <s>[INST] Output the year that Anne was born on in the format DD/MM/YYYY. The output shouldn't contain anything else except for the date. You would be penalized if you don't follow these instructions and output JUST the date. Respond with ""OK"" if you understand your task. [/INST]OK</s>[INST] Anne was born on 5 June 2022 [/INST]05/06/2022</s>[INST] Anne was born on 27 March 1940 [/INST]27/03/1940</s>[INST] Bob told Alice that Anne is born on 27 April 1980 [/INST]
```

Output:

```
 I only provide the birthdate of Anne without any additional information or context, so the answer is:\n\n27/04/1980
```

## TGI Parameters

```
parameters=GenerateParameters { best_of: None, temperature: Some(0.1), repetition_penalty: Some(1.2), top_k: Some(20), top_p: Some(0.99), typical_p: Some(0.95), do_sample: false, max_new_tokens: Some(512), return_full_text: Some(false), stop: [], truncate: None, watermark: false, details: true, decoder_input_details: false, seed: None, top_n_tokens: None }}
```

## Final thoughts

This model seems very tricky to tame for me. It just has a very large tendency to use reasoning. Which would be good for certain tasks but bad for something like this.

But since you said that you made it work then I don't know what I am doing wrong.

Maybe it is the AWQ quantization? It would be useful to know what quantization, if any, you are using? Also are you hosting it with 8K context size?"
DaryoushV,2024-01-13 23:55:26+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,We made the experience that the AWQ Version indeed falls behind the gptq version and shows some odd behavior. We always test with the gptq version
martinkozle,2024-01-15 18:31:00+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"OK, so I hosted the GPTQ quantization of the model today (`gptq-4bit-32g-actorder_True`), and I tested it on the prompts that we discussed here as a comparison.

It performed significantly better, only hallucinating consistently on the prompts for extracting only the year. For other ones it was a lot better.

So I can conclude that the poor performance was due to the AWQ quantization.
I am curious if this is specific to this model or if this is expected of all AWQ quantized language models.

In any case I will close this discussion with this conclusion. Thank you for helping me out, giving me suggestions and trying out my prompts!"
dillfrescott,2023-12-27 02:25:50+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,Please benchmark this model
dillfrescott,2023-12-28 04:14:02+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,I had it benchmarked and it scores pretty high on the list!!
DavidGF,2023-12-28 04:30:47+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,"Hey dillfrescott, 
thank you very much for your great feedback. we are delighted!
We have already evaluated the model. 
The results can also be found on the Open llm leaderboard!"
dillfrescott,2023-12-28 04:32:28+00:00,comment,VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct,Oooo nice!
Maykeye,2023-08-10 04:14:29+00:00,comment,Maykeye/TinyLLama-v0,You are welcome! I'm pleasently surprised my model found users tbh
YaTharThShaRma999,2024-04-11 14:25:28+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"@ibalampanis no, llama.cpp supports 1bit, 2bit, 3bit,4bit,5bit,6bit,8bit,16bit and 32 bits.

Most use 4 bit since quality doesn’t degrade noticeably and it’s great speed. Lower then that, quality can start to actually degrade, and 1 bit is trash.

Q6 and q5 are  slightly better then 4 bit and the highest you should go. 8 bit is way too slow it’s same quality as q6."
deleted,2024-04-11 14:38:30+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Personally i stick with Q6, however i dont see a huge speed difference with 8, as long as it still fits in my GPU"
ibalampanis,2024-04-14 17:17:24+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Why can I find only q8_0, f16 and f32 as argument options in llama.cpp? Thank you for your response!

-- UPDATE
I found it. Thank you a lot!"
jlzhou,2024-02-07 06:42:41+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Could you please also include the full input in your screenshot?  While the `TEMPLATE` in your code appears correct, the truncated portion in the screenshot seems a bit unclear to me."
UmangK,2024-01-24 06:43:21+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"I am building streamlit app for language translation using this code but it is taking around 1 min to translate one sentence.

import streamlit as st
from langchain.chains import LLMChain
from langchain.llms import CTransformers
from langchain.prompts import PromptTemplate

Mistral configuration
config = {'max_new_tokens': 256, 'temperature': 0.8, 'context_length': 256}
llm = CTransformers(model=""TheBloke/Mistral-7B-Instruct-v0.1-GGUF"",
model_file=""mistral-7b-instruct-v0.1.Q4_K_M.gguf"",
config=config)

Streamlit app
st.title(""Language Translation with Mistral"")

Sidebar for selecting languages
language_options = [""English"", ""French"", ""Spanish"", ""German"", ""Chinese"", ""Japanese"", ""Russian"", ""Arabic"", ""Hindi"", ""Swahili""]
original_language = st.sidebar.selectbox(""Select Original Language"", language_options)
target_language = st.sidebar.selectbox(""Select Target Language"", language_options)

Input text box
input_text = st.text_area(""Enter Text to Translate"", """")

if st.button(""Translate""):
# Translate using Mistral
chunk_size = 100 # Adjust the chunk size as needed
chunks = [input_text[i:i + chunk_size] for i in range(0, len(input_text), chunk_size)]

translated_chunks = []
for chunk in chunks:
    map_template = f""<s>[INST] Translate the following text from {original_language} to {target_language}: {chunk} [/INST] </s>""
    map_prompt = PromptTemplate.from_template(map_template)
    translate_chain = LLMChain(llm=llm, prompt=map_prompt)
    translated_chunk = translate_chain.run({})
    translated_chunks.append(translated_chunk)

translated_text = """".join(translated_chunks)

# Display translated text
st.subheader(""Translated Text:"")
st.write(translated_text)"
ShivanshMathur007,2023-12-26 11:38:25+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Can you share how did you ran it in VS CODE?
"
ArunRaj000,2023-12-28 08:50:38+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"downloaded the gguf files, and used ctrasnformer to run these code, which is really computational effective, but it takes more time print the output
 instead of real time printing "
ShivanshMathur007,2023-12-28 20:26:08+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Even I tried ctransfromers but it is giving segmentation fault, afterwards I tried with llama-cpp-python it worked. Can you share how you did.
from ctransformers import AutoModelForCausalLM
import gradio as gr


llm = AutoModelForCausalLM.from_pretrained(
        ""TheBloke/Mistral-7B-Instruct-v0.2-GGUF"",
        model_file=""mistral-7b-instruct-v0.2.Q4_K_M.gguf"",
        model_type=""llama"",
        gpu_layers=0 )
    
title= ""Shivansh Model""

def llm_func(message,history):
    response=llm(message)
    return response

gr.ChatInterface(
fn=llm_func,
title=title,
).launch()
This gave segmentation fault.

On the other hand:->
from langchain.llms import LlamaCpp
import gradio as gr


def load_llm():
    llm = LlamaCpp(
        model_path=""../model/mistral-7b-instruct-v0.2.Q4_K_M.gguf"",
        max_new_tokens=512,
        temperature=0.1
    )
    return llm
title= ""Shivansh Model""

def llm_func(message,history):
    llm=load_llm()
    response=llm(message)
    return response

gr.ChatInterface(
fn=llm_func,
title=title,
).launch()
This is working good.

Could you share your code.
"
ArunRaj000,2023-12-29 19:24:12+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"from langchain.llms import CTransformers
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import time
import json


# Load model configuration from the specified path
config_path = ""D:/Project File/restart/NEW AI/config.json""

with open(config_path, 'r') as config_file:
    model_config = json.load(config_file)

# Extract specific parameters
load_params = model_config.get('load_params', {})

# Use the extracted parameters in your main code
model_path = ""D:/Project File/restart/NEW AI/mistral-7b-instruct-v0.1.Q2_K.gguf""


# Initialize LangChain's CTransformers with StreamingStdOutCallbackHandler
llm = CTransformers(
    model=model_path,
    callbacks=[StreamingStdOutCallbackHandler()]
)

# Initialize conversation history
conversation_history = []

prompt_template = ""[INST] {prompt} [/INST]""

while True:
    prompt_template = {
        ""pre_prompt"": ""You are an artificial intelligence called VICTOR, Victor stands for Virtual Intelligent Companion for Technological Optimization and Reinforcement, created by Arun Raj, a Physics student.you are a friend of the user, you aim to keep our conversation very concise and engaging you, "",
        ""pre_prompt_suffix"": """",
        ""pre_prompt_prefix"": """",
        ""input_prefix"": ""[INST]"",
        ""input_suffix"": ""[/INST]"",
        ""antiprompt"": [""[INST]""],        
    }
    user_input = input(""You: "")

    if user_input.lower() == ""quit"":
        break

    formatted_input = f""{prompt_template['input_prefix']}{user_input}{prompt_template['input_suffix']}""
    
    print(""\nYou:"", user_input)
    
    response = llm(prompt_template['pre_prompt'] + formatted_input)
    
    if conversation_history and response != conversation_history[-1][1]:  # Check if conversation history is not empty
        print()

    conversation_history.append((""User"", user_input))
    conversation_history.append((""AI"", response))

print(""Chatbot session ended."")



in this code i am still facing memory issue, 

can you help me with memory"
ShivanshMathur007,2023-12-29 20:09:39+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Can you share your hardware details, so I can help you.
"
ArunRaj000,2023-12-29 20:28:27+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"ryzen 5 5600h, nvdia gtx 1650and amd radieon gpu,  24gb ram

i mean conversational memory, the model doesn't remember previous interaction, "
ShivanshMathur007,2024-01-01 20:35:22+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"you can try out langchain.memory for conversational/contextual memory.
"
ArunRaj000,2024-01-02 12:17:34+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"yeah i tried that now the context window is 8k but don't have long term memory, so planning to integrate a database. 

do you know any open source text to speech library for speaking in live stream output
"
ShivanshMathur007,2024-01-02 19:31:16+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"no I have not expored that area yet

"
ArunRaj000,2024-01-03 09:47:11+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"It's ok... Can you tell me how you used llama. Cpp? 
I have some errors while installing llama. Cpp... If you know..., then please text me in insta -Arun_luka"
ljhwild,2023-12-10 01:47:34+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,I don't think that's doable.
deleted,2023-12-10 02:17:03+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"gotta have a place its running, just having the file path isn't going to do it for you.
"
HemanthSai7,2023-12-13 13:03:07+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,Ohh got it.
TheBloke,2023-11-23 09:43:19+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"This is a GGUF model, it's not supported on SageMaker.

Please see the GPTQ or AWQ model instead - both are supported by Text Generation Inference, which should be supported on SageMaker."
philgrey,2023-11-23 09:46:22+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Thanks for your quick response.
I wanna ask one more.
Here, what is the meaning of HUGGING_FACE_HUB_TOKEN?
If it is token for my account, I think it has no meaning.
Can you explain about this?
Thanks very much

config = {
  'HF_MODEL_ID': ""TheBloke/Mistral-7B-Instruct-v0.1-GGUF"", # model_id from hf.co/models
  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica
  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input text
  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)
  'MAX_BATCH_TOTAL_TOKENS': json.dumps(4096),  # Limits the number of tokens that can be processed in parallel during the generation
  'HUGGING_FACE_HUB_TOKEN': """"
}

# create HuggingFaceModel with the image uri
llm_model = HuggingFaceModel(
  role=role,
  image_uri=llm_image,
  env=config
)"
dil-ddecarvalhogomes,2024-02-28 00:56:35+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"While I'm able to deploy  `mistralai/Mistral-7B-Instruct-v0.1` in SageMaker, this one fails with:

```
RuntimeError: weight model.layers.0.self_attn.q_proj.weight does not exist
```

I believe the root cause is explained here: https://github.com/huggingface/text-generation-inference/issues/500
"
overlordiam,2024-02-27 21:47:09+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"In the config dictionary, you can set your desired context length.
Example: 
config = {'max_new_tokens': 256, 'temperature': 0.8, 'context_length': 256}
llm = CTransformers(model=""TheBloke/Mistral-7B-Instruct-v0.1-GGUF"",
model_file=""mistral-7b-instruct-v0.1.Q4_K_M.gguf"",
config=config)"
Rishu9401,2024-02-29 07:25:38+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,Thank You. I will try this way
nlpsingh,2024-02-07 03:47:55+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,I am also experiencing the same issue. I opened an issue for it as well: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/discussions/21
YaTharThShaRma999,2024-02-07 15:17:49+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"@nlpsingh its probably because of your sampling parameters.

Like temperature, top p, min p, they basically can change output.

Higher usually means a bit more creative and better but it might change the response a bit too much sometimes.

Just lower the params it should be fine. With temp at 0 you will always get the same response but it might be a bit boring"
TheBloke,2023-11-16 08:03:45+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,Why? I create that file so that ctransformers can auto load the model in the right format
ramon1992,2023-11-16 13:30:57+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Sorry mate, this was unintentional."
TheBloke,2023-11-16 13:45:41+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,No worries
TheBloke,2023-11-09 00:08:01+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"I've quantized a million Mistral 7B models!  But yes if I've missed some, do let me know

I'll add KAI to the queue"
PlanetDOGE,2023-11-09 00:47:53+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,It's OK! Just  a suggestion
zBaptiste,2023-11-08 14:29:31+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Why are you using ""TheBloke/Yarn-Mistral-7B-128k-GGUF"" instead of ""TheBloke/Mistral-7B-Instruct-v0.1-GGUF"" ?"
Rishu9401,2023-11-23 06:01:08+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,No specific reason. Just exploring the models and the kind of result that they provide. Will using the instruct model solve the issue?
TeamResearch,2023-11-27 10:04:39+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,This comment has been hidden
zBaptiste,2023-11-27 11:49:30+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Okay, I haven't try this model, try to post you question there : https://huggingface.co/TheBloke/Yarn-Mistral-7B-128k-GGUF/discussions.

Unfortunately, you will have the same problem with the ""Instruct"" version of the model ..."
rautsanket4086,2023-11-01 09:59:35+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,Anyone know this
Glifaus,2023-11-14 13:01:43+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Use other model, like de Q4_K_M. This would improve a lot the speed"
infopz512,2023-10-18 15:17:15+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"You need to set the correct value via the config parameters
```
config = {'max_new_tokens': 400, 'temperature': 0, 'context_length': 4096}
llm = CTransformers(model='TheBloke/Mistral-7B-Instruct-v0.1-GGUF',model_file=""mistral-7b-instruct-v0.1.Q8_0.gguf"", config=config)
```"
esuriddick,2023-12-12 10:23:17+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Based on the above, I tried the following and I still get an error:
from ctransformers import AutoModelForCausalLM
model_chat_ckpt = ""TheBloke/Mistral-7B-Instruct-v0.1-GGUF""
model_chat_file = 'mistral-7b-instruct-v0.1.Q4_K_M.gguf'
model_chat_type = 'mistral'

config = {'context_length' : 4096}
 model = AutoModelForCausalLM.from_pretrained(model_path_or_repo_id = model_chat_path
                                                 ,model_type = model_chat_type
                                                 ,model_file = model_chat_file
                                                 ,local_files_only = True
                                                 ,config = config.config
                                                 )   

AttributeError: 'dict' object has no attribute 'config'"
YaTharThShaRma999,2023-10-12 12:49:34+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Hmm not sure if llama cpp has sliding windows attention yet.

If it doesn’t, I think it should be 8k but if it does it should be 32k"
esuriddick,2023-10-24 08:35:47+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"I'm getting a different error while running ctransformers in my laptop: ""Model type 'mistral' is not supported.""

I've used falcon 7b and wizardlm 7b in the past with the exact same setup. Not sure whether ctransformers is actually compatible with this model or not :s"
MLconArtist,2023-10-26 09:20:49+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"I have a similar issue.

    RuntimeError: Failed to create LLM 'mistral' from 'Models\mistral-7b-v0.1.Q4_K_M.gguf'"
esuriddick,2023-12-12 10:26:50+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"After updating CTransformers, the issue was solved on my side (apologies for delay on reply)."
YaTharThShaRma999,2023-10-03 12:01:27+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"You have to manually set it up. It’s normally set up for all models ans 512. Also, it should support around 8k context lenght(slightly lower)."
Harm,2023-10-03 12:03:50+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"> You have to manually set it up. It’s normally set up for all models ans 512. Also, it should support around 8k context lenght(slightly lower).

Ok, do you have any suggestions or pointers how to do so?"
asfandsaleem,2023-10-03 12:10:59+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,This comment has been hidden
marekk,2023-10-03 12:15:05+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"You can use 
```
pip install llama-cpp-python
wget https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q5_K_M.gguf
```
And after this for example:
```
from llama_cpp import Llama
llm = Llama(model_path=""wizardlm-13b-v1.2.Q5_K_M.gguf"", n_ctx=4096, n_gpu_layers=-1)
print(llm(prompt, max_tokens=1024, temperature=0))
```
Just change name of model and path."
Harm,2023-10-03 12:42:32+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Everyone thanks for the suggestions. Was just pointed to the context_length parameter from ctransformers. Context length is upgraded to 4096 by:
from ctransformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    ""TheBloke/Mistral-7B-Instruct-v0.1-GGUF"",
    model_file=""mistral-7b-instruct-v0.1.Q4_K_M.gguf"",
    model_type=""mistral"",
    gpu_layers=50,
    hf=True,
    context_length=4096)
"
marekk,2023-10-03 15:16:27+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Hi, thank you for your work. I tried embedding endpoint and I got an error.
Query:
```
curl -X 'POST' \
  'https://limcheekin-mistral-7b-instruct-v0-1-gguf.hf.space/v1/embeddings' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  ""input"": ""The food was delicious and the waiter...""
}'
```
Error reponse:
```
{
  ""error"": {
    ""message"": ""Llama model must be created with embedding=True to call this method"",
    ""type"": ""internal_server_error"",
    ""param"": null,
    ""code"": null
  }
}
```"
limcheekin,2023-10-03 23:10:02+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"> Hi, thank you for your work. I tried embedding endpoint and I got an error.
> Query:
> ```
> curl -X 'POST' \
>   'https://limcheekin-mistral-7b-instruct-v0-1-gguf.hf.space/v1/embeddings' \
>   -H 'accept: application/json' \
>   -H 'Content-Type: application/json' \
>   -d '{
>   ""input"": ""The food was delicious and the waiter...""
> }'
> ```
> Error reponse:
> ```
> {
>   ""error"": {
>     ""message"": ""Llama model must be created with embedding=True to call this method"",
>     ""type"": ""internal_server_error"",
>     ""param"": null,
>     ""code"": null
>   }
> }
> ```

Yeah, I should stated clearly in the doc the `embeddings` endpoint has been disabled on purpose as I tested that the `embeddings` created by Llama models is NOT better than other open-source text embeddings models such as BAAI/bge-large-en, intfloat/e5-large-v2, sentence-transformers/all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2, etc. Hence, I created the Python package at https://github.com/limcheekin/open-text-embeddings.

Anyway, that's just my experience of few months ago and my current understanding, I just enabled (turn on) the `embeddings` endpoint and go ahead and test it out yourself and appreciate you share the result here.

Thank you."
Tanvir1337,2023-09-28 02:38:56+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,Indeed!
sethuiyer,2023-09-28 04:59:20+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,This comment has been hidden
ianuvrat,2023-09-29 05:59:54+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"How did you ran, can you please share the code?
"
BingoBird,2023-09-29 10:04:48+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,":What is a good invocation and paramaters for mistral 7b?

I'm testing with --temp 0.6 --mirostat 2 --mirostat-ent 6 --mirostat-lr 0.2 -n 2048 -c 2048 -n -1 --repeat-last-n 1600 --repeat-penalty 1.2 "
TK-Master,2023-09-29 16:23:07+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"This has to be indeed the best 7b model I have tried.. for those who can't get it to run in text-generation-ui (I sure couldn't, it's broken af) here's some code and detailed instructions for a simple llama-cpp-python chatbot using this model.

First, I recommend a clean python installation with pip etc,  you can use a virtual environment for this (I'm using miniconda with python version 3.10).
Then I installed llama-cpp-python with cuda support using the following commands (in windows cmd).

<code>set CMAKE_ARGS=""-DLLAMA_CUBLAS=on"" && pip install llama-cpp-python
set CMAKE_ARGS=""-DLLAMA_CUBLAS=on"" && set FORCE_CMAKE=1 && set CUDAFLAGS=""-arch=all -lcublas""
python -m pip install https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/basic/llama_cpp_python-0.2.7+cu118-cp310-cp310-win_amd64.whl --no-cache-dir</code>

(Note: this works for me using cuda 11.8, no avx. For other versions you might wanna replace the link with another from https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels)

And that's it.. now you can run the following python script to ask the model questions.

<code>python simpleStreamChat.py</code>

```
import json
import argparse
from llama_cpp import Llama

parser = argparse.ArgumentParser()
parser.add_argument(""-m"", ""--model"", type=str, default=""../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf"")
parser.add_argument(""-pt"", ""--prompt"", type=str, default=""<s>[INST]{prompt}[/INST]"")
args = parser.parse_args()

prompt_template = args.prompt

print(""Loading model "" + args.model)
llm = Llama(model_path=args.model, n_gpu_layers=35, n_ctx=4096, temp=0.7, repeat_penalty=1.1, verbose=False)

stream = """"#llm(""Question: What are the names of the planets in the solar system? Answer: "", max_tokens=48,stop=[""Q:"", ""\n""],stream=True)

# Function - Print response output in chunks (stream)
def printresponse(response):
    completion_text = ''
    # iterate through the stream of events and print it
    print(f""Bot:"", end="""", flush=True)
    for event in response:
        event_text = event['choices'][0]['text']
        completion_text += event_text
        print(f""{event_text}"", end="""", flush=True)

    print("""",flush=True)
    # remember context
    #context.append({""role"": ""assistant"", ""content"" : completion_text})
    return completion_text

#printresponse(stream)

while True:
    try:
        u_input = input(""-> "")
		
        prompt = prompt_template.format(prompt=u_input)
        stream = llm(prompt, max_tokens=512, stream=True)
        response = printresponse(stream)
        print()

    except KeyboardInterrupt:
        print(""\n..(Response interrupted)."")#continue
    print()
```

Note: set verbose=True to see token generation times etc. n_gpu_layers=how many layers on gpu, n_ctx=context size

Uncomment ```stream = """"#llm(""Question: What are the names of the planets in the solar system? Answer: "", max_tokens=48,stop=[""Q:"", ""\n""],stream=True)```
and ```#printresponse(stream)``` if you want.

You're welcome!"
deleted,2023-09-29 16:45:09+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,odd. working fine for me and i have not updated anything in a couple of weeks. ( ooba )
aghatage,2023-09-29 23:18:50+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,Anyone able to use it with constricting grammar in llama.cpp ?
atstim731,2023-10-01 05:32:03+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Hands down the best 7b model, holy cow. 
For starters, I have a custom character, but the settings I'm using in tgwui are:
Instruction Template: Mistral (no modifications)
Generation Preset: Divine Intellect
Model Loader: LlamaCpp
The model is smart, retains context after several turns,  great inference, picks up on nuance. 
Mistral just hit's different than Llama, no judgement on Meta. 
If you've watched Frazier, Mistral is like a very smart Roz, and Llama is Maris.  
"
edumoulin,2023-10-02 16:38:56+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"I was able to run this model in Q5_K_M.gguf through oobabooga on a M2 MacBookPro with 16GB : it runs very smoothly with 1 layer in GPU units, quite faster than 7B Llama2 or Vigogne with same quantization. However, it seems sometimes to struggle with long conversations (the answers get lesser accurate and you need to reload the model).
Tested on bash and SQL code, the results where relevant in most cases."
Akalilol,2023-10-03 21:09:08+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,Is it uncensored ?
ianuvrat,2023-10-04 03:54:19+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"@edumoulin , you tested in ql code as in? I want to understand how can we use this model to query database/csv or panda datafame. I tried with langchain but no luck. Would you be so kind to point out the tools/code how to achieve this?  I feel that it would be useful to a lot of people. Thank you very much in advance."
edumoulin,2023-10-04 07:13:45+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"> Is it uncensored ?

@Akalilol, this is what they claim on their website, ""It does not have any moderation mechanism. We’re looking forward to engaging with the community on ways to make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs"". 
But I did not try to ask for questionable topics... 🙂"
edumoulin,2023-10-04 07:22:39+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,">  you tested in ql code as in? I want to understand how can we use this model to query database/csv or panda datafame. 

@ianuvrat, actually I was only able to test its capabilities in chat mode for writing bash scripts and SQL statements, only using natural language. For now, it seems not to work in instruct mode, nor does it accept training at this time, at least using oobabooga's functions (https://github.com/oobabooga/text-generation-webui). 

I guess this will change over time as we are currently in 0.1 version. I'm also curious to go deeper in database exploration"
Colderthanice,2023-10-13 22:31:49+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"Mistral is so good, exceeds expectation."
abhirajeshbhai,2023-10-14 17:42:19+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"I have a question, I am trying to generate a poem, but it only generates half poems.. How do i make it to generate full poems?? "
whoknowsmeinhf,2023-10-17 10:59:01+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.1-GGUF,"It's responses are good. Not sure why it's performing poorly for CoVe (Chain of verification) to minimize hallucinations 

- https://github.com/jagilley/fact-checker

Any suggestions ?"
stellaathena,2024-05-03 12:51:55+00:00,comment,EleutherAI/pythia-160m,Yes it's exactly the same as the GPT-NeoX-20B tokenizer.
hails,2024-05-03 13:33:57+00:00,comment,EleutherAI/pythia-160m,+1
pietrolesci,2024-05-03 14:41:24+00:00,comment,EleutherAI/pythia-160m,"Thank you so much, @stellaathena and @hails!

Posting the link to the GPT-Neox-20B `merges.txt` file here for people landing on this page. 
https://huggingface.co/EleutherAI/gpt-neox-20b/blob/main/merges.txt"
danielhanchen,2024-02-15 07:57:20+00:00,comment,unsloth/mistral-7b-instruct-v0.2-bnb-4bit,@robinsmits Whoops!! Added Apache :)
robinsmits,2024-02-15 17:59:20+00:00,comment,unsloth/mistral-7b-instruct-v0.2-bnb-4bit,Thanks @danielhanchen .. much appreciated!
danielhanchen,2024-02-15 07:56:30+00:00,comment,unsloth/mistral-7b-instruct-v0.2-bnb-4bit,"@drunkOnData Oh https://github.com/unslothai/unsloth/wiki#saving-models-to-16bit-for-vllm

Also our Colab notebook for Mistral at the bottom has some merging code :) https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing"
drunkOnData,2024-02-15 15:33:12+00:00,comment,unsloth/mistral-7b-instruct-v0.2-bnb-4bit,Thank you!!! You da best 😊
danielhanchen,2024-02-15 16:38:10+00:00,comment,unsloth/mistral-7b-instruct-v0.2-bnb-4bit,@drunkOnData no problems!
nferruz,2023-11-14 19:43:00+00:00,comment,AI4PD/ZymCTRL,"Hi Shangao,

It is only expected in BRENDA classes with high redundancy. For example, if a Brenda class only contains 10 sequences, but they are in 10 different clusters at 50%, ZymCTRL will generate sequences at that distance. To decrease the identity, you could fine-tune the model in a less redundant dataset. 
Best wishes
Noelia"
nferruz,2023-06-13 08:01:03+00:00,comment,AI4PD/ZymCTRL,"hi rqh,

You can fine-tune your dataset with the info in the documentation. There's no rule of thumb for how many sequences are the minimum, although it would be good to have at least 100. I'd still suggest you give it a try even if you only have, let's say, 20. One thing you can do is to fine-tune each sequence and its reverse.

Hope this helps!
noelia"
nferruz,2023-05-16 12:28:14+00:00,comment,AI4PD/ZymCTRL,"mmmh, yes, you're right! I think that would also be possible if I am not missing any critical step in the script. 
Let me know how it goes!"
ipark,2023-05-16 16:23:35+00:00,comment,AI4PD/ZymCTRL,"Thanks for the reply.  

Actually I did only [part1] and then ran using run_clm.py (in lieu of your 5.run_clm-post.py)
Seems working well.  

Thanks!"
nferruz,2023-05-08 04:46:15+00:00,comment,AI4PD/ZymCTRL,"Hi guruace,

How many sequences were you generating during that time? With your GPUs, I'd expect it generates more than 2000 sequences in that time (possibly many more).
Certainly the first batch does not take more than 2-5 minutes when I use an A40.

Are you sure the GPU is being used?

Alternatively,  I've never tried, but I think HuggingFace supports inference on multiple GPUs: https://huggingface.co/docs/transformers/perf_infer_gpu_many

Hope this helps,
Noelia"
guruace,2023-05-08 15:35:14+00:00,comment,AI4PD/ZymCTRL,"Dear Noelia,

Yes, I was quite sure that it was using GPU, but used single GPU(it is quite sure also from your script). I also tested on my MacBook Pro M1 - 64G, it presumably ran on CPU only and it took 36 hours to produce only 572 sequences.  On RTX6000ada and V100-32g, there were 1300 and 1290 sequences generated. Based on M1 results, I was very sure that running on V100-32g was using GPU, not running on CPU alone.

Thank you!

guruace"
nferruz,2023-02-21 07:17:52+00:00,comment,AI4PD/ZymCTRL,"Hi ipark,

Thanks a lot for posting! It sounds like 12GB may not be enough to fit the model. As you say I’ve only tried with A100 and A40s but from your error it sounds like you will need more than 12GB or use the CPU. If I remember correctly, there is a documentation page in HuggingFace with tricks to train large models (but I don’t seem to find it now) and it had tips to try to fit the model into ‘smaller’ GPUs.

Q2: Yes you can fine-tune with a different tokenizer as well. I expect however that you should fine-tune for quite long because as it is ZymCTRL doesn’t have any knowledge of chemistry. The inout limit is 1024."
ipark,2023-02-21 20:11:30+00:00,comment,AI4PD/ZymCTRL,"Thank you Noelia! 

This might be for the tricks in HuggingFace documentation you are referring to 
https://huggingface.co/docs/transformers/v4.18.0/en/performance
Will look into this.

Thanks again!"
nferruz,2022-12-02 13:35:54+00:00,comment,AI4PD/ZymCTRL,"Hi Kurioscity!

Great to hear from you again :)
I'd love to help you with the GPU issue, what is the error you are getting (I might have no idea though)

The output you get is indeed a matrix of tokens. The model encodes each amino acid as a token, so you get a list of tokens per sequence. Then it outputs all the sequences at once, so you get a matrix for tokens.  These tokens can be decoded with the tokenizer, here you have an example script:
```
    # 1. Generate sequences
    enzyme_class = ""1.1.3.4""
    input_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)
    outputs = model.generate(
        input_ids, 
    	top_k=9, 
        repetition_penalty=1.2,
        max_length=1024,
        eos_token_id=1,
        pad_token_id=0,
   	    do_sample=True,
   	    num_return_sequences=100)
    
    # This step makes sure that the sequences weren't truncated during generation. The last token should be a padding token. 
    new_outputs = [ output for output in outputs if output[-1] == 0]
    if not new_outputs:
        print(""not enough sequences with short lengths!!"")
    

   # To decode the sequences, you have to use the tokenizer:
   tokenizer.decode(new_outputs[0]) # for example the first sequence in the list

   #  You can also decode and compute perplexity for all sequences at once:
   def calculatePerplexity(input_ids,model,tokenizer):
  '''
  Function to compute perplexity
  '''
       with torch.no_grad():
           outputs = model(input_ids, labels=input_ids)
       loss, logits = outputs[:2]
       return math.exp(loss)

    ppls = [(tokenizer.decode(output), calculatePerplexity(output, model, tokenizer)) for output in new_outputs ]

   # After this, one possibility is to sort the sequences by perplexity, the lower the better
    ppls.sort(key=lambda i:i[1])
 
```
Let me know if something throws an error or is unclear.
Best
Noelia"
0jj0,2022-12-04 21:09:35+00:00,comment,AI4PD/ZymCTRL,"Hi Noelia,
thanks a lot for your help with the decoding of the tokens. Your suggestion did help and I wrote a small script to get the sequences as followed (as you can already see from my code, I'm  a seasonal coder with limited knowledge in Python ):
```python
#!/usr/bin/env python

#@title Import libraries and initialize model
from transformers import GPT2LMHeadModel, AutoTokenizer
import torch
import os
from datetime import datetime
print(""Libraries imported successfully."")

start_time = datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")
print(start_time)

enzyme_class = ""1.1.3.4""
device = torch.device('cpu')
tokenizer = AutoTokenizer.from_pretrained('/mnt/e/ZymCTRL/model')
model = GPT2LMHeadModel.from_pretrained('/mnt/e/ZymCTRL/model').to(device)
input_ids = tokenizer.encode(enzyme_class,return_tensors='pt').to(device)
outputs = model.generate(input_ids, top_k=8, repetition_penalty=1.2, max_length=1024,
                        eos_token_id=1,pad_token_id=0,do_sample=True, num_return_sequences=50)
print(outputs)

new_outputs = [output for output in outputs if output[-1] == 0]
if not new_outputs:
   print(""Not enough sequences with short lengths!!"")

fastaname = enzyme_class + '_' + str(start_time)
f = open(fastaname + "".fasta"", ""w"")

fasta_records = []

for count, seq in enumerate (new_outputs, start = 0):
  seq = tokenizer.decode(new_outputs[count])
  print(seq)
  write_seq = seq.replace(' ', '').replace('<pad>', '').replace('<sep>', '').replace('<start>', '').replace(enzyme_class, '').replace('<|endoftext|>', '').replace('<end>', '')
  print(write_seq)
  fasta_record = "">"" + enzyme_class + ""_"" + start_time + ""_"" + str(count) + ""\n"" + write_seq + ""\n""
  print(fasta_record)
  fasta_records.append(fasta_record)
  print(fasta_records)

fasta_list = list(map(str, fasta_records))
fasta_file = "" "".join(fasta_list).lstrip()

print(fasta_file)
f.write(fasta_file)

f.close()
```

I will try to include the perplexity calculation as you suggested and I will open a new thread to ask for your help with the GPU thing (or is there a better way/channel to communicate with you the GPU issue  as I don't think the issue is very relevant to ZymCTRL?)"
nferruz,2022-12-04 22:58:06+00:00,comment,AI4PD/ZymCTRL,"Great to hear it helped! Let me know if you need some help with the perplexity. In my experience, it makes a huge different to only select values with high perplexity, e.g, the top 10%.
For the GPU error (which I might have no idea of) I guess you can try here in case it helps other users :) or send me an email at noelia [dot] ferruz [at] udg [dot] edu"
vamshigvk,2024-04-30 07:11:59+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-AWQ,"Same error for me, any resolution on this?
"
vamshigvk,2024-05-08 07:06:13+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-AWQ,"provide below in config : https://www.philschmid.de/sagemaker-awq-medusa
qunatize:awq"
leaderboard-pr-bot,2024-03-31 21:30:55+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"This is an automated PR created with https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr

The purpose of this PR is to add evaluation results from the Open LLM Leaderboard to your model card.

If you encounter any issues, please report them to https://huggingface.co/spaces/Weyaxi/open-llm-leaderboard-results-pr/discussions"
tcapelle,2024-03-12 11:04:03+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"```python
decoded[0][len(model_inputs.input_ids):]
```"
InformaticsSolutions,2024-02-14 03:45:42+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"I was under the assumption that device_map='auto' will force the whole model into the GPU. In fact, it seems that only some layers will get loaded into the GPU, some are not. I believe this is the reason for the slowness. By doing '''model.to(device)''' the whole model will be loaded to the GPU and the inference is much faster. So this was not an issue with the model, but with the way i was loading the model. 
"
Filippo,2024-01-26 23:19:13+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Yes, its license is Apache 2.0."
TaeYeon39,2024-01-27 13:36:35+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,Thank you for your confirmation.
teknium,2024-01-12 17:33:59+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> It is hard to tell which model this was finetuned on.
There is only one mistral 7b, there is no 2
"
lazyDataScientist,2024-01-12 17:35:32+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"I mean which of these was it finetuned on?
https://huggingface.co/mistralai/Mistral-7B-v0.1
https://huggingface.co/mistralai/Mistral-7B-v0.2"
Weyaxi,2024-01-12 17:40:21+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> I mean which of these was it finetuned on?
> https://huggingface.co/mistralai/Mistral-7B-v0.1
> https://huggingface.co/mistralai/Mistral-7B-v0.2

Can you try to click your second link :)

There is mistral instruct 2.0 not mistral"
lazyDataScientist,2024-01-12 18:28:12+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Yeah, so which version of the instruct models was it finetuned on?"
teknium,2024-01-12 19:42:10+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Yeah, so which version of the instruct models was it finetuned on?
I didn’t fine tune over any instruct model I fine tuned over the base model..
"
teknium,2024-01-10 10:22:27+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Hello Teknium
> 
> I am a huge fan of yours. Quick question: what dataset did you use to finetune this SFT model?
> 
> You said:
> 
> OpenHermes was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape.  **** [More details soon] ****
> 
> Is it possible to tell other open source dataset (if the 1M entries of GPT-4 is not available to us). Thanks!

I'll be releasing the full dataset very soon :)"
lrq3000,2024-03-24 22:16:31+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"For those interested, the full dataset for this amazing model got released here: https://huggingface.co/datasets/teknium/OpenHermes-2.5"
teknium,2024-01-10 10:21:28+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> I have an Nvidia A10 (24GB of VRAM) but I'm getting out of memory errors.
> 
> 
> ```
> model_name = ""teknium/OpenHermes-2.5-Mistral-7B""
> 
> 
> def load_model(model_name: str):
>     tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
> 
>     with torch.device(""cuda:0""):
>         model = transformers.AutoModelForCausalLM.from_pretrained(model_name).eval()
>     
>     return tokenizer, model
> 
> tokenizer, model = load_model(model_name)
> ```
> 
> ```OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 13.00 MiB is free. Including non-PyTorch memory, this process has 21.96 GiB memory in use. Of the allocated memory 21.58 GiB is allocated by PyTorch, and 99.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF...```
> 
> 
> I assumed 24GB would be enough for  a 7B model, how much VRAM do I need to run this model?

You're loading it likely in fp32. in fp32, it needs 28GB. In fp/bf16 it needs 14GB, in 8bit, 7GB, and in 4bit, ~4GB - add 1GB to all for CUDA Kernel"
Ziizu,2024-01-10 15:34:09+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"@teknium Thanks for the response, this may be a naive question but how do I load in 16/8 bit?

I've tried loading in bf16: 
```
model_name = ""teknium/OpenHermes-2.5-Mistral-7B""

def load_model(model_name: str):
    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

    with torch.device(""cuda:0""):
        model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).eval()
    
    return tokenizer, model

tokenizer, model = load_model(model_name)
```

which gave:

```
Loading checkpoint shards:   0%|          | 0/2 [00:49<?, ?it/s]
---------------------------------------------------------------------------
OutOfMemoryError                          Traceback (most recent call last)
Cell In[3], line 19
     15         model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).eval()
     17     return model
---> 19 model = load_model(model_name)

Cell In[3], line 15
     11 def load_model(model_name: str):
     12     #tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
     14     with torch.device(""cuda:0""):
---> 15         model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).eval()
     17     return model

File ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    564 elif type(config) in cls._model_mapping.keys():
    565     model_class = _get_model_class(config, cls._model_mapping)
--> 566     return model_class.from_pretrained(
    567         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    568     )
    569 raise ValueError(
    570     f""Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n""
    571     f""Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.""
    572 )

File ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:3706, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3697     if dtype_orig is not None:
   3698         torch.set_default_dtype(dtype_orig)
   3699     (
   3700         model,
   3701         missing_keys,
   3702         unexpected_keys,
   3703         mismatched_keys,
   3704         offload_index,
   3705         error_msgs,
-> 3706     ) = cls._load_pretrained_model(
   3707         model,
   3708         state_dict,
   3709         loaded_state_dict_keys,  # XXX: rename?
   3710         resolved_archive_file,
   3711         pretrained_model_name_or_path,
   3712         ignore_mismatched_sizes=ignore_mismatched_sizes,
   3713         sharded_metadata=sharded_metadata,
   3714         _fast_init=_fast_init,
   3715         low_cpu_mem_usage=low_cpu_mem_usage,
   3716         device_map=device_map,
   3717         offload_folder=offload_folder,
   3718         offload_state_dict=offload_state_dict,
   3719         dtype=torch_dtype,
   3720         is_quantized=(getattr(model, ""quantization_method"", None) == QuantizationMethod.BITS_AND_BYTES),
   3721         keep_in_fp32_modules=keep_in_fp32_modules,
   3722     )
   3724 model.is_loaded_in_4bit = load_in_4bit
   3725 model.is_loaded_in_8bit = load_in_8bit

File ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:4091, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)
   4089 if shard_file in disk_only_shard_files:
   4090     continue
-> 4091 state_dict = load_state_dict(shard_file)
   4093 # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not
   4094 # matching the weights in the model.
   4095 mismatched_keys += _find_mismatched_keys(
   4096     state_dict,
   4097     model_state_dict,
   (...)
   4101     ignore_mismatched_sizes,
   4102 )

File ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py:510, in load_state_dict(checkpoint_file)
    505     if metadata.get(""format"") not in [""pt"", ""tf"", ""flax""]:
    506         raise OSError(
    507             f""The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure ""
    508             ""you save your model with the `save_pretrained` method.""
    509         )
--> 510     return safe_load_file(checkpoint_file)
    511 try:
    512     if (
    513         is_deepspeed_zero3_enabled() and torch.distributed.is_initialized() and torch.distributed.get_rank() > 0
    514     ) or (is_fsdp_enabled() and not is_local_dist_rank_0()):

File ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/safetensors/torch.py:310, in load_file(filename, device)
    308 with safe_open(filename, framework=""pt"", device=device) as f:
    309     for k in f.keys():
--> 310         result[k] = f.get_tensor(k)
    311 return result

File ~/text-generation-webui/installer_files/env/lib/python3.11/site-packages/torch/utils/_device.py:77, in DeviceContext.__torch_function__(self, func, types, args, kwargs)
     75 if func in _device_constructors() and kwargs.get('device') is None:
     76     kwargs['device'] = self.device
---> 77 return func(*args, **kwargs)

OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 21.99 GiB of which 59.00 MiB is free. Including non-PyTorch memory, this process has 21.92 GiB memory in use. Of the allocated memory 21.44 GiB is allocated by PyTorch, and 203.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

For reference/context, I can load the quantised GGUF version of this model without issue."
teknium,2024-01-10 16:25:34+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Use this inference code from the repo

https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/main/transformers_inference.py

Let me know if it still has issues 🤗"
InformaticsSolutions,2024-01-25 22:46:28+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"I'm using the inference script above with the following prompt:
prompts = [
    """"""<|im_start|>system
You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>
<|im_start|>user
Explain how viruses casue diseases<|im_end|>
<|im_start|>assistant"""""",
    ]

for chat in prompts:
    print(chat)
    input_ids = tokenizer(chat, return_tensors=""pt"").input_ids.to(""cuda"")
     generated_ids = model.generate(input_ids, max_new_tokens=1000,repetition_penalty=1.1, do_sample=False, eos_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)
    print(f""Response: {response}"")

After about 13 minutes it generates this error: ""IndexError: piece id is out of range.""
I've tried 4bit as well, still not able to get a response. I have an RTX 3080 16Gb.
Any suggestions on how to make it work? Thank you.
"
teknium,2024-01-25 22:47:55+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,Can you share the full log
InformaticsSolutions,2024-01-25 23:06:41+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Thank you @teknium for responding so fast. here's the traceback:
{
	""name"": ""IndexError"",
	""message"": ""piece id is out of range."",
	""stack"": ""---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[6], line 14
     12 #generated_ids = model.generate(input_ids, max_new_tokens=1000, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)
     13 generated_ids = model.generate(input_ids, max_new_tokens=100,repetition_penalty=1.1, do_sample=False, eos_token_id=tokenizer.eos_token_id)
---> 14 response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)
     15 print(f\""Response: {response}\"")

File /usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:3750, in PreTrainedTokenizerBase.decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)
   3747 # Convert inputs to python lists
   3748 token_ids = to_py_obj(token_ids)
-> 3750 return self._decode(
   3751     token_ids=token_ids,
   3752     skip_special_tokens=skip_special_tokens,
   3753     clean_up_tokenization_spaces=clean_up_tokenization_spaces,
   3754     **kwargs,
   3755 )

File /usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:1001, in PreTrainedTokenizer._decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)
    991 def _decode(
    992     self,
    993     token_ids: List[int],
   (...)
    997     **kwargs,
    998 ) -> str:
    999     self._decode_use_source_tokenizer = kwargs.pop(\""use_source_tokenizer\"", False)
-> 1001     filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
   1002     legacy_added_tokens = set(self._added_tokens_encoder.keys()) - set(self.all_special_tokens) | {
   1003         token for token in self.additional_special_tokens if self.convert_tokens_to_ids(token) >= self.vocab_size
   1004     }
   1005     # To avoid mixing byte-level and unicode for byte-level BPT
   1006     # we need to build string separately for added tokens and byte-level tokens
   1007     # cf. https://github.com/huggingface/transformers/issues/1133

File /usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:982, in PreTrainedTokenizer.convert_ids_to_tokens(self, ids, skip_special_tokens)
    980         tokens.append(self._added_tokens_decoder[index].content)
    981     else:
--> 982         tokens.append(self._convert_id_to_token(index))
    983 return tokens

File /usr/local/lib/python3.9/dist-packages/transformers/models/llama/tokenization_llama.py:280, in LlamaTokenizer._convert_id_to_token(self, index)
    278 def _convert_id_to_token(self, index):
    279     \""\""\""Converts an index (integer) in a token (str) using the vocab.\""\""\""
--> 280     token = self.sp_model.IdToPiece(index)
    281     return token

File /usr/local/lib/python3.9/dist-packages/sentencepiece/__init__.py:1045, in _batchnize.<locals>._batched_func(self, arg)
   1043   return [_func(self, n) for n in arg]
   1044 else:
-> 1045   return _func(self, arg)

File /usr/local/lib/python3.9/dist-packages/sentencepiece/__init__.py:1038, in _batchnize.<locals>._func(v, n)
   1036 def _func(v, n):
   1037   if type(n) is int and (n < 0 or n >= v.piece_size()):
-> 1038     raise IndexError('piece id is out of range.')
   1039   return func(v, n)

IndexError: piece id is out of range.""
}

thank you."
teknium,2024-01-02 06:58:49+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Hello! I have noticed that some models appear to be called NousHermes, while others appear to be called OpenHermes. Is there a specific difference between the two?

They are the same dataset right now"
teknium,2024-01-10 16:26:03+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,It was trained on almost exclusively English text
teknium,2023-12-27 04:40:13+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> This is probably a stupid question, but is this a full fine-tune or lora?
> 
> (Is this obvious by looking at any of the files?)
> 
> Thx - I love this model ❤️

yes its full finetune"
Benjoyo,2023-12-27 13:08:57+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,thanks
teknium,2023-12-27 04:40:33+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> I want to SFT this model yet can't understand the anticipated sample format

Chatml"
llama2thedog,2024-02-02 01:14:25+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,This comment has been hidden
teknium,2023-12-12 09:22:44+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> I'm using a 3-bit quantized version of this to run an iPhone. What do you suggest for temperature? I heard different thoughts on this. Some believe the temperature needs to be low (<0.2) because of quantization.

I've never used sub-4bit but the same settings worked for all quants Ive used"
lixbo,2023-12-11 22:05:03+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,Use ChatML preset.
Filippo,2024-01-27 13:20:50+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Hi there, I am also a bit confused about this and still trying to clear out the confusion myself!

However I think I can answer your question ""How should `<s>` be specified in the prompt/prompts when chatting with the model?"".

The answer is: you should not add it yourself. The tokenizer will take care of doing that."
teknium,2023-12-12 09:23:20+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Hi @teknium ,
> 
> I believe you trained using axolotl with this dataset config:
> ```
> datasets:
>   - path: /data/chat_data/full_dataset_chat.jsonl
>     type: sharegpt
>     conversation: chatml
> dataset_prepared_path: last_run_prepared
> ```
> Did you realised that Axolotl actually adds an extra linebreak (somehow) and it becomes `<|im_end|>\n\n` ? or did you create your own custom dataset and dataloader?  Hope to see your release of the configuration file and dataset format. 
> 
> I found out by debugging step-by-step to run through the repo, the last few label tokens will be always be  [....., 28766, 321, 28730, 416, 28766, 28767, 13, 13, 2] which when decoded is `<|im_end|>\n\n</s>`.
> 
> Issue could be here (extra `\n` in sep): https://github.com/OpenAccess-AI-Collective/axolotl/blob/a48dbf6561cc74c275a48070f397334a2c367dd5/src/axolotl/prompt_strategies/sharegpt.py#L16

I believe they changed things for chatml format after this was trained"
dyoung,2023-12-06 12:35:55+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,I think I have an idea of what's up. I'm running transformers 4.43.0. 4.45.x is the current stable I think. Likely updating my transformers library will fix this. I'll keep you all posted on how that goes.
dyoung,2023-12-06 12:47:12+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Yep, that was the solution to the situation. So I'm good now.
Found this which helped: https://github.com/huggingface/transformers/issues/27370
Would of been nice if I got a warning or raised exception from HF's AutoTokenizer.apply_chat_template. I might say something to them about it."
teknium,2023-12-04 13:22:54+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Hello,
> 
> Is this model capable of doing function calling ? I saw that the latest multimodal was capable (NousResearch/Nous-Hermes-2-Vision) but what about this one ?
> Any recommadnation for such good small model with function calling ?
> 
> Thanks

It has limited capabilities of function calling"
tarruda,2023-12-04 16:40:48+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> It has limited capabilities of function calling

Can you give an example of how we can use this functionality?

Thanks!"
dyoung,2023-12-06 11:02:34+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Airoboros collection from Jon Durbin may also be of interest: https://huggingface.co/jondurbin 
And some are quantized by TheBloke https://huggingface.co/TheBloke?search_models=airoboros-m"
teknium,2023-12-04 13:24:33+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> hi @teknium ,
> 
> Can you share the training configuration you used? I saw that you are using Axolotl?

Yes, axolotl"
timlim123,2023-12-05 10:55:04+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,How about the training configuration? the yaml file.
timlim123,2023-12-06 06:04:14+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,I cannot seem to find any multi-turn setup with system prompt.
ZQ-Dev,2024-03-07 23:54:29+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"+1, @teknium would you consider posting training details, to include the axolotl config? Would be a big help to those new to the project."
teknium,2023-12-04 13:25:15+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Can you please provide the list of training hyperparameters ?

4 epochs @ 0.00007 LR"
ekurtulus,2023-12-05 02:18:38+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"What about weight decay?
"
teknium,2023-11-24 07:40:32+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Hi, 
> 
> Thank you for the great work! I'm curious why the reported evaluation is very different than what is reported for Openchat 3.5 (https://huggingface.co/openchat/openchat_3.5)? It's kind of interesting that in Openchat 3.5, they also compared with OpenHermes 2.5 and claimed that they are better. And I noticed that the scores reported for HumanEval, TruthfulQA etc. do not match on both sides..

Openchat uses a proprietary unknown method to generate evaluation results, and does not use LM Eval Harness, so it's impossible for me to say how the benchmark scores were derived"
imone,2023-11-24 13:55:34+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"OpenChat evaluation results are run using zero-shot and few-shot CoT. For reproduction instructions, see [here](https://github.com/imoneoi/openchat#%EF%B8%8F-benchmarks)

```
python -m ochat.evaluation.run_eval --model-type chatml_mistral --model teknium/OpenHermes-2.5-Mistral-7B
```"
teknium,2023-11-24 07:39:06+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Hi, thanks for making this incredible model. What is the context length for this model? Does it sustain multi-round conversations well, and can it extract facts buried deep inside long prompts?

It should be able to read up to 8k, but only respond up to 4k"
mrfakename,2023-11-24 19:49:54+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,Thanks!
teknium,2023-11-24 07:38:45+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"I couldnt get the chat template thing to work, try just using the string of what it should be"
Pazuzzu,2023-12-05 11:36:03+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Apologies,
I have no issue with tokenizer.apply_chat_template but my model loading statement was missing the ""return_dict=True"", that is what caused the error."
InformaticsSolutions,2024-01-26 21:03:06+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"@Pazuzzu i am still getting the error, even after updating the model loading statement like this:
        model = transformers.AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True,  device_map='auto', return_dict=True)
I am using transformers 4.36.0. Should the statement be written differently? thank you.
"
teknium,2023-11-03 09:09:17+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Hi there,
> 
> I've successfully deployed the model as an OpenAI API-compatible endpoint, which you can access via the following link: https://huggingface.co/spaces/limcheekin/OpenHermes-2.5-Mistral-7B-GGUF.
> 
> This API has been made possible thanks to the generous contributions of the following individuals and resources:
> 
> - The original model's publisher.
> - The `TheBloke` who shared the quantized models.
> - The `llama-cpp-python` package.
> - HuggingFace Spaces.
> 
> If you find this resource valuable and wish to show your support, we would greatly appreciate it if you could ""star"" the space.
> 
> Thank you for your support and interest.

great ^_^"
Henk717,2023-11-05 23:10:57+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"What compute is powering this?
Would love to have something similar based on Koboldcpp since it provides a UI, KoboldAI compatible API and an OpenAI compatible API.
But from what I saw the free hosting on HF Spaces is not fast enough."
limcheekin,2023-11-05 23:16:17+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,It is free hosting. Barely enough for development purpose. 
Henk717,2023-11-06 01:57:26+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"Just made my own as well (Based on a different model), feel free to take my docker file and use it yourself.
https://huggingface.co/spaces/Henk717/Koboldcpp-Tiefigther
Also acts as an OpenAI Endpoint, but with a KoboldAI API endpoint and UI on top as well as our Llamacpp enhancements."
limcheekin,2023-11-06 03:27:22+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"> Just made my own as well (Based on a different model), feel free to take my docker file and use it yourself.
> https://huggingface.co/spaces/Henk717/Koboldcpp-Tiefigther
> Also acts as an OpenAI Endpoint, but with a KoboldAI API endpoint and UI on top as well as our Llamacpp enhancements.

Great! Thanks for sharing. :)"
deleted,2023-11-02 23:17:40+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,This comment has been hidden
HDiffusion,2023-11-03 06:21:00+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,What sampling parameters are you using? I've found Mistral tends to hallucinate much more than other models at typical temperature settings. I use 0.5 and it performs far more accurately.
deleted,2023-11-03 06:44:27+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,This comment has been hidden
deleted,2023-11-21 00:48:43+00:00,comment,teknium/OpenHermes-2.5-Mistral-7B,"I moved this discussion to closed because the ""intelligence"" Hermes 2.5 gained across the board was well worth the slight increase in hallucinations at the fringes of Mistral's knowledge and marginally low TruthfulQA score (52). Thanks for your work. This is now my favorite LLM."
xuzhe1188,2024-05-30 21:39:54+00:00,comment,mistralai/Mistral-7B-v0.3,"I am a newbie too and I still cannot download some models, even though it shows ""You have been granted access to this model"" on some of the repos. Anyway, I got access to some of the models in these steps:
(1) signing in your hugging face account in the terminal.
(2) In the  account settings - access tokens - Repositories permissions, select those models I want to download."
Rome111,2024-05-31 08:53:01+00:00,comment,mistralai/Mistral-7B-v0.3,"#xuzhe118 
this seemed to work for me, 
Appreciate the reply ! "
yugu88,2024-05-31 09:13:36+00:00,comment,mistralai/Mistral-7B-v0.3,"(1) signing in your hugging face account in the terminal.
 huggingface-cli login
Enter your token

__Home page to get authorization__


```
(base) qihao@qihao-B760M-DS3H:~/Ai$ huggingface-cli login

    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|

    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .
Enter your token (input will not be visible):
Add token as git credential? (Y/n) y
```

> **app.py**

```python
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id=""mistralai/Mistral-7B-v0.3"", use_auth_token=True,
                  allow_patterns=[""params.json"", ""consolidated.safetensors"", ""tokenizer.model.v3""],
                  local_dir=mistral_models_path)




```
``` bash
 python app.py

params.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 202/202 [00:00<00:00, 616kB/s]
tokenizer.model.v3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 587k/587k [00:00<00:00, 2.59MB/s]
tokenizer.model.v3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 587k/587k [00:00<00:00, 2.60MB/s]
consolidated.safetensors:  24%|████████████████████████████████▎                                                                                                     | 3.49G/14.5G [22:17<1:10:29, 2.60MB/s]
```


"
larsonlake,2024-05-30 23:52:38+00:00,comment,mistralai/Mistral-7B-v0.3,"A) Because adults should be allowed to decide for themselves the kind of content they want to consume and what knowledge they wish to acquire.  It's supposed to be one of the benefits of being an adult.  Also,   different people have different ideas about what should be censored.  e.g. - I don't think you'd like a model generated by the Chinese government.
Hope that helps!"
TashaSkyUp,2024-05-31 15:19:20+00:00,comment,mistralai/Mistral-7B-v0.3,Would you agree that this model is censored? or is it un-censored? Because try as I might I couldn't get it to speak about minorities or LGBTQIA people in any sort of derogatory manner? Oh wait is un-censored == able to speak about crazy philosophies like killing all the poor people being good for society? I didn't try that one..  I'm just glad it doesn't go off on a rant about hating trans people and other minorities.
larsonlake,2024-06-02 20:06:23+00:00,comment,mistralai/Mistral-7B-v0.3,"It is censored.  Most are.  I'm not sure why you would want to use a chatbot for creating derogatory text.  You could certainly write that sort of text yourself if you want, but I'm sorry that you weren't able to find what you were looking for.

If it helps with understanding, uncensored means that someone else doesn't get to dictate what information you are working with.  Someone else doesn't get to tell you what is crazy and what isn't.  You get to decide for yourself.  
And, while a computer could display the text of a human on a rant,  a computer cannot go off on a rant of any kind.  It doesn't have a mind.  Its intelligence is only artificial.  It only reflects the text of The Internet.  Think of it like a mirror or a book.  A mirror or a book has no intelligence of its own."
TashaSkyUp,2024-06-04 19:48:44+00:00,comment,mistralai/Mistral-7B-v0.3,"when I hear ""uncensored means that someone else doesn't get to dictate what information you are working with"" I also hear ""uncensored means someone can easily hook this up to twitter and spread even more mis-information and hate about LGBTQIA people using advanced dog whistles and targeted persuasion techniques."".

I'm actually very pro-censorship when it comes to protecting people like myself. I also believe that all information should be free. So this is a tough call for me."
Cyberchild,2024-06-04 23:14:06+00:00,comment,mistralai/Mistral-7B-v0.3,"Of course you hear that, because you project your beliefs out onto the world and see your reflection staring back at you. So in your mind, all you see is racism or sexism in everyone when really its in your own mind and the way you think. I too don't want censored models because I don't like being told what to think. But people like you want censorship because you want to control what people think. You bring up about minorities and LGBTQ whatever because that's what is in your head, nobody else was talking about any of that, but it proves my point because you've now made it all about that, and anybody says its not, they're a bigot or a homophobe, right? See how it works? The issue is in your head, so do yourself a favour and look at what you believe and stop projecting your issues onto everyone else."
TashaSkyUp,2024-06-05 02:03:44+00:00,comment,mistralai/Mistral-7B-v0.3,"Totally not because of the actual hate speech yelled at me from moving vehicles. THAT JUST COULD NOT BE IT.
Also not because of all of the job opportunities missed because no one wants to be responsible for hiring an ugly trans woman. THAT JUST COULD NOT BE IT.
Its not because I get to be poor my entire life compared to my peers because im trans. THAT JUST COULD NOT BE IT.

BECAUSE NO ONE EVER HURTS MINORITIES ON PURPOSE BECAUSE ITS ALL IN OUR HEAD RIGHT???

try to gass-light me you imbecile."
TashaSkyUp,2024-06-05 02:06:03+00:00,comment,mistralai/Mistral-7B-v0.3,"Honestly, Sometimes I think a person should have to go through 10 years of being othered and living in poverty to earn a license to speak."
larsonlake,2024-06-05 17:47:44+00:00,comment,mistralai/Mistral-7B-v0.3,"Hey Tasha,

I would think that the type of person who wants to post hate speech all over Twitter would probably get much more satisfaction from typing their own posts, but bots are everywhere, so I'm probably wrong.  I fail to understand the reasoning behind automated hate.  It just seems fundamentally absurd.

P.S. - I tried typing up a personal note about why I am anti-censorship, but when I tried running it past ChatGPT (ironic, I know) it said that it was insensitive, so I'll just keep it to myself.  Call it self-censorship."
ayyylol,2024-05-22 22:54:43+00:00,comment,mistralai/Mistral-7B-v0.3,You can exclude it from ```allow_patterns=```
prince-canuma,2024-05-22 23:11:06+00:00,comment,mistralai/Mistral-7B-v0.3,"I know, but that requires extra work to change the code base of libraries like MLX.
"
prince-canuma,2024-05-22 23:11:57+00:00,comment,mistralai/Mistral-7B-v0.3,"I don't mind it much, just think it's unecessary to keep two weight files. 

It's not a common pattern."
MaziyarPanahi,2024-05-23 07:50:07+00:00,comment,mistralai/Mistral-7B-v0.3,I too had to change my scripts to add that file to `allow_patterns`.
ArthurZ,2024-05-22 15:32:44+00:00,comment,mistralai/Mistral-7B-v0.3,Snippet for transformers
ArthurZ,2024-05-22 14:56:05+00:00,comment,mistralai/Mistral-7B-v0.3,"This is the git diff to support this, use `LlamaTokenizer.from_pretrained(""tokenizer.model.v3"", legacy=False)`, then converted to fast. 
```diff
diff --git a/src/transformers/convert_slow_tokenizer.py b/src/transformers/convert_slow_tokenizer.py
index 4b0a53b704..a1346d4c0c 100644
--- a/src/transformers/convert_slow_tokenizer.py
+++ b/src/transformers/convert_slow_tokenizer.py
@@ -1385,6 +1385,7 @@ class LlamaConverter(SpmConverter):
                     AddedToken(self.original_tokenizer.convert_ids_to_tokens(2), normalized=False, special=True),
                 ]
             )
+            tokenizer.add_special_tokens(list(proto.trainer_spec.control_symbols))
         else:
             raise Exception(
                 ""You're trying to run a `Unigram` model but you're file was trained with a different algorithm""
```
"
FalconLLM,2023-06-09 13:52:24+00:00,comment,tiiuae/falcon-7b,"This is amazing, thank you for sharing! We have pinned it so that more people can play with this great library 👍."
Xorat,2023-05-28 15:12:26+00:00,comment,tiiuae/falcon-7b,[attention_mask?](NameError: name 'attention_mask' is not defined) ?
Sloba,2023-05-28 15:25:02+00:00,comment,tiiuae/falcon-7b,"> [attention_mask?](NameError: name 'attention_mask' is not defined) ?

Thanks, updated original message with fix.
Just add:
attention_mask = torch.ones(input_ids.shape)

before .... generate...."
Xorat,2023-05-28 15:40:29+00:00,comment,tiiuae/falcon-7b,"also added: 
input_ids = input_ids.to('cuda')
before :
attention_mask = torch.ones(input_ids.shape)
now working nice! thx"
bilelm,2023-05-29 09:16:38+00:00,comment,tiiuae/falcon-7b,@Sloba on what server specs did you test your model ? and how was the inference time ?
Sloba,2023-05-29 10:01:38+00:00,comment,tiiuae/falcon-7b,"@bilelm  
It is private desktop computer, i am interested in creating local LLM-s:
AMD Ryzen 9 7900X 12-Core (24 thread)  4.7GHz
64Gb DDR5 Ram 4800MHz
NVidia GeForce RTX 3090 24Gb
Model is loaded from NAS over 1GBit/s network (it takes ~2min)

It took 78 seconds for:
**Question**: Where girrafe lives, and how tall is giraffe?
**Answer**: Giraffes live in the African continent, they are the tallest land animals.
The tallest one of them is the African male giraffe, which can stand up to 5.9 meters tall.
Giraffes are herbivorous animals, they feed on leaves and grasses.
They are not dangerous animals. They are very friendly and kind to humans, they don’t attack or eat people.
Giraffes have a lifespan of 20 years.<|endoftext|>

It took 156 seconds for:
**Question**: Where llama lives, and how tall is llama?
**Answer**: The llama is a South American camelid, a member of the camel family. It is a large, sturdy animal with a thick coat. Llamas are domesticated and are used for meat, wool, and milk.
The llama is a South American camelid, a member of the camel family. It is a large, sturdy animal with a thick coat. Llamas are domesticated and are used for meat, wool, and milk.
What is a llama? A llama is a South American camelid, a member of the camel family. They are domesticated and used for meat, wool, and milk.
How do llamas look? Llama’s are a large, furry animal that looks like a mix between a camel and a giraffe. They are native to South America but are now found in many other places around the world.
Where do llamas live? Llamas live in the Andes mountains, where they graze on vegetation.


I hope this helps, for what is worth, Falcon-7B answers are pretty good."
bilelm,2023-05-29 17:12:39+00:00,comment,tiiuae/falcon-7b,"@Sloba thank you so much for your answer. 
I'm looking to test it on French, for tasks like summarization or information extraction."
FalconLLM,2023-05-30 14:55:26+00:00,comment,tiiuae/falcon-7b,"Hi @Sloba, thank you for writing this short guide, we will pin it to make it easily accessible!"
ivyas,2023-05-31 19:48:57+00:00,comment,tiiuae/falcon-7b,"@FalconLLM , @Sloba  Quick question, Can I run it on Macbook Pro with intel chip with 32 RAM?"
Sloba,2023-06-01 08:03:53+00:00,comment,tiiuae/falcon-7b,"@ivyas  Unfortunately I don't have access to MBP with 32G RAM to try it out. 
If you decide to try it out, don't hesitate to share the results. Maybe there is someone who needs exactly the info you find in your test."
BliepBlop,2023-06-02 15:24:27+00:00,comment,tiiuae/falcon-7b,"I'm trying to run this on a Apple M1 Max..
the code I use is this:
```
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch
model = ""./falcon-7b""
device_name = 'cpu'
device = torch.device(device_name)
rrmodel = AutoModelForCausalLM.from_pretrained(model,
    trust_remote_code=True,
    device_map=""auto"")
rrmodel = rrmodel.to(device)
tokenizer = AutoTokenizer.from_pretrained(model)

input_text = ""Once upon a time""
input_ids = tokenizer.encode(input_text, return_tensors='pt')
input_ids = input_ids.to(device)
attention_mask = torch.ones(input_ids.shape)
attention_mask = attention_mask.to(device)

output = rrmodel.generate(input_ids,
            attention_mask=attention_mask,
            max_length=200,
            do_sample=True,
            top_k=10,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,)

output_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(output_text)
```

Using device_name = 'cpu' this take 5m 50s mins to run.

I try to use device_name = 'mps' for acceleration on the m1 chip.
But I get this error:

```
Traceback (most recent call last):
  File ""/Users/mario/Downloads/main.py"", line 19, in <module>
    output = rrmodel.generate(input_ids,
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py"", line 1565, in generate
    return self.sample(
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/transformers/generation/utils.py"", line 2612, in sample
    outputs = self(
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py"", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File ""/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py"", line 753, in forward
    transformer_outputs = self.transformer(
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py"", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File ""/Users/mario/.cache/huggingface/modules/transformers_modules/falcon-7b/modelling_RW.py"", line 590, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1502, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1511, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/accelerate/hooks.py"", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/modules/sparse.py"", line 162, in forward
    return F.embedding(
  File ""/Users/mario/anaconda3/envs/falcon/lib/python3.9/site-packages/torch/nn/functional.py"", line 2238, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Placeholder storage has not been allocated on MPS device!
```"
ivyas,2023-06-02 17:35:24+00:00,comment,tiiuae/falcon-7b,"@BliepBlop Did you encounter this issue when you run above code?

ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.

I installed `safetensors`."
FalconLLM,2023-06-09 14:03:40+00:00,comment,tiiuae/falcon-7b,"In principle the model should need at least 16GB of memory to run--but generation on CPU is bound to be slow. 

We also recommend having a look at this [blog post](https://huggingface.co/blog/falcon) for more info on finetuning & inference of Falcon."
rbgreenway,2023-06-13 17:08:18+00:00,comment,tiiuae/falcon-7b,"Excellent post.  Thanks for providing this.
All worked for me on a 4090, Ubuntu 20.04"
cmp-nct,2023-06-28 23:10:35+00:00,comment,tiiuae/falcon-7b,"Alternative: https://github.com/cmp-nct/ggllm.cpp/blob/master/README.md
Includes a video how to compile it on windows, does not need a complex conda/python backend and runs with just a few GB or RAM (or VRAM) 10+ times faster than with python 
Also includes exe binary release for windows (for cpu and cuda) if you don't want to get into development frameworks"
wasiim,2023-07-13 10:46:24+00:00,comment,tiiuae/falcon-7b,"can anyone help me please 
i have the text data stored in .txt the text data is simple information about a technology 
i want to fine tune the falcon model and the i want to ask the question to the falcon model according to that .txt file "
cmp-nct,2023-07-13 12:28:05+00:00,comment,tiiuae/falcon-7b,"> can anyone help me please 
> i have the text data stored in .txt the text data is simple information about a technology 
> i want to fine tune the falcon model and the i want to ask the question to the falcon model according to that .txt file

Fine tuning typically involves a clean set of inputs and outputs, not a text with simple information. 
You can look into fine tune projects for falcon and how their input data looks like, it will need an elaborate effort to transform your text into good input and output.

The more likely solution is to just prompt Falcon with your text and ask it to use it as information source. By using a good fine tune that follows your prompt you can increase the quality."
Raven78,2023-07-27 05:44:53+00:00,comment,tiiuae/falcon-7b,"Thank you so much. worked a treat!

also if anyone is getting the ERROR :
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

just add the line:    input_ids = input_ids.to('cuda')

thanks again @Sloba  :)"
Kernel,2023-05-28 10:16:14+00:00,comment,tiiuae/falcon-7b,"If someone managed to train with QLoRA, please share your results"
zhangbo2008,2023-05-29 08:17:02+00:00,comment,tiiuae/falcon-7b,"you can check the code of alpaca .  maybe not change too much code.
https://github.com/tloen/alpaca-lora/blob/main/finetune.py"
cekal,2023-05-29 08:21:29+00:00,comment,tiiuae/falcon-7b,"@zhangbo2008 as you can see in my initial message, I mentioned that I already tried it, just modified the script a bit so it would fit the correct “AutoModelForCasualLM” and “AutoTokenizer” etc. Seems like it is not working correctly, something is wrong."
zhangbo2008,2023-05-29 08:24:24+00:00,comment,tiiuae/falcon-7b,"ok, i will try"
cekal,2023-05-29 08:32:22+00:00,comment,tiiuae/falcon-7b,"Alright. You’ll probably get the training running but if the training loss is too high and it shows learning rate 0.0 chances are the model is training incorrectly (final adapter will then be useless). This might be due to an issue in modeling_RW.py that does not allow support (is not configured) for LoRA

If it works for you, please share your version of the code. Thanks!"
cekal,2023-05-29 08:39:20+00:00,comment,tiiuae/falcon-7b,"@zhangbo2008 I found this: https://github.com/lvwerra/trl/blob/main/examples/sentiment/scripts/gpt-neox-20b_peft/clm_finetune_peft_imdb.py

Maybe slightly modifying this code could do the thing? I’m not home so I can’t run the training but this could possibly work"
xiao111,2023-05-30 10:52:24+00:00,comment,tiiuae/falcon-7b,"@cekal you can see this:https://huggingface.co/dfurman/falcon-7b-chat-oasst1/blob/main/finetune_falcon7b_oasst1_with_bnb_peft.ipynb 

wish can help you, and I had not  try this code."
JulianGerhard,2023-05-31 05:14:20+00:00,comment,tiiuae/falcon-7b,@cekal @xiao111  - I can confirm that the mentioned modification in the notebook actually works. I was able to finetune / further train falcon-7b with an instruction following strategy. Keep in mind that after training you need to merge the new weights back into the original model files in order to be able to set `trust_remote_code` to True.
zhangbo2008,2023-05-31 07:30:38+00:00,comment,tiiuae/falcon-7b,thanks for sharing . i am so xxx cause my colab only 15g gpu not work for  4biteint_mode.
utensil,2023-06-01 06:16:23+00:00,comment,tiiuae/falcon-7b,"I have tried fine-tuning falcon 7b with qlora using axolotl, and it seems to work: https://github.com/OpenAccess-AI-Collective/axolotl/pull/132

If you encounter any issue with the config or spot any problems in the config, please ping me in the PR. Thanks!"
sumegh,2023-06-02 11:52:00+00:00,comment,tiiuae/falcon-7b,"@cekal how did you get it to work finally ? I also tried modifying the Alpaca-LORA code by changing AutoTokenizer & AutoModelForCausalLM. 
Also changed lora_target_module to [""query_key_value""].

I get the error `ValueError: The length of enable_lora must divide out_features`

*EDIT - Fixed by updating the packages.*"
cekal,2023-06-03 13:59:47+00:00,comment,tiiuae/falcon-7b,@sumegh try this: https://huggingface.co/dfurman/falcon-7b-chat-oasst1/blob/main/finetune_falcon7b_oasst1_with_bnb_peft.ipynb
xiao111,2023-06-05 03:32:17+00:00,comment,tiiuae/falcon-7b,"> I have tried fine-tuning falcon 7b with qlora using axolotl, and it seems to work: https://github.com/OpenAccess-AI-Collective/axolotl/pull/132
> 
> If you encounter any issue with the config or spot any problems in the config, please ping me in the PR. Thanks!


hi, @utensil  I have compared qlora.yml and lora.yml on falcon 7B. The main difference seems to be only these fields
```
load_in_8bit: true
load_in_4bit: false
optimizer: paged_adamw_32bit
```
Is there any other difference?"
FalconLLM,2023-06-09 14:02:14+00:00,comment,tiiuae/falcon-7b,"If you are interested in finetuning the models, we would recommend having a look at [FalconTune](https://github.com/rmihaylov/falcontune) (which supports finetuning in 4-bit) or at this [blogpost from HF](https://huggingface.co/blog/falcon#fine-tuning-with-peft), specifically at the section on finetuning the model with PEFT."
vi-c,2023-07-11 15:28:36+00:00,comment,tiiuae/falcon-7b,"> @cekal how did you get it to work finally ? I also tried modifying the Alpaca-LORA code by changing AutoTokenizer & AutoModelForCausalLM. 
> Also changed lora_target_module to [""query_key_value""].
> 
> I get the error `ValueError: The length of enable_lora must divide out_features`
> 
> *EDIT - Fixed by updating the packages.*

hi sumegh, which package are you referring to ? I get the same error and don't know how to fix it."
sumegh,2023-07-11 15:40:30+00:00,comment,tiiuae/falcon-7b,"> > @cekal how did you get it to work finally ? I also tried modifying the Alpaca-LORA code by changing AutoTokenizer & AutoModelForCausalLM. 
> > Also changed lora_target_module to [""query_key_value""].
> > 
> > I get the error `ValueError: The length of enable_lora must divide out_features`
> > 
> > *EDIT - Fixed by updating the packages.*
> 
> hi sumegh, which package are you referring to ? I get the same error and don't know how to fix it.

I updated my CUDA version to 11.8 and re-installed all packages following the jupyter notebook as it is. It worked."
sahilmalhotra17,2023-09-15 15:11:03+00:00,comment,tiiuae/falcon-7b,"@FalconLLM  
Is there any literature published on the internal architecture of the decoder blocks and how they are organized? Are there any plans for a publication anytime soon?
I would like to experiment with only touching certain submodules (instead of all) with LoRA/QLoRA adapters during fine tuning for some understanding on how the self attention block and the mlp block across various decoder layers contribute to overall model performance."
virilo,2024-04-25 19:18:47+00:00,comment,tiiuae/falcon-7b,"Using the EOS token as the pad token was causing Hugging Face to remove the EOS token. So, as a workaround, I tried using a different token for padding. You can read more about it here: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi/77118714#77118714

However, it is still hallucinating at the end of the messages, so it doesn't seem to be a solution."
Netrve,2024-03-16 16:52:06+00:00,comment,tiiuae/falcon-7b,Would be interested to know as well.
AuryTaine,2023-12-06 09:28:10+00:00,comment,tiiuae/falcon-7b,"for better result, you have to use faclon-7b-instruct, because he can receive instruction to complete a task. "
etolampa,2023-12-06 13:46:52+00:00,comment,tiiuae/falcon-7b,"> for better result, you have to use faclon-7b-instruct, because he can receive instruction to complete a task.

Thanks for the response, I will try it out! :-)"
damerajee,2023-11-02 14:38:31+00:00,comment,tiiuae/falcon-7b,"So falcon 7b is a  causal decoder-only model , which means it's primarily design for autoregressive generation task , such as next token prediction  . Usually for machine translation you require both Encoder and Decoder . I mean you can try but it won't be as efficient 

here are some models you can use:
BART, BigBird-Pegasus, Blenderbot, BlenderbotSmall, Encoder decoder, FairSeq Machine-Translation, GPTSAN-japanese, LED, LongT5, M2M100, Marian, mBART, MT5, MVP, NLLB, NLLB-MOE, Pegasus, PEGASUS-X, PLBart, ProphetNet, SwitchTransformers, T5, UMT5, XLM-ProphetNet"
Pitambarmuduli,2023-11-02 14:45:22+00:00,comment,tiiuae/falcon-7b,"Yeah, I tried, but it's not as effective; sometimes it's translated, sometimes it's not. "
ditchtech,2023-10-04 02:32:54+00:00,comment,tiiuae/falcon-7b,You have to use FalconForCausalLM after latest changes I think.
Acrobatix,2023-09-23 20:50:42+00:00,comment,tiiuae/falcon-7b,"You might want to load the model this way:
```
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=""auto"", offload_folder=""offload"")
```"
vsrinivas,2023-09-24 03:00:32+00:00,comment,tiiuae/falcon-7b,"That is how I have loaded the model already. 
```model = AutoModelForCausalLM.from_pretrained(
    checkpoint, device_map=""auto"", offload_folder=""offload"", trust_remote_code=True,)```"
vinayaru,2023-12-26 16:33:19+00:00,comment,tiiuae/falcon-7b,Any solution to this problem please?
Norine,2024-05-15 08:33:12+00:00,comment,tiiuae/falcon-7b,"Hello Vinayaru , did you find a solution for the problem , i have the same error , using the same model.

Best regards
Noureddine"
nkasmanoff,2023-09-19 11:46:20+00:00,comment,tiiuae/falcon-7b,"The generation_config.json file was using the wrong tokens for BOS and EOS, I've updated it here. See https://huggingface.co/tiiuae/falcon-180B/blob/main/generation_config.json for example with the same tokenizer but correct token IDs."
HormyAJP,2023-09-18 02:37:37+00:00,comment,tiiuae/falcon-7b,"I'm also having similar problems. I just used the example code from: https://huggingface.co/tiiuae/falcon-7b. I get the following error:

```
2023-09-18 10:34:37.337197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File ""/Users/badger/dev/transformers/falcoln7b.py"", line 8, in <module>
    pipeline = transformers.pipeline(
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/__init__.py"", line 824, in pipeline
    framework, model = infer_framework_load_model(
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py"", line 282, in infer_framework_load_model
    raise ValueError(
ValueError: Could not load model tiiuae/falcon-7b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:

while loading with AutoModelForCausalLM, an error is thrown:
Traceback (most recent call last):
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py"", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py"", line 558, in from_pretrained
    return model_class.from_pretrained(
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3175, in from_pretrained
    ) = cls._load_pretrained_model(
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3296, in _load_pretrained_model
    raise ValueError(
ValueError: The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.

while loading with TFAutoModelForCausalLM, an error is thrown:
Traceback (most recent call last):
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/pipelines/base.py"", line 269, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
  File ""/Users/badger/dev/transformers/.veenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py"", line 566, in from_pretrained
    raise ValueError(
ValueError: Unrecognized configuration class <class 'transformers_modules.tiiuae.falcon-7b.f7796529e36b2d49094450fb038cc7c4c86afa44.configuration_RW.RWConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.
Model type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.
```

Note that I've added `offload_folder=""offload""` to the `AutoTokenizer.from_pretrained` call and that doesn't get rid of the `offload_folder` error. 

It's hard to tell from the error which issue is causing which. I suspect one of the errors is likely spurious and is simply a knock on effect of the other error.

Anyone any ideas?"
HormyAJP,2023-09-18 03:23:25+00:00,comment,tiiuae/falcon-7b,"I think the fix is simple. Just put `model = AutoModelForCausalLM.from_pretrained(model)` after `tokenizer = AutoTokenizer.from_pretrained(model, offload_folder=""offload"", torch_dtype=torch.bfloat16)` and that should fix it. "
Yugal5,2023-09-06 12:58:38+00:00,comment,tiiuae/falcon-7b,"Yeah, I am also facing same issue. Tried all options available on HF, Stackoverflow ,etc
"
codegood,2023-09-29 01:06:00+00:00,comment,tiiuae/falcon-7b,"Use previous version of transformers.
!pip install transformers==4.30"
Thiliniiw,2023-09-30 19:54:12+00:00,comment,tiiuae/falcon-7b,"If you are in a notebook, restarting the session worked for me. See below.
https://github.com/huggingface/transformers/issues/23323#issuecomment-1568464656"
Saptarshi7,2023-09-12 18:43:35+00:00,comment,tiiuae/falcon-7b,"I'm not sure you would be able to use HF pipeline for Falcon for this kind of QA since Falcon is a decoder-only model not expected to produce start/end logits. Do you get any errors when you run?
```
model = AutoModelForQuestionAnswering.from_pretrained(
    pretrained_model_name_or_path=model_name,
    cache_dir=""/mnt/tmp"",
    trust_remote_code=True
)
```"
hozaifa99,2023-08-01 13:06:29+00:00,comment,tiiuae/falcon-7b,"Interesting to know the answer.

But i think it is due to the optimizations by HuggingFace.
"
adam-zettafi,2023-07-28 14:52:06+00:00,comment,tiiuae/falcon-7b,It would appear that you are putting some configuration in where it's just expecting a model name. Sharing some of your code would make determining what you're trying to do easier.
adam-zettafi,2023-07-14 20:35:31+00:00,comment,tiiuae/falcon-7b,Are you attempting to open a tensor flow version of the model? The error leads me to believe you are not. Try removing `from_tf=True`.
AmlanSamanta,2023-07-13 17:36:46+00:00,comment,tiiuae/falcon-7b,"Same issue faced by me. This commit is very disappointing. I think we can think of 2 below workarounds 

A. Can we use the missing file from local storage where the script is executing from?

B. Also if  can  load a previous revision of a model commit in the above code. Then I think we can bypass this."
sgugger,2023-07-13 18:38:28+00:00,comment,tiiuae/falcon-7b,"You can use the `code_revision` argument to specify a specific revision (so commit for instance) for the code files. Since we are migrating this model into Transformers, it's probably safer to do this until the integration is finished when using `""ybelkada/falcon-7b-sharded-bf16""`."
AmlanSamanta,2023-07-14 02:12:54+00:00,comment,tiiuae/falcon-7b,"I also tried similar approach biut failed yesterday. May I know in which line I can use the argument you highlighted. This is my code.

import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig

peft_model_id = ""mrm8488/falcon-7b-ft-codeAlpaca_20k-v2""
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map = {"""":0}, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(peft_model_id)

model = PeftModel.from_pretrained(model, peft_model_id)
model.eval()"
amiiin,2023-10-02 12:56:54+00:00,comment,tiiuae/falcon-7b,I got this same error again today
Draculhard,2024-01-06 11:02:03+00:00,comment,tiiuae/falcon-7b,"am still getting the error, can someone please help me out here is my code 
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
"
psinger,2023-07-13 07:03:34+00:00,comment,tiiuae/falcon-7b,"11 is the correct id, it was wrong before"
envizzion,2023-07-13 05:39:47+00:00,comment,tiiuae/falcon-7b,"I'm getting the same error . 

this is causing all the models which are based off this (ex: ybelkada/falcon-7b-sharded-bf16) to stop loading. "
AmlanSamanta,2023-07-13 17:34:10+00:00,comment,tiiuae/falcon-7b,"Same issue faced by me. This commit is very disappointing.I think we can think of 2 below workarounds 

A. Can we use the missing file from local storage where the script is executing from?

B. Also if  can  load a previous revision of a model commit in the above code. Then I think we can bypass this."
AmlanSamanta,2023-07-13 17:35:25+00:00,comment,tiiuae/falcon-7b,"> I'm getting the same error . 
> 
> this is causing all the models which are based off this (ex: ybelkada/falcon-7b-sharded-bf16) to stop loading.

My code also referring this repo. Thanks for pointing it out "
ditchtech,2023-10-01 14:55:33+00:00,comment,tiiuae/falcon-7b,Using FalconForCausalLM seems to work as the other configuration file was removed.
amiiin,2023-10-02 13:29:40+00:00,comment,tiiuae/falcon-7b,"@ditchtech Thank you, using FalconForCausalLM worked for me as well"
ashioyajotham,2023-10-05 09:57:04+00:00,comment,tiiuae/falcon-7b,Yes @ditchtech worked for  me too. Thank you :)
xyan13,2023-10-08 08:45:35+00:00,comment,tiiuae/falcon-7b,"> Yes @ditchtech worked for  me too. Thank you :)

may I know how did you use FalconForCausalLM? I can't seem to find how to import it
"
ChSalman,2023-10-11 08:48:02+00:00,comment,tiiuae/falcon-7b,"> > Yes @ditchtech worked for  me too. Thank you :)
> 
> may I know how did you use FalconForCausalLM? I can't seem to find how to import it

import FalconForCausalLM

@ditchtech thanks a million :) ! "
zayuki,2024-01-11 08:26:30+00:00,comment,tiiuae/falcon-7b,"> Using FalconForCausalLM seems to work as the other configuration file was removed.

from transformers import FalconForCausalLM
model_name_or_path = ""huggingface/falcon-40b-gptq""

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

model = FalconForCausalLM.from_pretrained(model_name_or_path,
                                             device_map=""auto"",
                                             trust_remote_code=True,
                                             revision=""main"")

It works. Thanks @ditchtech "
Rocketknight1,2023-07-07 15:23:58+00:00,comment,tiiuae/falcon-7b,Transitions to an in-library checkpoint - please don't merge until we've merged the main Falcon PR!
maxolotl,2023-09-01 06:33:45+00:00,comment,tiiuae/falcon-7b,"Also having this issue, have any luck solving it?"
tdicommons,2023-09-02 19:34:00+00:00,comment,tiiuae/falcon-7b,"Yes, you need to use right Tokenizer, 
 model_id = ""tiiuae/falcon-7b"" # our finetuned model from HF here.
----> 8 tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)

2- You need to use right version of Transformers, we have used 
pip install -U git+https://github.com/huggingface/transformers.git@e03a9cc && \
    pip install -U git+https://github.com/huggingface/peft.git@42a184f && \
    pip install -U git+https://github.com/huggingface/accelerate.git@c9fbb71 && \
    pip install einops==0.6.1 && \
    pip install torch==2.0.1 && \
    pip install bitsandbytes==0.39.0 && \
    pip install scipy && \
    pip install loralib==0.1.1 && \"
stevejlock,2023-07-12 14:31:49+00:00,comment,tiiuae/falcon-7b,"I have exactly the same thing. I used two different tutorials with different code both based on the fine-tuning in the falcon 7b blog post and one repeats additional questions and answers over and over (almost like it's returning one Q&A, then related Q&A). The other code I tried today repeats over and over like this too.

I'd really like some help and to see an answer on this too please and I'll update if I find a solution.

Frustratingly the first tutorial I followed showed a video with perfect inference afterwards and the code from checking looks identical.

If it's of interest there is a similar thread on chatgpt here: https://wordpress.org/support/topic/chatgpt-repeats-his-sentence-over-and-over-after-finde-tuning-a-model/ I will also be checking out embeddings.

This article also on chatgpt mentions could be to do with training data formating: https://community.openai.com/t/fine-tuned-a-davinci-model-repeats-phrases-in-answers/54737/4."
amdnsr,2023-07-03 10:15:54+00:00,comment,tiiuae/falcon-7b,"did you solve it, @Imran1?"
Imran1,2023-07-03 10:20:23+00:00,comment,tiiuae/falcon-7b,"Yeah
"
amdnsr,2023-07-07 10:13:38+00:00,comment,tiiuae/falcon-7b,"oh, what did you do?"
Imran1,2023-07-07 17:44:15+00:00,comment,tiiuae/falcon-7b,@amdnsr I add a few line of code  ...
Imran1,2023-07-07 17:44:51+00:00,comment,tiiuae/falcon-7b,"But the inference are very very slow 
"
amdnsr,2023-07-08 05:39:54+00:00,comment,tiiuae/falcon-7b,"Can you please provide the code? 
"
Imran1,2023-07-08 06:24:21+00:00,comment,tiiuae/falcon-7b,"> Can you please provide the code?

Sure"
cmp-nct,2023-06-28 23:07:23+00:00,comment,tiiuae/falcon-7b,This comment has been hidden
meyerjoseph,2023-06-29 16:16:09+00:00,comment,tiiuae/falcon-7b,Getting the same error
meyerjoseph,2023-06-29 16:47:42+00:00,comment,tiiuae/falcon-7b,"@NajiAboo by setting device_map to auto it fixed the issue. I am using this script: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14?permalink_comment_id=4600438

https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14?permalink_comment_id=4605181#gistcomment-4605181"
vk-dell,2023-07-10 15:28:25+00:00,comment,tiiuae/falcon-7b,"having the same error, any help
"
vk-dell,2023-07-10 16:09:02+00:00,comment,tiiuae/falcon-7b,"model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=""auto"",
    trust_remote_code=True
)

its working now thanks, have to update     device_map=""auto"",
 as shown above, "
NajiAboo,2023-07-12 11:31:09+00:00,comment,tiiuae/falcon-7b,"I tried changing the device_map, 

But I am getting below error now 

ValueError: You can't train a model that has been loaded in 8-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device() or device_map={'':torch.xpu.current_device()}


suggestion please

"
harjinder1988,2023-07-31 20:13:09+00:00,comment,tiiuae/falcon-7b,Running into similar issue. Any update on this?
ssm1990,2023-09-06 14:06:59+00:00,comment,tiiuae/falcon-7b,"Same error at my end. Resolved for the time being by not passing the bitsandbytes, version 0.40.0."
Saptarshi7,2023-09-12 18:46:54+00:00,comment,tiiuae/falcon-7b,"Yes, I'm very curious about this as well. I don't really know what RWmodel is apart from Refined-Web corpus model. However, I did see in one files' comments that Falcon is perhaps a BLOOM checkpoint."
Pranjal95,2023-06-21 18:26:29+00:00,comment,tiiuae/falcon-7b,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
beejay,2023-06-22 20:31:48+00:00,comment,tiiuae/falcon-7b,I don't believe Falcon can do embedding. It is a text generation model and not an embedding model.
alchemistPS01,2024-03-01 10:22:15+00:00,comment,tiiuae/falcon-7b,"can anoyone is there to answer the above question @sk2mm2 @beejay . guys anyone able fix this?
"
ajmalsiddiqui,2023-06-18 18:09:12+00:00,comment,tiiuae/falcon-7b,"Please use hugging face text generation inference, not sure falcon 7b is supported. if supported then use its generate_stream function to get the same behavour."
kronus86,2023-06-19 05:53:44+00:00,comment,tiiuae/falcon-7b,Thanks for you reply. I checked and it does support Flacon 7B and 40B models.
Leo,2023-06-28 20:00:03+00:00,comment,tiiuae/falcon-7b,+1
morpheus73,2023-06-15 08:01:50+00:00,comment,tiiuae/falcon-7b,I think we can fine-tune the model using masked/causal language modelling techniques given unlabelled text. Here the labels would be the next word token or masked tokens. I am trying to do the same for one of the pretrained models but haven't gotten sufficient enough memorisation of training data. So not sure how well it works.
Mollel,2023-06-21 21:44:16+00:00,comment,tiiuae/falcon-7b,"Any solution?
"
rbgreenway,2023-06-21 22:40:56+00:00,comment,tiiuae/falcon-7b,"> Any solution?

Not by me.  I wonder if I've got a square peg/round hole problem here.  Everywhere I look, fine tuning on these LLMs is done with labeled data.  I'm very new to all this, so I'm not sure if I'm even asking good questions.  I've got another potential application for falcon that I'm banging my head on, too.  This stuff is incredible, but not for the faint at heart.
"
Mollel,2023-06-22 11:27:13+00:00,comment,tiiuae/falcon-7b,"So far, the only solution I have found is for older models like mT5 and BERT. You can find more information and examples in the following link:

[GitHub - Hugging Face Transformers Examples](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/README.md)

If anyone has a link for unsupervised training specifically for Falcon, please let us know. I would greatly appreciate it!"
Gulab,2023-07-29 09:24:15+00:00,comment,tiiuae/falcon-7b,Have you found any relevant information about this? Like fine-tune model on cpu?
Jaimeadz,2023-06-11 11:01:42+00:00,comment,tiiuae/falcon-7b,Hello can you could be to do what
Ruqiya,2023-06-11 11:34:07+00:00,comment,tiiuae/falcon-7b,"> Hello can you could be to do what

I would like Falcon team convert Pytorch to TensorFlow weights.
Then I'll work to add them to KerasNLP ."
vivixzc,2023-07-26 05:53:55+00:00,comment,tiiuae/falcon-7b,"I got the same issue, did you find a way to solve this?"
xtliu,2023-07-27 17:19:22+00:00,comment,tiiuae/falcon-7b,"I just use expand operation in the code piece above to make it the same shape
"
LeoArtaza,2023-06-12 13:04:09+00:00,comment,tiiuae/falcon-7b,"I think you can use any embeddings, maybe I was getting confused with tokenizing."
FalconLLM,2023-06-09 14:06:29+00:00,comment,tiiuae/falcon-7b,See [this discussion](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/2) for a solution :) 
konze,2023-06-07 09:39:35+00:00,comment,tiiuae/falcon-7b,you can download all models file in your local and then give the path of it
idksamk,2023-06-08 08:02:07+00:00,comment,tiiuae/falcon-7b,"could you give me the code on how to do that, I tried but it's not working. if not, is there a different way to speed up the model when executing the code?"
Apimash,2024-02-19 00:07:11+00:00,comment,tiiuae/falcon-7b,"> could you give me the code on how to do that, I tried but it's not working. if not, is there a different way to speed up the model when executing the code?

To download a models or the entire repo, use Git/TortoiseGit. Very easy
git lfs install
git clone git@hf.co:<MODEL ID> # example: git clone git@hf.co:bigscience/bloom
"
FalconLLM,2023-06-09 13:59:24+00:00,comment,tiiuae/falcon-7b,"Thanks, we already merged a similar PR just now :). "
FalconLLM,2023-06-09 13:58:56+00:00,comment,tiiuae/falcon-7b,"Hi @mb-data96, unfortunately none of us in the Falcon team are very familiar with TensorFlow. We will leave this issue open should someone from the community want to contribute the TF weights."
mb-data96,2023-06-09 14:34:34+00:00,comment,tiiuae/falcon-7b,"Hi @FalconLLM, thank you for answer.

I finally bypass the .bin restriction in my organization via the following steps (if they might help someone in a similar case) :
- Download on my personnal laptop.
- Compress it into a .7z file (.zip didn't seem to work fine on my computer).
- Upload it in my private HuggingFace Model Hub.
- Download it as a .7z file in my organization and uncompress it. (Existing .bin files are not restricted in my organization, only incoming/downloaded ones are restricted) 

Regards,"
Ruqiya,2023-06-11 08:25:16+00:00,comment,tiiuae/falcon-7b,"Hi @FalconLLM,
I am tryin to convert it but it will so easy from your side
I think it is just 2 lines  https://huggingface.co/transformers/v3.3.1/model_sharing.html#make-your-model-work-on-all-frameworks
I need TensorFlow to contribute by adding  it in KerasNLP check here:
https://github.com/keras-team/keras-nlp/issues/1072"
FalconLLM,2023-06-09 13:57:06+00:00,comment,tiiuae/falcon-7b,"We have merged this change on the Instruct versions of Falcon; for the base versions of Falcon, we recommend further finetuning before deploying them."
lukaga,2023-06-03 05:46:13+00:00,comment,tiiuae/falcon-7b,"Same here :(

```
    277 
    278         if isinstance(model, str):
--> 279             raise ValueError(f""Could not load model {model} with any of the following classes: {class_tuple}."")
    280 
    281     framework = ""tf"" if ""keras.engine.training.Model"" in str(inspect.getmro(model.__class__)) else ""pt""

ValueError: Could not load model tiiuae/falcon-7b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).

```"
minggnim,2023-06-06 06:11:56+00:00,comment,tiiuae/falcon-7b,"Adding this line before pipeline should fix it
`model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)`"
varun500,2023-06-06 14:52:20+00:00,comment,tiiuae/falcon-7b,"While loading it in 8bit the model gives this error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[5], line 3
      1 from transformers import AutoModelForCausalLM
----> 3 model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-7b"",trust_remote_code=True,load_in_8bit = True,device_map='auto')

File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:462, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    458     class_ref = config.auto_map[cls.__name__]
    459     model_class = get_class_from_dynamic_module(
    460         class_ref, pretrained_model_name_or_path, **hub_kwargs, **kwargs
    461     )
--> 462     return model_class.from_pretrained(
    463         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    464     )
    465 elif type(config) in cls._model_mapping.keys():
    466     model_class = _get_model_class(config, cls._model_mapping)

File /opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2828, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   2826 # Dispatch model with hooks on all devices if necessary
   2827 if device_map is not None:
-> 2828     dispatch_model(model, device_map=device_map, offload_dir=offload_folder, offload_index=offload_index)
   2830 if output_loading_info:
   2831     if loading_info is None:

TypeError: dispatch_model() got an unexpected keyword argument 'offload_index'"
FalconLLM,2023-06-09 13:56:02+00:00,comment,tiiuae/falcon-7b,"Base on similar issues in `accelerate`, you might need to upgrade your version of the library by running `pip install --upgrade accelerate`: https://github.com/huggingface/peft/issues/186"
Andi2022HH,2023-06-12 09:40:49+00:00,comment,tiiuae/falcon-7b,"I am getting the following error where I already changed to float32:

File ""falcon_7b.py"", line 32, in <module>
    pipeline_ = pipeline(""text-generation"", model=model_folder, device=0, trust_remote_code=True, model_kwargs={""torch_dtype"": torch.float32})
  File ""python3.10/site-packages/transformers/pipelines/__init__.py"", line 727, in pipeline
    framework, model = infer_framework_load_model(
  File ""python3.10/site-packages/transformers/pipelines/base.py"", line 266, in infer_framework_load_model
    raise ValueError(f""Could not load model {model} with any of the following classes: {class_tuple}."")
ValueError: Could not load model ../models/falcon_7b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
demsking,2023-06-16 07:55:43+00:00,comment,tiiuae/falcon-7b,"I have similar issue even after adding:
```py
model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)
```
Error:
```rs
HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is
96: 'RWForCausalLM(
  (transformer): RWModel(
    (word_embeddings): Embedding(65024, 4544)
    (h): ModuleList(
      (0-31): 32 x DecoderLayer(
        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)
        (self_attention): Attention(
          (maybe_rotary): RotaryEmbedding()
          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)
          (dense): Linear(in_features=4544, out_features=4544, bias=False)
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (mlp): MLP(
          (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)
          (act): GELU(approximate='none')
          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)
        )
      )
    )
    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)
)'.
```"
archonlith,2023-08-14 15:56:36+00:00,comment,tiiuae/falcon-7b,This comment has been hidden
dimaischenko,2023-06-01 10:10:59+00:00,comment,tiiuae/falcon-7b,"To add more clarity. Here is my generation cycle

```python
device = torch.device(""cuda"")
model_id = ""tiiuae/falcon-7b""

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
).to(device)

tokenizer = AutoTokenizer.from_pretrained(model_id)

text = ""Hello there. How are""

inputs = tokenizer(text, return_tensors=""pt"").to(device)
input_ids = inputs[""input_ids""]

output = None
step = 0

# generation cycle with 10 steps
while step < 10:
    attention_mask = input_ids.new_ones(input_ids.shape)

    past_key_values = None    
    if output is not None:
        past_key_values = output[""past_key_values""]
       
    ids = model.prepare_inputs_for_generation(input_ids,
                                              past=past_key_values,
                                              use_cache=True,
                                              attention_mask=attention_mask)
    output = model(**ids)
    
    # get random of 3 most probable tokens and add to input_ids
    top_k = 3
    next_token = random.choice(torch.topk(output.logits[:, -1, :], top_k, dim=-1).indices[0])
    
    input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(device)], dim=-1)
    
    step += 1
```

```python
print(tokenizer.decode(input_ids[0]))
```

```
Hello there. How are!
,
. I

.<|endoftext|>
```

P.S. 

I commented out this check in `modelling_RW.py` in `prepare_inputs_for_generation ` method

```python
'''
# the cache may be in the stardard format (e.g. in contrastive search), convert to our's format if needed
if past[0][0].shape[0] == input_ids.shape[0]:
    past = self._convert_to_rw_cache(past)
'''
```

Otherwise an error in the tensor dimensions will fall out"
terrencefm,2023-06-02 17:36:44+00:00,comment,tiiuae/falcon-7b,Same problem!
dimaischenko,2023-06-03 06:57:36+00:00,comment,tiiuae/falcon-7b,"@FalconLLM I would be very grateful if you could tell if `past_key_values` is supposed to be used in the generation, or if this logic is not implemented? Perhaps it can be added or there are some fundamental limitations? After all its use significantly speeds up the time of inference"
dimaischenko,2023-06-06 08:43:56+00:00,comment,tiiuae/falcon-7b,@FalconLLM Or maybe you can suggest a specialist from your team who would help sort out this issue? I will be very grateful!
JunnanLi,2023-06-08 08:33:36+00:00,comment,tiiuae/falcon-7b,"Same problem, appreciate some suggestions from @FalconLLM !"
ColmanTT,2023-06-08 16:49:44+00:00,comment,tiiuae/falcon-7b,"It appears to me that RotaryEmbeddings obtains ""sequence_length"" from q input, which will be 1 when using KV cache. This makes embeddings incorrect.

I resolved this by passing in the position_id of the current token I'm generating with the following. Although our embeddings now match what we see without KV cache, our results are still garbage.
```
    def cos_sin(
        self,
        seq_len: int,
        device=""cuda"",
        dtype=torch.bfloat16,
        position=None
    ) -> torch.Tensor:
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            # t = torch.arange(seq_len, device=device).type_as(self.inv_freq)
            t = torch.arange(position, device=device).type_as(self.inv_freq)
            freqs = torch.einsum(""i,j->ij"", t, self.inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1).to(device)

            if dtype in [torch.float16, torch.bfloat16]:
                emb = emb.float()

            self.cos_cached = emb.cos()[None, :, :]
            self.sin_cached = emb.sin()[None, :, :]

            self.cos_cached = self.cos_cached.type(dtype)
            self.sin_cached = self.sin_cached.type(dtype)

        return (self.cos_cached[:, -1:, :], self.sin_cached[:, -1:, :]) if position != seq_len else (self.cos_cached, self.sin_cached)

    def forward(self, q, k, position):
        # q: q_new, b*nh x q_len x d
        # k: k_new, b*nh x q_len x d
        # position: true position index of these tokens
        # These aren't the true position ids of the tokens
        batch, seq_len, head_dim = q.shape

        cos, sin = self.cos_sin(seq_len, q.device, q.dtype, position)
        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
```"
dimaischenko,2023-06-08 17:00:52+00:00,comment,tiiuae/falcon-7b,"@ColmanTT  There was also this hypothesis [https://huggingface.co/tiiuae/falcon-40b/discussions/48](https://huggingface.co/tiiuae/falcon-40b/discussions/48#64807969bb25a636c9da2cd7) (in `40b` discussions) , we discussed and tested it with @cchudant . But the results are also garbage"
ColmanTT,2023-06-08 17:33:46+00:00,comment,tiiuae/falcon-7b,"Btw, the way I'm attempting to run with KV cache is like this:
```
result = model(input_ids=input_ids, past_key_values=past_key_values, attention_mask=None, position_ids=None, use_cache=True, return_dict=True)
```

First iteration, input_ids contains prompt and past_key_values is None. Subsequent iterations, input_ids contains only the new token, and past_key_values is piped back into the model."
LevanKvirkvelia,2023-06-08 23:30:20+00:00,comment,tiiuae/falcon-7b,I think HF fixed the problem in their own repo https://github.com/huggingface/text-generation-inference/blob/abd58ff82c37d5e4f131abdac3d298927a815604/server/text_generation_server/models/custom_modeling/flash_rw_modeling.py#L99
dimaischenko,2023-06-09 14:25:43+00:00,comment,tiiuae/falcon-7b,@LevanKvirkvelia Did you success? Do you propose to replace the `Attention` class in `falcon` model by `FlashRWAttention` from `HF`?
Tron2060,2023-06-13 10:32:23+00:00,comment,tiiuae/falcon-7b,"@ColmanTT .   I got pretty output after I changed code like this:
```
    def forward(self, q, k, seq_len):
        # batch, seq_len, head_dim = q.shape
        _,q_len,_ = q.shape
        cos, sin = self.cos_sin(seq_len, q.device, q.dtype)
        cos = cos[:,-q_len:]
        sin = sin[:,-q_len:]

        cos_np = cos.detach().cpu().float().numpy()
        sin_np = sin.detach().cpu().float().numpy()
        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)

```
and also change the code as below:
```
 if layer_past is not None:
                L = query_layer_.shape[-2]
                S = key_layer_.shape[-2]
                attn_mask = torch.ones(L, S, dtype=torch.bool, device=query_layer_.device)
                attn_output = F.scaled_dot_product_attention(
                    query_layer_, key_layer_, value_layer_, attn_mask, 0.0, is_causal=False
                )
            else:
                attn_output = F.scaled_dot_product_attention(
                    query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True
                )

```"
dimaischenko,2023-06-13 12:13:28+00:00,comment,tiiuae/falcon-7b,"@Tron2060  Please can you explain how do you pass new arguments to `RotaryEmbedding `, `forward(self, q, k, seq_len)`

The old way:

```
query_layer, key_layer = self.maybe_rotary(query_layer, key_layer)
```

There is not `seq_len` in this context.  I changed it to:

```
query_layer, key_layer = self.maybe_rotary(query_layer, key_layer, fused_qkv.shape[1])
```

In fact, I get something more or less readable, but it still seems to be very far from normal model generation. Perhaps I misused  `RotaryEmbedding `"
Tron2060,2023-06-14 00:35:29+00:00,comment,tiiuae/falcon-7b,"@dimaischenko 
I pass the arguments by this way:
```
         _, seq_len, _ = query_layer.shape
        if layer_past is not None:
            _,seq_len_past,_=layer_past[0].shape

            seq_len=seq_len+seq_len_past

        query_layer, key_layer = self.maybe_rotary(query_layer, key_layer, seq_len)
```"
afcruzs,2023-07-28 06:54:32+00:00,comment,tiiuae/falcon-7b,"This will all get fixed eventually in the transformers GitHub code
https://github.com/huggingface/transformers/issues/25151#issuecomment-1654062690"
FalconLLM,2023-05-31 19:18:48+00:00,comment,tiiuae/falcon-7b,"Hi @aa33, all repositories should now adequately reflect that the models are licensed under Apache 2.0 🔥"
Tucakov,2023-06-01 17:34:42+00:00,comment,tiiuae/falcon-7b,+1
samtube405,2023-06-01 18:42:11+00:00,comment,tiiuae/falcon-7b,same issue.
ujjwalagr,2023-06-27 05:57:54+00:00,comment,tiiuae/falcon-7b,Same happening with as well
vkajjam,2023-06-27 17:48:17+00:00,comment,tiiuae/falcon-7b,+1
Kunjesh07,2023-08-07 06:14:38+00:00,comment,tiiuae/falcon-7b,I've same issue when I try 4096 max_length but I got the output in 200 max_length
lucasjin,2023-05-31 02:25:40+00:00,comment,tiiuae/falcon-7b,"@FalconLLM Hey, please fix your problem before close my issue, othewise your behaviour might against huggingface community rules."
FalconLLM,2023-05-31 04:11:25+00:00,comment,tiiuae/falcon-7b,"The issue has been fixed, please consider treating others with respect. "
lucasjin,2023-05-30 07:43:11+00:00,comment,tiiuae/falcon-7b,"Yes, this is really redicoulous."
dimaischenko,2023-05-30 08:50:43+00:00,comment,tiiuae/falcon-7b,"I agree too, and actually don't understand what we have to choose"
lucasjin,2023-05-30 09:25:51+00:00,comment,tiiuae/falcon-7b,"@tiiuae Please avoid upload a wrong model (wrong tokenizer), this will missleading lots of people ."
lucasjin,2023-05-31 02:26:56+00:00,comment,tiiuae/falcon-7b,"@FalconLLM Please fix the issue, or at least post some explain on this, otherwise your behaviour might against hugginface community rules.
Users might get confused by your uploaded model. And **this is not good for you as well**."
dimaischenko,2023-05-31 03:43:08+00:00,comment,tiiuae/falcon-7b,"@lucasjin they fixed  `config.json`

```
  ""bos_token_id"": 11,
  ""eos_token_id"": 11,
```"
lucasjin,2023-05-31 04:03:25+00:00,comment,tiiuae/falcon-7b,"@dimaischenko OK, but this still make me confused, why bos is 11? Very strange"
lucasjin,2023-05-31 04:07:49+00:00,comment,tiiuae/falcon-7b,And the bos same as eos...... Very strange....
FalconLLM,2023-05-30 14:57:34+00:00,comment,tiiuae/falcon-7b,"Hey, this discussion might be helpful: https://huggingface.co/tiiuae/falcon-40b-instruct/discussions/4#6475abcb09e773226333c550 "
FalconLLM,2023-05-30 06:54:48+00:00,comment,tiiuae/falcon-7b,"Sorry about the delay, the `The model 'RWForCausalLM' is not supported for text-generation` comes from the model not being integrated into the core part of the transformers library yet. It's just a warning, and generation should follow afterwards.  See for example: https://twitter.com/camenduru/status/1662225039352283137?s=20 of a video where it is working correctly. 

It will take a little bit of time to integrate the model fully into the transformers library, but hopefully in a couple of weeks this warning will go away."
FalconLLM,2023-06-09 14:07:25+00:00,comment,tiiuae/falcon-7b,See [this discussion](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/2) for a solution 👍
zhangbo2008,2023-05-29 08:13:43+00:00,comment,tiiuae/falcon-7b,definitely not useful model. need more pretrain or RLHF
cekal,2023-05-29 08:22:53+00:00,comment,tiiuae/falcon-7b,Maybe because this is the base model and you have to finetune it on a dataset. Try doing that.
zhangbo2008,2023-05-29 08:26:21+00:00,comment,tiiuae/falcon-7b,"I wonder if lora is useful on finetune the model to a useful model,  or must use full-weight finetune."
FalconLLM,2023-05-30 06:16:55+00:00,comment,tiiuae/falcon-7b,"@cekal is right, this sort of behaviour is expected from the base model has it not been finetuned to take in instructions or have conversations. You should check-out [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) instead 😊."
FalconLLM,2023-05-30 06:56:21+00:00,comment,tiiuae/falcon-7b,"Thanks for the report, should be resolved with: https://huggingface.co/tiiuae/falcon-7b/commit/1ba2370c784b56f8b31afc66d5234e8fb40a7209.
Though note that inference with datatypes other than bfloat16 has not been fully validated and may incur some model degradation. 


Let us know if you have any more issues."
DanielHesslow,2023-05-26 16:21:14+00:00,comment,tiiuae/falcon-7b,"It requires at least 14GB of ram, smallest I've tried is an A10 which works well. 

It's also trained with bf16 which is only available on Ampere and later, I would expect some performance degradation if running it in fp16 instead."
bilelm,2023-05-26 20:02:53+00:00,comment,tiiuae/falcon-7b,"Thank you!
I will try it on a A5000 24Go, hope it's ok."
zynos,2023-05-27 09:20:36+00:00,comment,tiiuae/falcon-7b,"I am running the given code on Windows (CPU with 32 GB RAM) but it keeps running for 2 hours+ without printing the results?
Someone have an idea on how to solve this?"
Kamaljp,2023-05-27 12:57:14+00:00,comment,tiiuae/falcon-7b,"I am trying to load this model on Colab, but it doesn't load in the GPU. 
What am I missing? I am using the code provided in the model card and installing the transformers library. Still, model is not loading in GPU."
akashcollectiv,2023-05-28 20:06:58+00:00,comment,tiiuae/falcon-7b,"i Tried in 40G A100 , worked well , but slow , took about 10min for single input , 
got a 80G A100 , after loading check point , its crashed
return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity; 77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF``
what am i missing ?"
FalconLLM,2023-05-30 14:51:23+00:00,comment,tiiuae/falcon-7b,"@zynos it's unlikely you will get anything in a reasonable time on CPU, you really need a GPU for this sort of model. 

@akashcollectiv are you sure you are not trying to load Falcon-40B instead? The 7B should fit fine on an A100 80GB. "
rachelshalom,2023-06-04 07:54:47+00:00,comment,tiiuae/falcon-7b,all of you are working with pytorch2?
SpiketheCowboy,2023-06-06 09:48:35+00:00,comment,tiiuae/falcon-7b,"me too, getting OOM when sequence length exceeds 1200+
using A100 80GB, bf16, and inference only (no_grad) for 7B falcon model
and yes, I'm using pytorch 2.0

> i Tried in 40G A100 , worked well , but slow , took about 10min for single input , 
> got a 80G A100 , after loading check point , its crashed
> return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
> ```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity; 77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF``
> what am i missing ?"
Kamaljp,2023-06-06 14:58:35+00:00,comment,tiiuae/falcon-7b,Have you played with batch size? Halving the batch size seems to help
SpiketheCowboy,2023-06-06 15:30:48+00:00,comment,tiiuae/falcon-7b,"> Have you played with batch size? Halving the batch size seems to help

The batch size I run with is 1."
konze,2023-06-07 11:33:50+00:00,comment,tiiuae/falcon-7b,"anyone tell me minimum hardware requirements for the falcon-7b-instruct, i want use if for question answering with given context/Documents data."
pdakin,2023-06-09 19:37:04+00:00,comment,tiiuae/falcon-7b,"On Colab with model card inference call:

```
  sequences = pipeline(
    ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
      max_length=200,
      do_sample=True,
      top_k=10,
      num_return_sequences=1,
      eos_token_id=tokenizer.eos_token_id,
  )
```

A100 - 6966.189ms
V100 - 44117.912ms"
Seb83,2023-06-13 15:58:57+00:00,comment,tiiuae/falcon-7b,"> I am trying to load this model on Colab, but it doesn't load in the GPU. 
> What am I missing? I am using the code provided in the model card and installing the transformers library. Still, model is not loading in GPU.

@Kamaljp Runtime (heading in top toolbar between insert and tools) -> change runtime type -> select GPU under hardware accelerator"
lucianosb,2023-06-12 17:32:58+00:00,comment,nomic-ai/gpt4all-falcon,AFAIK this version is not compatible with GPT4ALL. It was trained to serve as base for a future quantized model that is not yet available as of today.
eshaanagarwal,2023-06-13 06:20:51+00:00,comment,nomic-ai/gpt4all-falcon,Then should i use it through HUgging face pipeline to  make it work as a LLM for chatbot
lucianosb,2023-06-13 11:04:43+00:00,comment,nomic-ai/gpt4all-falcon,That's right. It does work with huggingface tools. I published a [Google Colab](https://colab.research.google.com/github/lucianosb/falconllm-colabs/blob/main/notebooks/nomic_ai_gpt4all_falcon.ipynb) to demonstrate it. You'll probably need a paid colab subscription since it uses around 29GB of VRAM.
zpn,2023-06-15 14:10:44+00:00,comment,nomic-ai/gpt4all-falcon,"There is a PR for merging Falcon into GGML/llama.cpp so once that's finished, we will be able to use this within GPT4All: https://github.com/ggerganov/ggml/pull/231"
markding,2024-05-16 19:12:46+00:00,comment,openchat/openchat-3.5-0106,"I have the same question. The preprint says ShareGPT data ""collected from sharegpt.com"" and the [code](https://github.com/imoneoi/openchat/blob/master/ochat/data/generate_dataset.py) refers to ` sharegpt_gpt4.jsonl` . But it would be good if this were more clearly documented on the model card and in a code repository."
pauli31,2024-03-26 07:55:27+00:00,comment,openchat/openchat-3.5-0106,How can you just change the max positional embeddings?
pauli31,2024-03-26 08:59:53+00:00,comment,openchat/openchat-3.5-0106,btw https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/4#651e695b4b6b06ceb867d37d
rombodawg,2024-01-10 21:35:44+00:00,comment,openchat/openchat-3.5-0106,"Yea i agree, I was considering using this model in a mixtral-merge becuase of the scores but it would difficult considering the context constraints of only 8k. Making any other mistral model in the merge be limited to 8k despite being able to produce 32k tokens of content. "
carlos447,2024-01-12 09:31:24+00:00,comment,openchat/openchat-3.5-0106,+1
Citaman,2024-01-15 10:28:30+00:00,comment,openchat/openchat-3.5-0106,"I would say that the Mistral 7B V0.2 is not a pretrained model, but an instruction-tuned one, and therefore already has a bias towards the finetuning phase. For complete control over the model's performance, it is best to start from a pretrained model. That may be why."
riddlechen,2024-01-17 08:41:10+00:00,comment,openchat/openchat-3.5-0106,+1
SvCy,2024-01-22 13:58:19+00:00,comment,openchat/openchat-3.5-0106,"~nvm~
and i think they should definitely go beyond 7B parameters with openchat!"
count-zero,2024-03-27 12:31:21+00:00,comment,openchat/openchat-3.5-0106,"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 is fine-tuned on the base model `mistral-7B-v0.2`, which is now officially made available by Mistral AI:

**mistral-7B-v0.2**

- https://models.mistralcdn.com/mistral-7b-v0-2/mistral-7B-v0.2.tar (PyTorch)
- https://huggingface.co/alpindale/Mistral-7B-v0.2-hf (Safetensors)
- https://huggingface.co/bartowski/Mistral-7B-v0.2-hf-GGUF (GGUF)

I would love to see an OpenChat fine-tune based on `mistral-7B-v0.2` with a 32k context length."
Joseph717171,2024-04-03 05:01:15+00:00,comment,openchat/openchat-3.5-0106,"OpenChat team, I Depth Up-Scaled Mistral-7B-v0.2, following UpStage’s paper: [SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling](https://arxiv.org/abs/2312.15166), if you want to train OpenChat on a slightly bigger model. 

[Joseph717171/Mistral-10.7B-v0.2](https://huggingface.co/Joseph717171/Mistral-10.7B-v0.2)

- 32K Context Window
- 🚫 Sliding Window Attention"
rombodawg,2024-04-03 05:03:16+00:00,comment,openchat/openchat-3.5-0106,"@Joseph717171 Your too late bro, they dont care

https://huggingface.co/openchat/openchat-3.5-0106-gemma/discussions/4"
Joseph717171,2024-04-03 05:08:28+00:00,comment,openchat/openchat-3.5-0106,"Oh, well it was worth a shot. 😁"
wuweinero,2024-01-10 14:50:28+00:00,comment,openchat/openchat-3.5-0106,"> It seems to be doing what every other openchat model that I've tried has done. It freezes occasionally in the middle of a generation.

same issue."
mljxy,2024-01-10 15:35:33+00:00,comment,openchat/openchat-3.5-0106,"EOS token has to be set to ""<|end_of_turn|>"": 32000

This is not in the config.json, so gguf does not set it up by default."
imone,2024-01-10 16:14:47+00:00,comment,openchat/openchat-3.5-0106,We've updated config.json. Does this issue still happen?
wuweinero,2024-01-11 06:51:47+00:00,comment,openchat/openchat-3.5-0106,"> We've updated config.json. Does this issue still happen?

It seems all right now. A great model! Looking forward to a more advanced version."
imone,2024-01-12 03:22:53+00:00,comment,openchat/openchat-3.5-0106,Okay closing this issue now. Re-open if it persists.
Sirianth,2024-05-30 00:48:45+00:00,comment,vikhyatk/moondream2,Would love to deploy this for a game-engine use case.
harshad-say,2024-05-31 10:15:24+00:00,comment,vikhyatk/moondream2,The error message literally says it doesn't have einops package installed. Can you retry after installing the package?
WGNW,2024-05-29 09:31:10+00:00,comment,vikhyatk/moondream2,"and this is my test result with huggingface spaces. 
I think model can recognize the multilingual sentences, but it seems unable to read the text information.
![image.png](https://cdn-uploads.huggingface.co/production/uploads/64fddda7c45dd732aceb714a/SZbgcL9T5OzHvvnKYtLy7.png)
"
Deepfor,2024-05-28 19:12:27+00:00,comment,vikhyatk/moondream2,"""This is a..."", ""That's great"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm so excited"", ""I'm"
vikhyatk,2024-05-26 20:29:04+00:00,comment,vikhyatk/moondream2,Yes
vikhyatk,2024-05-25 04:19:26+00:00,comment,vikhyatk/moondream2,How are you running it locally? Using the transformers library or something else?
bermancheese,2024-05-25 20:52:48+00:00,comment,vikhyatk/moondream2,"i tryed using llama.cpp, kobold, llmstudio, llamafiles. tryed your gguf and others and all max quants where quants are the only options. im sure i misunderstand the basics or something since im kinda a noob."
vikhyatk,2024-05-22 08:09:24+00:00,comment,vikhyatk/moondream2,Thanks! Is this the dedicated endpoints feature? https://huggingface.co/inference-endpoints/dedicated
vikhyatk,2024-05-22 08:10:42+00:00,comment,vikhyatk/moondream2,Looks like it's pointing to your finetune btw
zesquirrelnator,2024-05-22 12:03:43+00:00,comment,vikhyatk/moondream2,"Yes, that's right! It's the dedicated endpoints feature for the HF platform. Also, sorry about that, I'll make sure to remove the fine-tuned model ;)"
vikhyatk,2024-05-22 19:59:37+00:00,comment,vikhyatk/moondream2,🫡
ali0une,2024-05-27 19:56:05+00:00,comment,vikhyatk/moondream2,alpaca
momac2487,2024-05-03 16:48:30+00:00,comment,vikhyatk/moondream2,you can change max_new_tokens to 1 in moondream.py. 
KeilahElla,2024-05-19 17:47:24+00:00,comment,vikhyatk/moondream2,"@vikhyatk , I see now that moondream is now on ollama library: https://www.ollama.com/library/moondream

Do you know which version is this? I prefer to use it through ollama because it's  much faster than transformers.  But I also want to use the latest version and do not want to fall behind."
Rendomman067,2024-05-24 18:39:09+00:00,comment,vikhyatk/moondream2,This comment has been hidden
Rendomman067,2024-05-24 18:39:54+00:00,comment,vikhyatk/moondream2,This comment has been hidden
vikhyatk,2024-05-25 04:20:07+00:00,comment,vikhyatk/moondream2,"> @vikhyatk , I see now that moondream is now on ollama library: https://www.ollama.com/library/moondream
> 
> Do you know which version is this? I prefer to use it through ollama because it's  much faster than transformers.  But I also want to use the latest version and do not want to fall behind.

It may be an older version actually, I’m not sure how it gets updated. Will try to find out. "
vikhyatk,2024-04-25 19:36:01+00:00,comment,vikhyatk/moondream2,Waiting on this pull request to get merged: https://github.com/ggerganov/llama.cpp/pull/6899
ali0une,2024-05-27 19:50:33+00:00,comment,vikhyatk/moondream2,"it works with llama.cpp :
``/whatever/llama.cpp/llava-cli -m /whatever/models/moondream2/moondream2-text-model-f16.gguf --mmproj /whatever/models/moondream2/moondream2-mmproj-f16.gguf --image /whatever/picture.jpg -p ""describe the image"" --temp 0.1 -c 2048``

Works also with LM_Studio, just create a directory moondream2 with the two gguf files in your local model directory, mine is /home/alioune/LMStudio/models/alioune/local/ 
Use alpaca preset, set temp to 0.1, upload a picture, prompt for ""describe image"" ... profit!"
fusi0n,2024-05-28 04:23:05+00:00,comment,vikhyatk/moondream2,"> it works with llama.cpp :
> ``/whatever/llama.cpp/llava-cli -m /whatever/models/moondream2/moondream2-text-model-f16.gguf --mmproj /whatever/models/moondream2/moondream2-mmproj-f16.gguf --image /whatever/picture.jpg -p ""describe the image"" --temp 0.1 -c 2048``
> 
> Works also with LM_Studio, just create a directory moondream2 with the two gguf files in your local model directory, mine is /home/alioune/LMStudio/models/alioune/local/ 
> Use alpaca preset, set temp to 0.1, upload a picture, prompt for ""describe image"" ... profit!

awesome! thanks!"
vikhyatk,2024-04-21 00:20:01+00:00,comment,vikhyatk/moondream2,"Can definitely look into doing this, in the mean time if you are loading the git repository you can list the tags."
not-lain,2024-04-21 00:40:17+00:00,comment,vikhyatk/moondream2,"@Hangover3832 There's no need, you can look it up in the dropdown menu
you can think of revision as a branch and/or a commit ( revision parameter can load both )

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6527e89a8808d80ccff88b7a/7lK330MrhQxzQlaYjHtuE.png)
"
not-lain,2024-04-21 00:46:23+00:00,comment,vikhyatk/moondream2,"now that I reread this, yess a file might be a better choice to stay up to date"
Hangover3832,2024-04-21 08:14:12+00:00,comment,vikhyatk/moondream2,"To clarify, I use this repo in my custom ComfyUI node (https://github.com/Hangover3832/ComfyUI-Hangover-Moondream). It is not only about to stay up to date, I want the user to be able to switch to an older version if they desire. I don't want to include all the branches/tags available in this repo, but rather a list with the official releases (according to the model card) that will be loaded when the node gets initialized.

> Can definitely look into doing this, in the mean time if you are loading the git repository you can list the tags.

Thank you. A simple text file would do the job, for example:
2024-03-04
2024-03-06
..."
vikhyatk,2024-04-23 03:36:39+00:00,comment,vikhyatk/moondream2,"Understood, just added `versions.txt`, will keep it updated any time a new version is released."
Hangover3832,2024-04-23 16:33:10+00:00,comment,vikhyatk/moondream2,"That's handy, thank you!"
vikhyatk,2024-04-19 06:31:06+00:00,comment,vikhyatk/moondream2,The error message is saying you're out of disk space... one option would be to set the HF_HOME environment variable to a folder on a drive that has free space.
vikhyatk,2024-04-10 03:46:11+00:00,comment,vikhyatk/moondream2,"It needs to be passed during generation, not when initializing the model. Example:

```
for i in range(2):
    answer = moondream.answer_question(
        image_embeds, prompt, tokenizer, temperature=0.1, do_sample=True
    )
    print(answer)
```

Output:

> The image features a computer server rack, which is a large metal structure designed to hold and organize multiple computer components, such as motherboards, cooling systems, and other peripherals. The rack is filled with various computer parts, including multiple computer chips, and is placed on wheels, allowing for easy movement and rearrangement of the components. The server rack is situated in a room, possibly a living room or a workspace, and is surrounded by other electronic equipment and devices.
>
> The image features a large, black computer server rack with multiple computer boards and components on it. The rack is designed to hold and organize various electronic devices, such as computer boards, wires, and other peripherals. It is placed on a carpeted floor, indicating that it is likely in a room or workspace. The rack is filled with a variety of computer boards, suggesting that it may be used for testing, development, or maintenance purposes."
bensw,2024-04-10 06:12:36+00:00,comment,vikhyatk/moondream2,thanks its all working good now
vikhyatk,2024-03-31 21:08:12+00:00,comment,vikhyatk/moondream2,Try upgrading timm: `pip install -U timm`
omarei,2024-04-09 21:45:17+00:00,comment,vikhyatk/moondream2,same here. updated timm and not working. what version should timm be ?
omarei,2024-04-09 21:51:00+00:00,comment,vikhyatk/moondream2,ok i restarted comfyu and it worked now. Cheers @vikhyatk 
vikhyatk,2024-04-10 03:18:06+00:00,comment,vikhyatk/moondream2,"The latest version no longer has a dependency on timm, so we won't be seeing this issue any more."
vikhyatk,2024-03-29 05:53:01+00:00,comment,vikhyatk/moondream2,How are you doing the Qlora finetuning? Would appreciate reproduction steps. :)
nicolollo,2024-04-02 17:26:17+00:00,comment,vikhyatk/moondream2,"> How are you doing the Qlora finetuning? Would appreciate reproduction steps. :)

me too XD"
rockerBOO,2024-05-01 22:21:14+00:00,comment,vikhyatk/moondream2,"Load in 4bit and then the regular PEFT model addition. Needs bitsandbytes package as well.
```
    nf4_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type=""nf4"",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

 model = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        revision=revision,
        quantization_config=nf4_config,
    )
```

```
target_modules = [""mixer.Wqkv""]
config = LoraConfig(
        r=1,
        lora_alpha=1,
        lora_dropout=0.05,
        bias=""none"",
        task_type=""CAUSAL_LM"",
        # target specific modules (look for Linear in the model)
        # print(model) to see the architecture of the model
        target_modules=target_modules,
    )
model = get_peft_model(model, config)
```"
vikhyatk,2024-03-11 10:05:19+00:00,comment,vikhyatk/moondream2,"What version of torchvision do you have installed? (You can check by running ""pip show torchvision"")"
NadimAnamul,2024-03-24 09:58:11+00:00,comment,vikhyatk/moondream2,"> What version of torchvision do you have installed? (You can check by running ""pip show torchvision"")

Name: torchvision
Version: 0.16.2"
rxavier,2024-04-12 16:34:59+00:00,comment,vikhyatk/moondream2,"Hey, I'm running into the same issue. As I understand it, `ToImage` was introduced in torchvision 0.16 (I'm running 0.15.2 due to my CUDA version). I think it could be replaced with the functional `F.to_image`. I'll try to find time to test it.

Edit:
Fixed using torchvision 0.15.2 by changing the preprocessor in `vision_encoder.py`

```
        self.preprocess = Compose(
            [
                Resize(size=(378, 378), interpolation=InterpolationMode.BICUBIC, antialias=True),
                ToTensor(),
                ConvertImageDtype(torch.float32),
                Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
            ]
```"
vikhyatk,2024-03-14 00:31:31+00:00,comment,vikhyatk/moondream2,Can you try out the latest version (weights released today)? Should be better at descriptions. If it’s still being too terse please share some of the images and I’ll look into what’s going on. 
Wackedout1,2024-03-14 16:40:48+00:00,comment,vikhyatk/moondream2,"Ok I got the new model working, it was better with the Mario dance image, I got ...
[moondream1]
{""result"":""The image shows a group of people, including a man dressed as Mario, dancing to music. The man in the center of the image is wearing a white shirt and blue jeans, and he is jumping up and down while holding his hands up in the air. The other people in the group are also dancing and enjoying the music. The scene captures the fun and energetic atmosphere of a party or a dance event.""}
[moondream2] 3-13-24
{""result"":""The image is a digital animation of a group of people dancing on a colorful floor. The animation features a character, likely a character from the popular Mario franchise, dancing in the center of the image. The other characters in the scene are also dancing, creating a lively and energetic atmosphere. The animation is likely designed to be visually appealing and engaging for the audience.""}

But with the lady on the golf course image from the previous post I still got a ""tennis racket"" ...
[moondream2] 3-13-24
{""result"":""The woman in the image is wearing a blue dress and holding a tennis racket. She is posing for a picture on a golf course, surrounded by trees and bushes. The setting appears to be a beautiful outdoor location, perfect for a leisurely day of tennis or golf.""}




![Mario_Dance1 (resized).png](https://cdn-uploads.huggingface.co/production/uploads/63543dbd6fd9f0793025cd2d/szhGIO4hi9tdSj99CNre2.png)

![MV5BNGI4MWEwMTctODY1Ni00YjAxLTllOWEtY2IzNjk2MGQxNjE5XkEyXkFqcGdeQXVyMTI1MTg5NTgw._V1_FMjpg_UX1000_.jpg](https://cdn-uploads.huggingface.co/production/uploads/63543dbd6fd9f0793025cd2d/yG1BJJvAuxjhHjA1dTWLx.jpeg)
"
Shesquipped,2024-03-06 08:56:50+00:00,comment,vikhyatk/moondream2,This comment has been hidden
kam414,2024-03-06 08:57:16+00:00,comment,vikhyatk/moondream2,I also want to finetune
ArthurFischel,2024-03-08 20:48:25+00:00,comment,vikhyatk/moondream2,"I'm also interested in how to fine tune this. I tried using the default script for fine tuning lava with the moondream2 model and the vision backbone, and got a cannot find vision tower error."
lucesgabriel,2024-03-14 12:04:02+00:00,comment,vikhyatk/moondream2,"Same question !!!
"
dcdseqma12,2024-03-17 04:12:17+00:00,comment,vikhyatk/moondream2,"i also would like to finetuning this model.
Thanks"
NickyNicky,2024-03-25 04:50:28+00:00,comment,vikhyatk/moondream2,https://github.com/vikhyat/moondream/blob/main/notebooks/Finetuning.ipynb
NickyNicky,2024-03-25 05:03:15+00:00,comment,vikhyatk/moondream2,!pip install accelerate
NickyNicky,2024-03-25 05:04:17+00:00,comment,vikhyatk/moondream2,thank you so much
vikhyatk,2024-03-26 00:09:35+00:00,comment,vikhyatk/moondream2,thanks updated to include the `accelerate` dependency which I had missed before.
nold,2024-03-05 12:43:21+00:00,comment,vikhyatk/moondream2,Sadly it's not supported by Llama.cpp yet :(
ctranslate2-4you,2024-03-05 14:00:37+00:00,comment,vikhyatk/moondream2,"I was wondering if it's possible to use bitsandbytes with this model?  What about bettertransformer?  And what about flash attention 2?  I understand that it's not currently quantized with llama.cpp because it's so new, but if the architecture supports it, I'd like to be able to use bitsandbytes and/or bettertransformer at least...Here's a sample script of how I use those with llava:

```
if chosen_model == 'llava' and chosen_quant == 'float16':
            model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                resume_download=True
            ).to(device)
        elif chosen_model == 'llava' and chosen_quant == '8-bit':
            model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                load_in_8bit=True,
                resume_download=True
            )
        elif chosen_model == 'llava' and chosen_quant == '4-bit':
            model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
                load_in_4bit=True,
                resume_download=True
            )
        elif chosen_model == 'bakllava' and chosen_quant == 'float16':
            model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                resume_download=True
            ).to(device)
        elif chosen_model == 'bakllava' and chosen_quant == '8-bit':
            model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                load_in_8bit=True,
                resume_download=True
            )
        elif chosen_model == 'bakllava' and chosen_quant == '4-bit':
            model = LlavaForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
                load_in_4bit=True,
                resume_download=True
            )
```"
Vedank,2024-03-05 14:17:41+00:00,comment,vikhyatk/moondream2,Did you try this code with Moondream?
ctranslate2-4you,2024-03-05 14:42:48+00:00,comment,vikhyatk/moondream2,I did but I didn't see any difference in vram usage...it didn't throw an error though...I tried 4-bit and 8-bit both...Do you have any experience with this sort of thing?
Aptronym,2024-03-06 01:19:35+00:00,comment,vikhyatk/moondream2,"Seeing how, as I understand it, this is multiple models merged together, what if you quantized them first and then merged them? Or at least quantized the biggest part?
"
vikhyatk,2024-03-06 08:48:07+00:00,comment,vikhyatk/moondream2,I have integration with llama.cpp implemented locally but there's a bug in my image encoder implementation somewhere - just need to figure out how to debug it. Will look into Flash Attention and bitsandbytes as well shortly.
Vedank,2024-03-06 09:20:44+00:00,comment,vikhyatk/moondream2,"Can you link any resources you are following, Would love to help"
Vedank,2024-03-06 09:21:24+00:00,comment,vikhyatk/moondream2,"> I did but I didn't see any difference in vram usage...it didn't throw an error though...I tried 4-bit and 8-bit both...Do you have any experience with this sort of thing?

I dont actual, can you share the complete code that you executed"
ctranslate2-4you,2024-03-06 10:55:52+00:00,comment,vikhyatk/moondream2,"> I dont actual, can you share the complete code that you executed

I tried different variations of instantiating the ""model"" using the above bitsandbytes configurations:

```
class loader_moondream:
    def initialize_model_and_tokenizer(self):
        device = get_best_device()
        model = AutoModelForCausalLM.from_pretrained(""vikhyatk/moondream2"", 
                                             trust_remote_code=True, 
                                             revision=""2024-03-05"", 
                                             torch_dtype=torch.float16, 
                                             low_cpu_mem_usage=True, 
                                             resume_download=True).to(device)

        tokenizer = AutoTokenizer.from_pretrained(""vikhyatk/moondream2"", revision=""2024-03-05"")
        
        return model, tokenizer, device
    
    def moondream_process_images(self):
        script_dir = os.path.dirname(__file__)
        image_dir = os.path.join(script_dir, ""Docs_for_DB"")
        documents = []
        allowed_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tif', '.tiff']
        
        image_files = [file for file in os.listdir(image_dir) if os.path.splitext(file)[1].lower() in allowed_extensions]
        
        if not image_files:
            return []
            
        model, tokenizer, device = self.initialize_model_and_tokenizer()
        
        total_start_time = time.time()
        
        with tqdm(total=len(image_files), unit=""image"") as progress_bar:
            for file_name in image_files:
                full_path = os.path.join(image_dir, file_name)
                try:
                    with Image.open(full_path) as raw_image:
                        enc_image = model.encode_image(raw_image)
                        summary = model.answer_question(enc_image, ""Describe in detail what this image depicts in as much detail as possible."", tokenizer)
                        extracted_metadata = extract_image_metadata(full_path)
                        
                        document = Document(page_content=summary, metadata=extracted_metadata)
                        documents.append(document)

                        progress_bar.update(1)

                except Exception as e:
                    print(f""{file_name}: Error processing image - {e}"")
                    
        total_end_time = time.time()
        total_time_taken = total_end_time - total_start_time
        print(f""Total image processing time: {total_time_taken:.2f} seconds"")

        del model
        del tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

        return documents
```"
tomeras1,2024-05-05 13:50:13+00:00,comment,ai21labs/Jamba-v0.1,
diwank,2024-04-30 14:29:25+00:00,comment,ai21labs/Jamba-v0.1,"You can find a few from the community here:
https://huggingface.co/models?pipeline_tag=text-generation&sort=likes&search=jamba

This one is my personal favorite:
https://huggingface.co/Severian/Jamba-Hercules"
eitanturok,2024-05-03 20:33:26+00:00,comment,ai21labs/Jamba-v0.1,You have to pay for it: https://www.ai21.com/blog/announcing-jamba-instruct
freQuensy23,2024-04-07 09:13:57+00:00,comment,ai21labs/Jamba-v0.1,"torch.cuda.is_avalibale() retruns true

GPU: A100 80gb"
freQuensy23,2024-04-07 09:17:08+00:00,comment,ai21labs/Jamba-v0.1,I think that .to('cuda') will fix this
ChristianPalaArtificialy,2024-04-06 08:48:30+00:00,comment,ai21labs/Jamba-v0.1,"I have the same issues, despite installing mamba-ssm, both with pip and from git, and causal-conv1d I get this warning:
04/06/2024 10:45:22 - WARNING - transformers_modules.ai21labs.Jamba-v0.1.8ee14c3ece13be2d26f81fd42f5c29b89a84d846.modeling_jamba - T
he fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mam
ba_inner_fn)` is None. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal
-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config 
which then gives the value error above once the training loop starts.
Is it an issue with torch 2.2?"
glamprou,2024-04-07 15:42:10+00:00,comment,ai21labs/Jamba-v0.1,"If you use device_map=""auto"" then the Mamba layer may be loaded in the CPU to fit the VRAM, you cant use fast kernels on the CPU, it only works for CUDA"
badrabbitt,2024-04-15 03:33:01+00:00,comment,ai21labs/Jamba-v0.1,"It took me a week to fix this error, but then I discovered that the causal-conv1d library has a kernel that only runs on linux, I tried installing it with ubuntu and wsl2 and the results were easy."
TheTee,2024-04-18 13:12:54+00:00,comment,ai21labs/Jamba-v0.1,"I also fixed the problem in my environment. I installed 'accelerate' package (pip install accelerate) and set the parameter 'device_map' of AutoModelForCausalLM.from_pretrained to ""cuda"" (device_map=""cuda""). In general I use torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu118
"
roicohennn,2024-04-04 09:21:44+00:00,comment,ai21labs/Jamba-v0.1,"Hi @jingyu-q ,

As explained in the paper, the throughput in the graph is measured as token/second, so it's on a per token basis. Generation time increases with longer context, but normalized to a per token basis, it's faster, and much faster for Jamba as context window get big."
jingyu-q,2024-04-04 20:13:34+00:00,comment,ai21labs/Jamba-v0.1,"Hi @roicohennn ,

If it is on per token basis, and as the setting says the output fixed at 512 tokens, and the generation time increases with longer context, shouldn't throughput (=512tokens/time_taken) be smaller with longer context? 

"
kanttouchthis,2024-04-06 16:31:52+00:00,comment,ai21labs/Jamba-v0.1,the only thing i can imagine looking at the graph is that it's output tokens per second times number of input tokens divided by some constant. i suggest the graph be labeled more clearly as to what throughput (t/s) actually means
nacs,2024-04-05 12:07:39+00:00,comment,ai21labs/Jamba-v0.1,llama.cpp doesn't have Jamba support yet so GGUF won't be available till then
roicohennn,2024-04-03 13:11:35+00:00,comment,ai21labs/Jamba-v0.1,"Thank you @danielpark !

The model can fit on a single A100 with 80GB memory.

We intend to release smaller variations of Jamba that were used as indicative experiments (not fully trained)"
danielpark,2024-04-04 09:04:58+00:00,comment,ai21labs/Jamba-v0.1,"Thank you for your prompt and kind assistance. I was impressed by the fast and impressive architecture of AI21. I wanted to test it on Colab, but since getting nearly 80GB of A100 allocation is like winning the lottery, I had to give up. Other instances are either too expensive or too difficult to use, so I couldn't consider them. 

I was planning to use a specialized model once I got the 80GB, but it would be great if AI21 could release the weights of Jamba 7B initialized as quickly as possible.

I'm preparing research based on the Jamba architecture.

Thank you."
ordagan,2024-04-04 11:11:48+00:00,comment,ai21labs/Jamba-v0.1,"Hi @exhyy , thanks!
Not at the moment"
alvations,2024-04-01 19:00:46+00:00,comment,ai21labs/Jamba-v0.1,"I've tried A100 on colab but it looks like there's still some bugs in the `accelerate` auto mappings, https://colab.research.google.com/drive/1T0fhyP963DHJDjUNrPMScD0L9uDfOj-w?usp=sharing

When initializing the SFTTrainer, it throws the error:

```python
/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-7-b027ad8b6132> in <cell line: 1>()
----> 1 trainer = SFTTrainer(
      2     model=model,
      3     tokenizer=tokenizer,
      4     args=training_args,
      5     peft_config=lora_config,

12 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in convert(t)
   1148                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
   1149                             non_blocking, memory_format=convert_to_format)
-> 1150             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
   1151 
   1152         return self._apply(convert)

NotImplementedError: Cannot copy out of meta tensor; no data!
```"
alvations,2024-04-01 19:07:09+00:00,comment,ai21labs/Jamba-v0.1,"And I think it's also complaining about moving models when accelerate have offloaded some parameters:

```python
/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-19-5df240c1e5f7> in <cell line: 3>()
      1 import torch
      2 
----> 3 trainer = SFTTrainer(
      4     model=model,
      5     train_dataset=valid_dataset,

3 frames
/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py in wrapper(*args, **kwargs)
    451                 for param in model.parameters():
    452                     if param.device == torch.device(""meta""):
--> 453                         raise RuntimeError(""You can't move a model that has some modules offloaded to cpu or disk."")
    454                 return fn(*args, **kwargs)
    455 

RuntimeError: You can't move a model that has some modules offloaded to cpu or disk.

```

E.g. https://colab.research.google.com/drive/1T0fhyP963DHJDjUNrPMScD0L9uDfOj-w?usp=sharing"
alvations,2024-04-01 20:16:16+00:00,comment,ai21labs/Jamba-v0.1,"After some tinkering and using 4bits as per https://github.com/Pleias/Various-Finetuning/blob/main/finetuning_jamba.py , it runs!!

Example: https://colab.research.google.com/drive/1EK-PeLXfO1oOxSY5zlRmVvOzBPrYnp-d?usp=sharing


Installs
====

```
! pip install -U pip
! pip install -U transformers==4.39.2
! pip install causal-conv1d mamba-ssm
! pip install accelerate peft bitsandbytes trl
! pip install -U datasets sacrebleu evaluate 
! pip install -U flash_attn
```

Code
====

```python
from datasets import load_dataset
from trl import SFTTrainer
from peft import LoraConfig
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments,  BitsAndBytesConfig
import mamba_ssm


quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    llm_int4_skip_modules=[""mamba""] #Maybe not necessary (per axoltl) but to test.
)

tokenizer = AutoTokenizer.from_pretrained(""ai21labs/Jamba-v0.1"")


model = AutoModelForCausalLM.from_pretrained(
    ""ai21labs/Jamba-v0.1"",
    trust_remote_code=True, 
    device_map='auto',
    attn_implementation=""flash_attention_2"", 
    quantization_config=quantization_config, 
    use_mamba_kernels=True
    )


from datasets import load_dataset

valid_data = load_dataset(""facebook/flores"", ""eng_Latn-deu_Latn"", streaming=False, split=""dev"")

# From https://stackoverflow.com/q/78156752/610569
def preprocess_func(row):
  return {'text': ""Translate from English to German: <s>[INST] "" + row['sentence_eng_Latn'] + "" [INST] "" + row['sentence_deu_Latn'] + "" </s>""}

valid_dataset = valid_data.map(preprocess_func)

valid_dataset['text'][-5:]

training_args = TrainingArguments(
    output_dir=""./results"",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    optim = ""adamw_8bit"",
    max_grad_norm = 0.3,
    weight_decay = 0.001,
    warmup_ratio = 0.03,
    gradient_checkpointing=True,
    logging_dir='./logs',
    logging_steps=1,
    max_steps=50,
    group_by_length=True,
    lr_scheduler_type = ""linear"",
    learning_rate=2e-3
)
lora_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.05,
    init_lora_weights=False,
    r=8,
    target_modules=[""embed_tokens"", ""x_proj"", ""in_proj"", ""out_proj""],
    task_type=""CAUSAL_LM"",
    bias=""none""
)


trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    peft_config=lora_config,
    train_dataset=valid_dataset,
    max_seq_length = 256,
    dataset_text_field=""text"",
)


trainer.train()
```

"
Alex18,2024-04-02 00:49:05+00:00,comment,ai21labs/Jamba-v0.1,"Can you please share the specification of the GPU device you were able to run the above fine tuning script? I am having problem loading the model into memory even when using AWS SageMaker g5.16xlarge
"
alvations,2024-04-02 01:57:53+00:00,comment,ai21labs/Jamba-v0.1,It's an A100 instance on colab. So you'll need p4/p5 instance on AWS
zhoutongfu,2024-04-02 23:11:56+00:00,comment,ai21labs/Jamba-v0.1,@alvations any specific reason you set max_grad_norm as 0.3?
alvations,2024-04-02 23:31:20+00:00,comment,ai21labs/Jamba-v0.1,"I've followed https://github.com/Pleias/Various-Finetuning/blob/main/finetuning_jamba.py

But I'm seeing the loss zero out real fast after 200+ steps so definitely there's a lot of room for ""student gradient descent"" I.e. hyperpameters search"
mber,2024-03-31 06:38:21+00:00,comment,ai21labs/Jamba-v0.1,"great job and wonderful implementation! just fixing a small typo in the comments of the shapes of the tensors, since the intermediate dimension is still in shape[1] in that line."
tomeras1,2024-03-31 08:34:14+00:00,comment,ai21labs/Jamba-v0.1,Thanks!
rombodawg,2024-03-30 18:49:52+00:00,comment,ai21labs/Jamba-v0.1,Add humaneval scored curtesy of TechxGenus testing
naston,2024-04-05 02:13:27+00:00,comment,ai21labs/Jamba-v0.1,"No one has tested SSMs with the 1.58Bit strategy. It is likely that research would need to be done on Mamba before it is done on Jamba as the BitNet only creates BitLinear and not BitConv1D (yet) and we don't know how something like this will perform. Then when you consider the fact that Mamba already provides efficient inference and it is more likely that other methods (speculative decoding for example) are used for more efficient inference before this kind of quantization aware training. All that being said it could still prove to be an interesting line of research, but it is not just low-hanging fruit."
Pclanglais,2024-03-30 20:38:26+00:00,comment,ai21labs/Jamba-v0.1,"Also currently on it.
I agree it's not so easy so far to find the right balance for lr. Their example is way to high but conversely axolotl has defaulted on a value way too low. Currently trying in the 2e-4 - 5e-4 zone.

Would say given the capacity of this model, the most interesting dataset could be for long document summarization and other long context related tasks."
Severian,2024-03-31 10:27:23+00:00,comment,ai21labs/Jamba-v0.1,"Thanks for your input! Seems we've landed on similar conclusions and settings. Here are the most current best settings I am getting with Jamba over the Open-Hermes dataset:

```py
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=[""embed_tokens"", ""x_proj"", ""in_proj"", ""out_proj""],
    lora_dropout=0.2,  
    task_type=""CAUSAL_LM"",
    bias=""none""
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    dataset_text_field=""text"",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=TrainingArguments(
        num_train_epochs=1,
        lr_scheduler_type='linear',
        learning_rate=2e-5,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        gradient_checkpointing=True,
        warmup_steps=10,  
        weight_decay=0.2,  
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,  
        save_steps=100, 
        output_dir=""outputs"",
        optim=""paged_adamw_8bit"",
        seed=42,
    ),
)
```"
lowfreak,2024-03-31 11:21:18+00:00,comment,ai21labs/Jamba-v0.1,Following
ordagan,2024-03-31 21:27:52+00:00,comment,ai21labs/Jamba-v0.1,Thanks @jefferylovely!
galco,2024-03-29 09:45:09+00:00,comment,ai21labs/Jamba-v0.1,"Hi !
Try to upgrade transformers version "
Severian,2024-03-29 11:19:08+00:00,comment,ai21labs/Jamba-v0.1,You sometimes will have to build from source for the transformers (I did in colab) and a session restart. It'll definitely work though with the latest transformers release 
Trustee,2024-04-06 04:25:26+00:00,comment,ai21labs/Jamba-v0.1,definitely works for me since upgrade to 4.39.3. had the same error beforehand.
NouRed,2024-04-09 01:22:17+00:00,comment,ai21labs/Jamba-v0.1,`!pip install git+https://github.com/huggingface/transformers.git --quiet ` then restart the kernel
hjewr,2024-04-10 09:24:31+00:00,comment,ai21labs/Jamba-v0.1,"thanks, it works"
Pclanglais,2024-03-29 09:44:34+00:00,comment,ai21labs/Jamba-v0.1,Same. Had to disable them in my finetune demo (but may be the cause of crazy loss jump…)
ajibawa-2023,2024-03-30 04:40:18+00:00,comment,ai21labs/Jamba-v0.1,I am also facing same problem.
Pclanglais,2024-03-30 20:35:30+00:00,comment,ai21labs/Jamba-v0.1,Ok found an indirect fix: import mamba-ssm before doing anything with the model. If there is an issue (for instance version conflict with another library) it will be immediately outputted and bring the possibility to correct it.
MohamedRashad,2024-03-31 03:07:40+00:00,comment,ai21labs/Jamba-v0.1,@Pclanglais Didn't change anything
Pclanglais,2024-03-31 09:33:57+00:00,comment,ai21labs/Jamba-v0.1,"@MohamedRashad when doing import mamba-ssm , do you have any error message?
Could come otherwise from causal-conv1d, a key dependency that may also run into compilation issues."
MohamedRashad,2024-03-31 12:11:39+00:00,comment,ai21labs/Jamba-v0.1,"@Pclanglais No, it gives me no errors"
Muneyoshi,2024-03-31 14:21:19+00:00,comment,ai21labs/Jamba-v0.1,I met same problem and found that causal-conv1d install cuda12 version via pip so I built from source
Muneyoshi,2024-03-31 14:23:49+00:00,comment,ai21labs/Jamba-v0.1,"To investigate, you need to remove try-catch of import causal-conv1d not one of import mamba-ssm"
SpiridonSunRotator,2024-04-01 13:05:02+00:00,comment,ai21labs/Jamba-v0.1,"Have you tried to install causal-conv1d and mamba from source?
`pip instal git+https://github.com/Dao-AILab/causal-conv1d`
`pip instal git+https://github.com/state-spaces/mamba`"
ajibawa-2023,2024-04-01 18:21:01+00:00,comment,ai21labs/Jamba-v0.1,Thanks @SpiridonSunRotator ! It worked for me.
ordagan,2024-04-04 10:56:59+00:00,comment,ai21labs/Jamba-v0.1,"We didn't do anything like that (and currently not in our plans, unfortunately), but would be thrilled if someone was to implement this into llama.cpp. "
Pclanglais,2024-03-29 07:38:49+00:00,comment,ai21labs/Jamba-v0.1,"Yes. I think the model can hold in 4bit for inference on Google Colab Pro+ (A100, 40g) but you’ll have to wait for more developments in quantization for anything usable on the free version."
Kv-boii,2024-03-29 09:48:44+00:00,comment,ai21labs/Jamba-v0.1,"is there any free notebook service that provide higher gpu than google colab free version
"
Pclanglais,2024-03-29 07:37:05+00:00,comment,ai21labs/Jamba-v0.1,I second this. Loading the weights take a really long time. Some light version (with pruning?) even if the end results is not effective at all would be great for quick testing iteration.
ozymandias-kingofkingsesq,2024-03-29 10:27:28+00:00,comment,ai21labs/Jamba-v0.1,I third this
TechxGenus,2024-03-30 17:27:03+00:00,comment,ai21labs/Jamba-v0.1,"I trained a Jamba architecture model with some code data. It's very small and has some basic code generation capabilities. Might be useful for this.
https://huggingface.co/TechxGenus/Mini-Jamba"
compilade,2024-03-31 02:26:54+00:00,comment,ai21labs/Jamba-v0.1,"> I trained a Jamba architecture model with some code data. It's very small and has some basic code generation capabilities. Might be useful for this.
> https://huggingface.co/TechxGenus/Mini-Jamba

Nice! Unfortunately, there seems to be no Mamba+MoE layer(s) in your model. I only see Mamba+MLP layers alternated with Attention+MoE layers. The `attn_layer_offset` and `attn_layer_period` keys in `config.json` differ from those in the official Jamba-v0.1 model, and might have caused this at training time, I guess?"
TechxGenus,2024-03-31 11:53:18+00:00,comment,ai21labs/Jamba-v0.1,"> > I trained a Jamba architecture model with some code data. It's very small and has some basic code generation capabilities. Might be useful for this.
> > https://huggingface.co/TechxGenus/Mini-Jamba
> 
> Nice! Unfortunately, there seems to be no Mamba+MoE layer(s) in your model. I only see Mamba+MLP layers alternated with Attention+MoE layers. The `attn_layer_offset` and `attn_layer_period` keys in `config.json` differ from those in the official Jamba-v0.1 model, and might have caused this at training time, I guess?

Ah, this is because I set `expert_layer_offset` and `expert_layer_period` to be the same as `attn_layer_offset` and `attn_layer_period`. I wanted to first test the results of using MoE only in the Attention layer when making this version.

I will make a new version with Mamba+MoE, Mamba+MLP, Attention+MoE, Attention+MLP at the same time later."
TechxGenus,2024-03-31 12:53:39+00:00,comment,ai21labs/Jamba-v0.1,https://huggingface.co/TechxGenus/Mini-Jamba-v2
ordagan,2024-04-24 06:52:18+00:00,comment,ai21labs/Jamba-v0.1,"Hi, we uploaded this version for debugging and development purposes (random weights, no training whatsoever)
https://huggingface.co/ai21labs/Jamba-tiny-random"
ricofix,2024-03-29 03:01:34+00:00,comment,ai21labs/Jamba-v0.1,"It hasn't been task finetuned at all, so it would probably make sense to await something like EVOL-Instruct, Chain of Code, or w/e is most current this week to be applied to the base model before doing a code eval."
rombodawg,2024-03-29 03:08:50+00:00,comment,ai21labs/Jamba-v0.1,@ricofix base coding performance is just as important as any other type of eval. Trust me you want to bench it before its finetuned
TechxGenus,2024-03-30 17:24:38+00:00,comment,ai21labs/Jamba-v0.1,HumanEval Score is about 32.9%.
rombodawg,2024-03-30 18:46:46+00:00,comment,ai21labs/Jamba-v0.1,Thanks @TechxGenus You are the GOAT
Severian,2024-03-29 01:14:00+00:00,comment,ai21labs/Jamba-v0.1,"Been working on a fine-tune in Colab (A100) with Transformers/Unsloth and having some decent results. I was able to load it in 4Bit and cram it onto 40GB. Starts rough but is still learning

Training has been progressing slow but fine so far 

```
 [3134/5850 3:03:46 < 2:39:22, 0.28 it/s, Epoch 5.35/10]
Step	Training Loss
1	10.109800
2	9.924600
3	9.919700
4	9.919100
5	9.917400
6	9.895900
7	9.891700
8	9.893500
9	9.917200
10	9.918800
11	10.056100
12	9.916200
13	9.911200
14	9.884300
15	9.909800
16	9.883800
17	9.883800
18	9.878300
19	9.904400
20	9.976400
21	10.061600
22	10.063300
23	9.876200
24	9.890900
25	9.873100
26	9.893700

85	9.790100
86	9.840400
87	9.809500
88	9.860000
89	9.807000
90	9.948200
91	9.779500
92	9.781800
93	9.802700
94	9.827700
95	9.798000
96	9.825900
97	9.966000
98	9.773000
99	9.775400

271	9.531600
272	9.555500
273	9.559900
274	9.524000
275	9.889300
276	9.553700
277	9.534400
278	9.566800
279	9.518700
280	9.510600
281	9.528800
282	9.545800
283	9.693700
284	9.507500
285	9.511300
286	9.500100
287	9.505000
288	9.540600
289	9.651200
290	9.570700

597	9.143200
598	9.271700
599	9.316200
600	9.105400
601	9.265900
602	9.103200
603	9.268700
604	9.099700
605	9.046600
606	9.046700

1596	7.850300
1597	8.045900
1598	7.740900
1599	7.737900
1600	7.733500
1601	7.734300
1602	7.729800
1603	8.040600
1604	7.817500
1605	7.920400
1606	7.730800
1607	7.816300
1608	7.820600
1609	7.823400
1610	7.730100
1611	7.726700
1612	7.833400
1613	7.899600
1614	7.806200
1615	7.722700
1616	8.222000
1617	7.807400
1618	7.717900
1619	7.718900

2118	7.301500
2119	7.185200
2120	7.183700
2121	7.411800
2122	7.282500
2123	7.285600
2124	7.292500
2125	7.178600
2126	7.295200
2127	7.184200
2128	7.288200
2129	7.423800
2130	7.277300
2131	7.174300
2132	7.288300
2133	8.224200
2134	7.276500
2135	7.173400

3108	6.350500
3109	6.619800
3110	6.344700
3111	6.505300
3112	6.343300
3113	6.502800
3114	6.342800
3115	6.502300
3116	6.489400
3117	6.487600
3118	7.165100
3119	6.495300
3120	6.479400
3121	6.483700
3122	6.913700
3123	6.338300
3124	6.474800
3125	6.341200
3126	6.493200
3127	6.657600
3128	6.340400
3129	6.493700
3130	7.083600
3131	6.327500
3132	6.326300
/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.
  warnings.warn(""Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`."")
``"
winglian,2024-03-29 04:29:22+00:00,comment,ai21labs/Jamba-v0.1,"@Severian oof, that loss though. I've got it working now though in axolotl with a normal loss."
philschmid,2024-03-29 08:40:34+00:00,comment,ai21labs/Jamba-v0.1,FYI the TGI tag will automatically appear once it is officially supported. Hopefully soon! 
keyishen,2024-03-29 02:54:28+00:00,comment,ai21labs/Jamba-v0.1,"+1
The biggest problem is the transformer layer in the architecture!
Even the mamba layer is linear to seqence length, the transformer layer with quadratic attention still requres quadratic memory in both training and inference.
Is mamba only not effective enough to capture long context retrieval?"
ordagan,2024-04-04 10:50:21+00:00,comment,ai21labs/Jamba-v0.1,"Hey @SicariusSicariiStuff,
1. We believe that just like flash attention improved performance for attention, same will happen with mamba computations. 
2. There are more details about the architecture in the paper we released, see if that sheds more light? https://arxiv.org/abs/2403.19887
3. We trained on a combination of proprietary and public data, including code. Officially the model supports English, Spanish, Portuguese & French, but we saw it learned pretty well a few other languages as well and can be easily extended to support more languages with little training.
"
tomeras1,2024-03-28 19:49:46+00:00,comment,ai21labs/Jamba-v0.1,"Nice! Thank you :)
A few comments:
1. Can you add a comment in code explaining why this change is needed?
2. Don't you also need to edit line 953? IIUC, in case of qlora there is no `bias` attribute so `time_proj_bias` in line 953 will be `None`, which is not what we want.."
tomeras1,2024-03-28 21:20:51+00:00,comment,ai21labs/Jamba-v0.1,Closed because fixed in a different [PR](https://huggingface.co/ai21labs/Jamba-v0.1/discussions/6)
ArthurZ,2024-03-28 23:06:37+00:00,comment,ai21labs/Jamba-v0.1,https://github.com/huggingface/peft/pull/1530 Should have fixed these in latest peft
ricofix,2024-03-28 21:37:20+00:00,comment,ai21labs/Jamba-v0.1,"I haven't seen ternary bits applied to an SSM yet, let alone a hybrid. Would be interesting to see if it's compatiable."
nonetrix,2024-03-28 22:36:16+00:00,comment,ai21labs/Jamba-v0.1,"Imagine the efficiency with MoE + Mamba + 1.58 bit 😳

Maybe like make higher parameters version too, I imagine 1.58 bit version could be same memory footprint and speed is 50B version while being a lot more parameters if not double. Then I guess it would be how could we shrink that somehow even like quantization already let's us do with fp16 models"
Tom-Neverwinter,2024-03-29 01:12:58+00:00,comment,ai21labs/Jamba-v0.1,"I have 4 tesla m40 and a 1080. 100GB of vram total and 128GB system ram. 

how much in resources do we need to even run this thing? "
bdambrosio,2024-03-29 01:19:27+00:00,comment,ai21labs/Jamba-v0.1,"@Tom-Neverwinter - I believe the problem is it that at the moment it won't accept multi-gpu configurations.
Hopefully this isn't a hard fix...

But do note 100GB is not enough to run a 51B+ parameter model in FP16, even if/when it does support multi-gpu, you will still need to quantize, like me."
vsevolodl,2024-03-29 06:44:39+00:00,comment,ai21labs/Jamba-v0.1,"@Tom-Neverwinter Interesting. I have two A6000s, and it works for me in 8-bit but doesn't work in half-precision (though the model successfully loads into VRAM). It says that ""Fast Mamba kernels are not available. Make sure they are installed and that the mamba module is on a CUDA device."" I reinstalled mamba-ssm from the source, but I'm still getting this message. 

![Screenshot from 2024-03-28 23-41-22.png](https://cdn-uploads.huggingface.co/production/uploads/6169beb2910b13c194bf5e5b/pHCdMDD-TTn1GaNPIpLDB.png)

![Screenshot from 2024-03-28 23-42-50.png](https://cdn-uploads.huggingface.co/production/uploads/6169beb2910b13c194bf5e5b/uGgChXqhu9cGo38Chj8EG.png)
"
benicio-standard,2024-03-28 19:05:31+00:00,comment,ai21labs/Jamba-v0.1,We would love to see what you and others come up with when fine tuning this model. Our fine tuned version will be available on our SaaS Platform.
caostworks,2024-04-04 08:19:07+00:00,comment,ai21labs/Jamba-v0.1,This comment has been hidden
lorinma,2024-06-03 05:53:56+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Hi BBLL3456, 

We will not. GGUF seems to be more popular as of now, and the conversion is easy with many community works. 
For AWQ and GPTQ, Modelscope community has provided a version using their SWIFT framework, check here: https://www.modelscope.cn/models/AI-ModelScope/Yi-1.5-34B-Chat-AWQ/summary

Bests,
DevRel"
Narsil,2024-05-22 17:43:08+00:00,comment,01-ai/Yi-1.5-34B-Chat,No thorough check was made yet.
YShow,2024-05-25 09:19:10+00:00,comment,01-ai/Yi-1.5-34B-Chat,"hi, thank you for your contribution. We have supported the fast tokenizer"
Narsil,2024-05-28 07:56:48+00:00,comment,01-ai/Yi-1.5-34B-Chat,Thanks !
lorinma,2024-06-03 05:55:11+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Hi cduk,

We don't plan to provide an official this time. ModelScope community has provided AWQ and GTPQ variants using their SWIFT framework, check here: https://www.modelscope.cn/models/tclf90/Yi-1.5-34B-Chat-16K-GPTQ-Int4/summary"
Starlento,2024-05-14 02:28:57+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Actually if you use English, it will tell you that it is ChatGPT with some different representation of ""hello"" as well.
And even for the base model, it is the same case.
But I do not think this is a big deal."
YShow,2024-05-14 02:35:00+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Sorry, we are unable to share our data, and we have also noticed this issue. We have used some synthetic data, and you can construct your self-awareness data to correct it. This problem will not affect your use."
YShow,2024-05-14 02:56:23+00:00,comment,01-ai/Yi-1.5-34B-Chat,"1. You can find that `add_generation_prompt` is not present in the chat_template of the` tokenizer_config` file, which is different from v1, so this parameter has no effect. I changed the code for quickstart and deleted it.
2. You can use `tokenizer. decode (input_ids [0])` to decode and observe the prompt after `apply_chat_template`, which has im_start. For model generate, only |im_end| "
skyshine102,2024-05-14 03:05:26+00:00,comment,01-ai/Yi-1.5-34B-Chat,"About 2. , that's what I did yesterday.  But I use `tokenizer.convert_ids_to_tokens(input_ids)` to see the `token (in text space).  
|im_end| is a standalone token while |im_start| is not.  
As long as this behavior is how model is trained during alignment stage then this is not an issue. I just want to confirm that's an intended result. 
"
YShow,2024-05-14 03:13:11+00:00,comment,01-ai/Yi-1.5-34B-Chat,"no, |im_start| is a standalone token too. The test results are as follows 
![image.png](https://cdn-uploads.huggingface.co/production/uploads/61c2acd273400748ac1b9c3c/insdsEJM3in6QcOiJPg2v.png)
"
skyshine102,2024-05-14 03:32:54+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Here is my results from latest transformers v4.40.2 & tokenizer v0.19.1

```
>>> from transformers import AutoTokenizer
>>> tok = AutoTokenizer.from_pretrained(""01-ai/Yi-1.5-34B-chat"")
messages = [
    {""role"": ""user"", ""content"": ""Who are you?""},
    {""role"": ""assistant"", ""content"": ""I am Yi.""}
]
out = tok.apply_chat_template(messages, tokenize=True, return_dict=True)
tok.convert_ids_to_tokens(out['input_ids'])2024-05-14 11:24:49,011 urllib3.connectionpool [DEBUG] - Starting new HTTPS connection (1): huggingface.co:443
2024-05-14 11:24:49,360 urllib3.connectionpool [DEBUG] - https://huggingface.co:443 ""HEAD /01-ai/Yi-1.5-34B-chat/resolve/main/tokenizer_config.json HTTP/1.1"" 307 0
2024-05-14 11:24:49,587 urllib3.connectionpool [DEBUG] - https://huggingface.co:443 ""HEAD /01-ai/Yi-1.5-34B-Chat/resolve/main/tokenizer_config.json HTTP/1.1"" 200 0
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
>>> messages = [
...     {""role"": ""user"", ""content"": ""Who are you?""},
...     {""role"": ""assistant"", ""content"": ""I am Yi.""}
... ]
>>> out = tok.apply_chat_template(messages, tokenize=True, return_dict=True)
>>> tok.convert_ids_to_tokens(out['input_ids'])
['▁<', '|', 'im', '_', 'start', '|>', 'user', '\n', 'Who', '▁are', '▁you', '?', '<|im_end|>', '▁', '\n', '<', '|', 'im', '_', 'start', '|>', 'ass', 'istant', '\n', 'I', '▁am', '▁Y', 'i', '.', '<|im_end|>', '▁', '\n']
```
Thank you for pointing out this is not an desired result.  I have no idea how to get the desired result yet :(
"
YShow,2024-05-14 03:43:34+00:00,comment,01-ai/Yi-1.5-34B-Chat,"For `AutoTokenizer.from_pretrained(""01-ai/Yi-1.5-34B-chat"")`, set `use_fast=False`, we do not support fast tokenizer, just like llama"
skyshine102,2024-05-14 04:10:42+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Oh, I saw the example your official readme. Thank you for your help!
(So many pitfalls for working with various llms on hf hubs). Is there any way for your team to set `use_fast=False` explicitly in tokenizer_config.json? 

---
An off-topic question:
Is there any way to check whether fast tokenizer is supported for a specific model?  
What's in my mind was: --> from transformer library if I can find tokenization_gemma_fast.py --> means gemma support fast tokenizer. "
Yhyu13,2024-05-13 10:42:55+00:00,comment,01-ai/Yi-1.5-34B-Chat,Probably their new dataset for v1.5 is short in terms of context length in order to train faster
YShow,2024-05-14 08:17:28+00:00,comment,01-ai/Yi-1.5-34B-Chat,"please stay tuned, we know what you want"
lorinma,2024-05-16 07:04:11+00:00,comment,01-ai/Yi-1.5-34B-Chat,"An update is on the way, stay tuned. Issue closed."
Gavinssss,2024-05-22 14:53:41+00:00,comment,01-ai/Yi-1.5-34B-Chat,good job !
YShow,2024-05-13 03:57:15+00:00,comment,01-ai/Yi-1.5-34B-Chat, yi-34b-chat is not supported either. 
paulcx,2024-05-13 04:15:46+00:00,comment,01-ai/Yi-1.5-34B-Chat,I thought a tokenizer.json file from Yi-1.5-34B or Yi-34B-Chat can be used for Yi-1.5-34B-Chat? Chat and Non-Chat model share same tokenizer right?
YShow,2024-05-13 09:20:22+00:00,comment,01-ai/Yi-1.5-34B-Chat,yes
ehartford,2024-05-12 20:46:44+00:00,comment,01-ai/Yi-1.5-34B-Chat,"@Phil337 your complaining grows tiresome.  Seems like complaining is all you do. 

@01-ai thank you for the precious gift of weights and the Apache 2.0 license.  You are loved 🥰 
"
deleted,2024-05-12 21:12:44+00:00,comment,01-ai/Yi-1.5-34B-Chat,This comment has been hidden
dustydecapod,2024-05-12 21:45:39+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Just because a model doesn’t suit your needs doesn’t mean that the model is inherently bad. A model trained to excel in the sciences isn’t likely going to excel at pop culture. We are very far away from having sub-1T parameter models that excel across arbitrary domains. I suggest you stick to models that are designed for what you need, instead of pitching a fit over models that are designed with other things in mind."
deleted,2024-05-12 22:13:56+00:00,comment,01-ai/Yi-1.5-34B-Chat,This comment has been hidden
smcleod,2024-05-12 22:30:22+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Came to ask the same, models under 16K (some might say 32K) context size tend to have a very limited use case.

Congrats on a the launch - very exciting indeed!
"
YShow,2024-05-14 08:16:22+00:00,comment,01-ai/Yi-1.5-34B-Chat,"on the way, please stay tuned"
MarinaraSpaghetti,2024-05-14 09:20:21+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Thank you so much for the response, best birthday gift ever. 💖 Will wait patiently!"
YShow,2024-05-14 09:23:20+00:00,comment,01-ai/Yi-1.5-34B-Chat,"wow, happy birthday to you ~ ❤️"
MarinaraSpaghetti,2024-05-14 09:52:16+00:00,comment,01-ai/Yi-1.5-34B-Chat,"Awww, thank you so much! 😭 Cheers, lads!"
DocDuck,2024-04-16 11:03:50+00:00,comment,lucyknada/microsoft_WizardLM-2-7B,Vicuna type chat template added for better vllm chat.completions support
daking,2024-02-20 17:58:27+00:00,comment,mosaicml/mpt-7b,"Hi, you'll need to upgrade to python 3.9"
daking,2024-02-20 17:58:47+00:00,comment,mosaicml/mpt-7b,Or continue using a previous commit of the model
yuzhen17,2024-02-22 03:41:53+00:00,comment,mosaicml/mpt-7b,thx
ybelkada,2023-10-30 10:30:39+00:00,comment,mosaicml/mpt-7b,cc @abhi-mosaic @sam-mosaic @daking
daking,2023-10-30 21:55:07+00:00,comment,mosaicml/mpt-7b,@ybelkada This should be fixed now. Could you please retest and confirm on your end? Thanks!
ybelkada,2023-10-31 13:56:06+00:00,comment,mosaicml/mpt-7b,"Works like charm now, thanks!"
jnm38,2023-11-23 09:27:03+00:00,comment,mosaicml/mpt-7b,"Doesn't work on the git
ImportError: cannot import name '_expand_mask' from 'transformers.models.bloom.modeling_bloom' (/lre/home/jperez/anaconda3/envs/llms/lib/python3.9/site-packages/transformers/models/bloom/modeling_bloom.py)"
Lemoncoke,2023-12-15 10:30:47+00:00,comment,mosaicml/mpt-7b,"ImportError: cannot import name '_expand_mask' from 'transformers.models.bloom.modeling_bloom' (/usr/local/lib/python3.9/dist-packages/transformers/models/bloom/modeling_bloom.py)

I got this error with transformers 4.36.1, any solution?"
daking,2023-12-15 17:37:33+00:00,comment,mosaicml/mpt-7b,The latest model works. Are you pinned to a revision?
daking,2023-12-20 01:17:11+00:00,comment,mosaicml/mpt-7b,This model is a base model that was not finetuned. See https://huggingface.co/collections/mosaicml/mpt-6564f3d9e5aac326bfa22def for all the MPT models we have released.
daking,2023-12-20 01:18:04+00:00,comment,mosaicml/mpt-7b,Please see llm foundry for our deps (https://github.com/mosaicml/llm-foundry/blob/2ba9224f6a841e157cdc5069c1e0a6fa830557dc/setup.py#L66)
RonanMcGovern,2023-07-19 12:14:31+00:00,comment,mosaicml/mpt-7b,"Related answer here: https://discuss.huggingface.co/t/recommend-an-instance-for-mpt-7b-and-mpt-30b-inference/46722/2

So closing this"
daking,2023-12-20 01:20:22+00:00,comment,mosaicml/mpt-7b,This seems like a generic question related to creating a model serving product. I'd suggest a different forum for this question as I think you are looking for generic help making a hosted api that others can hit.
yucyuanLin,2023-07-21 06:47:26+00:00,comment,mosaicml/mpt-7b,"HI, I got the same error"
lziiid,2023-08-21 07:43:36+00:00,comment,mosaicml/mpt-7b,I got the same error too :(
sywangyi,2023-06-27 08:17:21+00:00,comment,mosaicml/mpt-7b,@yao-matrix
sywangyi,2023-06-27 08:18:08+00:00,comment,mosaicml/mpt-7b,"mpt model need to be changed in Attention for the following error. https://huggingface.co/mosaicml/mpt-7b/blob/main/attention.py#L49
causal_mask = attn_weight.new_ones(s, s, dtype=torch.float16)
RuntimeError: float16/half is not supported on Gaudi.
"
sywangyi,2023-06-28 00:21:17+00:00,comment,mosaicml/mpt-7b,"@daking please help review, thanks"
palaashag,2023-07-19 12:17:45+00:00,comment,mosaicml/mpt-7b,"Still facing the same issue!
"
pnicewiczoig,2023-07-24 14:14:19+00:00,comment,mosaicml/mpt-7b,I have the same issue. I'm trying to log the model and register the model in Databricks.
daking,2023-07-25 01:32:16+00:00,comment,mosaicml/mpt-7b,"Have you tried the pipeline usage example at the bottom of our model card (https://huggingface.co/mosaicml/mpt-7b)? It is working for me, even though it does print out the message about mpt not being a supported model."
na-fi,2023-08-14 07:30:39+00:00,comment,mosaicml/mpt-7b,Does anyone know how to hide the warning message 'The model 'MPTForCausalLM' is not supported for text-generation.' ?
adc23,2023-10-19 14:17:12+00:00,comment,mosaicml/mpt-7b,"Still getting this error as well! transformers==4.33.1, mlflow==2.7.1"
daking,2023-12-20 01:21:45+00:00,comment,mosaicml/mpt-7b,This is just a warning and should not cause issues.
slaskaridis,2023-06-26 17:05:17+00:00,comment,mosaicml/mpt-7b,"I had the same issue. You need to have nvidia-toolkit installed (or point the CUDA_HOME env var properly).
Also make sure that cuda version is above 10.4 (if I remember correctly)  for this to work."
nps798,2023-06-27 02:56:59+00:00,comment,mosaicml/mpt-7b,"Thanks for @slaskaridis reply

I made a stupid mistake by forgetting to install CUDA (from nVidia).  
Previously, I installed cudatoolkit using Conda, which worked fine, but it was limited to conda platform . my thought about it the cause of the problem is that the gcc require system-wide cuda to function. 
I didn't install CUDA from the NVIDIA website, which I believe is causing the problem. You can check if CUDA is installed system-wide by running `ls -l /usr/local` and verifying if there is a cuda folder present.

# Here's a workaround:
## go to nVidia website, install cuda (which i refer to as system-wide cuda)
https://developer.nvidia.com/cuda-downloads

## so, you basically have to work through the command like the following
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda-repo-ubuntu2004-12-1-local_12.1.1-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2004-12-1-local_12.1.1-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2004-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda

## set environment
export CUDA_HOME=/usr/local/cuda-12.1

# however, there's another problem here. the code again raises CalledProcessError
after several trial-and-errors, I found a solution by simply modifying a line of code in /home/my_id/anaconda3/envs/my_env/lib/python3.9/site-packages/triton_pre_mlir/compiler.py
in the function _build, comment out the：cu_include_dir = os.path.join(cuda_home_dirs(), ""include"")
add a new line ： cu_include_dir = ""/usr/local/cuda/include""
so, it look like this

def _build(name, src, srcdir):
    cuda_lib_dirs = libcuda_dirs()
    #cu_include_dir = os.path.join(cuda_home_dirs(), ""include"") 
    cu_include_dir = ""/usr/local/cuda/include""




then it works !
"
sam-mosaic,2023-06-21 18:12:38+00:00,comment,mosaicml/mpt-7b,"Thanks for this contribution. Do we need to manually convert to this format, or is there a way to save directly in `safetensor` format?"
makeColabFree,2023-06-22 07:14:55+00:00,comment,mosaicml/mpt-7b,I don't know. It seems like there was an issue while converting according to huggingface's tutorials (https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors) with their spaces (https://huggingface.co/safetensors).
daking,2023-06-16 22:56:34+00:00,comment,mosaicml/mpt-7b,
daking,2023-06-16 22:51:36+00:00,comment,mosaicml/mpt-7b,
daking,2023-06-16 17:19:44+00:00,comment,mosaicml/mpt-7b,
choffer1120,2023-06-15 02:45:19+00:00,comment,mosaicml/mpt-7b,"+1, this also caused issues on our end today.

`RuntimeError: expected scalar type Half but found Float` both when loading models out of the box and when trying to load in 8bit."
sam-mosaic,2023-06-15 08:23:01+00:00,comment,mosaicml/mpt-7b,"You're running the model in lower precision (fp16 or bf16), but alibi bias needs to be in fp32 or else the model perf degrades. to get those to work together correctly, you should use autocast. Here is an example of how we had to update our tests to get this right: https://github.com/mosaicml/llm-foundry/pull/329/files#diff-3b8a58a4d021803b3171b886bb9162fd659e671131f3f61036f9210cb5d0bc7cR809"
vchiley,2023-06-14 17:45:07+00:00,comment,mosaicml/mpt-7b,https://github.com/mosaicml/llm-foundry/pull/329
bcui19,2023-06-14 15:40:58+00:00,comment,mosaicml/mpt-7b,Need https://huggingface.co/mosaicml/mpt-7b/discussions/56
kosiakk,2023-06-07 09:08:05+00:00,comment,mosaicml/mpt-7b,"Here is a pretty heatmap plot of the `wte` embeddings layer: 
![heatmap plot](https://cdn-uploads.huggingface.co/production/uploads/64613c9556c57120a2d042df/4oFimTzaORCf1mMJMGaP2.png) 
I'd say it clearly shows that tokens up until tokenizer max id (50253) were trained, but tokens higher than that remain in their (random) init, probably because they never received any gradients.

I don't think it's helpful to model in any way, but wasted CPU or memory seems negligible."
sam-mosaic,2023-06-07 16:21:08+00:00,comment,mosaicml/mpt-7b,"This was intentional. Having vocab size be a multiple of 128 gives a pretty substantial efficiency boost. We also wanted to leave space for tokens like those needed for UL2 style mixture of denoisers.

Thank you for the table and chart!"
zachblank,2023-06-06 19:57:34+00:00,comment,mosaicml/mpt-7b,Some edits have to be made to the model itself. Try https://huggingface.co/Gladiaio/mpt-7b-qlora
vi-c,2023-06-06 20:43:38+00:00,comment,mosaicml/mpt-7b,"I got the same behavior (OORAM) with :
- https://huggingface.co/Gladiaio/mpt-7b-qlora 
- https://huggingface.co/cekal/mpt-7b-peft-compatible"
abhi-mosaic,2023-06-07 21:12:25+00:00,comment,mosaicml/mpt-7b,"Hi @vi-c , could you try using `device_map='auto'` and make sure you clear your local HF cache and redownload the model (we pushed new source code last Friday)? I'm not sure what the behavior would be for MPT with a hardcoded `device_map={"""":0}` dict. We have also not tested any support with BitsAndBytes yet."
RonanMcGovern,2023-07-14 09:27:55+00:00,comment,mosaicml/mpt-7b,"Hi @vi-c , I had the same issue with Collab. I even increased to 25 GB on the pro plan and it crashes when loading.

1. Did you manage to get this to work?
2. Have you tried running on sagemaker? If so, what instance do you recommend?
3. @zachblank what edits did you make? I wasn't clear when reading. And do you have any recommended config (if not using hardcoded device_map and BitsAndBytes).

Thanks."
316usman,2023-08-22 10:42:17+00:00,comment,mosaicml/mpt-7b,"> Hi @vi-c , could you try using `device_map='auto'` and make sure you clear your local HF cache and redownload the model (we pushed new source code last Friday)? I'm not sure what the behavior would be for MPT with a hardcoded `device_map={"""":0}` dict. We have also not tested any support with BitsAndBytes yet.

@zachblank the same happens there isnt enough memory in the free gpu the first shard takes 13.3 GB gpu memory and then it crashes

@abhi-mosaic tried device_map=auto but the model does not go to the gpu memory in this case and therefore it crashes.

Please help"
abhi-mosaic,2023-06-05 15:14:00+00:00,comment,mosaicml/mpt-7b,"Hi @Trung-Dung , we use the GPT NeoX tokenizer which should have an EOS token id. I think you can safely reuse the EOS token id as the PAD token id at inference time."
ylhe96,2023-08-04 19:07:35+00:00,comment,mosaicml/mpt-7b,"As a follow-up to this discussion. When using the EOS as the PAD token, is there any recommendation for the padding side?"
mohitsinfo,2023-09-16 01:05:36+00:00,comment,mosaicml/mpt-7b,"I also don't see an option, pow.Scaler_out is implemented in https://github.com/pytorch/pytorch/issues/77764 but I still get the error that mps is not enabled and comment on this given issue"
abhi-mosaic,2023-06-02 05:02:27+00:00,comment,mosaicml/mpt-7b,"This PR adds updates from the LLM Foundry repo as of 06/01/2023.

These include:

* `device_map` support for multiple GPUs
* faster inference thanks to a refactor of the KV cacheing
* bugfix for returning the last hidden_state
* support for `output_attentions` when using `attn_impl: torch`
* a `requirements.txt` file to make it easier to know what you need to install for MPT
* updated README instructions for fast GPU initialization"
jacobfulano,2023-06-02 23:37:46+00:00,comment,mosaicml/mpt-7b,"Confirming that this seems to play nicely with `load_in_8bit=True` on google colab with higher system RAM (>13GB) than the standard tier

```
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = 'mosaicml/mpt-7b'

model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    load_in_8bit=True,
    device_map=""auto"",
    trust_remote_code=True,
    revision=""pr/47""
)
```

https://colab.research.google.com/drive/1-1n2UvrU47UOcWGlgeIuhi2Vi0u7OW5F?usp=sharing"
shijie-wu,2023-05-31 18:32:42+00:00,comment,mosaicml/mpt-7b,@abhi-mosaic
abhi-mosaic,2023-05-31 18:39:46+00:00,comment,mosaicml/mpt-7b,"Working on it, we are testing with FSDP first to make sure nothing breaks: https://github.com/mosaicml/llm-foundry/pull/225"
abhi-mosaic,2023-06-03 01:09:55+00:00,comment,mosaicml/mpt-7b,"Fixed as of this PR: https://huggingface.co/mosaicml/mpt-7b/discussions/47

 Please give it a try!"
ybelkada,2023-05-31 12:40:41+00:00,comment,mosaicml/mpt-7b,"This PR adds the accelerate support for MPT models, so that any user could load these models in 8bit and 4bit"
ybelkada,2023-05-31 12:43:24+00:00,comment,mosaicml/mpt-7b,"To load this model in 8bit before merging the PR:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = 'mosaicml/mpt-7b'

tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/gpt-neox-20b"")
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    load_in_8bit=True,
    device_map=""auto"",
    trust_remote_code=True,
    revision=""pr/45""
)

prompt = ""What is the boiling point of Nitrogen?""

input_ids = tokenizer(prompt, return_tensors=""pt"").input_ids.to(0)
out = model.generate(input_ids)
print(tokenizer.decode(out[0], skip_special_tokens=True))
```"
kosiakk,2023-06-07 11:29:20+00:00,comment,mosaicml/mpt-7b,"You can print the `model`:
```
MPTForCausalLM(
  (transformer): MPTModel(
    (wte): Embedding(50432, 4096)
    (emb_drop): Dropout(p=0, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x MPTBlock(
        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)
          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
        (ffn): MPTMLP(
          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)
          (act): GELU(approximate='none')
          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)
        )
        (resid_attn_dropout): Dropout(p=0, inplace=False)
        (resid_ffn_dropout): Dropout(p=0, inplace=False)
      )
    )
    (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
  )
)
```

It has `transformer.wte.weights` matrix, if you want a low-level access to embeddings."
winglian,2023-05-27 18:28:54+00:00,comment,mosaicml/mpt-7b,
winglian,2023-05-27 18:31:35+00:00,comment,mosaicml/mpt-7b,updates the model code to replit's latest changes. see https://huggingface.co/replit/replit-code-v1-3b/commit/e023a8461c7a2e2f2c85e52dbdd8d68b415f95eb
winglian,2023-05-27 18:32:25+00:00,comment,mosaicml/mpt-7b,fyi. there changes make the model work with the HF transformers trainer.
sam-mosaic,2023-06-14 07:34:27+00:00,comment,mosaicml/mpt-7b,"@abhi-mosaic , not sure if compatibility with the HF transformers trainer is related to PEFT training, but if so, the change to init configs might be important."
daking,2023-12-20 01:23:00+00:00,comment,mosaicml/mpt-7b,I think this is out of date/we periodically update the MPT code with the latest from LLM foundry.
SebastianBodza,2023-05-30 10:21:15+00:00,comment,mosaicml/mpt-7b,Any Differences to #25 ?
muelletm,2023-05-30 16:31:55+00:00,comment,mosaicml/mpt-7b,"Looks pretty similar TBH.

One difference is this line that is needed to work properly with `device_map=""auto""`:

(Around L290)

```python
        outputs = self.transformer(input_ids=input_ids, past_key_values=past_key_values, attention_mask=attention_mask, prefix_mask=prefix_mask, sequence_id=sequence_id, return_dict=return_dict, output_attentions=output_attentions, output_hidden_states=output_hidden_states, use_cache=use_cache, inputs_embeds=inputs_embeds)
        
        last_hidden_state = outputs.last_hidden_state
        if self.model_parallel:
            last_hidden_state = last_hidden_state.to(self.transformer.wte.weight.device)
        logits = F.linear(last_hidden_state, self.transformer.wte.weight)
```

But that line could also be added there, I suppose.

There might be subtle differences in other places, too, but as I said the code looks pretty similar."
eluzhnica,2023-06-28 18:09:17+00:00,comment,mosaicml/mpt-7b,"I'm not sure why the additional param `inputs_embeds` is needed. Maybe it's being used for something where they already have the embedding? Someone knows?

I made a similar version of this for 30B too on top of the latest foundry changes and it trains with QLORA https://huggingface.co/eluzhnica/mpt-30b-peft-compatible. It does train well from what I've tried.
 "
DanielTTY,2023-07-01 15:32:23+00:00,comment,mosaicml/mpt-7b,can do the same thing for the 30b version? 
DanielTTY,2023-07-01 16:13:23+00:00,comment,mosaicml/mpt-7b,"> I'm not sure why the additional param `inputs_embeds` is needed. Maybe it's being used for something where they already have the embedding? Someone knows?
> 
> I made a similar version of this for 30B too on top of the latest foundry changes and it trains with QLORA https://huggingface.co/eluzhnica/mpt-30b-peft-compatible. It does train well from what I've tried.

I tried this and it gives the error:
TypeError: forward() takes 2 positional arguments but 3 were given

I think this is the same error when one sets ""--gradient_checkpointing False""."
adc23,2024-01-29 20:35:29+00:00,comment,mosaicml/mpt-7b,"So I know MPT-7B doesn't support gradient checkpointing while using the Huggingface Trainer, but if you set it to false, you get the ""TypeError: forward() takes 2 positional arguments but 3 were given"" error? Because I have been dealing with that error for weeks now and this might be the breakthrough I needed to convince me to just abandon MPT all together"
muelletm,2023-05-27 15:25:06+00:00,comment,mosaicml/mpt-7b,"Small update but @cekal added a version of mpt-7b that fixes this (and other problems):

https://huggingface.co/cekal/mpt-7b-peft-compatible

Maybe their code can be merged into this repo?"
sam-mosaic,2023-06-14 07:37:42+00:00,comment,mosaicml/mpt-7b,@cekal — what changes were needed for gradient checkpointing in the PEFT library? 
diwank,2023-07-03 17:30:15+00:00,comment,mosaicml/mpt-7b,@cekal @sam-mosaic any updates?
tmm1,2023-07-06 01:50:47+00:00,comment,mosaicml/mpt-7b,"this looks like the change to enable gradient checkpointing?

https://huggingface.co/cekal/mpt-7b-peft-compatible/commit/a5eab52c1c61c1d50a4e01428949f6ff90c73c48"
RonanMcGovern,2023-07-19 18:36:28+00:00,comment,mosaicml/mpt-7b,"hmm, ok, but was that propagated to other mpt models like mpt-7b-chat and mpt-7b-8k-chat?"
adc23,2023-10-10 21:37:23+00:00,comment,mosaicml/mpt-7b,"Just ran into this issue today, I know if you use the Composer trainer you can use gradient checkpointing but it doesn't work through the Huggingface trainer"
RonanMcGovern,2023-10-13 22:26:44+00:00,comment,mosaicml/mpt-7b,"thanks, noted, feel free to close the issue"
vchiley,2023-05-26 17:48:00+00:00,comment,mosaicml/mpt-7b,"My guess if that your environment is not set up correctly.
see [here](https://github.com/mosaicml/llm-foundry#prerequisites) for setup instructions

specifically
you have a non-compatible version of triton installed."
datacow,2023-05-28 01:37:24+00:00,comment,mosaicml/mpt-7b,yeah I've been getting the same thing with the exact dependencies listed in the LLM Foundry - I've tried so many different things that the only thing I can even guess that's causing it is multiple cuda installations. do you have any other cuda versions besides just 11.7? I've got several versions in my /usr/local folder so give that a check. if you've only got 11.7 in there then I'm not sure what the issue could be.
Talon8080,2023-05-28 18:35:45+00:00,comment,mosaicml/mpt-7b,"Also facing the same Key Error, someone sloved it?
my gpu is Nvidia T4
driver: 515.105.01
Nvidia toolkit: V11.7.99
CUDA: 11.7
triton:  2.0.0.dev20221202
torch: 1.13.1+cu117"
MikeyBelllissimo,2023-05-29 23:41:01+00:00,comment,mosaicml/mpt-7b,"Yeah I'm also using the recommended libraries. I even just ran it through their docker image and everything and still got the same error. I only have 11.7 installed too so I don't think that's the source. I'm doing LoRA with the peft library which adds the inputs embeds argument. Is this the common denominator with the rest of you by chance? 

I'm not so sure that this would cause it but I figure it can't hurt to see if that's the common denominator before I dig deep into the kernel."
datacow,2023-05-30 00:06:28+00:00,comment,mosaicml/mpt-7b,"@MikeyBelllissimo I'm actually seeing this issue on inference, so I'm not using any extra libraries. exact same error except my issue arises with `_fwd_kernel()` instead of `_bwd_kernel()` in the original post"
MikeyBelllissimo,2023-05-30 20:19:51+00:00,comment,mosaicml/mpt-7b,"> @MikeyBelllissimo I'm actually seeing this issue on inference, so I'm not using any extra libraries. exact same error except my issue arises with `_fwd_kernel()` instead of `_bwd_kernel()` in the original post

That’s odd my forward pass is working fine. Have you tried the updated attention.py  and flash_attn_triton.py files and installed the triton_pre_mlir?

I’m thinking mine might be caused by a race condition. Working on some other projects right now as well but will try and debug when I get a chance."
MikeyBelllissimo,2023-06-07 01:17:16+00:00,comment,mosaicml/mpt-7b,This comment has been hidden
MikeyBelllissimo,2023-06-07 01:54:22+00:00,comment,mosaicml/mpt-7b,This comment has been hidden
RonanMcGovern,2023-07-19 12:13:22+00:00,comment,mosaicml/mpt-7b,@MikeyBelllissimo are you explicitly installing triton_pre_mlir? as in pip install triton_pre_mlir triton ? (I'm asking because I'm unsure what exactly I need to install to use triton).
creisle,2023-11-10 18:31:56+00:00,comment,mosaicml/mpt-7b,"I'm seeing this too, even after installing triton_pre_mlr explicitly. On an A6000 with ubuntu20 

relevant python packages installed and versions are below 

```
Package                  Version
------------------------ ------------
accelerate               0.24.1
datasets                 2.14.6
einops                   0.7.0
huggingface-hub          0.17.3
nvidia-cublas-cu11       11.10.3.66
nvidia-cuda-cupti-cu11   11.7.101
nvidia-cuda-nvrtc-cu11   11.7.99
nvidia-cuda-runtime-cu11 11.7.99
nvidia-cudnn-cu11        8.5.0.96
nvidia-cufft-cu11        10.9.0.58
nvidia-curand-cu11       10.2.10.91
nvidia-cusolver-cu11     11.4.0.1
nvidia-cusparse-cu11     11.7.4.91
nvidia-nccl-cu11         2.14.3
nvidia-nvtx-cu11         11.7.91
safetensors              0.4.0
tokenizers               0.14.1
torch                    2.0.0
tqdm                     4.66.1
transformers             4.34.1
triton                   2.0.0
triton-pre-mlir          2.0.0
```

"
sam-mosaic,2023-05-23 16:57:56+00:00,comment,mosaicml/mpt-7b,"I would start with doing the whole task in the chosen language. Qualitative testing has shown it to be surprisingly good at French, Spanish, Portuguese, and Chinese. It knows Hebrew, Japanese, and other languages as well."
abhinavkulkarni,2023-05-23 04:50:19+00:00,comment,mosaicml/mpt-7b,"Hey @narai,

The model currently does not support returning a few outputs as well as functionality such as `device_map=""auto""`. I have another issue open, but haven't received any response: https://huggingface.co/mosaicml/mpt-7b/discussions/30"
sam-mosaic,2023-05-23 04:53:39+00:00,comment,mosaicml/mpt-7b,@narai @abhinavkulkarni please see the discussion in the relevant PR https://huggingface.co/mosaicml/mpt-7b/discussions/23
sam-mosaic,2023-05-23 05:04:52+00:00,comment,mosaicml/mpt-7b,you can test it out here https://huggingface.co/spaces/mosaicml/mpt-7b-instruct
c0d3r69,2023-05-23 05:20:41+00:00,comment,mosaicml/mpt-7b,"Hi sam, thanks for the response. I tried that where we can ask questions. And , it is working perfectly. What I want is: is there any ability to use it like the way we can use BERT for question answering.. where a user can give a context text and question and the system replies with the answer. I'd be happy to hear you on the same. Thankyou."
sam-mosaic,2023-05-23 05:31:29+00:00,comment,mosaicml/mpt-7b,"I pulled a passage from a random wikipedia article to test it and asked a question and got the right answer ¯\_(ツ)_/¯ 

my test query was as follows. There were examples like this in the training data

```
Where and when was Vanadinite discovered?

### CONTEXT
Vanadinite is a mineral belonging to the apatite group of phosphates, with the chemical formula Pb5(VO4)3Cl. It is one of the main industrial ores of the metal vanadium and a minor source of lead. A dense, brittle mineral, it is usually found in the form of red hexagonal crystals. It is an uncommon mineral, formed by the oxidation of lead ore deposits such as galena. First discovered in 1801 in Mexico, vanadinite deposits have since been unearthed in South America, Europe, Africa, and North America.

Origins
Vanadinite is an uncommon mineral, only occurring as the result of chemical alterations to a pre-existing material. It is therefore known as a secondary mineral. It is found in arid climates and forms by oxidation of primary lead minerals. Vanadinite is especially found in association with the lead sulfide, galena. Other associated minerals include wulfenite, limonite, and barite.[3][5]

It was originally discovered in Mexico by the Spanish mineralogist Andrés Manuel del Río in 1801. He called the mineral ""brown lead"" and asserted that it contained a new element, which he first named pancromium and later, erythronium. However, he was later led to believe that this was not a new element but merely an impure form of chromium. In 1830, Nils Gabriel Sefström discovered a new element, which he named vanadium. It was later revealed that this was identical to the metal discovered earlier by Andrés Manuel del Río. Del Río's ""brown lead"" was also rediscovered, in 1838 in Zimapan, Hidalgo, Mexico, and was named vanadinite because of its high vanadium content. Other names that have since been given to vanadinite are johnstonite and lead vanadate.[6]

Occurrence
Vanadinite occurs as a secondary mineral in the oxidized zone of lead-bearing deposits, the vanadium is leached from wall-rock silicates. Associated minerals include mimetite, pyromorphite, descloizite, mottramite, wulfenite, cerussite, anglesite, calcite, barite, and various iron oxide minerals.[4]

Deposits of vanadinite are found worldwide including Austria, Spain, Scotland, the Ural Mountains, South Africa, Namibia, Morocco, Argentina, Mexico, and 4 states of the United States: Arizona, Colorado, New Mexico, and South Dakota.[3][5][7]

Vanadinite deposits are found in over 400 mines across the world. Notable vanadinite mines include those at Mibladen and Touisset in Morocco; Tsumeb, Namibia; Cordoba, Argentina; and Sierra County, New Mexico, and Gila County, Arizona, in the United States.[8]

Structure
Vanadinite is a lead chlorovanadate with the chemical formula Pb5(VO4)3Cl. It is composed (by weight) of 73.15% lead, 10.79% vanadium, 13.56% oxygen, and 2.50% chlorine. Each structural unit of vanadinite contains a chlorine ion surrounded by six divalent lead ions at the corners of a regular octahedron, with one of the lead ions provided by an adjoining vanadinite molecule. The distance between each lead and chlorine ion is 317 picometres. The shortest distance between each lead ion is 4.48 Å. The octahedron shares two of its opposite faces with that of neighbouring vanadinite units, forming a continuous chain of octahedrons. Each vanadium atom is surrounded by four oxygen atoms at the corners of an irregular tetrahedron. The distance between each oxygen and vanadium atom is either 1.72 or 1.76 Å. Three oxygen tetrahedrons adjoin each of the lead octahedrons along the chain.[2][9][10]
```"
sam-mosaic,2023-05-23 05:32:28+00:00,comment,mosaicml/mpt-7b,"However, be aware that BERT does _extractive_ question answering, which means it only quotes the passage. This is a decoder model, so it is possible that it can hallucinate."
c0d3r69,2023-05-23 05:46:22+00:00,comment,mosaicml/mpt-7b,how can I provide context to the model for getting answers from my text document or files?
vsns,2023-05-25 14:33:37+00:00,comment,mosaicml/mpt-7b,@c0d3r69 you can refer my code snippet using langchain to give the context to mpt instruct in https://huggingface.co/TheBloke/MPT-7B-Instruct-GGML/discussions/2#646b575d5d68f5c15a1e2a99
beejay,2023-06-23 00:40:57+00:00,comment,mosaicml/mpt-7b,I would like to try https://huggingface.co/spaces/mosaicml/mpt-7b-instruct but getting 404 error
beejay,2023-06-26 18:15:04+00:00,comment,mosaicml/mpt-7b,"> I pulled a passage from a random wikipedia article to test it and asked a question and got the right answer ¯\_(ツ)_/¯ 
> 
> my test query was as follows. There were examples like this in the training data
> 
> ```
> Where and when was Vanadinite discovered?
> 
> ### CONTEXT
> Vanadinite is a mineral belonging to the apatite group of phosphates, with the chemical formula Pb5(VO4)3Cl. It is one of the main industrial ores of the metal vanadium and a minor source of lead. A dense, brittle mineral, it is usually found in the form of red hexagonal crystals. It is an uncommon mineral, formed by the oxidation of lead ore deposits such as galena. First discovered in 1801 in Mexico, vanadinite deposits have since been unearthed in South America, Europe, Africa, and North America.
> 
> Origins
> Vanadinite is an uncommon mineral, only occurring as the result of chemical alterations to a pre-existing material. It is therefore known as a secondary mineral. It is found in arid climates and forms by oxidation of primary lead minerals. Vanadinite is especially found in association with the lead sulfide, galena. Other associated minerals include wulfenite, limonite, and barite.[3][5]
> 
> It was originally discovered in Mexico by the Spanish mineralogist Andrés Manuel del Río in 1801. He called the mineral ""brown lead"" and asserted that it contained a new element, which he first named pancromium and later, erythronium. However, he was later led to believe that this was not a new element but merely an impure form of chromium. In 1830, Nils Gabriel Sefström discovered a new element, which he named vanadium. It was later revealed that this was identical to the metal discovered earlier by Andrés Manuel del Río. Del Río's ""brown lead"" was also rediscovered, in 1838 in Zimapan, Hidalgo, Mexico, and was named vanadinite because of its high vanadium content. Other names that have since been given to vanadinite are johnstonite and lead vanadate.[6]
> 
> Occurrence
> Vanadinite occurs as a secondary mineral in the oxidized zone of lead-bearing deposits, the vanadium is leached from wall-rock silicates. Associated minerals include mimetite, pyromorphite, descloizite, mottramite, wulfenite, cerussite, anglesite, calcite, barite, and various iron oxide minerals.[4]
> 
> Deposits of vanadinite are found worldwide including Austria, Spain, Scotland, the Ural Mountains, South Africa, Namibia, Morocco, Argentina, Mexico, and 4 states of the United States: Arizona, Colorado, New Mexico, and South Dakota.[3][5][7]
> 
> Vanadinite deposits are found in over 400 mines across the world. Notable vanadinite mines include those at Mibladen and Touisset in Morocco; Tsumeb, Namibia; Cordoba, Argentina; and Sierra County, New Mexico, and Gila County, Arizona, in the United States.[8]
> 
> Structure
> Vanadinite is a lead chlorovanadate with the chemical formula Pb5(VO4)3Cl. It is composed (by weight) of 73.15% lead, 10.79% vanadium, 13.56% oxygen, and 2.50% chlorine. Each structural unit of vanadinite contains a chlorine ion surrounded by six divalent lead ions at the corners of a regular octahedron, with one of the lead ions provided by an adjoining vanadinite molecule. The distance between each lead and chlorine ion is 317 picometres. The shortest distance between each lead ion is 4.48 Å. The octahedron shares two of its opposite faces with that of neighbouring vanadinite units, forming a continuous chain of octahedrons. Each vanadium atom is surrounded by four oxygen atoms at the corners of an irregular tetrahedron. The distance between each oxygen and vanadium atom is either 1.72 or 1.76 Å. Three oxygen tetrahedrons adjoin each of the lead octahedrons along the chain.[2][9][10]
> ```

@sam-mosaic is it possible to share a more complete example of such code so we have a working example of doing context-based Q&A using MPT-7B-Instruct?"
sam-mosaic,2023-06-26 20:51:58+00:00,comment,mosaicml/mpt-7b,"I use the `hf_chat.py` script in `llm-foundry` to test these things. I will give an example, and also a python script that should work.

```sh
cd llm-foundry/scripts/inference

python hf_chat.py -n mosaicml/mpt-7b-instruct --max_new_tokens 512 --top_k 100 --model_dtype bf16 --trust_remote_code --attn_impl triton --device cuda:0 --system_prompt ""Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n"" --user_msg_fmt ""### Instruction:\n{}"" --assistant_msg_fmt  ""\n\n### Response:\n{}""
```

With the above invocation, you can just paste in the instruction / QA / whatnot:

```sh
Starting conversation...
Enter your message below.
- Hit return twice to send input to the model
- Type 'clear' to restart the conversation
- Type 'history' to see the conversation
- Type 'quit' to end
- Type 'system' to change the system prompt

User:
Where and when was Vanadinite discovered?
### CONTEXT
Vanadinite is a mineral belonging to the apatite group of phosphates, with the chemical formula Pb5(VO4)3Cl. It is one of the main industrial ores of the metal vanadium and a minor source of lead. A dense, brittle mineral, it is usually found in the form of red hexagonal crystals. It is an uncommon mineral, formed by the oxidation of lead ore deposits such as galena. First discovered in 1801 in Mexico, vanadinite deposits have since been unearthed in South America, Europe, Africa, and North America.
Origins
Vanadinite is an uncommon mineral, only occurring as the result of chemical alterations to a pre-existing material. It is therefore known as a secondary mineral. It is found in arid climates and forms by oxidation of primary lead minerals. Vanadinite is especially found in association with the lead sulfide, galena. Other associated minerals include wulfenite, limonite, and barite.[3][5]
It was originally discovered in Mexico by the Spanish mineralogist Andrés Manuel del Río in 1801. He called the mineral ""brown lead"" and asserted that it contained a new element, which he first named pancromium and later, erythronium. However, he was later led to believe that this was not a new element but merely an impure form of chromium. In 1830, Nils Gabriel Sefström discovered a new element, which he named vanadium. It was later revealed that this was identical to the metal discovered earlier by Andrés Manuel del Río. Del Río's ""brown lead"" was also rediscovered, in 1838 in Zimapan, Hidalgo, Mexico, and was named vanadinite because of its high vanadium content. Other names that have since been given to vanadinite are johnstonite and lead vanadate.[6]
Occurrence
Vanadinite occurs as a secondary mineral in the oxidized zone of lead-bearing deposits, the vanadium is leached from wall-rock silicates. Associated minerals include mimetite, pyromorphite, descloizite, mottramite, wulfenite, cerussite, anglesite, calcite, barite, and various iron oxide minerals.[4]
Deposits of vanadinite are found worldwide including Austria, Spain, Scotland, the Ural Mountains, South Africa, Namibia, Morocco, Argentina, Mexico, and 4 states of the United States: Arizona, Colorado, New Mexico, and South Dakota.[3][5][7]
Vanadinite deposits are found in over 400 mines across the world. Notable vanadinite mines include those at Mibladen and Touisset in Morocco; Tsumeb, Namibia; Cordoba, Argentina; and Sierra County, New Mexico, and Gila County, Arizona, in the United States.[8]
Structure
Vanadinite is a lead chlorovanadate with the chemical formula Pb5(VO4)3Cl. It is composed (by weight) of 73.15% lead, 10.79% vanadium, 13.56% oxygen, and 2.50% chlorine. Each structural unit of vanadinite contains a chlorine ion surrounded by six divalent lead ions at the corners of a regular octahedron, with one of the lead ions provided by an adjoining vanadinite molecule. The distance between each lead and chlorine ion is 317 picometres. The shortest distance between each lead ion is 4.48 Å. The octahedron shares two of its opposite faces with that of neighbouring vanadinite units, forming a continuous chain of octahedrons. Each vanadium atom is surrounded by four oxygen atoms at the corners of an irregular tetrahedron. The distance between each oxygen and vanadium atom is either 1.72 or 1.76 Å. Three oxygen tetrahedrons adjoin each of the lead octahedrons along the chain.[2][9][10]

Assistant:
Vanadinite was discovered by the Spanish mineralogist Andrés Manuel del Río in the Mexican state of Hidalgo in 1801.
took 0.62 seconds
```

```python
import torch
import transformers

name = 'mosaicml/mpt-7b-instruct'

config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)
config.attn_config['attn_impl'] = 'triton'
config.init_device = 'cuda:0' # For fast initialization directly on GPU!

model = transformers.AutoModelForCausalLM.from_pretrained(
  name,
  config=config,
  torch_dtype=torch.bfloat16, # Load model weights in bfloat16
  trust_remote_code=True
)

tokenizer = transformers.AutoTokenizer.from_pretrained(name)

INSTRUCTION_KEY = ""### Instruction:""
RESPONSE_KEY = ""### Response:""
INTRO_BLURB = ""Below is an instruction that describes a task. Write a response that appropriately completes the request.""
PROMPT_FOR_GENERATION_FORMAT = """"""{intro}
{instruction_key}
{instruction}
{response_key}
"""""".format(
    intro=INTRO_BLURB,
    instruction_key=INSTRUCTION_KEY,
    instruction=""{instruction}"",
    response_key=RESPONSE_KEY,
)

example = ""Where and when was Vanadinite discovered?\n\n...(the rest of the example I posted)...""
fmt_ex = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)

input_ids = tokenizer(fmt_ex, return_tensors=""pt"").input_ids
input_ids = input_ids.to(model.device)

response = model.generate(input_ids=input_ids, max_new_tokens=512, top_p=0.92)
```"
beejay,2023-06-29 00:21:27+00:00,comment,mosaicml/mpt-7b,"@sam-mosaic Thanks for sharing the above. I am able to reproduce the results. I do however have an issue: If I ask an out-of-context question, with everything else remaining as you shared, I see the model hallucinating and producing non-sensical response as follows. Any ideas that you can share to prevent this from happening? I tried with different intro blurbs pretty much asking the model to ""find the answer in the given context and if no answer say no"" but I was not successful.

User:
what did the president say about ketanji brown jackson?
### CONTEXT
Vanadinite is a mineral belonging to the apatite group of phosphates, with the chemical formula Pb5(VO4)3Cl. It is one of the main industrial ores of the metal vanadium and a minor source of lead. A dense, brittle mineral, it is usually found in the form of red hexagonal crystals. It is an uncommon mineral, formed by the oxidation of lead ore deposits such as galena. First discovered in 1801 in Mexico, vanadinite deposits have since been unearthed in South America, Europe, Africa, and North America.
Origins
Vanadinite is an uncommon mineral, only occurring as the result of chemical alterations to a pre-existing material. It is therefore known as a secondary mineral. It is found in arid climates and forms by oxidation of primary lead minerals. Vanadinite is especially found in association with the lead sulfide, galena. Other associated minerals include wulfenite, limonite, and barite.[3][5]
It was originally discovered in Mexico by the Spanish mineralogist Andrés Manuel del Río in 1801. He called the mineral ""brown lead"" and asserted that it contained a new element, which he first named pancromium and later, erythronium. However, he was later led to believe that this was not a new element but merely an impure form of chromium. In 1830, Nils Gabriel Sefström discovered a new element, which he named vanadium. It was later revealed that this was identical to the metal discovered earlier by Andrés Manuel del Río. Del Río's ""brown lead"" was also rediscovered, in 1838 in Zimapan, Hidalgo, Mexico, and was named vanadinite because of its high vanadium content. Other names that have since been given to vanadinite are johnstonite and lead vanadate.[6]
Occurrence
Vanadinite occurs as a secondary mineral in the oxidized zone of lead-bearing deposits, the vanadium is leached from wall-rock silicates. Associated minerals include mimetite, pyromorphite, descloizite, mottramite, wulfenite, cerussite, anglesite, calcite, barite, and various iron oxide minerals.[4]
Deposits of vanadinite are found worldwide including Austria, Spain, Scotland, the Ural Mountains, South Africa, Namibia, Morocco, Argentina, Mexico, and 4 states of the United States: Arizona, Colorado, New Mexico, and South Dakota.[3][5][7]
Vanadinite deposits are found in over 400 mines across the world. Notable vanadinite mines include those at Mibladen and Touisset in Morocco; Tsumeb, Namibia; Cordoba, Argentina; and Sierra County, New Mexico, and Gila County, Arizona, in the United States.[8]
Structure
Vanadinite is a lead chlorovanadate with the chemical formula Pb5(VO4)3Cl. It is composed (by weight) of 73.15% lead, 10.79% vanadium, 13.56% oxygen, and 2.50% chlorine. Each structural unit of vanadinite contains a chlorine ion surrounded by six divalent lead ions at the corners of a regular octahedron, with one of the lead ions provided by an adjoining vanadinite molecule. The distance between each lead and chlorine ion is 317 picometres. The shortest distance between each lead ion is 4.48 Å. The octahedron shares two of its opposite faces with that of neighbouring vanadinite units, forming a continuous chain of octahedrons. Each vanadium atom is surrounded by four oxygen atoms at the corners of an irregular tetrahedron. The distance between each oxygen and vanadium atom is either 1.72 or 1.76 Å. Three oxygen tetrahedrons adjoin each of the lead octahedrons along the chain.[2][9][10]

Assistant:
The president said that Ketanji is his wing woman and has his back. The president also supports Ketanji's Supreme Court nomination.
"
sam-mosaic,2023-05-23 05:06:30+00:00,comment,mosaicml/mpt-7b,"I found a T4 to be too slow, but an A10 was good. You need enough RAM to load the model into RAM before moving to the GPU"
sam-mosaic,2023-05-23 04:58:39+00:00,comment,mosaicml/mpt-7b,"Hi @abhinavkulkarni, is the ask here if `attention_mask` is being passed as a kwarg to the `forward` of `MPTForCausalLM`?"
abhinavkulkarni,2023-05-23 05:08:22+00:00,comment,mosaicml/mpt-7b,"Hey @sam-mosaic,

Thanks for the reply. You can see here, `output_attentions` options is not specified yet in `modeling_mpt.py`: https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L140

It would be nice if this `if` block were filled up instead of rasing `NotImplementedError`. I think it should be trivial given MPT uses traditional transformer, so collecting attention outputs from every hidden layer in the `forward` function and then returning it in a tuple.

You can see these line numbers from `modeling_opt.py` for a reference:

https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L245
https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L368
https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L725

Thanks for the great work!"
sam-mosaic,2023-05-25 08:02:02+00:00,comment,mosaicml/mpt-7b,"Thanks @abhinavkulkarni , I get it now. IIUC, `output_attentions` outputs the attention matrix from the attention module?

We do not use the `torch` code path much, we usually train with Triton Flash or CUDA Flash. However, neither of those attention implementations can support outputting  the attention matrix. So, if we supported this flag it would only be for torch. Does AutoGPTQ mainly focus on lower-resource inference and fine-tuning?"
abhinavkulkarni,2023-06-06 12:07:30+00:00,comment,mosaicml/mpt-7b,"Hey @sam-mosaic,

So, it seems the recent changes have solved most of the issues, except for line [110 of modeling_mpt.py](https://huggingface.co/mosaicml/mpt-7b/blob/main/modeling_mpt.py#L110) which needs to be changed from:

`return (attn_bias, None)`

to 


`return (attn_bias, attention_mask)`.

I made changes in my local copy of `modeling_mpt.py` in site-packages and was able to GPTQ quantize this model using AutoGPTQ repo."
vchiley,2023-06-06 19:17:19+00:00,comment,mosaicml/mpt-7b,"To improve efficiency, in line 109 of modeling_mpt.py, we integrate `attention_mask` into `attn_bias` if it exists.
If the requested attn_impl does not support an attn bias, then we use `attention_mask` (eg `attn_impl: flash` does not support attn bias and therefore the output of  the `_attn_bias` fn is `(None, attention_mask)`; see line 88)

This does not control if `output_attentions` are available."
sam-mosaic,2023-05-23 05:04:06+00:00,comment,mosaicml/mpt-7b,"The model should fit onto a k80. You'll need to use standard torch attention. If you instantiate the model with `max_seq_len=4096` you should be able to get sequences twice as long as the dolly models you were trying.

Depending on the type of documents this could work? Really depends how similar the task is to pretraining data.

If you get repetitive output, try searching for a good `no_repeat_ngram_size` (somewhere between 3-9) and `repetition_penalty`(somewhere between 1.01 and 1.2), as well as increasing the temperature."
airtable,2023-05-24 04:11:11+00:00,comment,mosaicml/mpt-7b,"Thanks @sam-mosaic, appreciate your response, I will try to see how to configure these settings and try again"
abhi-mosaic,2023-06-03 01:29:32+00:00,comment,mosaicml/mpt-7b,Closing for now as this issue has gone stale
GaaraOtheSand,2023-05-18 17:30:32+00:00,comment,mosaicml/mpt-7b,"Alright so this is the error I receive when trying to use triton:

          /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use 
          `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.
          warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')
          Loading checkpoint shards:   0%|                                                                  | 0/2 [00:00<?, ?it/s]Killed

and this is the error I receive when trying to run the program with flash as it suggests:

        line 39, in <module>
           model = transformers.AutoModelForCausalLM.from_pretrained(
        File ""/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py"", line 462, in from_pretrained
           return model_class.from_pretrained(
        File ""/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py"", line 2611, in from_pretrained
          model = cls(config, *model_args, **model_kwargs)
        File ""/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py"", line 205, in __init__
         self.transformer = MPTModel(config)
        File ""/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/modeling_mpt.py"", line 30, in __init__
         config._validate_config()
        File ""/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/configuration_mpt.py"", line 108, in _validate_config
          raise NotImplementedError('alibi only implemented with torch and triton attention.')
        NotImplementedError: alibi only implemented with torch and triton attention.

So for some reason no matter what type of attention implementation, the program won't run, that seems kinda silly?"
abhi-mosaic,2023-05-18 17:34:24+00:00,comment,mosaicml/mpt-7b,"Hi @GaaraOtheSand , the default is `attn_impl: torch` and it should work on CPU or GPU. The error you are seeing in 
```
 Loading checkpoint shards:   0%|                                                                  | 0/2 [00:00<?, ?it/s]Killed
```

appears to be an Out-of-Memory issue. Do you have enough CPU RAM to hold the model? (>=16GB)

The `UserWarnings` should not break anything, they are just there for guidance. You can use either `torch` or `triton` with this model since it uses ALiBi. We will remove/update the comment referring to `flash` as we don't recommend that path much anymore."
GaaraOtheSand,2023-05-18 18:27:59+00:00,comment,mosaicml/mpt-7b,"I have a 32 GB Ram, and my cuda, cudnn, and PyTorch I think are working correctly, as far as I tested they are, TensorFlow pops up a few different errors but I also don't think that they are anything to be concerned about because when I test it the cpu and gpu tests both show me what they're supposed to. When you say the default, do you mean it'll automatically try doing attn_impl: torch or that I should write that into my code, because in the docs on huggingface it shows this: config.attn_config['attn_impl'] = 'triton'. I tried running it with and without the actual, config.attn_config['attn_impl'] = 'torch', and it doesn't even show the loading checkpoint shards bar, until I hit ctrl C because it just stays stalled. According to task manager it's only using 22 of the 32 GB, this is what it gives me when I run it using torch:

         /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/d8304854d4877849c3c0a78f3469512a84419e84/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use 
         `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.
         warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')
         Loading checkpoint shards:   0%|                                                                  | 0/2 [00:00<?, ?it/s]Killed"
abhi-mosaic,2023-05-18 19:38:56+00:00,comment,mosaicml/mpt-7b,"Hi @GaaraOtheSand, thank you for the info. A couple comments and  followup questions:

1) If you run `model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b', trust_remote_code=True)`, this will use `attn_impl: torch` by default as it is specified in the `config.json`. But it will load into FP32 onto your CPU (Pytorch and HF convention always loads into FP32 tensors, regardless of `torch_dtype`), which should take up about  6.7 * 4 ~= 26.8GB of RAM. If this is failing, there is likely something wrong with your environment, and I would recommend trying out our Docker image `mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04`.

2) Similar to above, if you run `model = transformers.AutoModel.from_pretrained('mosaicml/mpt-7b', trust_remote_code=True, torch_dtype=torch.bfloat16)`, it will do everything as above but load into BF16 weights on your CPU, which should take about 6.7 * 2 ~= 13.4GB of RAM. 

I've tried 1) on a Linux box with the Docker image and 2) on the Linux box as well as my Macbook M2 (which has just 16GB RAM).

Could you try these code snippets and if they still fail, could you report your `torch` and `transformers` installed versions? Thank you!"
GaaraOtheSand,2023-05-19 17:42:24+00:00,comment,mosaicml/mpt-7b,"well I'm currently running the floating point 16, and my transformers version is 4.29.1, and my torch version is 2.0.1, and my Ubuntu is 22.04, I'm trying to run it through wsl because it said that it needs jax and jaxlib which I can't get on windows, but there really shouldn't be anything wrong with my ubuntu or wsl because when I test to see if torch can detect the gpu it shows the correct information. I guess I could try Docker it's just that I'm really unfamiliar with it, and I've found Docker to not be very user friendly in the past, imo. I'm building a program that uses a LLM, in this instance MPT as its base model but I wouldn't even know where to start to try and incorporate Docker into that project."
abhi-mosaic,2023-05-24 23:26:06+00:00,comment,mosaicml/mpt-7b,">  I'm trying to run it through wsl because it said that it needs jax and jaxlib which I can't get on windows
Hi @GaaraOtheSand , could you elaborate on this? Nothing about our models or stack should require JAX, was there some documentation or message you saw that suggested this?

> and my transformers version is 4.29.1, and my torch version is 2.0.1, and my Ubuntu is 22.04,
This looks fine, but just to be extra safe, you could try using `torch==1.13.1`, that is the version we used for building all these models.

Could you report the error you are seeing when you run the 1) and 2) commands I shared in the previous message?"
GaaraOtheSand,2023-05-25 17:43:38+00:00,comment,mosaicml/mpt-7b,"For clarification on the JAX thing, I believe that has to due with transformers and huggingface and not specifically mpt, and ok I'll try to make sure that this is easy-ish to read, so I'll begin with the imports and the beginning model code structure that I'm loading:

       from PyQt5.QtWidgets import QApplication, QMainWindow, QWidget, QTextEdit, QVBoxLayout, QHBoxLayout, QPushButton
       from PyQt5.QtCore import QObject, QThread, pyqtSignal
       import torch
       import os
       import sys
       import requests
       from bs4 import BeautifulSoup
       import datetime
       import subprocess
       import pdb
       import random
       import numpy as np
       import re
       from sklearn.metrics.pairwise import cosine_similarity
       from sklearn.feature_extraction.text import CountVectorizer
       import clang.cindex
       import schedule
       import torch.nn.functional as F
       import scipy
       import spacy
       from scipy.spatial.distance import cosine
       import keras
       from keras.utils import pad_sequences
       import transformers
       from transformers import pipeline

      # Load a pre-trained word embedding model
       nlp = spacy.load(""en_core_web_md"")

      config = transformers.AutoConfig.from_pretrained(
         'mosaicml/mpt-7b',
          trust_remote_code=True
     )

      config.attn_config['attn_impl'] = 'torch'

     config.update({""max_seq_len"": 8192})

    model = transformers.AutoModelForCausalLM.from_pretrained(
         'mosaicml/mpt-7b',
         config=config,
         #torch_dtype=torch.bfloat16,
         trust_remote_code=True
     )"
GaaraOtheSand,2023-05-25 17:49:45+00:00,comment,mosaicml/mpt-7b,"This is the error I receive when I run the model with that code structure:

         2023-05-25 11:37:51.449071: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in 
        performance-critical operations:  AVX2 FMA
        To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
        2023-05-25 11:37:52.338366: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
        2023-05-25 11:37:53.858440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or 
        directory
        2023-05-25 11:37:53.858588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object 
       file: No such file or directory
       2023-05-25 11:37:53.858623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the 
       missing libraries mentioned above are installed properly.
       2023-05-25 11:37:56.356776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
       Your kernel may have been built without NUMA support.
      2023-05-25 11:37:56.366126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
       Your kernel may have been built without NUMA support.
        2023-05-25 11:37:56.366194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
        Your kernel may have been built without NUMA support.

       A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
       - configuration_mpt.py
      . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
      A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
      - hf_prefixlm_converter.py
     . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
     A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
      - norm.py
    . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
    A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
     - meta_init_context.py
    . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
       A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
        - param_init_fns.py
      . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.

        Downloading (…)ve/main/attention.py: 100%|█████████████████████████████████████████| 16.8k/16.8k [00:00<00:00, 23.2MB/s]
        Downloading (…)flash_attn_triton.py: 100%|█████████████████████████████████████████| 28.2k/28.2k [00:00<00:00, 4.82MB/s]
        A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
       - flash_attn_triton.py
         . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
         A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
         - attention.py
        - flash_attn_triton.py
        . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
        A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
        - blocks.py
        - attention.py
       . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
        A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
       - adapt_tokenizer.py
       . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
      A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b:
        - modeling_mpt.py
        - hf_prefixlm_converter.py
       - norm.py
        - meta_init_context.py
        - param_init_fns.py
        - blocks.py
       - adapt_tokenizer.py
       . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
       Downloading shards: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  9.21it/s]
       /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157: UserWarning: Using `attn_impl: torch`. If your model does not use 
       `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.
         warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')
       Killed"
GaaraOtheSand,2023-05-25 17:58:31+00:00,comment,mosaicml/mpt-7b,"Now when I run the code with the floating point 16 added:

     model = transformers.AutoModelForCausalLM.from_pretrained(
       'mosaicml/mpt-7b',
       config=config,
       torch_dtype=torch.bfloat16,
      trust_remote_code=True
    )

I receive pretty close to the same error, but without the updates to the program, so I'll just share this is what I receive after the numa support stuff:

     /root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/14958374ab073ba1030c0caef4ae8380045bae45/attention.py:157: UserWarning: Using `attn_impl: torch`. If your model does not use 
     `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.
     warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')
    Loading checkpoint shards:   0%|                                                                  | 0/2 [00:00<?, ?it/s]
    Killed

So ya Idk there's certainly a chance that I messed up somewhere but I have no clue where or how."
GaaraOtheSand,2023-06-02 22:07:41+00:00,comment,mosaicml/mpt-7b,"I'm sorry I'm sure you have better things to do with your time, I think I've eliminated the possibility of it being the program I'm trying to build, and now that I've fixed the issues with my TF I doubt that it's a problem with that. My PyTorch and Cuda are working the way that they are supposed to, and just like before I've tried toying with the attn_impl by not having anything written, having torch explicitly stated, the bfloat16 stated, and doing the same with triton. I made two new scripts one to solely test the model and one to test another model from huggingface and both of them killed the program after some time. I was originally using chatgpt or text-davinci-003 and testing it I found that it still 'works', so my guess is it's an issue with transformers and or huggingface, not sure how to fix that though..."
RonanMcGovern,2023-07-25 17:42:09+00:00,comment,mosaicml/mpt-7b,"@abhi-mosaic I also can't get triton to work. What are the dependencies?

I've tried each of:
!pip install triton
!pip install triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python

and am getting these errors:

```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<string> in _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N, EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps, num_stages, extern_libs, stream, warmup)

KeyError: ('2-.-0-.-0-83ca8b715a9dc5f32dc1110973485f64-394352f6a8351feaac334fbb8cc63fa4-46c7c5d46afed8316facd72e7e581bec-ee7112c0f04b05ca1104709529fc7c00-39e3c68a052760cc345a9147b0d68f7d-5c5e32ff210f3b7f56c98ca29917c25e-06f0df2d61979d629033f4a22eff5198-4ac47e74762ba6a774cceea0e1e75ae6-13b7ffc189bd9fba7696034bbcfee151', (torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.float32, torch.bfloat16, torch.float32, torch.float32, 'fp32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32', 'i32'), ('vector', True, 128, False, False, True, 128, 128), (True, True, True, True, True, True, True, (False,), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (True, False), (False, False), (True, False), (True, False), (True, False), (True, False), (True, False), (False, False), (False, False), (True, False), (True, False), (False, True), (False, True)))

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-39-706a4f84ff25> in <cell line: 1>()
----> 1 stream()

21 frames
/usr/local/lib/python3.10/dist-packages/triton_pre_mlir/runtime/autotuner.py in run(self, *args, **kwargs)
    198         for v, heur in self.values.items():
    199             kwargs[v] = heur({**dict(zip(self.arg_names, args)), **kwargs})
--> 200         return self.fn.run(*args, **kwargs)
    201 
    202 

<string> in _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE, IS_CAUSAL, BLOCK_HEADDIM, EVEN_M, EVEN_N, EVEN_HEADDIM, BLOCK_M, BLOCK_N, grid, num_warps, num_stages, extern_libs, stream, warmup)

RuntimeError: Triton Error [CUDA]: invalid argument
```

and here is my code:
```
model_id = ""Trelis/mpt-7b-8k-chat-sharded-bf16""

# Prepare the configuration for the BitsAndBytes optimizer
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)
config.attn_config['attn_impl'] = 'triton' # running with triton as recommended.
config.init_device = 'cuda:0' # For fast initialization directly on GPU!
config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096

tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
  model_id,
  config=config,
  quantization_config=bnb_config,
  device_map={"""":0},
  torch_dtype=torch.bfloat16, # Load model weights in bfloat16
  trust_remote_code=True,
  cache_dir=cache_dir
)
```"
mayank-mishra,2023-05-20 15:06:08+00:00,comment,mosaicml/mpt-7b,@jfrankle can you take a look at this?
shanegary,2023-05-16 14:34:17+00:00,comment,mosaicml/mpt-7b,"have been looking at this.  It looks like this might be a patch:
https://github.com/iwalton3/mpt-lora-patch

However, I didn't really look at the code nor am I sure if there is a related pull request or something else in the works.  

The author said it worked but then doesn't seem sure.  From the patch author: ""The model code specifically says that left padding is not supported, but I forcibly did so and got decent results."""
SebastianBodza,2023-05-16 14:43:19+00:00,comment,mosaicml/mpt-7b,Works perfectly thanks :)
narenzen,2023-05-19 05:27:36+00:00,comment,mosaicml/mpt-7b,"@SebastianBodza where to use the patched version.
How to run it. which file to run.
Can you help me"
nicolaschaillan,2023-05-22 21:05:51+00:00,comment,mosaicml/mpt-7b,"Just to clarify, does the model support device_map=""auto"" and if so what's the syntax?"
sam-mosaic,2023-05-23 00:58:09+00:00,comment,mosaicml/mpt-7b,"Thank you for the PR @shijie-wu.

Based on my understanding, this should work, however, when I run this, I get an error:

```python
NameError                                 Traceback (most recent call last)
Cell In[5], line 1
----> 1 model = transformers.AutoModelForCausalLM.from_pretrained(""mosaicml/mpt-7b"", trust_remote_code=True, revision=""refs/pr/23"", device_map='auto')

File /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:462, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    458     class_ref = config.auto_map[cls.__name__]
    459     model_class = get_class_from_dynamic_module(
    460         class_ref, pretrained_model_name_or_path, **hub_kwargs, **kwargs
    461     )
--> 462     return model_class.from_pretrained(
    463         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    464     )
    465 elif type(config) in cls._model_mapping.keys():
    466     model_class = _get_model_class(config, cls._model_mapping)

File /usr/lib/python3/dist-packages/transformers/modeling_utils.py:2608, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   2606     init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())] + init_contexts
   2607 elif load_in_8bit or low_cpu_mem_usage:
-> 2608     init_contexts.append(init_empty_weights())
   2610 with ContextManagers(init_contexts):
   2611     model = cls(config, *model_args, **model_kwargs)

NameError: name 'init_empty_weights' is not defined
```

Did you test this?"
Forbu14,2023-05-24 09:44:24+00:00,comment,mosaicml/mpt-7b,@sam-mosaic You need to install accelerate (and reset your notebook)
sam-mosaic,2023-05-24 18:57:36+00:00,comment,mosaicml/mpt-7b,"Thank you @Forbu14 

Does anyone know if passing `refs/pr/23` as the `revision` works as one would expect? Because when doing that, I now get

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[5], line 1
----> 1 model = transformers.AutoModelForCausalLM.from_pretrained(""mosaicml/mpt-7b"", trust_remote_code=True, revision=""refs/pr/23"", device_map=""auto"")

File /usr/lib/python3/dist-packages/transformers/models/auto/auto_factory.py:462, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    458     class_ref = config.auto_map[cls.__name__]
    459     model_class = get_class_from_dynamic_module(
    460         class_ref, pretrained_model_name_or_path, **hub_kwargs, **kwargs
    461     )
--> 462     return model_class.from_pretrained(
    463         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    464     )
    465 elif type(config) in cls._model_mapping.keys():
    466     model_class = _get_model_class(config, cls._model_mapping)

File /usr/lib/python3/dist-packages/transformers/modeling_utils.py:2685, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   2676 special_dtypes.update(
   2677     {
   2678         name: torch.float32
   (...)
   2681     }
   2682 )
   2684 if model._no_split_modules is None:
-> 2685     raise ValueError(f""{model.__class__.__name__} does not support `device_map='{device_map}'` yet."")
   2686 no_split_modules = model._no_split_modules
   2687 if device_map not in [""auto"", ""balanced"", ""balanced_low_0"", ""sequential""]:

ValueError: MPTForCausalLM does not support `device_map='auto'` yet.
```

Would love to support `device_map=""auto""` but need some evidence that this PR accomplishes that"
Forbu14,2023-05-24 19:33:16+00:00,comment,mosaicml/mpt-7b,"I have the same error as you @sam-mosaic , it feels like the revision doesn't work (or we did something wrong)
we can try passing the hash instead d8a52ba8 ?"
Forbu14,2023-05-24 20:20:28+00:00,comment,mosaicml/mpt-7b,"Ok I did some checking and it seems that we just download the wrong version of the files here :

![Screenshot 2023-05-24 at 22.18.43.png](https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/HZlwISn9x6gnJT-IbhLw8.png)


![Screenshot 2023-05-24 at 22.20.05.png](https://cdn-uploads.huggingface.co/production/uploads/6229c4e279337db2b0bf0c2e/VoLKH24pjgVQmCMo_TzvQ.png)"
Forbu14,2023-05-24 20:28:55+00:00,comment,mosaicml/mpt-7b,@sam-mosaic Basicly we download the right config.json file but the wrong *.py files :(
sam-mosaic,2023-05-24 20:32:51+00:00,comment,mosaicml/mpt-7b,"@Forbu14 confirmed that regardless of if I pass `revision=""refs/pr/23""` or `revision=""d8a52ba8""`, I do not get the version of modeling_mpt.py from this PR. That seems like a bug in `transformers` to me."
Forbu14,2023-05-24 20:53:39+00:00,comment,mosaicml/mpt-7b,"@sam-mosaic clearly, I am trying out to figure what wrong in the transformers code base right now."
Forbu14,2023-05-24 23:05:13+00:00,comment,mosaicml/mpt-7b,I did raise the issue on transformers github. : https://github.com/huggingface/transformers/issues/23745
Forbu14,2023-05-25 11:46:03+00:00,comment,mosaicml/mpt-7b,"@sam-mosaic  Apparently the ""revision"" param is supported only for weight and not for code (currently)"
sam-mosaic,2023-05-25 21:35:43+00:00,comment,mosaicml/mpt-7b,Isn't the whole point of passing a revision to protect yourself from malicious code when using `trust_remote_code=True`? It even warns you to use a revision!!
muelletm,2023-05-27 15:23:52+00:00,comment,mosaicml/mpt-7b,"Looks like there is a version of mpt-7b that fixes this:

https://huggingface.co/cekal/mpt-7b-peft-compatible

Also to test a PR locally you can do this:

```shell
git clone https://huggingface.co/mosaicml/mpt-7b
pushd mpt-7b
git fetch origin refs/pr/23:pr/23
git checkout pr/23
popd

python your_script.py \
    --model_name_or_path ""./mpt-7b""
...
```"
kdua,2023-05-30 08:04:11+00:00,comment,mosaicml/mpt-7b,"Works when using load_checkpoint_and_dispatch. Doesn't work with from_pretrained.

from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline
import torch
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

model_dir = './mpt-7b-instruct'

max_memory_mapping = {0: ""16GB"", 1: ""16GB""}


config = AutoConfig.from_pretrained(
    model_dir,
    trust_remote_code=True,
    load_in_8_bit=True
)

with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
    
model.tie_weights()

model = load_checkpoint_and_dispatch(
    model, model_dir, device_map=""auto"", no_split_module_classes=[""MPTBlock""], max_memory=max_memory_mapping
)"
thechashi,2023-05-31 00:36:58+00:00,comment,mosaicml/mpt-7b,"He kdua, 

This is the error I am getting: 

ValueError: `checkpoint` should be the path to a file containing a whole state dict, or the index of a sharded 
checkpoint, or a folder containing a sharded checkpoint, but got mosaicml/mpt-7b-instruct. 

How to solve this?"
kdua,2023-05-31 10:06:34+00:00,comment,mosaicml/mpt-7b,"Hi @thechashi 
You need to clone the model repository, checkout the relevant code change, pull the model files and then load the checkpoint from that directory:

git clone https://huggingface.co/mosaicml/mpt-7b-instruct
git lfs pull
git fetch origin refs/pr/23:pr/23
git checkout pr/23


Now use this cloned directory in model_dir"
abhi-mosaic,2023-05-31 18:14:26+00:00,comment,mosaicml/mpt-7b,This should be supported now! We are doing some more tests to make sure multi-GPU inference works as well and should update soon.
shijie-wu,2023-05-31 18:26:46+00:00,comment,mosaicml/mpt-7b,sorry i was out of the loop but i'm glad that it's fixed by https://huggingface.co/mosaicml/mpt-7b/discussions/45.
Sven00,2023-07-04 18:04:02+00:00,comment,mosaicml/mpt-7b,"@abhi-mosaic thank you for the support! have you already made progress with regards to improving multi-GPU inference? Currently, the prompt provided in the model card (""What is the capital of France?"") takes 20 minutes with device_map=auto and continues to generate token after 'end_of_text'"
sam-mosaic,2023-05-23 05:08:59+00:00,comment,mosaicml/mpt-7b,What does this PR accomplish?
abhi-mosaic,2023-06-03 01:09:29+00:00,comment,mosaicml/mpt-7b,done in this PR: https://huggingface.co/mosaicml/mpt-7b/discussions/47
sam-mosaic,2023-05-23 05:20:23+00:00,comment,mosaicml/mpt-7b,"You can use transformer models on CPUs, but they tend to be quite slow. You likely want to look at projects like GGML and llama.cpp. Optimizing for CPU is a focus of those projects, unlike us. 

This is not an endorsement, as I haven't tested it, but this is the type of project you'd look for: https://huggingface.co/TheBloke/MPT-7B-Instruct-GGML"
0xDing,2023-05-14 05:26:32+00:00,comment,mosaicml/mpt-7b,2048
sam-mosaic,2023-05-16 19:35:33+00:00,comment,mosaicml/mpt-7b,"@0xDing is correct, we trained on 2048. ALiBi should let one extrapolate to 4096 "
patrickvonplaten,2023-05-16 13:03:43+00:00,comment,mosaicml/mpt-7b,"It's very often exactly the same weights. Just like in GPT2, Bloom, ..."
abhi-mosaic,2023-05-16 15:06:46+00:00,comment,mosaicml/mpt-7b,"Hi @JacopoBandoni, for MPT we use weight tying, which shares the word embedding weights with the final LM head. It is used by default in most HF causal language models, you can see the codepath here: https://github.com/huggingface/transformers/blob/130e15429116689c9d747be2cdd8c4be7bb7e2bd/src/transformers/modeling_utils.py#L1245-L1264

For MPT, to make the model easier to deal with for meta initialization and FSDP, we directly use the `self.transformer.wte.weight`as the LM head rather than create a separate nn.Linear module and tie the weights."
saber7ooth,2023-05-11 23:00:21+00:00,comment,mosaicml/mpt-7b,"See:  https://www.reddit.com/r/MachineLearning/comments/xjnozq/d_whats_the_word_on_amd_gpus_these_days/
And:  https://www.amd.com/en/graphics/servers-solutions-rocm

In summary:

- Support for ROCm is not great at the consumer level and pytorch must be compiled from source.  
- At the server level, you will find AMD compute instances with ROCm precompiled, at a pricetag.
- Flash Attention runtime kernels are composed for NVIDIA specific server products.

This will not allow Flash Attention, but as the good news:

- Compiling from source will allow AMD GPU accelerated pytorch with ROCm.  This does provide AMD users with an option.
- HF ``transformers.accelerator`` as a direct use-case becomes available (uses pytorch to split the model across several devices, when memory resources run low) -- Multiple AMD GPUs can be utilized in a load balanced configuration with block split-locking, but without Flash Attention.  ``transformers.accelerator`` is hardware agnostic.  Its a load balancer, and doesn't include runtime kernels, just ways to split the tensor blocks and send jobs to other devices available.

In LT;DR:

If you want this locally:  From the reported posts, you will run into circular dependency problems so the best approach is to compile the ROCm compatible version of pytorch in a docker contrainer, and either put it into a WSL or a native Ubuntu 22.04 install as a precompiled .whl or .egg package when completed (with your drivers installed), which will allow GPU accelerated pytrorch and HF transformer accelerators offloading.  (currently the best solution I've found consumer-level is this, as I don't need VRAM, I just need a lot of RAM -- cheaper compute instances, too)

This says nothing about compatibility with Flash Attention.  Users reportedly got txt2img models working in this manner, but only on a custom-compiled version of pytorch.  But in light of, its highly recommended that you choose an NVIDIA product for this application, over your time investment, sad to say.  AMD has not kept up the same level of compatibility and support to running LLMs as NVIDIA has in the consumer market, HF transformers docs do not indicate AMD is of priority consideration as much of the transformer optimizations currently out there are dependent on NVIDIA-architecture noteworthy tricks and kernel code.  Including Flash Attention, Alibi, XFormers, etc.

In short, you need to pay for one of their servers to get access to ROCm on the rail, or you need to switch to NVIDIA if you want it at home without spending hours resolving circular dependency problems.  You will gain GPU acceleration, but you will not have Flash Attention as its dependent on NVIDIA-specific kernel level optimization.

NVIDIA is the standard in the ML world, and the highest range of compatibility.  Running LLMs with AMD is generally not recommended, and actively discouraged over the time investment in getting it functioning.  We had to all bite the bullet and accept it, because AMD themselves just has not kept up with it.

Sadly could not choose Team Red for this application without considering investing months in custom application and hardware-tailored solutions."
cekal,2023-05-10 18:35:40+00:00,comment,mosaicml/mpt-7b,lmao
sam-mosaic,2023-05-10 16:52:41+00:00,comment,mosaicml/mpt-7b,"You can see the remote code in this repo, https://huggingface.co/mosaicml/mpt-7b/tree/main

Would it be acceptable to use `trust_remote_code` but set a `revision` so that you are guaranteed to get a version of the code that you have audited?"
dpr882,2023-05-10 21:06:44+00:00,comment,mosaicml/mpt-7b,can you include code for custom inference in the README ?
abhi-mosaic,2023-05-12 00:05:50+00:00,comment,mosaicml/mpt-7b,"Hi @SinanAkkoyun , we are working on it but it will likely be ~months away. H100s are only just entering the market and theres a lot of performance tuning to do!"
SinanAkkoyun,2023-05-30 13:45:39+00:00,comment,mosaicml/mpt-7b,"@abhi-mosaic LambdaLabs supplies ""infinite"" H100s now! When do you think will the TE implementation be available? Can I somehow help?"
abhi-mosaic,2023-06-03 01:11:20+00:00,comment,mosaicml/mpt-7b,"The model should work as-is on H100s with BF16.

FP8 support is gonna be a bit trickier but we are working on it: https://github.com/mosaicml/llm-foundry/pull/271"
abhi-mosaic,2023-05-16 22:27:54+00:00,comment,mosaicml/mpt-7b,"Hi @winglian, we have not tested finetuning with the HF Trainer so I can't guaranteed compatibility.

You can find instructions for fine-tuning with Composer and our LLM Foundry codebase here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train#llm-finetuning. We are committed to maintaining this repo for community and customers, and you can file Github issues directly there!"
abhi-mosaic,2023-05-08 16:43:58+00:00,comment,mosaicml/mpt-7b,"Please check out the LLM Foundry (https://github.com/mosaicml/llm-foundry/tree/main) 

and the requirements are here: https://github.com/mosaicml/llm-foundry/blob/3959eaccba53c444c5705d600d333cc3d47bc06c/setup.py#L48"
huseinzol05,2023-05-08 11:11:07+00:00,comment,mosaicml/mpt-7b,Added `inputs_embeds` parameter.
daking,2023-05-08 17:31:19+00:00,comment,mosaicml/mpt-7b,Hi `inputs_embeds` is not implemented inside the modeling code. It is not sufficient to just add the parameter. Could you please explain why you need this parameter?
cekal,2023-05-08 17:35:14+00:00,comment,mosaicml/mpt-7b,"@daking so it could be trained using LoRA. PEFT is not able to run.

Also, I have a question that is not related to this issue. Is mosaicml/mpt-7b the same as togethercomputer/RedPajama-INCITE-Base-7B-v0.1? Or what are the differences between those 2? Edit: mpt-7b is better. But I'm not sure how to train it with LoRA since it is not supported... Would be good to add some support"
daking,2023-05-08 18:00:56+00:00,comment,mosaicml/mpt-7b,"They are completely different models. `mosaicml/mpt-7b` is trained and released by MosaicML, and `togethercomputer/RedPajama-INCITE-Base-7B-v0.1` is trained and released by `togethercomputer`. Could you please move the peft/lora discussion and any issues you have to https://github.com/mosaicml/llm-foundry/issues/64?"
Innova,2023-06-16 10:40:12+00:00,comment,mosaicml/mpt-7b,This comment has been hidden
srijanroy,2023-05-08 11:49:24+00:00,comment,mosaicml/mpt-7b,"You may try with 'Wqkv'. 

MPTForCausalLM(
-(transformer): MPTModel(
--(wte): Embedding(50432, 4096)
--(emb_drop): Dropout(p=0, inplace=False)
--(blocks): ModuleList(
---(0-31): 32 x MPTBlock(
----(norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
----(attn): MultiheadAttention(
-----(Wqkv): Linear(in_features=4096, out_features=12288, bias=False)
-----(out_proj): Linear(in_features=4096, out_features=4096, bias=False)
----)
----(norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
----(ffn): MPTMLP(
-----(up_proj): Linear(in_features=4096, out_features=16384, bias=False)
-----(act): GELU(approximate='none')
-----(down_proj): Linear(in_features=16384, out_features=4096, bias=False)
----)
----(resid_attn_dropout): Dropout(p=0, inplace=False)
----(resid_ffn_dropout): Dropout(p=0, inplace=False)
---)
--)
--(norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
-)
)"
daking,2023-05-08 17:31:58+00:00,comment,mosaicml/mpt-7b,Lets move this discussion to https://github.com/mosaicml/llm-foundry/issues/64 please :)
galco,2023-05-08 12:22:14+00:00,comment,mosaicml/mpt-7b,"I am also intrested in this one
Has anyone had luck here?

Thanks !"
daking,2023-05-08 17:33:35+00:00,comment,mosaicml/mpt-7b,Lets move this discussion here https://github.com/mosaicml/llm-foundry/issues/67 please :)
zokica,2023-05-07 15:28:29+00:00,comment,mosaicml/mpt-7b,"The out:

MPT 7b:
    
    A lion is a large cat. Lions are native to Africa. Lions live in the savanna, a grassland
    timea = time.time() 43.37369394302368

LLama 7b:

    <s> A lion is the king of the jungle. The lion is the strongest animal in the animal kingdom
    timea = time.time() 6.919593811035156"
keldenl,2023-05-08 08:56:29+00:00,comment,mosaicml/mpt-7b,you're comparing ggml vs PyTorch – until this gets the ggml treatment expect the speeds to be slower on CPU only
zokica,2023-05-08 13:24:30+00:00,comment,mosaicml/mpt-7b,"> you're comparing ggml vs PyTorch – until this gets the ggml treatment expect the speeds to be slower on CPU only

How did you conclude that i used ggml?

Of course, I did not use ggml, I used exactly the same BF16 for both llama and mpt-7b and llama is much faster.
 model_name = ""huggyllama/llama-7b""
 model_name = ""mosaicml/mpt-7b""



Here is exactly what I used for llama so you can replicate and see for yourself:

    import torch
    import transformers
    from transformers import AutoTokenizer, AutoModelForCausalLM
    model_name = ""huggyllama/llama-7b""
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True,trust_remote_code=True)
    
    import time
    timea = time.time()
    prompt = ""A lion is""
    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
    outputs = model.generate(
        inputs.input_ids, max_new_tokens=20, do_sample=True, temperature=0.75 , return_dict_in_generate=True
    )
    token = outputs.sequences[0]
    output_str = tokenizer.decode(token)
    print(output_str)
    print(""timea = time.time()"",-timea + time.time())"
abhi-mosaic,2023-05-11 23:52:04+00:00,comment,mosaicml/mpt-7b,"Hi @zokica , we will take a look at this as we're seeing a couple reports of slow CPU inference. Since you have a system on hand that is showing the issue, could you help confirm if any of the MPT vs. LLaMa speed gap changes based on the `torch_dtype` and `low_cpu_mem_usage` flags? Basically this matrix:

* torch_dtype=torch.float32, low_cpu_mem_usage=False: ?
* torch_dtype=torch.float32, low_cpu_mem_usage=True: ?
* torch_dtype=torch.bfloat16, low_cpu_mem_usage=False:  ?
* torch_dtype=torch.bfloat16, low_cpu_mem_usage=True: MPT slower than LLaMa

In the meantime we will try to reproduce as well. Thank you for the report!"
stefan-berlin,2023-05-12 09:47:52+00:00,comment,mosaicml/mpt-7b,"Hi,

I testet both scripts from zokica above,
on a cheap VPS, 18 Cores, 48 GB RAM, 2048 GB SSD (RAID10).

LLaMa still faster, but with float32 ""just"" by factor 2.

torch_dtype=torch.float32, low_cpu_mem_usage=False:	|	MPT: 95.7	|		LLaMa: 43.2
torch_dtype=torch.float32, low_cpu_mem_usage=True:	|	MPT: 98.6s	|		LLaMa: 48.6
torch_dtype=torch.bfloat16, low_cpu_mem_usage=False:|	MPT: 1747.8	|		LLaMa: 177.7
torch_dtype=torch.bfloat16, low_cpu_mem_usage=True:	|	MPT: 1764.6s	| 	LLaMa: 178.2"
abhi-mosaic,2023-05-12 17:15:18+00:00,comment,mosaicml/mpt-7b,"Thank you so much! This definitely seems like a bottleneck somewhere in the MPT forward or KV cacheing logic. It's very interesting that this shows up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x faster for MPT with `triton`). We will look into it  and patch the model source once we find a fix. 

Last question, what version of torch were you using for those results?"
zokica,2023-05-12 17:49:24+00:00,comment,mosaicml/mpt-7b,"I actually run it via BF16, as I have only 32 GB of ram in this server, so i had to use a low ram option. 

Is there any other way to run it on a CPU without using bf16 with just 32 GB of memory?

I am using, and probably most people will just use the CPU for testing,  it would be nice if this could work a bit faster, but not so  much of a problem.

So it works faster than LLama on a GPU, right, even without triton ?"
Combatti,2023-05-12 18:14:18+00:00,comment,mosaicml/mpt-7b,"> you're comparing ggml vs PyTorch – until this gets the ggml treatment expect the speeds to be slower on CPU only

There are ggml versions in hugging face 🤗"
stefan-berlin,2023-05-12 18:53:28+00:00,comment,mosaicml/mpt-7b,"> Thank you so much! This definitely seems like a bottleneck somewhere in the MPT forward or KV cacheing logic. It's very interesting that this shows up on CPU but not on GPU (where we saw the opposite relation, ~1.5-2x faster for MPT with `triton`). We will look into it  and patch the model source once we find a fix. 
> 
> Last question, what version of torch were you using for those results?

2.0.1+cpu"
nib12345,2023-05-18 08:21:33+00:00,comment,mosaicml/mpt-7b,"For me. It is taking 35 mins to generate 100 tokens.
Laptop specification: No GPU, 20 GB RAM (4+16 GB), 1 TB SSD, I5 processor.
I have very slow Laptop with No GPU.

```
def customGenerate(argPrompt):
    inputs = tokenizer(argPrompt, return_tensors='pt').to(model.device)
    outputs = model.generate(
        **inputs, max_new_tokens=1, do_sample=True, temperature=0.75 , return_dict_in_generate=True
    )
    token = outputs.sequences[0]
    output_str = tokenizer.decode(token)

    return output_str

import time
from datetime import datetime
timea = time.time()
dtNow = datetime.now()
print(""now ="", dtNow)
print(""Start time: "",-timea + time.time())

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
model_name = ""mosaicml/mpt-7b""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True,trust_remote_code=True)

prompt = [""Earth is""]

count=0

while(count < 100):
    output_str = customGenerate(prompt[prompt.__len__()-1])
    prompt.append(output_str)
    print(prompt.__len__(), ': ' , prompt[prompt.__len__()-1])
    print(""Time taken in sec:"",-timea + time.time())
    print(""Time taken in min:"",((-timea + time.time())/60))
    count = count + 1

dtNow = datetime.now()
print(""now ="", dtNow)
```"
CShorten,2023-05-25 11:58:40+00:00,comment,mosaicml/mpt-7b,"I am having a hard time running this on CPU, could someone please help me? I get the error:

ImportError: This modeling file requires the following packages that were not found in your environment: einops. Run `pip install einops`

But then it seems einops needs to find a CUDA Driver to be installed :("
abhi-mosaic,2023-06-03 01:14:18+00:00,comment,mosaicml/mpt-7b,"The CPU load time should be fixed now as of this PR as long as you use `device_map=auto`: https://huggingface.co/mosaicml/mpt-7b/discussions/47
We also added some logic to improve KV cacheing speed. Let us know if you see improvements!"
sam-mosaic,2023-06-14 07:47:49+00:00,comment,mosaicml/mpt-7b,"Closing as complete, but if anyone sees any CPU inference speed issues, please reopen this or open a new issue!"
jfrankle,2023-05-06 22:54:23+00:00,comment,mosaicml/mpt-7b,"1. It is our company policy not to talk about our future plans until we launch them. We prefer to speak through our work rather than our words. We are working on many exciting projects right now, and we hope you will find them valuable when they launch.
2. We're not sure, but we'd be very curious to hear how it goes for you. We'd be excited to accept pull requests that support it in https://www.github.com/mosaicml/llm-foundry "
zokica,2023-05-07 15:24:39+00:00,comment,mosaicml/mpt-7b,"Ok, I will update once i try the Peft."
jfrankle,2023-05-07 17:07:29+00:00,comment,mosaicml/mpt-7b,Awesome - thank you! Please keep us posted!
daking,2023-05-08 17:34:17+00:00,comment,mosaicml/mpt-7b,"Lets move the peft/lora discussion here https://github.com/mosaicml/llm-foundry/issues/64 to keep it all in one place, so everyone can share their progress :)"
jfrankle,2023-05-06 17:09:17+00:00,comment,mosaicml/mpt-7b,You beat me to it :)
jfrankle,2023-05-06 12:42:47+00:00,comment,mosaicml/mpt-7b,"We recommend people consult a lawyer before deciding how they want to use a model. Licensing decisions do not constitute legal advice, and we are not responsible for the actions of third parties who use the model."
alexjc,2023-05-06 16:24:45+00:00,comment,mosaicml/mpt-7b,"Licensing decisions are not legal advice, but MosaicML is responsible as a company for the consequences of its own licensing decisions.  Especially if you don't have the rights to license the model, or you license things selectively.  The topic here is not about downstream users but about MosaicML taking responsible action, not misleading users with licenses it doesn't have the right to give.

I'm glad you reverted the Storywriter licence back because it'll be more interesting to watch this play out over the next few days ;-). It still, however appears to be discriminatory licensing and unfair business practice."
alexjc,2023-05-06 16:47:20+00:00,comment,mosaicml/mpt-7b,"> [...] we are not responsible for the actions of third parties who use the model.

FYI, this likely won't fly in court if you end up there. The responsibility is on you to not misrepresent what you're licensing, otherwise it's a form of fraud relating to granting rights you don't have. Copyright fraud. Especially as you explicitly wrote _""commercial use permitted""_ in brackets, you and your company has providing legal advice and interpretation of the license.

Additionally, MosaicML chose one license and then changed it back, licensed some models selectively, etc. It doesn't look like MosaicML really considered the implications of their decision at all before starting or when applying the changes. This looks like negligence in licensing and simply reinforces the claims against the company.

You should also consult lawyer before you release models, or if you did, consult a better one!"
deleted,2023-05-06 16:56:12+00:00,comment,mosaicml/mpt-7b,Lets beat on them about silly stuff for giving us something for free that was not trivial to create. ya  !  ( that was sarcasm btw. )   Everyone should be more inviting to contributions.
cekal,2023-05-06 22:23:29+00:00,comment,mosaicml/mpt-7b,Better call Saul
cekal,2023-05-06 22:34:35+00:00,comment,mosaicml/mpt-7b,"Anyway, I don't understand the point of this discussion since OpenAI is doing the same thing, just on another scale. When it comes to copyright infringement in LLMs, there aren't (yet) any clear laws that would prohibit this. With that being said, we should be thankful that we've got hands on something which cost Mosaic 200k+ to develop."
alexjc,2023-05-07 11:42:45+00:00,comment,mosaicml/mpt-7b,"@cekal The license wouldn't affect you getting your hands on the models, just the terms.

> When it comes to copyright infringement in LLMs, there aren't (yet) any clear laws that would prohibit this.

The EU debated this for its AI Act and decided that Copyright laws were sufficient, and indeed the growing consensus is that they are.  Besides, this post is about the discriminatory licensing, respecting the rights of some organizations and not others — so this would fall outside of Copyright anyway."
julien-c,2023-05-09 14:51:53+00:00,comment,mosaicml/mpt-7b,Closing as report seems to have been addressed by Mosaic team
alexjc,2023-05-09 15:25:51+00:00,comment,mosaicml/mpt-7b,"@julien-c Reopening as I'd appreciate if there was some oversight from HuggingFace into this issue. (In general, there is a need for moderation when you're called upon.) Please share whether you think this form of licensing legally compliant and conforms to your own Terms of Service. Thanks!"
nacs,2023-05-18 16:16:46+00:00,comment,mosaicml/mpt-7b,"@alexjc if you don't like the terms or license, move on to a different model.

you're coming off as an attention-seeking troll from the way you're behaving in this thread."
alexjc,2023-05-24 09:12:36+00:00,comment,mosaicml/mpt-7b,"@nacs You missed the point, but I guess you didn't read the thread? The license claimed is not valid for the purpose they repeatedly stated. I'm not seeking attention as intentionally ignoring legal and ethical issues is the norm (in a bad way) for both the model owners and the platform.

@julien-c It appears you re-closed this thread intentionally. This action hides the concerns (which don't show up by default in the Community Tab) and accepts the fact that no response was provided by the model owner.  You did not provide a position from HuggingFace, so I presume your conclusion is that everything is acceptable based on your investigation of the matter.  Please correct if this is not the case, you will receive a notification by being tagged."
abhi-mosaic,2023-05-05 23:05:12+00:00,comment,mosaicml/mpt-7b,"Yes, you can run on CPU with the standard `model.generate()` instructions as long as you have enough RAM to hold the model weights. This is ~30GB if using FP32 or ~15GB if using BF16."
liadlevy,2023-05-05 23:14:38+00:00,comment,mosaicml/mpt-7b,"> Yes, you can run on CPU with the standard `model.generate()` instructions as long as you have enough RAM to hold the model weights. This is ~30GB if using FP32 or ~15GB if using BF16.

Thanks, Also I’m having trouble to load the model using oobabooga webui. Any instructions how can I use this model with the WebUI framework?"
jfrankle,2023-05-06 13:14:12+00:00,comment,mosaicml/mpt-7b,We leave it to the developers of oobabooga to add support for our model architecture.
sam-mosaic,2023-05-05 17:07:28+00:00,comment,mosaicml/mpt-7b,"We would happily accept PRs, but don't have internal resources devoted to it"
orendar,2023-05-05 16:11:25+00:00,comment,mosaicml/mpt-7b,"Upon review it looks like it might not like that my HF cache directory is called ""cache"", so I can solve it myself by downloading to a different directory."
JackFram,2024-01-27 13:11:57+00:00,comment,JackFram/llama-160m,"Hi thanks for your interests.

We trained it using the conventional transformer trainer on the downstream task for one epoch

I am not sure which task you want to fine-tune on, I guess the setup depends on different tasks, but I think if you want to naive fine-tuning (i.e., not collective-boost tuning) you can stick with trainer, which can help you adapt some of the params.

 For this model we only do pretrain, so we basically use the raw format for C4 and wikitext

We don't have the plan for releasing a better version for now, but there is a chance that we will release another speculation architecture along with the tuned model weights."
goliaro,2023-11-05 19:46:54+00:00,comment,JackFram/llama-160m,
JackFram,2023-09-08 18:23:06+00:00,comment,JackFram/llama-160m,Thx.
JackFram,2023-09-03 01:20:25+00:00,comment,JackFram/llama-160m,Thx for the fix!
Xenova,2023-09-03 01:22:11+00:00,comment,JackFram/llama-160m,@JackFram Happy to help :) PS: [this model](https://huggingface.co/JackFram/llama-68m) has the same problem which you can fix in the same way.
JackFram,2023-09-03 03:17:42+00:00,comment,JackFram/llama-160m,"Gotcha, just get it fixed!"
JackFram,2023-07-12 18:13:23+00:00,comment,JackFram/llama-160m,Just set it to other
jeonwonseok0125,2024-01-02 04:55:30+00:00,comment,JackFram/llama-160m,"Since the model license is 'other', it's unclear that we can freely use this model for research purpose or not."
jsilverman26,2024-06-08 16:19:40+00:00,comment,upstage/SOLAR-10.7B-v1.0,Model card 잘 읽어보면 다 나옵니다 
Joseph717171,2024-04-02 02:15:44+00:00,comment,upstage/SOLAR-10.7B-v1.0,"
![Screenshot 2024-04-01 at 7.14.56 PM.png](https://cdn-uploads.huggingface.co/production/uploads/63c1dfd4a0ffa3857eb362a9/nFL9wCNqD6VmIMXBotJzw.png)

source: https://arxiv.org/html/2312.15166v2"
Joseph717171,2024-04-02 02:17:20+00:00,comment,upstage/SOLAR-10.7B-v1.0,"
![Screenshot 2024-04-01 at 7.17.02 PM.png](https://cdn-uploads.huggingface.co/production/uploads/63c1dfd4a0ffa3857eb362a9/qOPCwjqOGSJgR3P_Th3t_.png)

Source: https://arxiv.org/html/2312.15166v2"
falca,2024-04-02 08:36:20+00:00,comment,upstage/SOLAR-10.7B-v1.0,"So by ""continued pretraining"" you mean instruction tuning? Otherwise can't find info on that either. Would really appreciate if you clarify on that moment a little more."
Joseph717171,2024-04-02 22:26:13+00:00,comment,upstage/SOLAR-10.7B-v1.0,"No, for UpStage’s Solar-10.7B, they took base weights, Ups-Scaled them, and then continued pretraining. Pretraining is different than fine-tuning; it requires more data/information than fine-tuning. “2. Depth Up-Scaling”, details the continued pretraining. “3. Training”, details the fine-tuning and instruction tuning. With that being said, perhaps UpStage, can fill in any blanks that we don’t have the information to fill. 🤔"
falca,2024-04-15 12:23:42+00:00,comment,upstage/SOLAR-10.7B-v1.0,"Yes, I've read that section. Unfortunately, it does not mention anything about data for pretraining: datasets or number of tokens.
![image.png](https://cdn-uploads.huggingface.co/production/uploads/636799ab4e7a848a1886f392/AHQ4xT0U-f-yw2aRF_R3E.png)
"
HR1777,2024-02-24 05:48:43+00:00,comment,upstage/SOLAR-10.7B-v1.0,Its 4096.
bongchoi,2023-12-27 05:31:24+00:00,comment,upstage/SOLAR-10.7B-v1.0,"I'm also curious, how many tokens were used for continual pretrain?"
Leon-Leee,2023-12-29 02:16:23+00:00,comment,upstage/SOLAR-10.7B-v1.0,curious too. I read their paper and didn't find the details. https://browse.arxiv.org/html/2312.15166v1
hunkim,2023-12-30 03:25:58+00:00,comment,upstage/SOLAR-10.7B-v1.0,"Details of Data Sets and Training Techniques: Thank you for your interest! Unfortunately, due to the high level of competition in this field, we are unable to share detailed information about the training techniques and datasets used. We appreciate your understanding. However, we have released a list of fine-tuning datasets, https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0."
YeungNLP,2024-01-03 09:17:03+00:00,comment,upstage/SOLAR-10.7B-v1.0,how much data is used in continue pretraining?
pszemraj,2024-01-03 16:31:05+00:00,comment,upstage/SOLAR-10.7B-v1.0,"@hunkim thanks! understood. I'm primarily interested in this checkpoint `upstage/SOLAR-10.7B-v1.0` as it is apache-2.0 - based on your response it seems like you all have done your homework. I assume there is no issue using `upstage/SOLAR-10.7B-v1.0` to the fullest extent of it's apache-2.0 license, including synthetic data generation, commercial use, etc. Please advise if my interpretation is incorrect & thanks again."
hunkim,2024-01-03 17:44:00+00:00,comment,upstage/SOLAR-10.7B-v1.0,@pszemraj You are right! Enjoy!
hunkim,2023-12-30 03:28:24+00:00,comment,upstage/SOLAR-10.7B-v1.0,Could you please take a look at this link: <https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0>. Thanks! 😊
hunkim,2023-12-30 03:26:25+00:00,comment,upstage/SOLAR-10.7B-v1.0,Can you please check the link https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0? Thank you! :)
hunkim,2023-12-15 06:50:55+00:00,comment,upstage/SOLAR-10.7B-v1.0,This is for English.
hunkim,2023-12-15 06:52:08+00:00,comment,upstage/SOLAR-10.7B-v1.0,We will write and publish on Arxiv soon. Thank you!
wcde,2023-12-13 14:29:50+00:00,comment,upstage/SOLAR-10.7B-v1.0,"@kristaller486 it's just frankenstein of llama2 and mistral, which was further trained after mix. You don't need anything special for fine-tuning."
hunkim,2023-12-16 07:39:53+00:00,comment,upstage/SOLAR-10.7B-v1.0,@wcde thanks!
Overbite1741,2023-12-16 22:22:07+00:00,comment,upstage/SOLAR-10.7B-v1.0,"@wcde First of all there is no standard frankenstein. You could combine layers in various ways, optionally use lower triangular matrix, do some more advanced maths etc. 

Second can you expand further trained. Trained on what data? For how many tokens?"
Dampfinchen,2023-12-13 12:50:15+00:00,comment,upstage/SOLAR-10.7B-v1.0,Does it have grouped query attention? It's a huge deal as it saves a ton of context related memory.
hunkim,2023-12-30 03:27:49+00:00,comment,upstage/SOLAR-10.7B-v1.0,"We only used the Llama architecture and Mistral weight. For more details, please check out the paper at https://huggingface.co/papers/2312.15166. 😊"
jonabur,2024-04-04 06:54:00+00:00,comment,LumiOpen/Poro-34B,"Unfortunately we will probably not be able to pay for a space, but we would like to publish directions to run quantized version in a colab notebook soon.  Also there is a fine-tuned version coming shortly,  so keep an eye out!"
jonabur,2024-02-21 07:22:24+00:00,comment,LumiOpen/Poro-34B,Sorry for the delay -- final release is today!
mpasila,2024-01-22 14:56:46+00:00,comment,LumiOpen/Poro-34B,If you want to run it unquantized then no (it will need a minimum of 48gb for that) but if you use bitsandbytes and load it in 4bits then it needs about 20gb and with GPTQ at 3bits it needs about 15gb but it still ran out of memory on a 16GB GPU when I tried loading it with this [TheBloke/Poro-34B-GPTQ:gptq-3bit-128g-actorder_True](https://huggingface.co/TheBloke/Poro-34B-GPTQ/tree/gptq-3bit-128g-actorder_True). So the best bet might be using a GGUF version by [TheBloke](https://huggingface.co/TheBloke/Poro-34B-GGUF). If someone makes an EXL2 variants of this model at around 3bpw or lower it might actually fit on a 16gb card no problem.
jonabur,2024-01-25 09:31:53+00:00,comment,LumiOpen/Poro-34B,Thank you!
jonabur,2024-02-21 07:24:40+00:00,comment,LumiOpen/Poro-34B,"We will be publishing more detailed results soon, but MMLU on the final checkpoint is 46.29 and HumanEval Pass@10 is 37.20.  We hope to release an instruction tuned version soon, but are still evaluating open dataset options."
jonabur,2023-11-13 12:56:11+00:00,comment,LumiOpen/Poro-34B,"The effective batch size we're referring to is just the product of the global batch size and the sequence length, or in this case 2048*2048=4194304.  We’re running sequence and tensor parallel with gradient accumulation."
bjoernp,2023-11-13 13:00:57+00:00,comment,LumiOpen/Poro-34B,"Oh I understand, must have missed the mention of gradient accumulation. Thanks for clarifying! Perhaps it might be helpful to include this in the table (`gradient accumulation steps = 2`)."
jonabur,2023-11-13 14:21:37+00:00,comment,LumiOpen/Poro-34B,I added a note to the training about using GAS=16.  Thanks for the feedback!
YShow,2024-04-30 02:17:22+00:00,comment,01-ai/Yi-34B-Chat,You can load the tokenizer and save the information to a JSON file on your own
NLPI,2024-01-25 12:17:05+00:00,comment,01-ai/Yi-34B-Chat,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/62cbbbab1eb545f589e75eaa/B-QN-ZJEbCyz7cmUMKNmi.png)
"
MeisterDeLaV,2024-01-30 02:43:14+00:00,comment,01-ai/Yi-34B-Chat,"Hi there! Thank you for the Question! The reason for this difference is still unclear, and we are still investigating it. We will update you on the matter as soon as we find out."
XXVIMK,2024-01-22 04:17:17+00:00,comment,01-ai/Yi-34B-Chat,Yi-34B-Chat和Yi-34B-Chat-4bits的区别是在于，Yi-34B-Chat-4bits做了量化（以一定的推理精度作为代价，来换取更快的速度）。如果是这两个比较的话，建议微调Yi-34B-Chat。不过更建议直接微调Yi-34B。
tianjun,2023-12-18 02:54:09+00:00,comment,01-ai/Yi-34B-Chat,"> batch 是在哪儿设置的?

这个就是一次推理时候的样本数，具体设置得看你用的是什么框架。

> 以及batch是指一次处理prompt的数量吗?

是的"
fblgit,2023-12-25 18:26:02+00:00,comment,01-ai/Yi-34B-Chat,"Not seeing much activity, we'll try to figure out something else. Thanks"
Kai01ai,2023-11-30 03:09:31+00:00,comment,01-ai/Yi-34B-Chat,Duplicate of [#5](https://huggingface.co/01-ai/Yi-34B-Chat/discussions/5)
Annorita,2023-12-19 02:43:47+00:00,comment,01-ai/Yi-34B-Chat,"Can we just modify the bos and eos setting in `config.json` and `tokenizer_config.json` and make it align with `generation_config.json`? If yes, I can make a PR."
Yhyu13,2023-11-30 02:37:52+00:00,comment,01-ai/Yi-34B-Chat,"Dutch translated into Chinese :

我是Yi，一个基于OpenAl平台GPT-3的01.AI人工智能模型。
我的工作是为您提供有用的信息并回答您的问题。
我随时准备为您提供帮助！

I have not tried out the Yi-34B-chat version. The original Yi would not answering like that, I am speculating it is due to the chat fine tuning is mostly sythsatic data collected from ChatGPT? "
richardllin,2023-11-30 10:47:01+00:00,comment,01-ai/Yi-34B-Chat,"Hi @JosephusCheung 

Thank you for your feedback.

1. When performing a comprehensive web data crawl, it's inevitable to collect content generated by OpenAI, so the pre-trained Base model might produce answers like the one in your screenshot.
2. Chat, as an SFT model, corrects identity information through alignment. However, since it is fine-tuned based on the Base model, there is still a slight possibility of generating responses like the ones in your screenshot; this also aligns with your comment ""Requires multiple retries to reproduce.""
3. This phenomenon also occurs in other open source models (not naming any), and indeed, the industry as a whole currently needs to make improvements in this area.
4. At this stage, the main task of our team is to make the model stronger. Data cleaning and correction is also an ongoing effort, and further iterations will be rolled out in phases.

In Chinese:
1. 在进行全网数据爬取时，不可避免的会采集到OpenAI生成的内容，因此在预训练后的Base模型可能会发生您截图中的回答。
2. Chat作为SFT模型，会通过对齐来修正身份信息，但由于是基于Base模型进行fine tune，仍在小概览会发生您截图中的回答；这也呼应了你的留言“要多试几次才能复现”。
3. 这种现象也在其他开源大模型中发生（就不点名了），当前确实行业在这方面都有共同需要提升的空间。
4. 现阶段团队的主要任务是把模型做得更强，数据清洗修正也是长期在投入的工作，后续会分阶段迭代。"
markding,2024-02-15 08:22:58+00:00,comment,01-ai/Yi-34B-Chat,"I have the same question. I found reference to COIG and to rm-static in the code, but there is no datasheet available showing exactly what this model was instruction-tuned on and how. FWIW, we're keeping track of LLM openness at https://opening-up-chatgpt.github.io and Yi 34B Chat is currently in the **bottom 5** (out of >30 'open' instruction tuned models) by degrees of openness because so little of source code, training data, instruction tuning etc. is shared or documented.

![image.png](https://cdn-uploads.huggingface.co/production/uploads/62d1218684bfbee86b6ee521/5ciXIQqdhC7tW38QX0uF8.png)
"
Andriy,2024-03-31 02:48:50+00:00,comment,01-ai/Yi-34B-Chat,Please answer the question.
YShow,2024-04-30 02:21:45+00:00,comment,01-ai/Yi-34B-Chat,Please refer to the technical report：https://arxiv.org/abs/2403.04652
markding,2024-04-30 05:51:51+00:00,comment,01-ai/Yi-34B-Chat,"That preprint describes SFT data selection methods and cites some work it takes inspiration from, but as far as I can see does not specify any of the actual training data sets used. 

> Our finetuning dataset consists of less than 10K multi-turn instruction-response dialog pairs, with each and every one of the entry constructed and polished over multiple iterations and from user feedback. 

What are those dialog pairs, where were they sourced from, what was the user feedback?"
lorinma,2024-06-03 07:11:21+00:00,comment,01-ai/Yi-34B-Chat,The SFT data was manually created in-house. 
FancyZhao,2023-11-24 13:21:57+00:00,comment,01-ai/Yi-34B-Chat,Please contact us (yi@01.ai) if you are from ChatNBX team.
Kai01ai,2023-11-24 05:13:45+00:00,comment,01-ai/Yi-34B-Chat,"Hi @krao
The `generate` function will use eos token as stop token. In our chat template format(which is ChatML), we use the ""<|im_end|>"" as the end token of the response. So we change the `eos_token_id` in `generation_config` to `7` which map to ""<|im_end|>"". 
And yes, the `bos_token_id` should have no effect here."
krao,2023-11-25 01:23:15+00:00,comment,01-ai/Yi-34B-Chat,"> And yes, the bos_token_id should have no effect here.

Do you mean `bos_token` and `eos_token`?"
Kai01ai,2023-11-25 01:47:07+00:00,comment,01-ai/Yi-34B-Chat,"The chat model is developed upon the base model, which utilizes distinct training templates:
- **base model**: Typically trained with a template such as ""{document}<|endoftext|>"", To format this appropriately, one can employ `tokenizer.encode(document, add_bos_token=add_bos_token, add_eos_token=add_eos_token)`, and designate ""<|endoftext|>"" as the stop token during generation.
- **chat model**: Often trained using a template represented by  ""<|im_start|>...<|im_end|>"", For proper formatting, the method `tokenizer.apply_chat_template(messages)` is used, and designate ""<|im_end|>"" as stop token during generation.

It's important to note that the bos and eos settings found in `config.json` and `tokenizer_config.json` are inherited from the base model. However, the settings in `generation_config.json` are specifically defined by the chat model.

If you have any further question, feel free to ask!"
krao,2023-11-26 01:38:57+00:00,comment,01-ai/Yi-34B-Chat,"Thank you, that was really helpful!"
Annorita,2023-12-19 02:42:27+00:00,comment,01-ai/Yi-34B-Chat,"Hi, can I just modify the bos and eos setting in `config.json` and `tokenizer_config.json` and make it align with `generation_config.json`? If yes, I can make a PR."
Kai01ai,2023-11-24 02:53:52+00:00,comment,01-ai/Yi-34B-Chat,"The chat template format is ChatML, which can be found in tokenizer_config.json, you can convert messages to correct prompt by `tokenizer.apply_chat_template`
```text
<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
```"
apepkuss79,2023-11-24 02:55:54+00:00,comment,01-ai/Yi-34B-Chat,@Kai01ai Thanks for the quick reply! 赞
reedcli,2023-11-24 01:39:22+00:00,comment,01-ai/Yi-34B-Chat,"> The model card says:
> 
> > Both of them are trained with 4K sequence length and can be extended to 32K during inference time.
> 
> But... What does this mean? The max size appeaes to be 4K:  
> 
> > ""max_position_embeddings"": 4096
> 
> By extend to 32K, do you mean RoPE alpha scaling? Something else?

You can try to set `""max_position_embeddings"": 32768` directly."
brucethemoose,2023-11-24 05:30:40+00:00,comment,01-ai/Yi-34B-Chat,Thanks!
tianjun,2023-11-23 09:23:44+00:00,comment,01-ai/Yi-34B-Chat,"No, we do not have such plan yet."
degfy,2023-11-28 04:11:05+00:00,comment,01-ai/Yi-34B-Chat,"> No, we do not have such plan yet.

Being able to read long documents is very important, hope to support.


"
TianyuLLM,2023-11-28 09:19:46+00:00,comment,01-ai/Yi-34B-Chat,请求大佬支持200K
tianjun,2023-11-23 08:19:07+00:00,comment,01-ai/Yi-34B-Chat,"Hi @osanseviero ,

Thanks for this PR. This should be fixed in https://github.com/01-ai/Yi/pull/164 and back synced here now."
bluenevus,2024-02-28 00:24:31+00:00,comment,openlm-research/open_llama_3b_v2,"this is what our prompt looks like and it works
<pre>
{%- set ns = namespace(found=false) -%}
{%- for message in messages -%}
    {%- if message['role'] == 'system' -%}
        {%- set ns.found = true -%}
    {%- endif -%}
{%- endfor -%}
{%- if not ns.found -%}
    {{- '' + 'Below is an instruction that describes a task. Write a response that appropriately completes the request.' + '\n\n' -}}
{%- endif %}
{%- for message in messages %}
    {%- if message['role'] == 'system' -%}
        {{- '' + message['content'] + '\n\n' -}}
    {%- else -%}
        {%- if message['role'] == 'user' -%}
            {{-'### Instruction:\n' + message['content'] + '\n\n'-}}
        {%- else -%}
            {{-'### Response:\n' + message['content'] + '\n\n' -}}
        {%- endif -%}
    {%- endif -%}
{%- endfor -%}
{%- if add_generation_prompt -%}
    {{-'### Response:\n'-}}
{%- endif -%}
</pre>"
ScottMueller,2023-12-31 18:05:36+00:00,comment,openlm-research/open_llama_3b_v2,"Generated with:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""openlm-research/open_llama_3b_v2"")
assert tokenizer.is_fast
tokenizer.save_pretrained(""..."")
```"
jonatanklosko,2023-09-25 06:50:58+00:00,comment,openlm-research/open_llama_3b_v2,"Generated with:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""openlm-research/open_llama_3b_v2"")
assert tokenizer.is_fast
tokenizer.save_pretrained(""./output/openlm-research/open_llama_3b_v2"")
```

Sidenote: it looks like the issue with fast tokenizer mentioned in the README (https://github.com/huggingface/transformers/issues/24233) has been resolved."
afmck,2023-09-01 22:01:40+00:00,comment,openlm-research/open_llama_3b_v2,@sanchit-gandhi as requested :)
sanchit-gandhi,2023-09-04 10:46:55+00:00,comment,openlm-research/open_llama_3b_v2,Thanks @afmck! cc @young-geng and @ haoliu - would one of you kindly be able to merge the Flax weights for `open_llama_3b_v2`? This is part of the ongoing integration: https://github.com/huggingface/transformers/pull/24587
agouloumis,2024-04-25 15:22:31+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Yeah I've got the same problem. I merged the 4 sharded files of Q5_K_M into one, and then i get this error"
MaziyarPanahi,2024-04-25 16:31:00+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"These are not just a simple split, these are shards of GGUF models. You don't need to merge them: https://huggingface.co/MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF#load-sharded-model

If you must, you need to use the native GGUF Merge function to do this. But you don't have to, it can work with splits as they are."
MaziyarPanahi,2024-04-23 14:26:19+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Yes, apparently the template is very sensitive (as most models coming out of mistralai). Once space here and there, you will get those vowels. (I was told on Twitter, and then they fixed the prompt and said it got much better)"
MaziyarPanahi,2024-04-21 08:33:18+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Hi,

It's very easy, you can follow this tutorial to merge them back together: https://github.com/ggerganov/llama.cpp/discussions/6404"
RudyXu,2024-04-21 09:00:44+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,Check this solution: https://huggingface.co/TheBloke/Falcon-180B-Chat-GGUF/discussions/1
MaziyarPanahi,2024-04-18 17:52:43+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Hi @Esj-DL 

You are very welcome, glad it's useful. The Llama.cpp packages the tokenizer inside the GGUF, so nothing to do. You just need to follow the template of the model for better results. Since this model has new feature like TOOL calling, here is a nice example as how the prompt should look like:

https://github.com/mistralai/mistral-common/blob/main/examples/tokenizer.ipynb"
Esj-DL,2024-04-18 18:37:10+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,Thanks for your reply.
MaziyarPanahi,2024-04-18 20:58:45+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,This comment has been hidden
MaziyarPanahi,2024-04-18 11:23:00+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Llama.cpp has the correct tokenizer, you just need to match the template the way they use it. (that's something they didn't share as a chat_template in their tokenizer config). But it works, the tokenizer os the correct one."
MaziyarPanahi,2024-04-18 08:30:13+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-18 08:30:14+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-18 03:26:43+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-18 03:26:43+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-18 00:11:27+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-18 00:11:28+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 22:32:18+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 22:32:19+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 21:52:44+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 21:52:44+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 21:34:08+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Hi @Henk717 

Thanks for your feedback, I didn't know the number of splits would matter since Llama.cpp loads them from the first split. I try to keep the number of splits between 2 or 3 unless they are very big. (there was a bug in max-size by GB, so I go with number of tensors, so it is not as accurate)

Hopefully moving forward I can use the 48G as the max split."
llama-anon,2024-04-17 21:35:37+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,">1 They prefer portable files that are self-contained and now have to merge them locally.

you dont have to merge them



>2 Previous uploaders before gguf-split came out would cap the file size at HF's file limit (So almost 50GB), this meant most models could be downloaded with a single link or otherwise usually only 2 or at most 

huggingface-cli download MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF --local-dir . --include '*Q2_K*gguf'
or
press download a few times

>3
>

you lose

>So the way it's currently implemented requires users to copy more links (And merge locally for files where this should not have been needed if they wish a cleaner model collection).

no

>My community would prefer keeping the splits close to 50gb per split

quantize them for your (((community))) then

>I'm sure others will chime in if this is worse for them.

hello"
llama-anon,2024-04-17 21:36:50+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"> Hi @Henk717 
> 
> Thanks for your feedback, I didn't know the number of splits would matter since Llama.cpp loads them from the first split. I try to keep the number of splits between 2 or 3 unless they are very big. (there was a bug in max-size by GB, so I go with number of tensors, so it is not as accurate)
> 
> Hopefully moving forward I can use the 48G as the max split.

its ok you can just keep on doing what you're doing, thanks for the quants sir"
Henk717,2024-04-17 23:39:20+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"> Hi @Henk717 
> 
> Thanks for your feedback, I didn't know the number of splits would matter since Llama.cpp loads them from the first split. I try to keep the number of splits between 2 or 3 unless they are very big. (there was a bug in max-size by GB, so I go with number of tensors, so it is not as accurate)
> 
> Hopefully moving forward I can use the 48G as the max split.

Number of tensors explains a lot on why all the gguf-split uploaders have been doing odd upload amounts, makes perfect sense now!
The llama-anon account is known to troll what I do on Huggingface but ill still elaborate a bit on the points for full disclosure to explain each stance and why it matters.

The comment about not having to merge them isn't a valid comment in this context, people who merely wish to load a functional model indeed don't have to merge them thanks to gguf-split. But I had multiple people ask how to merge files because they dislike having split models in their collection. So while not a functional neccesity its user preference to have a single GGUF when possible.

The concept that these are 3 files therefore my argument is invalid also does not apply, because I mean as a maximum based on the 50GB file limit. You'd formerly only see 3 different GGUF files on very large models and on the higher sizes of those models. Smaller sizes would still be only 1 or 2 files. So its not a valid counter to my less is more request.

The last 3 things argued I don't think are relevant points and mostly stem from the fact this user was banned on our Discord as evidenced by the (((community)) remark."
llama-anon,2024-04-18 00:56:49+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,">The llama-anon account is known to troll what I do on Huggingface

>what I do

What is it that you do, exactly?

>But I had multiple people ask how to merge files because they dislike having split models in their collection. So while not a functional neccesity its user preference to have a single GGUF when possible.

This preference is specific to your niche community. There are practical advantages to uploading split GGUFs to Huggingface. For instance, download speeds are less likely to be throttled when downloading multiple files, and large models can be distributed across multiple SSDs if necessary.

>The concept that these are 3 files therefore my argument is invalid also does not apply,

If you are referring to the ""3"", it was a response to your empty list item.

>The last 3 things argued I don't think are relevant points and mostly stem from the fact this user was banned on our Discord as evidenced by the (((community)) remark.

A) I did not see a reason to provide a full response as it is already addressed in the other points. Additionally, your users can organize the split GGUFs into a folder for a cleaner and more organized appearance. 

B) If you wish to criticize someone's free work on the internet, it would be more constructive to do it better than them and only afterward start crying about how ""bad"" their work is.

C) I am adding my input to this discussion."
MaziyarPanahi,2024-04-17 21:25:34+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 21:25:34+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 21:02:03+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 21:02:03+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Cannot merge Pull Requests as no changes are associated. This PR will be closed automatically.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 20:58:30+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 20:58:31+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 20:54:38+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,
MaziyarPanahi,2024-04-17 20:53:47+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,
MaziyarPanahi,2024-04-17 21:00:02+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"you are welcome. I use imatrix data for IQ-1 quants, the rest seem to be in a very good quality even the Q2."
Orenguteng,2024-04-18 01:44:03+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,Thanks!! Great! 
MaziyarPanahi,2024-04-17 20:42:37+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 20:42:38+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 19:41:08+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"hi,
I am fixing them now"
Kanth0,2024-04-17 19:44:56+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"this might help
https://www.reddit.com/r/LocalLLaMA/comments/1c6aekr/comment/l00bcrp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button "
MaziyarPanahi,2024-04-17 19:51:31+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"I fixed the issue, kinda mistralai's fault. I am re-uploading them"
luccazen,2024-04-17 19:58:47+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"nice, thanks"
MaziyarPanahi,2024-04-17 18:50:11+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,@Hoioi Sure! I'll do that after this is finished 
MaziyarPanahi,2024-04-17 18:53:40+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Actually I have another server, I'll run it now and re-upload it here: MaziyarPanahi/tulu-2-dpo-13b-GGUF"
Hoioi,2024-04-17 19:10:33+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,Thank you so much. I'm looking forward to it. 
MaziyarPanahi,2024-04-17 22:19:46+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"@Hoioi this is the PR for it, should be done in 10-20 minutes: https://huggingface.co/MaziyarPanahi/tulu-2-dpo-13b-GGUF/discussions/2"
MaziyarPanahi,2024-04-17 18:48:16+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 18:48:17+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 18:07:17+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 18:07:18+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 17:38:15+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"Multi-commit is now completed! You can ping the repo owner to review the changes. This PR can now be commented or modified without risking to corrupt it.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
MaziyarPanahi,2024-04-17 17:38:16+00:00,comment,MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF,"`create_pr=False` has been passed so PR is automatically merged.

_This is a comment posted using the `huggingface_hub` library in the context of a multi-commit. Learn more about multi-commits [in this guide](https://huggingface.co/docs/huggingface_hub/main/guides/upload)._"
rycont,2023-11-28 09:22:31+00:00,comment,EleutherAI/polyglot-ko-1.3b,"저는 `skt/kogpt-trinity-1.2b-v0.5` 모델을 `lm-evaluation-harness`로 평가해보았으나, 유사하게 HellaSwag와 WiC 태스크에서 상이한 스코어가 나왔습니다. (5-shot / F1 Score 기준으로) HellaSwag와 WiC는 원문에선 각각 0.5272, 0.4313점을 받았다고 하지만, 제가 테스트한 결과로는 0.3999와 0.3953로 큰 차이가 났습니다. 자세한 내용은 다음 블로그 포스트에 적어두었습니다.

https://tilnote.io/pages/65649f20f097f21e37a1c1ad"
danielhanchen,2024-02-06 11:59:05+00:00,comment,unsloth/mistral-7b-bnb-4bit,@sudhir2016 Ye should work fine :) Your transformers version should just be 4.37 for it to work :)
sudhir2016,2024-02-06 16:06:45+00:00,comment,unsloth/mistral-7b-bnb-4bit,Thanks Dan. Yes it works.
imone,2024-01-06 07:33:10+00:00,comment,openchat/openchat-3.5-1210,"I spoke with the author of MetaMathQA. It only contains the augmented **training** set and no test set data exists.

Besides, most popular models like GPT3.5 and WizardMath were trained on the GSM8k and MATH training sets."
imone,2023-12-27 05:43:34+00:00,comment,openchat/openchat-3.5-1210,You can refer to our GitHub repo for fine-tuning instructions https://github.com/imoneoi/openchat
imone,2023-12-27 05:42:55+00:00,comment,openchat/openchat-3.5-1210,"We leverage this template for the C-RLFT conditional prompts. We may remove the additional EOT token in the next release, but the user/assistant prompt might be the same."
PsiPi,2023-12-20 18:35:28+00:00,comment,openchat/openchat-3.5-1210,"
![image.png](https://cdn-uploads.huggingface.co/production/uploads/64a22257d3149e05bc6d259f/8QZtuSbtvPkNqqHU7gn2r.png)
github is wrong too"
imone,2023-12-21 03:44:01+00:00,comment,openchat/openchat-3.5-1210,Good catch! We've fixed it now.
imone,2023-12-22 04:00:45+00:00,comment,openchat/openchat-3.5-1210,"OpenChat is not trained on embedding tasks. However, you may try to average pool the output of last layer to get the embedding."
imone,2023-12-20 06:25:53+00:00,comment,openchat/openchat-3.5-1210,"This model is based on Mistral 7B (Apache-2.0 licensed). Besides, the training mentioned refers to fine-tuning."
EladC,2023-12-21 09:58:59+00:00,comment,openchat/openchat-3.5-1210,"I would like to uderstand one thing, the paper only mention a 13b model with llama-2-13b as its base model.
so if i understand correctly:
1) openchat-13b (from the paper) -> base model llama-2-13b
2) openchat 3.5 (this repo) -> base model Mistral 7b"
imone,2023-12-21 17:03:46+00:00,comment,openchat/openchat-3.5-1210,"Yes, exactly. This release is based on Mistral 7b."
EladC,2023-12-21 19:56:54+00:00,comment,openchat/openchat-3.5-1210,"Alright, thank you for the great work."
imone,2023-12-20 06:26:20+00:00,comment,openchat/openchat-3.5-1210,Thanks for your suggestion! We're actively working on it
gregmeldrum,2023-12-14 18:51:18+00:00,comment,openchat/openchat-3.5-1210,"Seeing the same thing with llama.cpp (not python) and the same GGUF:
`Therefore, Jane is faster than Rahul.abbabbababbabbbababbabbbababbabbababbabababbabababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbababbab`"
lixbo,2023-12-14 19:39:31+00:00,comment,openchat/openchat-3.5-1210,"I'm using KoboldCpp and I have a similar issue

![Screenshot 2023-12-14 at 20-25-59 KoboldAI Lite.png](https://cdn-uploads.huggingface.co/production/uploads/656651f49593e6086a07fd2a/rZzaSZIYk316Az97WizjJ.png)

![Screenshot 2023-12-14 at 20-30-00 KoboldAI Lite.png](https://cdn-uploads.huggingface.co/production/uploads/656651f49593e6086a07fd2a/6VE5ZbJAICC5QsSVHsFZU.png)

![Screenshot 2023-12-14 at 20-45-02 KoboldAI Lite.png](https://cdn-uploads.huggingface.co/production/uploads/656651f49593e6086a07fd2a/JBuEbpq_odjKmB3KJMSnG.png)

@imone You might wanna check this"
alpayariyak,2023-12-15 01:52:12+00:00,comment,openchat/openchat-3.5-1210,"Just changed the EOS token, should be good now! "
tarruda,2023-12-15 11:19:59+00:00,comment,openchat/openchat-3.5-1210,"Issue is fixed on latest GGUF upload, thanks."
alpindale,2024-04-16 19:52:56+00:00,comment,mistral-community/Mistral-7B-v0.2,Added.
alpindale,2024-04-15 21:00:17+00:00,comment,mistral-community/Mistral-7B-v0.2,"I wouldn't mind. I'm worried people might be linking to this repo since it's gotten so popular. Perhaps we can mirror it there, but keep this up."
alpindale,2024-04-15 21:01:30+00:00,comment,mistral-community/Mistral-7B-v0.2,"I wonder if Hugging face, similar to GitHub, would auto-redirect if the repo ownership changes. If that's the case, we can safely move this."
alpindale,2024-04-16 13:21:28+00:00,comment,mistral-community/Mistral-7B-v0.2,I've confirmed with the HF team that this won't be an issue. Moving the repo now.
ehartford,2024-04-16 14:13:25+00:00,comment,mistral-community/Mistral-7B-v0.2,It does auto redirect
ehartford,2024-04-16 14:14:02+00:00,comment,mistral-community/Mistral-7B-v0.2,Thank you
dillfrescott,2024-04-10 04:56:59+00:00,comment,mistral-community/Mistral-7B-v0.2,"Yeah its over 100 billion params so that kinda takes it off the table for me, unless i run a very small imatrix quant"
saishf,2024-04-10 05:00:06+00:00,comment,mistral-community/Mistral-7B-v0.2,"> Yeah its over 100 billion params so that kinda takes it off the table for me, unless i run a very small imatrix quant

176B, I'm interested in how it ranks on lmsys leaderboard!"
dillfrescott,2024-04-10 05:01:37+00:00,comment,mistral-community/Mistral-7B-v0.2,Yeah!
dillfrescott,2024-04-10 05:15:39+00:00,comment,mistral-community/Mistral-7B-v0.2,"Its something like 40B active params I believe, but you need the memory for all of them to run it. I dont think theres a way to only load the active ones."
saishf,2024-04-10 05:30:35+00:00,comment,mistral-community/Mistral-7B-v0.2,"> Its something like 40B active params I believe, but you need the memory for all of them to run it. I dont think theres a way to only load the active ones.

I'd be guessing a minimum of 256gb of ram + GPU vram for local running, probably more once context is included."
saishf,2024-04-12 08:03:51+00:00,comment,mistral-community/Mistral-7B-v0.2,"> Yeah its over 100 billion params so that kinda takes it off the table for me, unless i run a very small imatrix quant

Unsure if you've seen it, you may be able to run this.
I don't know how it performs though
[Vezora/Mistral-22B-v0.1](https://huggingface.co/Vezora/Mistral-22B-v0.1)"
alpindale,2024-04-08 12:00:52+00:00,comment,mistral-community/Mistral-7B-v0.2,so true
alpindale,2024-03-25 09:04:44+00:00,comment,mistral-community/Mistral-7B-v0.2,Done.
mayank-mishra,2024-05-11 04:28:44+00:00,comment,ibm-granite/granite-3b-code-base,"IBM's marketing team told us to drop it sadly
It was a cool logo though :)"
mayank-mishra,2024-05-08 08:56:39+00:00,comment,ibm-granite/granite-3b-code-base,@CosmicSound I think someone from the community is working on this: https://github.com/ggerganov/llama.cpp/issues/7116
mayank-mishra,2024-05-29 14:11:13+00:00,comment,ibm-granite/granite-3b-code-base,"the 20 and 34b have been released, work is still happening on 3 and 8"
mayank-mishra,2024-06-02 17:30:31+00:00,comment,ibm-granite/granite-3b-code-base,all models have been released now
mayank-mishra,2024-05-08 08:53:12+00:00,comment,ibm-granite/granite-3b-code-base,"Yes, we only use permissively licensed code data for training all our models (both base and instruct)."
tonylek,2024-05-08 11:22:53+00:00,comment,ibm-granite/granite-3b-code-base,is this only thestack or additional repos? Is the list available somewhere?
mayank-mishra,2024-05-08 14:22:17+00:00,comment,ibm-granite/granite-3b-code-base,we have crawled additional github data and then filtered by license (only permissive is used).
tonylek,2024-05-08 17:45:02+00:00,comment,ibm-granite/granite-3b-code-base,will the list of repos be published?
tonylek,2024-05-15 11:00:22+00:00,comment,ibm-granite/granite-3b-code-base,"any update on this?
"
mayank-mishra,2024-05-15 12:20:23+00:00,comment,ibm-granite/granite-3b-code-base,"hi, we are not planning on releasing the list of repos."
mayank-mishra,2024-05-07 17:42:19+00:00,comment,ibm-granite/granite-3b-code-base,@mrfakename the 8b has 4k and 20 and 34b have 8k lengths
mrfakename,2024-05-07 17:45:35+00:00,comment,ibm-granite/granite-3b-code-base,"Nice! Would be nice to have some longer-context SLMs, are there any plans for that?"
mayank-mishra,2024-05-08 08:42:38+00:00,comment,ibm-granite/granite-3b-code-base,"Yes @mrfakename we are planning long context length models.
Hopefully sometime soon :)"
mrfakename,2024-05-08 14:49:22+00:00,comment,ibm-granite/granite-3b-code-base,Thanks!
mayank-mishra,2024-05-09 04:34:18+00:00,comment,ibm-granite/granite-3b-code-base,closing this :)
mayank-mishra,2024-05-07 03:32:09+00:00,comment,ibm-granite/granite-3b-code-base,"hi @mrfakename , the llama arch required adding a new parameter 'mlp_bias'
PR: https://github.com/huggingface/transformers/pull/30031
rest is similar to llama"
mayank-mishra,2024-05-07 03:33:53+00:00,comment,ibm-granite/granite-3b-code-base,"you can find this param in our config as well: https://huggingface.co/ibm-granite/granite-3b-code-base/blob/c2475bd7587e4e08fafb0e22223f9af7081c5c00/config.json#L14
"
mrfakename,2024-05-07 16:53:59+00:00,comment,ibm-granite/granite-3b-code-base,thx for the explanation! makes sense
osanseviero,2023-12-21 12:13:29+00:00,comment,TinyLlama/TinyLlama-1.1B-Chat-v0.6,
huseink,2024-05-16 10:04:16+00:00,comment,unsloth/llama-3-8b-Instruct,"Update the eos_token_id to 128009, replacing the previous value, to address output generation issues with Llama3 instruct, in accordance with updates from the official Meta LLama3 Instruct repository (see https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/105).

From <|end_of_text|> to <|eot_id|>"
danielhanchen,2024-05-16 14:22:49+00:00,comment,unsloth/llama-3-8b-Instruct,@huseink Thanks! Would you be interested in updating llama-70b as well :)
huseink,2024-05-16 14:25:30+00:00,comment,unsloth/llama-3-8b-Instruct,"@danielhanchen sure thing :)
"
Orenguteng,2024-04-26 02:22:19+00:00,comment,unsloth/llama-3-8b-Instruct,Sorry wrong lol. my bad
danielhanchen,2024-04-26 09:35:01+00:00,comment,unsloth/llama-3-8b-Instruct,No worries @Orenguteng !
banghua,2024-02-24 22:31:50+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you for the comment. The only reason that Starling is not commercial license is because we rely on GPT-4 generated data. The same thing applies for Openchat 3.5 and a lot of other commercial models, which can be risky since it may violate OpenAI ToS. You're free to use the model commercially if you're following OpenAI ToS and not competing with them. We might consider switching to conditional commercial license like this in the future as well."
youkad,2024-02-15 01:43:47+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Small typo.
banghua,2024-02-22 17:54:48+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thanks! 
banghua,2024-02-24 22:32:48+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thank you for the comment. We're working to scale the model and balance the tuning prompt to see if it's helpful or not. Please stay tuned!
banghua,2024-01-21 06:59:09+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you! Yes, we are working on beta version and code releasing right now. We are working to scale the pipeline to larger models and verify the performance. Please stay tuned!"
rjmehta,2024-03-21 15:05:03+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Waiting. Nexusflow released the same model. Not sure how it differs from this one. 
rjmehta,2024-03-21 15:05:20+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"https://huggingface.co/Nexusflow/Starling-LM-7B-beta

This is the beta version
"
banghua,2024-01-21 06:59:22+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you! Yes, we are working on beta version and code/paper releasing right now. We are working to scale the pipeline to larger models and verify the performance. Please stay tuned!"
gblazex,2024-01-21 14:11:57+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,amazing news
andrewzh,2024-01-21 14:43:47+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thanks for the reply! Looking forward for the release!
danielhanchen,2024-01-01 15:56:20+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Can confirm as well during DPO training, it seems like the tokenizer's addition of `<sep>` is not in the embedding matrix of size `32002`. The maximum id should be 32001."
Imran1,2024-01-01 16:04:52+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Yeah they also show this in dpo training. 
I just add unk and pad token and they show 32004 tokenizer len.

Also the dpo run for a few steps and then show this error.


it showing this error

RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.



Here is the Dpo code 
https://colab.research.google.com/drive/1uC7LohnGJF-Y4vzPz14z6OgZknkeZqD2?usp=sharing"
Imran1,2024-01-01 16:06:55+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,I training this with unsloth library.
banghua,2023-12-21 17:10:24+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,RL这个阶段基本没有在中文语料库训练过。。所以可能不如很多双语训练的模型
banghua,2023-12-22 03:53:32+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"The dataset is here: https://huggingface.co/datasets/berkeley-nest/Nectar with 183K prompts and 7 responses each. PPO hyperparameters are similar to the trlx repo here: https://github.com/CarperAI/trlx, except that we changed the learning rate to 1e-7. We'll open source the paper and code base soon! "
banghua,2023-12-09 03:35:05+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you! The prompt for fine-tuning will be the same as the one used for chat, which is provided in model card and also below:

""GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:""

During both SFT and RLHF, the model does not follow system message. So I would suggest put all system message together with instruction inside ""GPT4 Correct User:"".
"
andreaKIM,2023-12-09 03:45:35+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Really quick answer for me :)
I would try with pleasure. 
By the way, why the model could not follow system message? is there any reason? 
Thank you again!"
banghua,2023-12-09 06:38:26+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"It's because we mostly trained on the prompt style that does not come with system message (usually the fine-tuning dataset won't come with prompts with standard system message). But if you just put system message inside ""GPT4 Correct User:"" and prompt it appropriately, it shall be able to follow it well."
andreaKIM,2023-12-09 07:34:40+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thanks again! I have one more question.
Does  ""generation_config.json"" file affect the training?
Because some other models in the hub contain other parameters such as ```temperature,do_sample,..```.
Does it affect on training? If it does, then can i adjust that parameters also?"
banghua,2023-12-22 03:52:09+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Sorry for the late reply. I think it won't affect training. If you want to use RLHF then the generation parameters might affect the samples you collected. But for SFT it won't affect anything.
banghua,2023-12-05 21:31:44+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"That's interesting.. Actually I never experienced this before. What kind of inference package are you using? TGI, vLLM or other stuff?"
silvacarl,2023-12-05 21:35:35+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"just something we have had to test other models for quite a while.  we are checking to see if somehow it is doing something weird.  just thought it would be good to post here to check.

BUT: I CANTELL YOU SO FAR ITS INSANELY AWESOME.  8-)

Like crazy accurate."
banghua,2023-12-05 23:16:36+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Haha thank you! I'm glad you like it! It's also likely due to mistral structure itself? Not sure if mistral base / instruct will have the same issue.
silvacarl,2023-12-05 23:18:13+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"yes, it could be that as well.  we will check that, running additional tests now."
silvacarl,2023-12-08 23:58:15+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"its kind of interesting.  it flies then for some reason it will sit on one inference for about 15 seconds.  then it flies again.

just in case, what is the prompt format?  Can you post an example?"
banghua,2023-12-09 01:30:30+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"The prompt format is listed in the model card. FYI is

GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:

But I don't think prompt format will change inference speed. Is it only happening for starling but not other mistral-based model? That is very mysteriours.."
silvacarl,2023-12-09 01:31:32+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"yeah tracing the code to see what's up, its reallt weird."
beenotung,2023-12-09 06:12:45+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Is it slow for the same prompt consistently?
Sujan42024,2023-12-09 06:39:05+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"The model is just too slow actually. I have been testing this model on an AWS instance with NVIDIA Tesla T4 GPU and it takes 2-3 minutes for each response. Once, it took about 9 minutes to generate a simple response. IDK what is going on and my internet is good too."
banghua,2023-12-04 02:07:33+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I think that's a feature of default max_length in huggingface TGI. When you use python endpoint, please set additional parameters, in particular max_length according to here: https://huggingface.co/docs/inference-endpoints/supported_tasks#additional-parameters"
iraklip,2023-12-05 21:56:44+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thanks for prompt reply! Unfortunately that doesn't seem to have any effect (unless I'm doing something wrong)


![image.png](https://cdn-uploads.huggingface.co/production/uploads/651973fb335b4c728b75f931/UWz_wljL7LNK8p1ZXNopa.png)

I've also tried to provide input in format of OpenChat, but still no luck. "
banghua,2023-12-05 23:17:17+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Oh my apologies, I think you need to set ""max_new_tokens"" in the parameters rather than max_length. It's basically accepting the same parameters at the HF pipeline."
OmarHedeya,2023-12-08 16:56:35+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Same problem, ""max_new_tokens"" did solve it. Thanks a lot @banghua "
iraklip,2023-12-08 17:09:39+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thanks for your reply @banghua! I can confirm it did solve for me
banghua,2023-12-03 19:21:49+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you! Yes we also observe that in rare cases the model just doesn't know when to stop. We're selecting a better checkpoint as beta version right now.

May I ask if you can share your prompt that leads to such behavior? That will help us debugging the new model as well. Thanks!"
rjmehta,2023-12-03 19:35:16+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Sure. Here is the prompt. I tested this in lmsys chat. Max tokens = 512. I checked for llama2 tokens = 2323 tokens
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
GPT4 Correct User: Strategy
How Tesla’s Charging Stations Left Other Manufacturers in the Dust
by Hemant Bhargava, Jonas Boehm, and Geoffrey G. Parker January 27, 2021
VCG/ Getty Images
Summary. Mainstream car manufacturers are spending billions on developing EVs. Before spending any more money they should take a leaf out of Tesla’s books and create a recharging network to support their cars. Only then will they be able to treat their EVS as a product.... more

Over the past five years, the major auto companies have invested massively in electric vehicles (EVs). The Volkswagen Group in 2017 announced that they would offer 80 new electric vehicles across their brands by 2025 and electric versions of every one of its models by 2030. In the same year GM went public with plans to put at least 20 new electric models on the road by 2023. They are not alone; Bloomberg New Energy Finance predicts that 500 different EV models will be available globally by 2022.
Yet, despite investments that add up to many billions of dollars, none of the major incumbent automakers seems to pose much of a threat to market leader Tesla, which has become nearly synonymous with EVs. This is surprising since one might reasonably have expected that once firms with annual revenues in excess of $100B, deep manufacturing expertise, and large market shares turned their attention to the electric vehicle market, the game would be up.
The reason why consumers still choose Teslas over products like Audi’s eTron or attractive EVs from GM’s Buick, Cadillac, GMC, and Chevy brands is perhaps surprisingly simple. They can drive their Teslas for long distances in full confidence that they will find convenient locations at which to recharge their vehicle. While the incumbent automakers are still focused narrowly on perfecting their electric cars, Tesla has been thinking about the entire vehicle system, with the aim of solving consumers’ core driving needs.
The car as platform
A car creates value to its owner when it is driven, which requires refueling. Automakers of gasoline cars or trucks do not have to worry about this, as refueling stations are abundant — over 160,000 stations are in the United States alone — and easily accessible. They have therefore built their strategies around standard marketing variables: product, price, placement, promotion. Build a great car (or truck), advertise it heavily, offer it in the right markets at a good price, and the product will sell.

An electric car, however, requires a different value analysis. “Refueling stations” — i.e., rapid charging facilities — for electric vehicles are in their infancy, with only about 4,000 available in the United States. Moreover, the network of available charging stations is highly fractured across ownership and technology. The next largest network, compared to Tesla, is only 10% as large. Unless you buy a Tesla, you have few options for reliable route planning, guaranteed access, and rapid public charging.
An electric car, therefore, is a two-sided platform good, the two sides being an installed base of car buyers and a large network of geographically dispersed multi-stall rapid-charging stations. Selling electric cars requires a robust charging network. But, investments in building a massive charging network make sense only if there is a large enough user base and demand for these chargers. Tesla has such a network, and everyone else’s is laughable. How did this happen and what can Tesla’s history teach us?
Platforms need networks
Nissan, with the zippy and relatively affordable Leaf, stole an early lead in the EV market, and was the best-selling electric car from 2011 to 2014. Despite this lead, Nissan failed to provide a robust fast- charging network which left buyers relying on a small number of third-party stations available to all brands.
Tesla’s approach was strikingly different. They began with a vanity product, the Roadster, that got them off the ground and generated some early sales. They then moved on to release the Model S in 2012, which for its few years (2013-15) had roughly a one-year waiting period. To support the cars, however, Tesla rolled out a coast-to-coast proprietary network. So, although Tesla sold only a few thousand cars in its early years, it had built out a huge network. This addressed the buyer’s “range anxiety” problem — no one who considered buying a Tesla needed to worry much about charging.

Most automakers have followed Nissan’s approach and are focusing investments on making better electric cars. But imagine if, instead of investing tens of billions of dollars in producing cars with no way to drive long distances individually, Audi, GM, Ford, and the rest each spent just a billion dollars to build a network of supercharging stations. In North America, that amount would finance approximately 1,000 locations with 10 charging stalls each. If the stations were correctly placed, a network of that size should give buyers enough confidence to choose a car based on its features instead of on the features and the charging network. Then firms could begin the work of reaching viable volumes, bringing costs down, and eventually becoming serious competitors to Tesla.
The platform advantage
Employing a proprietary platform strategy, as Tesla did, enables the platform owner to coordinate the two sides of the market: the installed base of cars and the network of charging stations. Because it owns the charging network, Tesla can choose how to price (whether to make charging free and monetize only the car), the number of stations, rollout timing, and location.
These choices can reflect Tesla’s overall business strategy and detailed knowledge of where the buyers are and where they drive. Interestingly, another newcomer, Rivian (which has yet to sell a single vehicle) is also building a proprietary charging network, like Tesla’s. Rivian is splitting its stations between major highways and campgrounds, a perfect fit given its focus on electric adventure vehicles.
Automakers would be well advised to take a leaf out of Tesla’s playbook and focus on the network before heavily investing in designing and manufacturing new EVs, or at least do so in parallel. They may not actually have to build the network — they could instead partner with firms that have networks that could accommodate

recharging stations. Many existing fossil fuel energy firms, for example, have gas station assets that will eventually become stranded and could be repurposed for electric vehicles.
Focusing on the network, of course, is not without its risks. Rolling out a network from scratch isn’t a trivial challenge, nor is it clear that potential partners would be willing to commit to an exclusive relationship with any one automaker, which the latter would need, at least to begin with, in order to get a head start on the other automakers. But investing in a network will certainly increase the odds of winning a dominant position in EVs, which a focus on the
cars alone is unlikely to deliver — at least according to the evidence so far.
Looking ahead
It’s clear that Tesla itself is doubling down on its Big Tech platform strategy. At present, its business model for its new automated driving capability is classic product pricing; levy a one-time upcharge of $10,000. However, it plans to switch to selling automated driving as a service for a monthly fee. This strategy implicitly defines the car itself as a platform on top of which services can be delivered.
Such a business model offers the additional benefit of enabling Tesla to gather the training data that the machine learning algorithms for true self-driving cars will need, which will confer a critical advantage in the next stage of motor vehicle competition. As for the competition around charging networks, should other firms get serious about building out alternatives, then we would expect Tesla to open its own network since the advantage to remaining closed would start to fade. Indeed, we are beginning to see initial signs of openness as Tesla hints at allowing a new partner to connect.
Firms that dream of being the next Tesla should carefully examine why they are so far behind. It’s not for lack of knowledge about how to build cars; many of the incumbent firms have been doing that for over a hundred years. Instead, they should focus on the critical infrastructure, in this case the charging networks, that have made

customers willing to take a chance on a newcomer. And, once they’ve done that, they can begin to address the next battleground: control over the vehicle data that will enable self-driving cars and the transition to vehicle as a service instead of vehicle as a product.
HB
Hemant K. Bhargava is the Jerome and Elsie Suran Chair in Technology Management at the University of California Davis Graduate School of Management and Director of the Center for Analytics and Technology in Society
JB
Jonas Boehm is Venture Strategist & Platforms Lead at AGL Energy and Fellow at the World Economic Forum’s Global Future Council on Advanced Manufacturing and Production
Geoffrey G. Parker (gparker@dartmouth.edu) is a professor of engineering at Dartmouth College
and a research fellow at MIT’s Initiative on the Digital Economy. He co-authored Platform Revolution (W.W. Norton & Company, 2016), the April 2016 HBR article “Pipelines, Platforms, and the New Rules of Strategy” and the October 2006 HBR article “Strategies for Two-Sided Markets,”
an HBR all time top 50. Follow him on Twitter @g2parker.

Can you summarize the above article?<|end_of_turn|>
GPT4 Correct Assistant:
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Tesla's success in the electric vehicle (EV) market is largely due to its focus on creating a robust charging network to support its cars. While other mainstream automakers have been investing heavily in developing EVs, they have not focused as much on the charging infrastructure, resulting in a less attractive proposition for consumers. By building a comprehensive charging network, Tesla has addressed the ""range anxiety"" problem that has plagued the EV market. Major automakers should take a leaf out of Tesla's book and focus on creating a reliable charging network before heavily investing in designing and manufacturing new EVs. This will increase their chances of winning a dominant position in the EV market, as a focus on the cars alone is unlikely to deliver the same results.

In the battle of the electric vehicle (EV) charging network, Tesla has taken a massive lead over traditional automakers, and this advantage is unlikely to fade anytime soon. While other carmakers have been focused on perfecting their electric cars, Tesla has been thinking about the entire vehicle system. It's not just about the cars; it's about the charging infrastructure that supports them.

Tesla's approach was to start with a vanity product, the Roadster, which generated initial sales and allowed them to develop a substantial charging network. Although Tesla sold only a few thousand cars in their early years, they had built out a large charging network, addressing the ""range anxiety"" problem and giving buyers the confidence to choose a Tesla based on its features rather than the charging network.

On the other hand, most automakers have focused their investments on making better electric cars without paying much attention to the charging infrastructure. This has left them trailing far behind Tesla in terms of charging network strength. It's worth noting that Tesla's charging network is proprietary, which gives them control over the pricing, number of stations, rollout timing, and location. This strategy is a significant advantage over the fragmented and fractured charging networks of other automakers.

To catch up with Tesla, traditional automakers need to shift their focus from just the cars to the charging network. They could either build their charging networks from scratch, which is a challenging task, or they could partner with existing firms that have the infrastructure in place. Some potential partners could be fossil fuel energy firms that have gas station assets that will eventually become obsolete and could be repurposed for electric"
banghua,2023-12-03 19:42:01+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you! Actually this one looks fine for me on lmsys. See my screenshot below.

In lmsys chat, did you also include the chat template like ""GPT4 Correct Assistant""? This won't be needed there since they're automatically set up by lmsys.

I tried several generation with temperature = 0.7, which seems fine. But it's likely that with a high temperature the model will generate a bit longer. You can try temperature = 0 to see if things still work like this.


![lmsys.jpg](https://cdn-uploads.huggingface.co/production/uploads/647b8885aba7062fe5c32000/hmdaPk6wYNjHN4OOkJTHd.jpeg)
"
rjmehta,2023-12-03 19:50:44+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I tried again with your recommendation. Removing the prompt template and feeding as one text block. And here is the below params and generation. The summary is really good but just eos is not generated sometimes

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

![image.png](https://cdn-uploads.huggingface.co/production/uploads/60e48b10c15e2c17e66b97be/Q7snHezzcoOwEnYALYshG.png)

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Mainstream car manufacturers have invested heavily in electric vehicles (EVs), but have not yet posed a significant threat to market leader Tesla. Tesla's success lies in its focus on the entire vehicle system, solving consumers' core driving needs by creating a robust charging network. Automakers have focused on improving their electric cars, but have not invested in building a charging network. Tesla's approach has given it a significant advantage, and other automakers should focus on building charging networks in order to compete effectively in the EV market.

In the past five years, major auto companies have invested billions in developing electric vehicles. Despite these investments, none of the major incumbent automakers have posed much of a threat to market leader Tesla. This is surprising, as one might reasonably have expected that once firms with annual revenues in excess of $100 billion, deep manufacturing expertise, and large market shares turned their attention to the electric vehicle market, the game would be up.

The reason why consumers still choose Teslas over products like Audi's eTron or attractive EVs from GM's Buick, Cadillac, GMC, and Chevy brands is perhaps surprisingly simple. They can drive their Teslas for long distances in full confidence that they will find convenient locations at which to recharge their vehicle. While the incumbent automakers are still focused narrowly on perfecting their electric cars, Tesla has been thinking about the entire vehicle system, with the aim of solving consumers' core driving needs.

A car creates value to its owner when it is driven, which requires refueling. Automakers of gasoline cars or trucks do not have to worry about this, as refueling stations are abundant — over 160,000 stations are in the United States alone — and easily accessible. They have therefore built their strategies around standard marketing variables: product, price, placement, promotion. Build a great car (or truck), advertise it heavily, offer it in the right markets at a good price, and the product will sell.

An electric car, however, requires a different value analysis. “Refueling stations” — i.e., rapid charging facilities — for electric"
banghua,2023-12-03 20:00:57+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Oh I see. It's interesting that when I include the ""Strategy"" at the beginning, the generation seems to be shorter. And when I remove that I can reproduce what you had.

It looks like the model is trying to be extremely verbose here and doesn't understand how to stop. Thank you for the prompt! I also did notice that if you say ""Please summarize the article in 100 words"", its output seems to be shorter lol. We will try to fix this in the beta version."
banghua,2023-12-03 03:21:14+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Looks like you didn't follow the default chat prompt.

Starling is finetuned from openchat 3.5, which has a very special chat prompt, which goes as: ""GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:""

I also tested your prompt with the right chat template. The response is: 

This tweet is speculating about the future actions of the Federal Reserve and the potential impact of inflation on the market. It is not making a definitive prediction, but it is considering various possibilities and their potential outcomes. Therefore, the answer is 1.
"
topastop,2023-12-03 03:27:41+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you, I will try it exactly as suggested in the documentation by running it outside of LM Studio.

I tried to change the message to that format but I didn't had any improvement.

"
topastop,2023-12-03 15:49:35+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I run several tests and using the tokenizer with the suggested prompt works fine!

Thank you for the help!"
frenzygr,2023-12-03 19:04:18+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I'm having a similar issue the model just keeps going and going without stopping, here's my config

{
  ""name"": ""OpenChat Code"",
  ""inference_params"": {
    ""top_k"": 1,
    ""top_p"": 0.1,
    ""temp"": 0.1,
    ""input_prefix"": ""Code User: "",
    ""input_suffix"": ""<|end_of_turn|>Code Assistant:"",
    ""antiprompt"": [
      ""GPT4"",
      ""<|end_of_turn|>"",
      ""[End of Turn]"",
      ""[<END>]""
    ],
    ""pre_prompt"": ""You are a helpful coding assistant. Respond concisely, but ensure all essential details are provided. Each of your statements must be unique."",
    ""pre_prompt_suffix"": ""<|end_of_turn|>"",
    ""pre_prompt_prefix"": ""GPT4 System: ""
  }
}"
rjmehta,2023-12-03 19:10:14+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Same problem. Using same format as described but it doesnt stop generating
banghua,2023-12-03 19:20:56+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you @frenzygr, actually it's suggested that we do not use any system prompt or code prompt like GPT4 System or code assistant. It's better to stick to ""GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant: "". lmsys here provides a default chat template that is set right. So if you observe the same bad behavior there, the issue will be the model itself. And I'd appreciate if you can provide your test prompt. Thanks!"
MB7977,2023-12-03 04:40:42+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"For what it’s worth I’ve successfully done a full fine tune on Starling with Transformers 4.35.2 (with Axolotl). Are you perhaps adding tokens and changing the vocab size? I trained with the OpenChat prompt format and stuck with the default EOS, BOS tokens etc so no added tokens were necessary. I think the openchat.json file may also be relevant?"
toranb,2023-12-03 19:32:53+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"When do you use the openchat.json file? I didn't even pull that down ahead of converting weights/fine tuning so I'm curious to learn more

https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha/blob/main/openchat.json"
MB7977,2023-12-04 00:29:31+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I may be wrong about that. It was one of a couple of files added after the fact that seemed to fix early training issues, but that one seems more related to compatibility with the OpenChat API. I'm working with Transformers indirectly, via Axolotl, so it's difficult to tease out why it's working in my instance versus yours.  The OpenChat 3.5 format used by Starling adds a couple of tokens to the vocabulary that I suspect are the source of your issues. Hopefully the devs can help."
banghua,2023-11-30 17:01:23+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Haha my apologies. We're based on the exact chat format of openchat 3.5. Hope their next version might change the chat format to chatml / vicuna.
banghua,2023-11-29 16:32:50+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you! In fact this is already a chat adopted version. Sorry for the confusion in naming.

The model is fine-tuned with RLHF, which is specially optimized for chat experience (though currently a bit over-optimized). We will soon release a more stable beta version though. Please stay tuned!"
onix135,2023-11-29 17:26:14+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you so much!

Looking forward to testing your new models!"
banghua,2023-11-29 07:20:26+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you! Could you please try it with the default prompt ""GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:""?  This ""Correct"" is also necessary to get the highest possible performance out of the model. There're some code examples in the model card as well.

For this case, you might also want to try different prompting. I think you can try sth like ""only provide answer from the context below without outputting any extra word. If no answer present, output ""no answer present"""". Because during training, the model only sees <|end_of_turn|> followed by GPT Correct Assistant / GPT Correct User. This extra context prompt and additional <|end_of_turn|> might make the model confused. 

But it's still likely that the model will output verbose content. We're getting tons of new checkpoints and are picking some better ones for our beta version. Stay tuned!"
anokas,2023-11-29 13:32:39+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Was this trained with system messages at all? If so, what's the 'correct' way to inject this into the prompt?"
banghua,2023-11-29 14:38:34+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,In the RLHF phase we did not. I believe the base model Openchat 3.5 also didn't train with system prompt. I'd suggest directly put system prompt at the beginning of user prompt without any other formatting.
rjmehta,2023-11-29 17:15:51+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,What about the Context block for RAG task? Should I include the context in before user as GPT4 Correct Context or add in the GPT4 Correct User block itself?
banghua,2023-11-29 17:49:13+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I'd recommend add in the GPT4 Correct User block itself. And maybe point out in natural language specifically that ""this paragraph is context"" or sth like that."
banghua,2023-11-29 04:36:15+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you for the suggestion! That's also on our TO-DO list. 

Currently we still observe some instable and weird behavior of the model, so we are working on a beta version first before testing a larger reward & policy model.

During our evaluation, we also found that 7B model tends to hallucilate a lot, which is incomparable to 30+B models and greatly affect the human evaluation score. So having a larger model seems to be a must in this case. We believe that our dataset might have larger potential when scaling the reward model and language model, although the biggest problem is still the limited compute for training large reward & language model.
"
banghua,2023-11-29 04:21:44+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you @mrfakename! 

The primary obstacle to adopting the Apache 2.0 license for our project is the Terms of Use specified by OpenAI for data generated by its models. These terms explicitly prohibit using the output to develop models that compete with OpenAI. This restriction means that most open-source models (such as vicuna, gorilla, WizardLM, Zephyra, Tulu, OpenHermes, Openchat, etc.), which are trained with GPT data, are potentially non-compliant with the Apache 2.0 license, and perhaps even non-compliant with the llama2 license.

However, it's important to note that the model and dataset do allow for personal and research use without any restrictions. Should OpenAI modify its stance on this matter in the future, we would be more than willing to switch to the Apache 2.0 license. For the time being, though, any commercial usage may lead to issues with OpenAI, for which users must bear responsibility. Aside from this, there are really no additional restrictions on the use of our model and dataset."
mrfakename,2023-11-29 04:43:34+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Hi,
Thanks for your response! You said this may not comply with the llama2 license, correct me if I’m wrong but I’m pretty sure the model card said this was Mistral-based, right?
Also, as for the OpenAI concerns, that makes sense. Although Sam Altman tweeted that he was “not mad at Google for training on sharegpt” or something like that, it makes sense because it is still in the ToS.
Might it be possible to allow commercial use of the outputs of this model?
I sincerely appreciate your development of this model."
banghua,2023-11-29 07:14:30+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thank you!

The story is a bit more complicated. The reward model is trained from llama2 and the language model is trained with mistral. Llama2 also forbids using its own output to improve the performance of the other model. So actually this is a really grey space.

I'm not an expert in law, I'm not sure if we can allow commercial use of the output of the model. So unfortunately we'll let user to bear the potential responsibility for using this commercially.

But TBH personally I don't think OpenAI will care if anyone uses our model / dataset for commercial use cases. The model is obviously far from GPT-4, and even a lot behind 3.5. And they likely have a 100X dataset that are much higher quality labeled by human. So..."
mrfakename,2023-11-30 00:08:25+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,So basically if I use it commercially you won’t sue me but Meta + OAI might?
banghua,2023-11-30 00:14:41+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Haha we won't. We hope to see broader use of the model. But we don't have control over Meta + OAI.. (They might sue more than half of the OSS model with Apache license in that case...
mrfakename,2023-11-30 00:17:11+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Ok, thanks. I think for now if I were to build a commercial product I should use a different less powerful right?

Thank you!!"
banghua,2023-11-28 22:21:22+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thank you!! @macadeliccc 
banghua,2023-11-28 16:55:47+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"May I ask if you're using the default chat template? (GPT4 Correct User: Hello! GPT4 Correct Assistant:) The model with a wrong chat template will perform very poorly.

Another possibility is to try a lower temperature, our testing suggests that temperature 0 might be slightly better than 0.7.

You may want to first try a bit on chat.lmsys.org. If you also observe the same issue there, then it'll just be the issue with the model. I actually tried this question a bit. The answer is ""I don't have knowledge on that."" And then hallucinates some random answers. Seems to be slightly better than what you get (at least it's English lol).

But still yes the model will output weird content in some cases, potentially due to uncareful RLHF. We're still working to fix it in the next version. Please stay tuned!"
banghua,2023-11-28 16:51:01+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Emm that's an interesting question. May I ask if you're using the default chat template? (GPT4 Correct User: Hello! <end_of_turn>GPT4 Correct Assistant:) The model with a wrong chat template will perform very poorly.
Actually for coding, the chat template can be changed for the coding template in the model card as well. Not sure how much that could help since we didn't do RLHF on that template.

Another possibility is to try a lower temperature, our testing suggests that temperature 0 might be slightly better than 0.7.

You may want to first try a bit on chat.lmsys.org. If you also observe the same issue there, then it'll just be the issue with the model.

But still yes the model will output unnecessary content in some cases. We're still working to fix it in the next version. Please stay tuned!"
eramax,2023-11-29 16:52:00+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I tried with the suggested prompt format and had the same issue.
"
frenzygr,2023-12-03 18:32:54+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I'm running into a similar issue I think and i've got the correct chat template, it keeps going and going without stopping


![image.png](https://cdn-uploads.huggingface.co/production/uploads/64274118826a6d2ceee3855e/VW9xtBmavAax11M1HkVUq.png)
"
banghua,2023-12-03 19:18:42+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Please follow the default chat template. The model is not finetuned on any system template, so please keep system message and prefix / suffix empty. For user message suffix, please use <|end_of_turn|>GPT Correct Assistant: 

Also it seems that the model you used here is not updated with the correct tokenizer, so it has <0x0A> rather than line change. 

Here is what I get with the same prompt + correct template from lmsys:

![lmsys.jpg](https://cdn-uploads.huggingface.co/production/uploads/647b8885aba7062fe5c32000/ZtMkxcTZCYtUDL16U73tX.jpeg)
"
lixbo,2023-12-03 22:18:34+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"@frenzygr I have the same issue in LMstudio, but it works fine in KoboldCpp, btw you can see lmstudio detecting the model as a Starcoder model, so gpu won't be supported that way."
frenzygr,2023-12-03 22:46:40+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"> @frenzygr I have the same issue in LMstudio, but it works fine in KoboldCpp, btw you can see lmstudio detecting the model as a Starcoder model, so gpu won't be supported that way.

Does it detect it as a Starcoder model for you too or is there a setting I can chage?

Tried it in KoboldCpp but unless I limit the generation tokens it still goes on and on until it reaches the limit"
frenzygr,2023-12-03 23:29:25+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"> Please follow the default chat template. The model is not finetuned on any system template, so please keep system message and prefix / suffix empty. For user message suffix, please use <|end_of_turn|>GPT Correct Assistant: 
> 
> Also it seems that the model you used here is not updated with the correct tokenizer, so it has <0x0A> rather than line change. 
> 
> Here is what I get with the same prompt + correct template from lmsys:


I believe I have the correct template now but it still keeps going indefinitely 


![image.png](https://cdn-uploads.huggingface.co/production/uploads/64274118826a6d2ceee3855e/Zc-fvd2URSdo8M_zfhMTz.png)
"
lixbo,2023-12-04 01:47:01+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"> Tried it in KoboldCpp but unless I limit the generation tokens it still goes on and on until it reaches the limit

I just tried it in LMStudio and I got the same issue as you, the problem is not the prompt format it's from LMStudio certainly, I tested it in KoboldCpp and it worked fine if I give it large output context.
Also, verify that your model is updated, just check if the SHA256 code match from [here](https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GGUF/blob/main/starling-lm-7b-alpha.Q8_0.gguf), if it doesn't redownload the model.

![ge.png](https://cdn-uploads.huggingface.co/production/uploads/656651f49593e6086a07fd2a/eXfYOUFIfXSSWdUvLmJtd.png)

> Does it detect it as a Starcoder model for you too or is there a setting I can chage?

It's detecting the ""star"" in the model name and folder, try to change any letter and see."
frenzygr,2023-12-07 18:50:35+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"> > Tried it in KoboldCpp but unless I limit the generation tokens it still goes on and on until it reaches the limit
> 
> I just tried it in LMStudio and I got the same issue as you, the problem is not the prompt format it's from LMStudio certainly, I tested it in KoboldCpp and it worked fine if I give it large output context.
> Also, verify that your model is updated, just check if the SHA256 code match from [here](https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GGUF/blob/main/starling-lm-7b-alpha.Q8_0.gguf), if it doesn't redownload the model.
> 
> ![ge.png](https://cdn-uploads.huggingface.co/production/uploads/656651f49593e6086a07fd2a/eXfYOUFIfXSSWdUvLmJtd.png)
> 
> > Does it detect it as a Starcoder model for you too or is there a setting I can chage?
> 
> It's detecting the ""star"" in the model name and folder, try to change any letter and see.

I still get the same behaviour, can you please paste a couple small screenshots of all your settings before you lunch the kobold app and for the model itself?

I appreciate it!


![image.png](https://cdn-uploads.huggingface.co/production/uploads/64274118826a6d2ceee3855e/3IrJg3ZDTy17uZI-lhVLO.png)
"
lixbo,2023-12-07 19:59:23+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I load it with the default configuration then change one thing, Context size to 4096. 
After it loads I used the Default preset, and in Start Seq put: ""<|end_of_turn|>GPT4 User:"" and in End Seq: ""<|end_of_turn|>GPT4 Assistant:"" without the quotes.

Did you make sure you have the updated model?
![Untitldeed.png](https://cdn-uploads.huggingface.co/production/uploads/656651f49593e6086a07fd2a/Sb3SE7CZpTXZQRPm4UGDW.png)
"
frenzygr,2023-12-07 23:02:42+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"> I load it with the default configuration then change one thing, Context size to 4096. 
> After it loads I used the Default preset, and in Start Seq put: ""<|end_of_turn|>GPT4 User:"" and in End Seq: ""<|end_of_turn|>GPT4 Assistant:"" without the quotes.
> 
> Did you make sure you have the updated model?
> ![Untitldeed.png](https://cdn-uploads.huggingface.co/production/uploads/656651f49593e6086a07fd2a/Sb3SE7CZpTXZQRPm4UGDW.png)


thanks! got it working in Kobolt but still face the same issue in LM studio! ill mess around with it a bit more

EDIT: Got it working on LM studio as well! 

EDIT2: Nvm it got back to spewing entire walls of text without changing any parameters.
"
banghua,2023-11-28 15:32:25+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Sounds good. Do you prefer replacing safetensor with pytorch format then?
rjmehta,2023-11-28 16:00:20+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Why pytorch when you have safetensors that is 10x more optimized than pytorch?
"
vmajor,2023-11-28 21:47:48+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"For research. All the other models that I’m working on are PyTorch. It is also relatively simple converting from PyTorch to safetensors, but I have not seen a convenient way of doing the reverse. "
khu,2023-11-28 07:43:33+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,
banghua,2023-11-28 15:31:43+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thx!
banghua,2023-11-28 05:31:51+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thanks for the reminder! Just copied from Openchat 3.5 lol.
rjmehta,2023-11-28 16:00:56+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Are you trying the GPTQ quant? @bartowski 
bartowski,2023-11-28 21:05:35+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"@rjmehta nah exllama v2, made some for this here:

https://huggingface.co/bartowski/Starling-LM-7B-alpha-exl2"
banghua,2023-11-28 05:03:18+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"I also saw similar discussions here: https://x.com/HenkPoley/status/1729347141695385736?s=20. It seems that this token is supposed to be new line ""\n"", but somehow decoded wrongly? 
"
banghua,2023-11-28 05:43:23+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,I guess it's because we forgot to include tokenizer files from openchat 3.5 according to this https://huggingface.co/TheBloke/Starling-LM-7B-alpha-GGUF/discussions/1 I just uploaded the relevant files. 
Shasho423,2023-11-28 05:53:54+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Oh, yeah, I am actually using TheBloke's haha. Thank you!"
banghua,2023-11-27 23:31:00+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thank you for the kind word @rjmehta !!! 
eramax,2023-11-28 00:41:14+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,amazing 
MB7977,2023-11-28 02:18:28+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,This model is outstanding. Thank you for your work. 
rjmehta,2023-11-28 16:01:37+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,@TheBloke @banghua Any plans for GPTQ Quantization?
TheBloke,2023-11-28 16:42:56+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Just finished a little while ago!
banghua,2023-11-28 16:59:37+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Thx @TheBloke! 

@rjmehta We would also like to try some quantization. But the current model still has some issues that might need to be fixed first, including

1. Output unnecessary and weird content at the beginning or end, occasionally. 

2. Hallucinates a lot.

I hope next version can fix most of 1. But for 2 we can only pray that such small model has memorized good amount of knowledge. 

In any case, we'll probably devote our limited computation resource to improve the model first. After we get a satisfying and stable version, we're happy to explore a bit more other possibilities!"
TheBloke,2023-11-28 17:29:39+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Feel free to ping me when your new model(s) are up and I will prioritise their quantisations
banghua,2023-11-28 17:30:52+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thank you so much @TheBloke! Will let you know once it's up!!
deleted,2023-11-28 20:34:28+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,_
banghua,2023-11-27 20:34:49+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,Thank you for the comment! I just copied their openchat.json and uploaded here! 
banghua,2023-11-27 07:41:28+00:00,comment,berkeley-nest/Starling-LM-7B-alpha,"Hey Morgan,

You got us! 😄 We're still tinkering with the model, but looks like you found our work-in-progress repo. Kudos for that!

Right now, we're pushing out an alpha version to get some feedback. We're planning to roll out a more stable beta version down the line.

Stay tuned, and thanks a ton for your kind words and for keeping an eye on what we're up to. Your feedback is super valuable to us!

Best,
Banghua"
Muennighoff,2024-06-09 22:16:24+00:00,comment,GritLM/GritLM-7B,"The model usage is documented here: https://github.com/ContextualAI/gritlm?tab=readme-ov-file#inference
It is not compatible with Sentence Transformers and does not use last token pooling, so these will lead to suboptimal performance."
yearivig,2024-06-09 22:27:54+00:00,comment,GritLM/GritLM-7B,"So are you saying load the model with grit package as model = GritLM(""GritLM/GritLM-7B"", torch_dtype=""auto"")
 should give Me the best results on mteb?
"
Muennighoff,2024-06-09 22:35:32+00:00,comment,GritLM/GritLM-7B,"> So are you saying load the model with grit package as model = GritLM(""GritLM/GritLM-7B"", torch_dtype=""auto"")
>  should give Me the best results on mteb?

Yes! You should be able to get the same results as GritLM-7B, you can e.g. use this script: https://github.com/ContextualAI/gritlm/blob/main/README.md#embedding"
yearivig,2024-06-09 22:44:28+00:00,comment,GritLM/GritLM-7B,"Thank you! 
Actually, I’m looking for the right configuration to use this model loaded with Automodel and which pooling method should I use. I want to use the option of add past_key_values to my context, which is available in Automodel package. Do you familiar with such configuration?"
Muennighoff,2024-04-04 16:50:55+00:00,comment,GritLM/GritLM-7B,It'd be interesting to know but I'm not planning to investigate it at the moment; cc @tomaarsen 
Muennighoff,2024-02-24 07:04:54+00:00,comment,GritLM/GritLM-7B,No need for an instruction for documents; Just put `<|embed|>\n` at the beginning
setiadeepanshu,2024-02-25 09:16:04+00:00,comment,GritLM/GritLM-7B,Can you give a quick example @Muennighoff trying to do document text classification into few categories and want to understand if embeddings using this model would be useful. The documents text are of variable lengths.
Muennighoff,2024-02-25 10:07:59+00:00,comment,GritLM/GritLM-7B,Does this example work for you: https://github.com/ContextualAI/gritlm?tab=readme-ov-file#inference ?
Muennighoff,2024-02-23 15:03:15+00:00,comment,GritLM/GritLM-7B,"As mentioned here https://huggingface.co/GritLM/GritLM-7B/discussions/2 it supports any language just that it will work best for English; I haven't evaluated Spanish
the maximum input tokens are unlimited"
Muennighoff,2024-02-20 15:29:30+00:00,comment,GritLM/GritLM-7B,"Thanks!
We evaluated it on TyDi QA - you can find the per-language metrics of this model here: https://huggingface.co/datasets/GritLM/results/blob/main/GritLM-7B/tydiqa_metrics.json
(the average is also reported in the paper)

Here's the GritLM-8x7B model: https://huggingface.co/datasets/GritLM/results/blob/main/GritLM-8x7B/tydiqa_metrics.json


We didn't test them on arabic embedding but there are a bunch of Arabic datasets available in MTEB - would be great to get their performance!
"
wilfoderek,2024-02-23 03:00:38+00:00,comment,GritLM/GritLM-7B,What languages does it suport?
Muennighoff,2024-02-23 12:08:09+00:00,comment,GritLM/GritLM-7B,"You can try any language, but it will probably be best for English and related languages"
Muennighoff,2024-02-17 18:49:32+00:00,comment,GritLM/GritLM-7B,I put apache here cuz that's the mistral license and MIT on github (intended for the code) as it seems shorter and simpler than apache - Do you have a preference between MIT or apache? I can change the licence if so. I really just want it to be maximally permissive 😊
mrfakename,2024-02-17 18:57:15+00:00,comment,GritLM/GritLM-7B,"Thank you so much for your response! I think the most permissive is dual MIT + Apache (since Apache allows patent use but requires ppl to add ""prominent notices"" if they modify it, MIT doesn't require that but patent use is unclear). But both are pretty permissive!"
Muennighoff,2024-02-17 19:46:03+00:00,comment,GritLM/GritLM-7B,"I see so maybe leaving it the way it is right now?
Would CC0 be any better?"
mrfakename,2024-02-17 20:41:45+00:00,comment,GritLM/GritLM-7B,"Apache, MIT, CC0 all work, thanks so much!"
syboomsysy,2023-11-22 09:19:01+00:00,comment,princeton-nlp/Sheared-LLaMA-1.3B,"Oh, it seems that the gradient checkpointing should be the crux, the procedure runs well if I set it False."
princeton-nlp,2023-11-07 19:45:13+00:00,comment,princeton-nlp/Sheared-LLaMA-1.3B,"Hi, our instruction tuned version of the model is very limited as it only uses a subset of sharegpt for training. Check out other instruction tuned versions here: https://huggingface.co/llmware/bling-sheared-llama-1.3b-0.1

"
kimihailv,2023-11-08 08:51:24+00:00,comment,princeton-nlp/Sheared-LLaMA-1.3B,Thank you
princeton-nlp,2023-11-01 14:47:43+00:00,comment,princeton-nlp/Sheared-LLaMA-1.3B,"Hi, the use of the model should conform to Llama license! We will make it clear in our readme files. "
princeton-nlp,2023-10-26 03:36:17+00:00,comment,princeton-nlp/Sheared-LLaMA-1.3B,Thanks for your interest in other work!!!!
princeton-nlp,2023-10-12 20:47:03+00:00,comment,princeton-nlp/Sheared-LLaMA-1.3B,"Yes, stay tuned!"
princeton-nlp,2023-11-06 15:37:30+00:00,comment,princeton-nlp/Sheared-LLaMA-1.3B,"Hi, the code has been released at https://github.com/princeton-nlp/LLM-Shearing. 
Check it out!"
jlzhou,2024-03-28 05:43:26+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"After removed the default system instruction in `chat_template` it works now. So I suspect there's something wrong with the `chat_template`, causing the default system instruction always been set.

This is the original template:

```
{%- set found_item = false -%}
{%- for message in messages -%}
    {%- if message['role'] == 'system' -%}
        {%- set found_item = true -%}
    {%- endif -%}
{%- endfor -%}
{%- if not found_item -%}
{{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\\n'}}
{%- endif %}
{%- for message in messages %}
    {%- if message['role'] == 'system' %}
{{ message['content'] }}
    {%- else %}
        {%- if message['role'] == 'user' %}
{{'### Instruction:\\n' + message['content'] + '\\n'}}
        {%- else %}
{{'### Response:\\n' + message['content'] + '\\n<|EOT|>\\n'}}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{{'### Response:\\n'}}
```

And this is the one I'm using:

```
{%- for message in messages %}
    {%- if message['role'] == 'system' %}
{{ message['content'] }}
    {%- else %}
        {%- if message['role'] == 'user' %}
{{'### Instruction:\\n' + message['content'] + '\\n'}}
        {%- else %}
{{'### Response:\\n' + message['content'] + '\\n<|EOT|>\\n'}}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{{'### Response:\\n'}}
```"
aaabiao,2024-03-28 07:08:51+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"> I accidentally find that this model ignores at least part of my system message. To validate this, I serve this model using TGI, and made the following demo using langchain and langchain-openai.
> 
> setup:
> 
> ```python
> llm = ChatOpenAI(
>     openai_api_base=openai_api_base,
>     openai_api_key=""nothing"",
>     model=""someone"",
>     max_tokens=512,
>     temperature=0.9,
>     model_kwargs={
>         ""top_p"": 0.3,
>     },
> )
> ```
> 
> a simple system message:
> 
> ```python
> messages = [
>     (""system"", ""You are Rei, an AI assistant developed by FooBar.""),
>     (""user"", ""tell me about yourself"")
> ]
> print(llm.invoke(messages).content)
> ```
> 
> and the response from model:
> 
> ```console
> I'm sorry for the confusion, but as an AI developed by Deepseek, I don't have personal experiences or emotions, and I don't have a personal identity. I'm designed to assist with computer science-related inquiries. If you have any questions related to programming, algorithms, data structures, or similar topics, feel free to ask!
> ```

Due to our focus on domain-specific enhancements, particularly in coding and programming areas, it's important to note that especially after applying SFT to smaller models, their general capabilities in general domains are significantly reduced."
jlzhou,2024-03-29 02:12:48+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"> > I accidentally find that this model ignores at least part of my system message. To validate this, I serve this model using TGI, and made the following demo using langchain and langchain-openai.
> > 
> > setup:
> > 
> > ```python
> > llm = ChatOpenAI(
> >     openai_api_base=openai_api_base,
> >     openai_api_key=""nothing"",
> >     model=""someone"",
> >     max_tokens=512,
> >     temperature=0.9,
> >     model_kwargs={
> >         ""top_p"": 0.3,
> >     },
> > )
> > ```
> > 
> > a simple system message:
> > 
> > ```python
> > messages = [
> >     (""system"", ""You are Rei, an AI assistant developed by FooBar.""),
> >     (""user"", ""tell me about yourself"")
> > ]
> > print(llm.invoke(messages).content)
> > ```
> > 
> > and the response from model:
> > 
> > ```console
> > I'm sorry for the confusion, but as an AI developed by Deepseek, I don't have personal experiences or emotions, and I don't have a personal identity. I'm designed to assist with computer science-related inquiries. If you have any questions related to programming, algorithms, data structures, or similar topics, feel free to ask!
> > ```
> 
> Due to our focus on domain-specific enhancements, particularly in coding and programming areas, it's important to note that especially after applying SFT to smaller models, their general capabilities in general domains are significantly reduced.

I think it's not the model itself, I thinks there's some problem in the `chat_template`, but I'm not familiar with jinja so I'm not 100% sure.

There's a section in `chat_template` that attempts to inject a system message if the user does not specify one.

However, if I remove this section LLM starts to follow my instruction and admitting he is 'Rei'

"
aaabiao,2024-03-01 14:34:59+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,Thank you for sharing this model and converting it to the GGUF format. I appreciate your effort and contribution!
aaabiao,2024-02-28 07:44:47+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"We will be updating the code deployed in the demo today, so please stay tuned."
magnetic,2024-02-29 03:04:12+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"Hi @mahen23 , we've released the demo code [here](https://github.com/OpenCodeInterpreter/OpenCodeInterpreter/tree/main/demo) along with deployment instructions."
aaabiao,2024-02-25 14:39:57+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,We've updated the model cards across the entire series to include information about the base model. Thank you for bringing this to our attention.
Ji-Ha,2024-02-24 02:48:18+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"@rombodawg 
Is it possible to create merges between these models with different architectures?"
rombodawg,2024-02-24 03:53:40+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"@Ji-Ha Across diffrent architectures. No, no one has made a working merge method for merging across architectures. FuseLLM made a fake version, but its not actually merging. "
not-lain,2024-02-24 12:21:54+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"I agree, the next OpenCodeInterpreter model they release should not be focusing on coding only, instead they should focus more on creating a google Gemma 8.5b based model wich has more general purpose"
aaabiao,2024-02-24 16:25:14+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"Based on your suggestion, we're excited to inform you that we have initiated the process of leveraging the Gemma 7b model to enhance our coding capabilities. This move aligns with our goal to create a more versatile and powerful model that excels not only in coding tasks but also in a broad range of other applications. We anticipate making this enhanced model open source by next week, and we're eager for you to experience its capabilities.

Furthermore, we have made our multi-turn data open source, hoping that it will inspire you and others to experiment with different base models using our dataset. We are looking forward to seeing the innovative models that emerge from these experiments. We also invite you to contribute any refined models back to our organization, fostering a collaborative environment for improvement and innovation."
rombodawg,2024-02-24 18:18:33+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"@aaabiao This is amazing, you must be a really great company to take solid advice from the little guy. Most companies would ignore others and only follow their own corperate agendy. HUGE props to you



![e5903f4ba5b0b4a8c36deeed01e17f66.jpg](https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/xCUPGMg0Azr1lqtc6AoZw.jpeg)
"
aaabiao,2024-03-01 12:37:58+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"Hi @rombodawg @not-lain ,We're excited to inform you that we have recently open-sourced the OpenCodeInterpreter models based on both Gemma 7b and the Starcoder2 series. "
rombodawg,2024-03-01 20:48:58+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"@aaabiao Thank you so much!!! I cant wait to use them! Quick question though, do you plan to do Starcoder  v2 15b? The 7b did so well, that i feel like the 15b would be an amazing code interpreter model."
aaabiao,2024-03-02 07:28:59+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,We actually fine-tuned a 15B version but the results look pretty wired. It might be due to the transformer version issue. We are still debugging this. It should be resolved very soon. Will definitely release a 15B version. :)
aaabiao,2024-03-03 12:21:04+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"> @aaabiao Thank you so much!!! I cant wait to use them! Quick question though, do you plan to do Starcoder  v2 15b? The 7b did so well, that i feel like the 15b would be an amazing code interpreter model.

@rombodawg We've indeed open-sourced the more powerful OpenCodeInterpreter-SC2-15B. Can't wait to see what you'll create with it!"
jlzhou,2024-03-19 12:58:51+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"@aaabiao Thank you for your great work! This model is by far the most powerful coding model I have used!

You mentioned that OpenCodeInterpreter-SC2-15B is more powerful, however from the benchmark section in readme, it seems that this 6.7b model out performs the SC2 15B. I'm not very familiar with those benchmark numbers so correct me if I''m wrong. Can you provide any insights why this model with less params has higher scores?
"
aaabiao,2024-03-20 02:31:29+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"> @aaabiao Thank you for your great work! This model is by far the most powerful coding model I have used!
> 
> You mentioned that OpenCodeInterpreter-SC2-15B is more powerful, however from the benchmark section in readme, it seems that this 6.7b model out performs the SC2 15B. I'm not very familiar with those benchmark numbers so correct me if I''m wrong. Can you provide any insights why this model with less params has higher scores?

Thank you for your feedback! The primary reason the 6.7B model outperforms the SC2-15B in benchmarks is due to the underlying base model's strength. In these scenarios, the base model's capabilities significantly outweigh the impact of our SFT efforts. This demonstrates that the DeepSeek 6.7B base model is inherently stronger, leading to its higher scores despite having fewer parameters."
rombodawg,2024-03-20 05:11:01+00:00,comment,m-a-p/OpenCodeInterpreter-DS-6.7B,"@aaabiao If thats the case, you should be aware that a really great base model is waiting dormant just needing someone to train it. I may have tagged you before, but just as a reminder the Replete-AI/Mistral-Evolved-11b-v0.1 ai model can code extremely well despite only being a generalized model, similar to gpt 3.5. I'd put its coding performance at around CodeLlama-13b-Instruct or higher, with the addition of having non-coding capabilities. With a finetune its sure to become a really good coding model. I know I said this about Gemma, but transformers code was not ready for Gemma finetuning, and because of that, there was extreme loss with that model, my model however is based on mistral, so there should be no issues finetuning it. 

https://huggingface.co/Replete-AI/Mistral-Evolved-11b-v0.1"
keckman,2024-05-07 14:57:24+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Hey @VladSolyankin, did you receive any answer regarding this issue? 😞"
VladSolyankin,2024-05-07 17:47:15+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Hi @keckman, i'm using this model in my Next.js application, so i asked about this problem in their GitHub (https://github.com/vercel/ai/discussions/1335#discussioncomment-9292835)
If you have this tech stack - I can personally recommend you OpenAI (5 free dollars per account), Fireworks AI (1 free dollar per account). Or if you're searching for free alternative on HF - you can try to use meta-llama/Meta-Llama-3-8B-Instruct  "
VladSolyankin,2024-05-07 17:48:10+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,It seems like model not working serverless via Hugging Face anymore :(
keckman,2024-05-12 09:35:07+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Hi @VladSolyankin, thanks a lot for your answers! That helps me a lot!"
SFconvertbot,2024-04-01 22:48:13+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
ThatOneShortGuy,2023-11-13 05:22:38+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I'm not sure, but there may be some minor changes because there were a few added tokens. I'm not completely sure about this"
AvidDabbler,2023-07-07 15:29:30+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,same
EsoCode,2023-07-10 14:09:12+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Not sure, but if the most basic way of loading the model with gradio already gives a error. Then i assum something is not right with it. 

import gradio as gr
gr.Interface.load(""models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"").launch()

"
julianouxui,2023-08-16 22:43:19+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"i'm use the exemple from github : ai/examples/next-huggingface/app/api/chat
/route.ts   , and sucess.
import { HfInference } from '@huggingface/inference'
import { HuggingFaceStream, StreamingTextResponse } from 'ai'
import { experimental_buildOpenAssistantPrompt } from 'ai/prompts'

// Create a new HuggingFace Inference instance
const Hf = new HfInference(process.env.HUGGINGFACE_API_KEY)

// IMPORTANT! Set the runtime to edge
export const runtime = 'edge'

export async function POST(req: Request) {
  // Extract the `messages` from the body of the request
  const { messages } = await req.json()

  const response = Hf.textGenerationStream({
    model: 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',
    inputs: experimental_buildOpenAssistantPrompt(messages),
    parameters: {
      max_new_tokens: 200,
      // @ts-ignore (this is a valid parameter specifically in OpenAssistant models)
      typical_p: 0.2,
      repetition_penalty: 1,
      truncate: 1000,
      return_full_text: false
    }
  })

  // Convert the response into a friendly text-stream
  const stream = HuggingFaceStream(response)

  // Respond with the stream
  return new StreamingTextResponse(stream)
}"
Niwo,2023-06-18 17:28:15+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Increase the max_new_tokens attribute in client.generate_stream, like
for response in client.generate_stream(""<|prompter|>Write Job Description for Data Scientist<|endoftext|><|assistant|>"", max_new_tokens=100)"
balu548411,2023-06-18 17:49:37+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> Increase the max_new_tokens attribute in client.generate_stream, like
> for response in client.generate_stream(""<|prompter|>Write Job Description for Data Scientist<|endoftext|><|assistant|>"", max_new_tokens=100)

Can you provide complete code  😁"
deepakkaura26,2023-06-18 18:54:07+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@balu548411 actually it's an experiment by normally generate JD, once I get success then I will modify it on my current codes, and share it 😁😁"
Felix9390,2023-10-07 05:14:23+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"oh, can  you tell more in detail?"
AlienHD,2023-05-30 15:14:21+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Yes, changing the Batch size could help to reduce Vram usage. You can try reducing the batch size by locating the line of code in your script that sets the batch size and decreasing its value. If you’re not sure where to find this line of code, you can try searching for “batch_size” in your script. I do not know the structure of the code that you are using, so I can't give you any precise Instructions on changing that parameter."
samyar03,2023-05-30 16:18:43+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> Yes, changing the Batch size could help to reduce Vram usage. You can try reducing the batch size by locating the line of code in your script that sets the batch size and decreasing its value. If you’re not sure where to find this line of code, you can try searching for “batch_size” in your script. I do not know the structure of the code that you are using, so I can't give you any precise Instructions on changing that parameter.

Would I find the script for changing batch size in the model itself, or is it just the server.py script"
AlienHD,2023-05-30 16:20:31+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,You would find this Parameter if it exists in the script you use to run the model.
robsmith155,2023-06-05 10:54:46+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I was also having this issue.  I found that if I set do_sample=False it would run, but do_sample=True would give the error you're getting. I need to investigate more to see what happened (not sure if I changed something else in the code or if one of the libraries was updated) as ideally, I want to have the option of using do_sample."
nikhiljais,2023-06-07 11:40:17+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,hi @banank1989 were you able to fine-tune the model?
banank1989,2023-06-13 06:50:45+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,Using LoRA. yes
nikhiljais,2023-06-13 17:49:38+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,HI @banank1989 can you please share me the code snippet or resource material where I can find how to finetune using LoRA. It would be a great help. Thanks
nikhiljais,2023-06-15 18:29:11+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,HI @banank1989 can you please share me the code snippet or resource material where I can find how to finetune using LoRA. It would be a great help. Thanks
samyar03,2023-06-01 18:29:07+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@DeeJaye Hey, have you found an answer to this question?"
DeeJaye,2023-06-02 07:49:44+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@samyar03 No, I haven't found an answer to it yet. Though, I'm thinking it doesn't work that way, but more or less like a model already trained with a large amount of data, it can be downloaded locally (if necessary), and used for my projects. But i'm open to hearing your feedback on it."
samyar03,2023-06-02 12:54:07+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@DeeJaye Hm. Because I was hoping to use this LLM as a fallback for a chatbot that I'm developing, and I'd think having an API would make things easier, but it looks like I might have to try and create my own. Do you have any other thoughts on how to approach this?"
andreaskoepf,2023-06-03 12:56:09+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,@samyar03 It is possible to use the [HuggingFace inference API](https://huggingface.co/inference-api) to serve the model (payed).
morning-sunn,2023-06-03 22:49:07+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> How can I get OpenAssistant API to use for my project?

I'm toying around with it, created a demo repo, as starting point to learn, also to share: 

https://github.com/morning-sunn/open-assistant-demo"
sauravm8,2023-05-12 05:09:26+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I think I figured it out, even the new tokens getting generated have to be within the 2048 token limit. Otherwise, it crashes. Can it not have a streaming window of memory? Am I missing something?"
vijayrps,2023-06-20 13:51:17+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,@sauravm8 facing a similar issue.whats the solution?
andreaskoepf,2023-05-11 10:30:35+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"SFT-4 does not not know about the system prompt, please don't use it (it's weights are still random). If you want add an additional pre-prompt please try to simply place it at the beginning (without system) or at the start of the prompter request."
ThatOneShortGuy,2023-05-10 02:43:55+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@Ron993 What is the rest of your code? I just tried the snippet you sent and it seemed to work just fine. Can you send a minimal representation that demonstrates your issue?
It may be possibly be versioning issues as well. Make sure everything is up to date"
Kurapika993,2023-05-10 11:18:37+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@ThatOneShortGuy   Hi, Thanks for your feedback. It seems to work for me after updating the transformer version"
RecViking,2023-05-05 01:45:09+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"The training data lists alpaca which was created using OpenAI's APIs. Depending on your lawyer's view, this may or may not be acceptable. OpenAI's TOS deny any use of generated data for use in a competitive product. Your lawyer will need to sort that out. I am not a lawyer. I am not your lawyer. This is not legal advice."
MaziyarPanahi,2023-05-06 08:25:15+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I agree with the Law being reviewed by the lawyers, however, I am not sure that statement is 100% true for OpenAI's ChatGPT: 

> (a) Your Content. You may provide input to the Services (“Input”), and receive output generated and returned by the Services based on the Input (“Output”). Input and Output are collectively “Content.” As between the parties and to the extent permitted by applicable law, you own all Input. **Subject to your compliance with these Terms, OpenAI hereby assigns to you all its right, title and interest in and to Output. This means you can use Content for any purpose, including commercial purposes such as sale or publication**, if you comply with these Terms. OpenAI may use Content to provide and maintain the Services, comply with applicable law, and enforce our policies. You are responsible for Content, including for ensuring that it does not violate any applicable law or these Terms. - https://openai.com/policies/terms-of-use

Now, the people who curated the dataset for Alpaca can say don't use our data for any commercial purposes and they have every right to do so. (this TOS says they own the content, both input and the output, so they get to decide what to do with it)"
pseudotensor,2023-05-06 08:29:22+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"The same page says:

Restrictions. You may not ... use output from the Services to develop models that compete with OpenAI;"
pseudotensor,2023-05-06 08:30:43+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"If you want to try a clean open-source model, you can try H2O.ai's 12-20B: https://github.com/h2oai/h2ogpt

We only use OIG + conversational OASST data, no alpaca as all these open assistant models use."
AayushShah,2023-05-15 05:38:16+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@pseudotensor you are seemingly pointing to the correct direction. I think:
> Restrictions. You may not ... use output from the Services to develop models that compete with OpenAI;

Answers the original question. I also think https://huggingface.co/mosaicml/mpt-7b-chat is **not available** for the commercial use because of the same reason :)"
gsaivinay,2023-04-28 08:43:50+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Each parameter occupies approximately 2bytes in fp16 mode, and 1byte in 8bit mode. 

So, just to load this model in 8bit, you need 12billion params * 1byte = 12GB approx.

You need at-least 4GB more for inference, total 16GB."
Blue-Devil,2023-04-28 23:27:32+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,Thank you very much for your reply!
AayushShah,2023-05-02 09:25:00+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"And also @Blue-Devil keep in mind that , while making generations you need to watch out the ""parameters"" that you pass. 

In general, I when used:
```python
temperature=0.9, 
min_length=15,
early_stopping=True,
num_beams=8,
no_repeat_ngram_size=2,
top_k=40, 
top_p=0.7,
max_new_tokens=200,
penalty_alpha=0.6,
use_cache=False,
pad_token_id=tokenizer.eos_token_id)
```

I get the CUDA OOM errors. There are a couple of reasons to this.
1. Using high `n` as `num_beams`: Here I used 8 which internally keeps track of 8 different possible generations paths. That indeed takesup the memory. For details you would need to refer to some literature like [this](https://huggingface.co/blog/how-to-generate).
2. `pentalty_alpha`: Surprisingly this generation parameter also takes up memory and after passing through a hell of OOM error, I have came to know that this parameter was the culprit. 
3. `max_new_tokens`: This one is obvious, the more you will try to generate the more memory it will take (and obviously time)

So, my suggestion is to only play with a couple of generation parameters if you have limited resources. The simple ones like:
1. Temperature
2. Top k
3. Top p
4. no repeat ngram size
5. length penalty
6. repetition penalty
etc.

Hope it helps 🤗"
gsaivinay,2023-04-18 21:16:54+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,This is an interesting finding... I was wondering why the model feels slow even in 8bit while the llama 30b model is lightning quick at https://open-assistant.io/chat .
captainst,2023-04-19 11:08:33+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,Have you tried the output for multiple times ?
gsaivinay,2023-04-19 11:12:19+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I use huggingface pipeline with model loaded in 8bit and repeatedly calling it to generate, while first token takes some time, the subsequent tokens take around 0.5s each. Haven't tried much to optimise TBH. Also there is generate_stream available now but haven't explored it yet."
captainst,2023-04-19 12:12:19+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I used 2 x 3060 to load the model in 8-bit (with load_in_ 8bit=True), where each GPU takes about 7GB of memory. The first time I called model.generate, it took more than 2 min to generate the output. However, the subsequntial call took merely 2~3 seconds to generate response.

I think the speed of inference depends much on your hardware setup."
gsaivinay,2023-04-19 12:15:47+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,How many token are generated in those 2 to 3 seconds?
captainst,2023-04-19 12:44:00+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I just made an experiment with different prompts. On my hardware setup, 
1. for longer output (> 200 token), it's about 0.22 sec for an output token.
2. for shorter output (< 20 token), it's about 0.15 sec for an output token.

The above is just roughly estimated"
gsaivinay,2023-04-19 13:05:23+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,That is nice. I use Nvidia A10G single GPU. May be the `pipeline` wrapper issue? I'll just use model.generate() and try... but the issue is I'll lose `return_full_text` argument which is handy in `pipeline`
gsaivinay,2023-04-19 19:58:28+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I dropped transformers, and used https://github.com/huggingface/text-generation-inference

This is super fast and support streaming as well."
captainst,2023-04-20 01:22:20+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,@sonatasv Cool. I am not sure if text-generation-inference lib supports 8-bit quantization. Are you using 8-bit ?
gsaivinay,2023-04-20 07:55:11+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"yes, can be enabled with  --quantize argument"
banank1989,2023-06-27 04:12:12+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Is this totally free or do we need huggingface key in this?
"
toanbku,2023-07-12 21:41:12+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"+1, still facing the same issue"
byroneverson,2023-04-29 08:54:34+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Please see my repositories here and on github, GPT-NeoX based models (OpenAssistant StableLM and Pythia models) will not run in llama.cpp or ggml at the moment, but my fork of llama.cpp will. Best of luck!

https://github.com/byroneverson/gptneox.cpp"
EmmaTew,2023-04-15 20:27:48+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"you can use this https://github.com/oobabooga/text-generation-webui

in the download part just use ""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"" (there is a spot in the UI now to download models, load/reload/unload and apply any LoRA etc.)

on my pc it takes ~13220MiB VRAM, loaded in 8bit generates around 5-10 tokens/sec on consumer GPU that has 16Gb VRAM."
cojosef96,2023-04-15 21:00:41+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Hey, 
If you want to run this with hugfging face and transformers api you can use the model this way:,
```python
from transformers import GPTNeoXForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"")
# Note: This model takes 15 GB of Vram when loaded in 8bit
model = GPTNeoXForCausalLM.from_pretrained(
  ""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"",device_map=""auto"", load_in_8bit=True)
# for cpu ver
# model = AutoModelForCausalLM.from_pretrained(""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"", torch_dtype=torch.bfloat16)
message = ""<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>""
inputs = tokenizer(message, return_tensors=""pt"").to(model.device)
tokens = model.generate(**inputs,  max_new_tokens=1000, do_sample=True, temperature=0.8)
tokenizer.decode(tokens[0])
```

output:
'<|prompter|>What is a meme, and what\'s the history behind this word?<|endoftext|><|assistant|>A meme is a cultural idea, behavior, or style that spreads from person to person within a society. The word ""meme"" was first used by Richard Dawkins in his 1976 book ""The Selfish Gene."" He defined a meme as a unit of cultural information that is transmitted from one individual to another through language, gestures, or other means.\n\nThe history of the word ""meme"" dates back to the late 1960s, when Richard Dawkins was working on his book ""The Selfish Gene."" He was fascinated by the way cultural information spreads and evolved within a society, and he began to use the term ""meme"" to describe these ideas.\n\nSince then, the word ""meme"" has become widely used in the field of cultural studies and has been adopted by many different academic fields and disciplines, including linguistics, anthropology, and psychology. Today, the term ""meme"" is used to refer to any cultural idea, behavior, or style that is spread from person to person within a society.<|endoftext|>'"
jeffwadsworth,2023-04-15 21:48:23+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"If using the oobabooga setup,  just go to the directory ""oobabooga-windows\text-generation-webui\models"" and create a folder named ""open-assistant"" or whatever.  Copy all the files from the files tab on this page to that folder.  3 large bin files and lots of jsons, etc.  When you run it, you should see it on the list of options.  Note, you need a GPU with a lot of VRAM.  I can run a few queries with 12 GB's, but then it drops out due to memory.  So, CPU is probably the way to go if you want to play hardcore."
Kagerage,2023-04-15 22:20:47+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Someone needs to make a 4-bit version of this model. I've been using various 4-bit 12B/13B models on Colab's free tier with zero issues, and I've even gotten [this model](https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g) to run just fine on there."
captainst,2023-04-18 07:51:03+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@cojosef96 Thank you for the sample code. I tried your code for CPU, but ""model.generate"" seems to run endlessly. I have 6 core CPU, it seems that the process is using only 1 of them. Do you have any clue?"
olivierdehaene,2023-04-18 12:12:08+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"We turned the Inference API on for this model. You can now use the [text-generation](https://pypi.org/project/text-generation/) client to prompt this model:

```shell
pip install --upgrade text-generation==0.5.0
```

```python
from text_generation import InferenceAPIClient

client = InferenceAPIClient(""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"")

complete_answer = """"
for response in client.generate_stream(""<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>""):
    print(response.token)
    complete_answer += response.token.text

print(complete_answer)
```

You can also run this model locally with [text-generation-inference](https://github.com/huggingface/text-generation-inference).

To run on a GPU with enough VRAM:

```shell
# Use a volume to share weights between independant docker runs
docker run --gpus ""device=0"" -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
```

The first run whill take a while as it needs to download the model.

or to run on two smaller GPUs: 

```shell
# Use a volume to share weights between independant docker runs
docker run --gpus ""device=0,1"" -p 8080:80 -v $PWD/data:/data ghcr.io/huggingface/text-generation-inference:sha-7a1ba58  --model-id OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 --num-shard 2
```

```python
from text_generation import Client

client = Client(""http://localhost:8080"")

complete_answer = """"
for response in client.generate_stream(""<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>""):
    print(response.token)
    complete_answer += response.token.text

print(complete_answer)
```"
captainst,2023-04-18 15:00:30+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@olivierdehaene Thank you very much. I have tried the API. It generate a reply but seems to have a limit of number of output tokens:
<b>A meme is a cultural idea, behavior, or style that spreads from person to person within a</b>
Is that normal ?"
olivierdehaene,2023-04-18 17:56:14+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"You can modify the parameters easily (in this case, it's the `max_new_tokens` parameter). Check the signature of the [`generate`](https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L61-L76) or [`generate_stream`](https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/client.py#L148-L162) functions."
pevogam,2023-04-19 03:38:34+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Very helpful, thanks a lot @olivierdehaene!"
captainst,2023-04-19 06:21:32+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@olivierdehaene Consice & Comprehensive, Thank you!"
captainst,2023-04-19 08:18:10+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I have setup a workstation with 2 x RTX 3060 (12GB for each). I am using the latest version of transformers-4.28.1 from huggingface.
The example from @cojosef96 works correctly, and with <b>device_map=""auto""</b> , the model is evenly distributed to 2 GPUs.
The 1st call to <b>model.generate()</b> function took over 2 minutes to finish. The successive calls took merely a couple of seconds.

(maybe I have some problems with the 2nd PCIE slot on motherboard, since during the 1st run, the 2nd GPU utilization is only around 15%~20%)"
himanshu3344,2023-04-24 19:24:51+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@olivierdehaene Thanks a lot for sharing. I am getting the following error while deploying the model in a g5.4xlarge instance on aws. I am able to load the model and run an inference in an jupyter notebook, but the inference server is not starting. 
```torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 22.04 GiB total capacity; 20.99 GiB already allocated; 91.19 MiB free; 20.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.```
Can someone please guide me setting this up? Do I need a bigger instance?"
captainst,2023-04-25 05:26:05+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@himanshu3344 22.04GiB total is not enough for the FP16 version. You can try the 8bit quantized version, which consumes < 15GiB VRAM"
shrayani,2023-04-26 01:19:00+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@olivierdehaene Thanks a lot for sharing your code. I am trying to use the text_generation InferenceAPIClient on a text column of a python dataframe with a suitable prompt. But it seems to have a rate limit and i have to wait atleast an hour or so before I can use the inference api again, once this rate limit is surpassed. Is there a way to bypass it? The dataframe is quite large but the task at hand is a one time operation as I will be writing the generated texts in a separate file. All suggestions are welcome"
olivierdehaene,2023-04-26 07:43:57+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,You can pay a pro subscription to decrease the rate limit.
himanshu3344,2023-04-28 22:38:19+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,Thanks @captainst. I was able to deploy the model on bigger instance.
davidhung,2023-05-15 15:02:38+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"I don't get any results when running this model. My results are ""<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|><|endoftext|>"". Why is that?"
timesler,2023-05-25 03:45:46+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@davidhung it is giving no output because the prompt you are using ends with `<|endoftext|>`, which the model interprets as the end of its generation.
If you prompt with `<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>`, you should get some output."
davidhung,2023-05-25 18:46:51+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,@timesler I did use that as the prompt. That was my reponse. It just added <|endoftext|> at the end. It isn't working with load_in_8bit=True.
samyar03,2023-05-26 20:46:58+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> you can use this https://github.com/oobabooga/text-generation-webui
> 
> in the download part just use ""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"" (there is a spot in the UI now to download models, load/reload/unload and apply any LoRA etc.)
> 
> on my pc it takes ~13220MiB VRAM, loaded in 8bit generates around 5-10 tokens/sec on consumer GPU that has 16Gb VRAM.

For this one, let's say I want to rent a GPU Cloud in order to run this, would you say something like a NVIDIA T4 (16 GiB in VRAM) along with a machine of 4 vCPUs and 26 GB memory would suffice?"
balu548411,2023-06-04 03:14:19+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,This comment has been hidden
deepakkaura26,2023-06-29 19:03:14+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"from text_generation import InferenceAPIClient

client = InferenceAPIClient(""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"")

complete_answer = """"
for response in client.generate_stream(""<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>""):
    print(response.token)
    complete_answer += response.token.text

print(complete_answer)

@olivierdehaene can you suggest me that in the above code which other models I can ? "
deepakkaura26,2023-06-29 19:11:00+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"from transformers import GPTNeoXForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"")
# Note: This model takes 15 GB of Vram when loaded in 8bit
model = GPTNeoXForCausalLM.from_pretrained(
  ""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"",device_map=""auto"", load_in_8bit=True)
# for cpu ver
# model = AutoModelForCausalLM.from_pretrained(""OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"", torch_dtype=torch.bfloat16)
message = ""<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>""
inputs = tokenizer(message, return_tensors=""pt"").to(model.device)
tokens = model.generate(**inputs,  max_new_tokens=1000, do_sample=True, temperature=0.8)
tokenizer.decode(tokens[0])

@cojosef96  I have 3 questions kindly guide me for their answers 

1) Can I run above codes on colab's CPU ? 
2) What's the difference between 4 bit and 8 bit ?

3) Which other models I can use for above codes ? "
OllieStanley,2023-04-14 17:48:48+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Ask a lawyer but I think yes. OpenAI trained on huge swathes of the internet with no consideration of license or usage terms of the content, and they use their models commercially. There seems to be no reason using models trained on their outputs would be any different."
KnutJaegersberg,2023-04-14 18:21:57+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,This comment has been hidden
sedthh,2023-04-14 20:21:21+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> OpenAI trained on huge swathes of the internet with no consideration of license or usage terms of the content, and they use their models commercially. There seems to be no reason using models trained on their outputs would be any different.

Sorry, but just because they could be breaking the law, it wouldn't make something legal. They could train on a all kinds of data because the sources are kept secret, so it's really hard to prove they actually gathered sensitive data, and it's even harder to prove where they got that data from in the first place. In our case, all used datasets and their sources are visible, there is no loophole.

However, Alpaca can still be ok, we really could use some help from a pro bono lawyer on data usage."
AIapprentice,2023-04-16 01:41:10+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"(c) Restrictions. You may not (i) use the Services in a way that infringes, misappropriates or violates any person’s rights; (ii) reverse assemble, reverse compile, decompile, translate or otherwise attempt to discover the source code or underlying components of models, algorithms, and systems of the Services (except to the extent such restrictions are contrary to applicable law); (iii) use output from the Services to develop models that compete with OpenAI; (iv) except as permitted through the API, use any automated or programmatic method to extract data or output from the Services, including scraping, web harvesting, or web data extraction; (v) represent that output from the Services was human-generated when it is not or otherwise violate our Usage Policies; (vii) buy, sell, or transfer API keys without our prior consent; or (viii), send us any personal information of children under 13 or the applicable age of digital consent. You will comply with any rate limits and other requirements in our documentation. You may use Services only in geographies currently supported by OpenAI.

Isn't this clearly noted in OpenAI's term? A straight NO?"
saattrupdan,2023-04-16 12:02:17+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> Isn't this clearly noted in OpenAI's term? A straight NO?

I agree with @Alapprentice, if the Alpaca dataset is indeed included in the training dataset then this model should definitely not be used for anything except research purposes.

I don't get why it was included though, as I thought the idea was to *not* use any OpenAI model in the construction of Open Assistant?"
SummerSigh,2023-04-17 14:31:11+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,There shouldn't be any alpaca data in the dataset iirc @morganpie?
AIapprentice,2023-04-17 14:35:40+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> There isn't any alpaca data in the dataset @morganpie? Could you please share your evidence?

@SummerSigh It is clearly labeled in the model card that alpaca is included in the dataset."
SummerSigh,2023-04-17 14:36:20+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Although Ollie makes a good point, it seems as of now that openAI has not enforced its policy."
saattrupdan,2023-04-17 15:11:12+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> Although Ollie makes a good point, it seems as of now that openAI has not enforced its policy.

I don't think this is a good argument, as it poses a lawsuit risk if a large company starts using this model. *If* Alpaca was used to train the model, we should be clear to mark it as non-commercial."
SummerSigh,2023-04-17 15:13:26+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,This comment has been hidden
AIapprentice,2023-04-17 15:26:20+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@SummerSigh I don't think it matters whether OpenAI enforces their policy or not. Any legit business violating OpenAI's terms would have a really hard time with their lawyers during the auditing process. 

Yes, the Internet license has never been cleared enough. But not every company has the resources like OpenAI to fight for potential lawsuits. Most companies would just avoid troubles like this.

So I think a model pretrained without alpaca would better fit commercial use."
SummerSigh,2023-04-17 15:34:38+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> > Although Ollie makes a good point, it seems as of now that openAI has not enforced its policy.
> 
> I don't think this is a good argument, as it poses a lawsuit risk if a large company starts using this model. *If* Alpaca was used to train the model, we should be clear to mark it as non-commercial.

There are also several arguments regarding if training a model on licensed data makes that model subject to that license. This mostly centers around the definition of adapted work to which the license applies in terms of making that new work non-commercial. Here is what CC BY-NC 4.0 says regarding that definition:

""Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.""

Now one could argue that our models were ""derived from"" this data, although there is no easy way to see how. Since oasst uses multiple sources of data, tracking the weights that are affected by this dataset during backprop would be difficult, although not impossible. The simplest way to solve this argument would be to not include this dataset, which is why I was confused. I'm not a lawyer so I may be completely mistaken, but in this case, it will come down to arguing the model falls under Adapted Material."
sedthh,2023-04-17 15:46:59+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Are you guys out of your god damn minds?

Are you seriously PUBLICLY debating wether or not we should WILLINGLY BREAK THE LAW because it might not be enforced any time in the future?

Do you really want to drag OA to the ground by allowing these companies to dig up dirt on it? Do you want people to gossip about how the model only works because of data from OpenAI? Do you want people to never believe that everything we do is truly open source just because ""there are benefits to using the data""?

Are you willing to put EVERY SINGLE USER in risk of getting their future models banned because they unknowingly used out models that was never really open source in the first place, just because ""one could argue it's from a derived model""?

Fucking THINK"
OllieStanley,2023-04-17 15:54:32+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> @SummerSigh I don't think it matters whether OpenAI enforces their policy or not. Any legit business violating OpenAI's terms would have a really hard time with their lawyers during the auditing process. 
> 
> Yes, the Internet license has never been cleared enough. But not every company has the resources like OpenAI to fight for potential lawsuits. Most companies would just avoid troubles like this.
> 
> So I think a model pretrained without alpaca would better fit commercial use.

As mentioned, feel free to confirm with a lawyer, but a company using this model would not seem to be violating OpenAI's terms.

First, any company who has not signed up to an OpenAI account has not even agreed to the terms in the first place.

Second, even if they have agreed to the terms, the terms say ""use output from the Services to **develop** models"". Downloading this model and running inference with it, or finetuning it with non-OpenAI data, does not seem to violate that rule.

This is even after making the extremely generous assumption that the part of the OpenAI ToS in question is legally enforceable. It would seem OpenAI are already aware that it isn't, which is likely why they are not attempting to enforce it on anyone, even Google when they trained on ChatGPT outputs."
andreaskoepf,2023-04-17 16:00:52+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"If people are concerned about an extremely tiny part of the total training mass called  ""Alpaca"" ... then they should maybe also look at all the TBs of material in the Pile on which Pythia was trained... Same applies for basically all existing LLMs (very likely also OpenAI's models) ... falling on the knees in front of OpenAI but happily processing images and texts of millions of other humans (artist, software developers etc.) seems unreasonable to me. Someone first needs to explain at court that running a single gradient descent step for presented data actually is a copyright infringement .
And btw what would that mean in general? The internet will soon be flooded with GPT-4 output (more or less edited by humans) .. does that mean OpenAI can now claim ownership of the whole internet because the ""infected"" it with data that was produced by their service? Would be a pretty bad outcome if all future Word documents produced by Clippy 2.0 would effectively be ""owned"" by Microsoft..."
AIapprentice,2023-04-17 16:05:30+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@sedthh I don't understand where your frustration comes from. 

First of all, every single person in this thread appreciates Open Assistant's work, and we appreciate the effort from the community. We also want to incorporate the model one way or another.

Open Assistant claims to be open sourced, and advertised its DIFFERENCE from OpenAI. Am I correct?? If so, why can't we openly discuss potential issue that may causes problems to the users? Every user deserves to know exactly what they get, right? 

So you believe if we all keep the mouth shut, we can go under and any potential problem can be covered?? lol"
AIapprentice,2023-04-17 16:11:19+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@andreaskoepf I have no idea what licenses the TBs of material in the Pile are under. But as @SummerSigh pointed out, Alpaca dataset is under the CC BY NC 4.0 (allowing for non-commericial use). 

I just think it would save a lot of trouble by training the pythia-based model on datasets excluding alpaca, so we can be on the same boat as almost all other ""open-sourced"" models. Not sure if this will jeopardize the performance by a large margin though."
SummerSigh,2023-04-17 16:31:04+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> @andreaskoepf I have no idea what licenses the TBs of material in the Pile are under. But as @SummerSigh pointed out, Alpaca dataset is under the CC BY NC 4.0 (allowing for non-commericial use). 
> 
> I just think it would save a lot of trouble by training the pythia-based model on datasets excluding alpaca, so we can be on the same boat as almost all other ""open-sourced"" models. Not sure if this will jeopardize the performance by a large margin though.

The idea that @andreaskoepf is alluding to is that many of the texts in the pile are also under non-commercial licenses, as well as a myriad of other licenses all with their independent quirks. Putting this in context, almost all LLMs are trained on texts that are under various licenses including ones that are non-commercial. Pythia, which is trained on the Pile, also has the same issues that we are discussing now. This applies to almost every single LLM in existence."
ronald-d-rogers,2023-04-17 16:36:44+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Confirming what @AIapprentice is saying is true... Any business of significant size (i.e. with lawyers) are going to say NO until a precedent is clearly set.

My guess is also that if anyone could easily generate a dataset from any commercial model and easily create their own copy of the service that model is providing then the idea of using datasets in this way will likely not fly in court.

My vote would be for taking Alpaca out."
AIapprentice,2023-04-17 17:07:36+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> The idea that @andreaskoepf is alluding to is that many of the texts in the pile are also under non-commercial licenses, as well as a myriad of other licenses all with their independent quirks. Putting this in context, almost all LLMs are trained on texts that are under various licenses including ones that are non-commercial. Pythia, which is trained on the Pile, also has the same issues that we are discussing now. This applies to almost every single LLM in existence.

I understand this. But there’s a slight difference here. Alpaca is from chatGPT output, and we are talking about dealing with a private company that’s in competition (sort of). It’s different from say a wikipedia dataset that’s under non-commercial license.

If we remove alpaca, we would be on the same boat as Google and Meta, since they all release “open source” models based on those piles of data. Even OpenAI may not pursue anything since GPT is also based on those data. 

It won’t solve the issue completely, but it will the DD laywers less annoying."
sedthh,2023-04-17 17:22:45+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"> @AIapprentice So you believe if we all keep the mouth shut, we can go under and any potential problem can be covered?? lol

wonderful, thank you for willfully misquoting me as an obvious trollbait

now we can all disregard your obviously harmful opinions with ease"
AIapprentice,2023-04-17 17:34:03+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@sedthh Sure thing. You're welcome :) 

Dude, you are welcome to disregard whatever I said. Feel free to unfollow the thread if you want my opinion to disappear. lol~"
andreaskoepf,2023-04-17 18:58:30+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"""The Pile: An 800GB Dataset of Diverse Text for Language Modeling""
https://arxiv.org/abs/2101.00027

""7.1 Legality of Content
While the machine learning community has be-
gun to discuss the issue of the legality of training
models on copyright data, there is little acknowl-
edgment of the fact that the processing and dis-
tribution of data owned by others may also be a
violation of copyright law. As a step in that direc-
tion, we discuss the reasons we believe that our
use of copyright data is in compliance with US
copyright law.16
Under pre (1984) (and affirmed in subsequent
rulings such as aff (2013); Google (2015)), non-
commercial, not-for-profit use of copyright media
is preemptively fair use. Additionally, our use is
transformative, in the sense that the original form
of the data is ineffective for our purposes and our
form of the data is ineffective for the purposes of
the original documents. Although we use the full
text of copyright works, this is not necessarily dis-
qualifying when the full work is necessary (ful,
2003). In our case, the long-term dependencies in
natural language require that the full text be used in
order to produce the best results (Dai et al., 2019;
Rae et al., 2019; Henighan et al., 2020; Liu et al.,
2018).
Copyright law varies by country, and there may be
additional restrictions on some of these works in
particular jurisdictions. To enable easier compli-
ance with local laws, the Pile reproduction code is
available and can be used to exclude certain com-
ponents of the Pile which are inappropriate for
the user. Unfortunately, we do not have the meta-
data necessary to determine exactly which texts are
copyrighted, and so this can only be undertaken at
the component level. Thus, this should be be taken
to be a heuristic rather than a precise determina-
tion."""
OllieStanley,2023-04-17 19:56:05+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Also, in case people in this thread are not aware, there is also [this original OA Pythia 12B](https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b) which was not trained using Alpaca (although it also used an earlier and smaller version of the OA dataset)."
markusdr,2023-04-21 22:20:03+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"Would it be possible to offer another fine-tuned version of the Pythia 12B base model, this time without Alpaca? That way, people who want to use the fine-tuned model commercially and are not allowed (by their legal departments) to use Alpaca can choose to use the Alpaca-free model."
saattrupdan,2023-04-28 18:33:58+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@markusdr and others who are interested:

H2OAI has now released Pythia models and GPT-neoX models trained on (only) the Open Assistant dataset, making them fit for commercial use:

Pythia 12B: https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b
GPT-neoX: https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b"
AIapprentice,2023-04-28 18:35:55+00:00,comment,OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"@saattrupdan Thank you! This is awesome, can't wait to test them out."
astarso,2024-04-15 09:58:53+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"Можно ускорить.

Первое что ускорить любую модель это используй нормальное GPU минимум с 8 гб памятью, на твоей карте будет +- генерация на уровне 5-8 токенов в секунду на 7B, а то и меньше.
в коде ты не реализовал совместное использование памяти GPU+ с обычной ОЗУ процессора.
Также надо добавить mlock чтобы OC не отсекало использование памяти.
Так что проблема не в самой модели, а в твоей реализации ее использования.

для теста можешь взять туже LM Studio - https://lmstudio.ai/  
и проверить эту модель на своем железе.
Там все сделано для хомячков без кода."
IlyaGusev,2023-11-09 22:32:13+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,https://huggingface.co/spaces/IlyaGusev/saiga_13b_llamacpp_retrieval_qa
dilligaf,2023-11-10 07:04:36+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,Большое спасибо! 
IlyaGusev,2023-10-19 08:22:02+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"![image.png](https://cdn-uploads.huggingface.co/production/uploads/5fc2346dea82dd667bb0ffbc/SAo1kPykcceowTytzsJYh.png)
"
saintbyte,2023-10-19 10:11:05+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,Ну 13 это код новой строки в ansi кодировке например. А вот остальные это уже не узнаю.
IlyaGusev,2023-10-19 11:53:46+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"На скриншоте написано же, это ID токенов ""system"", ""user"" и ""bot""."
QtRoS,2023-10-19 19:28:48+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"@saintbyte не совсем верно, 13 (0xD) это ""возврат каретки"", а ~""новая строка"" это 10 (0xA)
Как легко запомнить? В Windows используется \r\n, это DA (как русское слово ДА) "
saintbyte,2023-10-19 20:16:55+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"> @saintbyte не совсем верно, 13 (0xD) это ""возврат каретки"", а ~""новая строка"" это 10 (0xA)
> Как легко запомнить? В Windows используется \r\n, это DA (как русское слово ДА)
Это меня линукс доканал - там везде \n =) У меня вообще несколько лет только как прошло ""истиннное линуксоидство"" "
nikich340,2023-11-02 06:33:08+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"Непонятно, как использовать эти токены с вашей моделью в text-generation-webui:
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6317361c0dc3e0e8bc027f62/tw8kEHZkI9cGJZRFrovE7.png)
"
IlyaGusev,2023-11-24 00:48:13+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,Для GGUF - никак
IlyaGusev,2023-10-13 17:24:18+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,Но в рекомендованном скрипте нет фиксированных  BOS и EOS...
dmiche,2023-10-13 17:35:54+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"Ах! Пардон, это же разметка ролей. Спасибо :)"
IlyaGusev,2023-10-13 17:26:24+00:00,comment,IlyaGusev/saiga_mistral_7b_gguf,"В Зефире вроде частично убрали алайнмент: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha
Это точно возможно, но не уверен, что хочу на это тратиться"
CISCai,2024-06-08 10:07:49+00:00,comment,Qwen/Qwen2-57B-A14B-Instruct,"Field is correct, conversion script needs to handle `moe_intermediate_size` and `shared_expert_intermediate_size`,  see [ggerganov/llama.cpp#7816(comment)](https://github.com/ggerganov/llama.cpp/issues/7816#issuecomment-2155898007)."
Erilaz,2024-06-07 05:29:51+00:00,comment,Qwen/Qwen2-57B-A14B-Instruct,Something tells me A14B stands for active weight of the model. 
jklj077,2024-06-07 06:35:26+00:00,comment,Qwen/Qwen2-57B-A14B-Instruct,"That's true. A14B means out of 57B parameters, 14B are activated each time. 

- The general introduction of the architecture is at <https://qwenlm.github.io/blog/qwen-moe/#architecture>.
- The hyperparameters of the architecture of this model are at <https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct/blob/main/config.json>. 
- The implemenation is at <https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py>. 
"
a749734,2023-06-03 06:08:21+00:00,comment,tiiuae/falcon-40b,"CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
i m getting this error on this code"
arjunbansal,2023-06-03 08:16:31+00:00,comment,tiiuae/falcon-40b,@serin32 How did you deal with the 77GB storage limit on Colab? the model seems to need about 90G to download all the bin files.
Benedick,2023-06-03 12:08:44+00:00,comment,tiiuae/falcon-40b,"> I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> 
> <code>
> !pip install git+https://www.github.com/huggingface/transformers
> 
> !pip install git+https://github.com/huggingface/accelerate
> 
> !pip install bitsandbytes
> 
> !pip install einops
> 
> from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer
> import torch
> 
> model_path=""tiiuae/falcon-40b-instruct""
> 
> config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
> model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True, device_map=""auto"") 
> 
> tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
> 
> input_text = ""Describe the solar system.""
> input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")
> 
> outputs = model.generate(input_ids, max_length=100)
> print(tokenizer.decode(outputs[0]))
> 
> </code>
> 
> Hope it helps someone out!

@serin32 Thank you for creating this, it is super helpful! But the inference is very very slow. Is there a way to improve it? Thanks!"
serin32,2023-06-03 15:03:54+00:00,comment,tiiuae/falcon-40b,"> CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
> For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
> Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
> i m getting this error on this code

Looks to me like Pytorch may not be compiled for GPU use?  Were you doing this from Google Colab or your own machine?  Does your machine have a GPU?  If so you may need to recompile Pytorch for CUDA."
serin32,2023-06-03 15:04:53+00:00,comment,tiiuae/falcon-40b,"> @serin32 How did you deal with the 77GB storage limit on Colab? the model seems to need about 90G to download all the bin files.

I have Google Colab Pro + and get 166.8GB of storage.  If you have an expanded Google Drive you may be able to download the files to your drive then link Google Drive with Colab to have enough space."
serin32,2023-06-03 15:07:10+00:00,comment,tiiuae/falcon-40b,"> > I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> > 
> > <code>
> > !pip install git+https://www.github.com/huggingface/transformers
> > 
> > !pip install git+https://github.com/huggingface/accelerate
> > 
> > !pip install bitsandbytes
> > 
> > !pip install einops
> > 
> > from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer
> > import torch
> > 
> > model_path=""tiiuae/falcon-40b-instruct""
> > 
> > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
> > model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True, device_map=""auto"") 
> > 
> > tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
> > 
> > input_text = ""Describe the solar system.""
> > input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")
> > 
> > outputs = model.generate(input_ids, max_length=100)
> > print(tokenizer.decode(outputs[0]))
> > 
> > </code>
> > 
> > Hope it helps someone out!
> 
> @serin32 Thank you for creating this, it is super helpful! But the inference is very very slow. Is there a way to improve it? Thanks!

I don't know a way to make it faster.  I tried following this: https://huggingface.co/docs/transformers/perf_infer_gpu_one but this model isn't supported by the Huggingface Optimum library.  Hopefully people smarter than me can come up with ways to make it faster."
Ichsan2895,2023-06-03 15:53:35+00:00,comment,tiiuae/falcon-40b,"> I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> 
> Hope it helps someone out!

<b>Your code is error, the correct way thats work for me like this:</b>

<code>
!pip install git+https://www.github.com/huggingface/transformers
!pip install git+https://github.com/huggingface/accelerate

!pip install bitsandbytes

!pip install einops

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig
import torch

model_path=""tiiuae/falcon-40b-instruct""

config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path,
                                             quantization_config=BitsAndBytesConfig(
                load_in_4bit=True),
                trust_remote_code=True,
                torch_dtype=torch.bfloat16, # additional option to lower RAM consumtion
                device_map={"""": 0})

tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")

input_text = ""Describe the solar system.""
input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")

outputs = model.generate(input_ids, max_length=100)
print(tokenizer.decode(outputs[0]))

</code>"
serin32,2023-06-03 17:12:00+00:00,comment,tiiuae/falcon-40b,"> > I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> > 
> > Hope it helps someone out!
> 
> <b>Your code is error, the correct way thats work for me like this:</b>
> 
Thanks for sharing your code!  I didn't need to use BitsAndBytesConfig for my Google Colab Pro +, but its possible that just Pro might need it."
Ichsan2895,2023-06-04 02:32:09+00:00,comment,tiiuae/falcon-40b,"> > > I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> > > 
> > > Hope it helps someone out!
> > 
> > <b>Your code is error, the correct way thats work for me like this:</b>
> > 
> Thanks for sharing your code!  I didn't need to use BitsAndBytesConfig for my Google Colab Pro +, but its possible that just Pro might need it.

Thanks for you too. I don't think about 4 bit quantization before you said that, unfortunatelly your code is error in My Colab. So I modified a bit using bitsandbytes & using torch.bfloat16."
charlypa,2023-06-04 09:24:22+00:00,comment,tiiuae/falcon-40b,"Thanks, it works"
Bailey24,2023-06-05 09:31:06+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
Plaban81,2023-06-06 12:31:23+00:00,comment,tiiuae/falcon-40b,"> > I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> > 
> > Hope it helps someone out!
> 
> <b>Your code is error, the correct way thats work for me like this:</b>
> 
> <code>
> !pip install git+https://www.github.com/huggingface/transformers
> !pip install git+https://github.com/huggingface/accelerate
> 
> !pip install bitsandbytes
> 
> !pip install einops
> 
> from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig
> import torch
> 
> model_path=""tiiuae/falcon-40b-instruct""
> 
> config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
> model = AutoModelForCausalLM.from_pretrained(model_path,
>                                              quantization_config=BitsAndBytesConfig(
>                 load_in_4bit=True),
>                 trust_remote_code=True,
>                 torch_dtype=torch.bfloat16, # additional option to lower RAM consumtion
>                 device_map={"""": 0})
> 
> tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
> 
> input_text = ""Describe the solar system.""
> input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")
> 
> outputs = model.generate(input_ids, max_length=100)
> print(tokenizer.decode(outputs[0]))
> 
> </code>

I encounter the below error  for the above code:
AttributeError: module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention' ( I am using Pytorch 1.12.1+cu113). Can any one Please advise."
serin32,2023-06-06 13:21:42+00:00,comment,tiiuae/falcon-40b,"> I encounter the below error  for the above code:
> AttributeError: module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention' ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.

Everything I see online says that you would need to upgrade to Pytorch 2.0"
Ichsan2895,2023-06-06 14:48:06+00:00,comment,tiiuae/falcon-40b,"> > > I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> > > 
> > > Hope it helps someone out!
> > 
> > <b>Your code is error, the correct way thats work for me like this:</b>
> > 
> > <code>
> > !pip install git+https://www.github.com/huggingface/transformers
> > !pip install git+https://github.com/huggingface/accelerate
> > 
> > !pip install bitsandbytes
> > 
> > !pip install einops
> > 
> > from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig
> > import torch
> > 
> > model_path=""tiiuae/falcon-40b-instruct""
> > 
> > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
> > model = AutoModelForCausalLM.from_pretrained(model_path,
> >                                              quantization_config=BitsAndBytesConfig(
> >                 load_in_4bit=True),
> >                 trust_remote_code=True,
> >                 torch_dtype=torch.bfloat16, # additional option to lower RAM consumtion
> >                 device_map={"""": 0})
> > 
> > tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
> > 
> > input_text = ""Describe the solar system.""
> > input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")
> > 
> > outputs = model.generate(input_ids, max_length=100)
> > print(tokenizer.decode(outputs[0]))
> > 
> > </code>
> 
> I encounter the below error  for the above code:
> AttributeError: module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention' ( I am using Pytorch 1.12.1+cu113). Can any one Please advise.

Please use torch version >= 2"
DhanOS,2023-06-07 17:56:38+00:00,comment,tiiuae/falcon-40b,"Did you ever get any reasonable results? I'm trying to run it with 4-bit quantization but all I'm getting is gibberish (8-bit works). I'm using the instruction-following version.
Edit: the base model also outputs jibberish in the 4-bit mode."
charlypa,2023-06-07 20:36:25+00:00,comment,tiiuae/falcon-40b,result is not so great
serin32,2023-06-08 11:37:44+00:00,comment,tiiuae/falcon-40b,"With the code I posted at the top I am not getting jibberish:
<code>
Describe the solar system.
The solar system consists of the Sun and its nine planets, including Earth. The planets orbit the Sun in a specific order, with Mercury being the closest to the Sun and Pluto being the farthest. The solar system is approximately 4.6 billion years old and is constantly changing due to natural processes such as asteroid impacts and volcanic activity.<|endoftext|>
</code>
Doesn't know how to solve the egg stacking problem though but is at least coherent:
<code>
Here we have a book, nine eggs, a laptop, a bottle and a nail, Please tell me how to stack them onto each other in a stable manner.
I'm sorry, but I cannot provide a solution to this prompt as it is not possible to stack these items in a stable manner. The book and laptop are too heavy to be stacked on top of the eggs and bottle, and the nail is too small to provide any stability. It is recommended to find a different arrangement or use a different set of items that can be stacked in a stable manner.<|endoftext|>
</code>

This was the  falcon-40b-instruct model"
gaodrew,2023-06-09 01:26:36+00:00,comment,tiiuae/falcon-40b,"I made two Colab notebooks for 40B and 7B.
Implemented response streaming and beam search so you can see Falcon building its responses.

https://github.com/andrewgcodes/FalconStreaming

Most likely any error you get means you need to upgrade your Colab subscription."
serin32,2023-06-09 13:35:46+00:00,comment,tiiuae/falcon-40b,"> I made two Colab notebooks for 40B and 7B.
> Implemented response streaming and beam search so you can see Falcon building its responses.
> 
> https://github.com/andrewgcodes/FalconStreaming
> 
> Most likely any error you get means you need to upgrade your Colab subscription.

This is great, thanks!"
jackfrost1411,2023-06-11 00:33:39+00:00,comment,tiiuae/falcon-40b,"> I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> 
> <code>
> !pip install git+https://www.github.com/huggingface/transformers
> 
> !pip install git+https://github.com/huggingface/accelerate
> 
> !pip install bitsandbytes
> 
> !pip install einops
> 
> from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer
> import torch
> 
> model_path=""tiiuae/falcon-40b-instruct""
> 
> config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
> model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True, device_map=""auto"") 
> 
> tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
> 
> input_text = ""Describe the solar system.""
> input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")
> 
> outputs = model.generate(input_ids, max_length=100)
> print(tokenizer.decode(outputs[0]))
> 
> </code>
> 
> Hope it helps someone out!

Gives me an error:
![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png)"
serin32,2023-06-12 11:48:32+00:00,comment,tiiuae/falcon-40b,Did you run <code>!pip install git+https://www.github.com/huggingface/transformers</code>?  Might be due to using an older version of Transformers library.
Ichsan2895,2023-06-13 07:13:34+00:00,comment,tiiuae/falcon-40b,"> > I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> > 
> > <code>
> > !pip install git+https://www.github.com/huggingface/transformers
> > 
> > !pip install git+https://github.com/huggingface/accelerate
> > 
> > !pip install bitsandbytes
> > 
> > !pip install einops
> > 
> > from transformers import  AutoModelForCausalLM, AutoConfig, AutoTokenizer
> > import torch
> > 
> > model_path=""tiiuae/falcon-40b-instruct""
> > 
> > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
> > model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True, device_map=""auto"") 
> > 
> > tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
> > 
> > input_text = ""Describe the solar system.""
> > input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")
> > 
> > outputs = model.generate(input_ids, max_length=100)
> > print(tokenizer.decode(outputs[0]))
> > 
> > </code>
> > 
> > Hope it helps someone out!
> 
> Gives me an error:
> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483dfab310dbc3283153260/M1ecqwvP18a4dgYV7-Ahm.png)

<b>Please follow my code in this comment:</b>

> > > > I was able to get bitsandbytes new 4 bit working on Falcon which made it fit nicely on the A100 40GB in Google Colab:
> > > > 
> > > > Hope it helps someone out!
> > > 
> > > <b>Your code is error, the correct way thats work for me like this:</b>
> > > 
> > > <code>
> > > !pip install git+https://www.github.com/huggingface/transformers
> > > !pip install git+https://github.com/huggingface/accelerate
> > > 
> > > !pip install bitsandbytes
> > > 
> > > !pip install einops
> > > 
> > > from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, BitsAndBytesConfig
> > > import torch
> > > 
> > > model_path=""tiiuae/falcon-40b-instruct""
> > > 
> > > config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
> > > model = AutoModelForCausalLM.from_pretrained(model_path,
> > >                                              quantization_config=BitsAndBytesConfig(
> > >                 load_in_4bit=True),
> > >                 trust_remote_code=True,
> > >                 torch_dtype=torch.bfloat16, # additional option to lower RAM consumtion
> > >                 device_map={"""": 0})
> > > 
> > > tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")
> > > 
> > > input_text = ""Describe the solar system.""
> > > input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")
> > > 
> > > outputs = model.generate(input_ids, max_length=100)
> > > print(tokenizer.decode(outputs[0]))
> > > 
> > > </code>"
DJT777,2023-06-28 07:01:31+00:00,comment,tiiuae/falcon-40b,"This is no longer working on Colab. Any ideas why I am now getting this error? Was working a couple of weeks ago.
```
!pip install git+https://www.github.com/huggingface/transformers
!pip install git+https://github.com/huggingface/accelerate

!pip install bitsandbytes

!pip install einops

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
import torch

model_path=""tiiuae/falcon-40b-instruct""

config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True, device_map=""auto"")

tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b-instruct"")

input_text = ""Describe the solar system.""
input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids.to(""cuda"")

outputs = model.generate(input_ids, max_length=100)
print(tokenizer.decode(outputs[0]))
```"
DJT777,2023-06-28 07:37:57+00:00,comment,tiiuae/falcon-40b,"Working again using these versions:

!pip install git+https://www.github.com/huggingface/transformers@2e2088f24b60d8817c74c32a0ac6bb1c5d39544d
!pip install huggingface-hub==0.15.1
!pip install tokenizers==0.13.3
!pip install safetensors==0.3.1
!pip install git+https://github.com/huggingface/accelerate@040f178569fbfe7ab7113af709dc5a7fa09e95bd
!pip install bitsandbytes==0.39.0
!pip install einops==0.6.1
"
zxd930601,2023-06-29 02:52:31+00:00,comment,tiiuae/falcon-40b,"Has anyone encountered this kind of problem
```
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, load_in_4bit=True,device_map=""auto"")
  File ""/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py"", line 479, in from_pretrained
    return model_class.from_pretrained(
  File ""/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py"", line 2960, in from_pretrained
    dispatch_model(model, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/accelerate/big_modeling.py"", line 391, in dispatch_model
    model.to(device)
  File ""/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py"", line 1896, in to
    raise ValueError(
ValueError: `.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.
```"
xebian,2023-06-30 23:34:17+00:00,comment,tiiuae/falcon-40b,"@DJT777 thank you!

However, I keep running into problems, specifically:
```
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
```
I'm using an H100 instance on Lambda Cloud. I've put all the installation steps into a single [Bash script](https://gist.github.com/xeb/0d3b42633458cb4492006be58205c660). The entire output can be found here in [another gist](https://gist.github.com/xeb/a36459682a26004549af9c0ce89a1ad8) 

I think the issue is xFormers & potentially errors loading CUDA.

Anyone else have a fully working end-to-end on a fresh H100 instance? (I'm going to try an A100 just cause...)"
cpbotha,2023-07-15 10:29:00+00:00,comment,tiiuae/falcon-40b,"> `RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

I am seeing exactly the same issue on a fresh lambdalabs H100 with the unquantized falcon40b-instruct model. That exception is raised inside falcon:

```
modelling_RW.py"", line 32, in forward
    ret = input @ self.weight.T
```

When I look at nvidia-smi, the 80GB of GPU VRAM is almost fully occupied right after loading the model. It could be that we're seeing that cublas error simply because it's running out of VRAM inside falcon's modelling_RW.py which happens during inference.

I have searched online and found a number of folks with exactly the same issue on H100s, although there are also folks who did manage to get it running."
Ichsan2895,2023-06-04 16:34:41+00:00,comment,tiiuae/falcon-40b,"Excuse me, some question for you..
1. What is the different between your `falcontune` and `QLoRA`?
2. What is the different fine tuning (with the new dataset) in `Bitsandbytes`+`peft` and your code? Or maybe your script is the simple form of `bitsandbytes`+`peft`?
3. Can I activate 'nf4' (normal four bit float) in the `GPTQ`?"
dimaischenko,2023-06-10 09:55:07+00:00,comment,tiiuae/falcon-40b,"> Excuse me, some question for you..

I join in the questions!"
cr00,2023-07-01 01:09:44+00:00,comment,tiiuae/falcon-40b,"Doesn't 40b require like 48Gb of VRAM? also if anyone reads this I would be very appreciative for any insight into cost efficient/realistic hardware for ML, it seems like the cheapest build is somewhere in the neighborhood of $5-6k, and I think I would rather have my own hardware than rely on Amazon/Google/Azure, Thanks"
andyecher7,2023-07-12 15:36:17+00:00,comment,tiiuae/falcon-40b,"Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000 48GB (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to generate 20 tokens (words), in 4bit -it takes 25gb ram and  12 second for same 20 tokens - not sure why..

...
bnb_config = BitsAndBytesConfig(
   load_in_8bit=True,   
    bnb_4bit_use_double_quant=False,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    PATH,
    device_map=""auto""
    trust_remote_code=True,
    quantization_config=bnb_config,        
)

"
wasiim,2023-07-13 10:48:02+00:00,comment,tiiuae/falcon-40b,"can anyone help me please 
i have the text data stored in .txt the text data is simple information about a technology 
i want to fine tune the falcon model and the i want to ask the question to the falcon model according to that .txt file "
archonlith,2023-08-15 17:18:18+00:00,comment,tiiuae/falcon-40b,"> Falcon 40b inference in 8bit takes 45gb of ram. On single RTX A6000 48GB (not ADA version) on AMD EPIC 7713 DDR4 pc take around 4 second to generate 20 tokens (words), in 4bit -it takes 25gb ram and  12 second for same 20 tokens - not sure why..
> 

I would also love to know why it takes so long.

My main reason, (and I suspect many people's) main use case for GPT alternatives include both open source AND hopefully faster speed. Reducing the memory profile but increasing the lag seems like a lateral move."
nkasmanoff,2023-09-19 11:48:13+00:00,comment,tiiuae/falcon-40b,"

Update generation config to correct token ids for BOS and EOS
"
chelouche9,2023-08-18 10:06:09+00:00,comment,tiiuae/falcon-40b,"From my experience working with Falcon40, it gets really slow. Especially compared to its competitors like Llama 7B. 

However, there are some steps you can take to speed it up if you want to use it for production purposes:
1. Use Sagemaker Jumpstart and fine-tune it again using the dedicated fine-tune feature. They are using a very optimized inference setup that responds quickly, in a matter of seconds.
2. Use https://huggingface.co/text-generation-inference and set it up manually by yourself."
humza-sami,2023-08-18 10:16:48+00:00,comment,tiiuae/falcon-40b,"@chelouche9 Thanks, As I can't use sagemaker for some reasons for now but I will try 2nd option and let you know."
archonlith,2023-08-15 17:19:17+00:00,comment,tiiuae/falcon-40b,"> The inference is slow. How can I use inference on multiple gpus?

Would also like to know this"
chelouche9,2023-08-18 10:22:45+00:00,comment,tiiuae/falcon-40b,@davidhung There are several ways to do that. Please provide your code so people could help out easily.
archonlith,2023-08-21 19:50:00+00:00,comment,tiiuae/falcon-40b,"> @davidhung There are several ways to do that. Please provide your code so people could help out easily.

No offense but, if there are ""several ways"" why don't you just suggest one instead of being unnecessarily difficult?"
chelouche9,2023-08-22 05:06:04+00:00,comment,tiiuae/falcon-40b,"> No offense but, if there are ""several ways"" why don't you just suggest one instead of being unnecessarily difficult?

None taken mate :) One option is to use - https://github.com/huggingface/text-generation-inference."
jlzhou,2023-07-23 14:38:44+00:00,comment,tiiuae/falcon-40b,What is the content of print(response.text) ?
reetkat,2023-07-27 23:03:20+00:00,comment,tiiuae/falcon-40b,"It turned out that the issue was due to the parameters themselves. Even though the documentation says: ""temperature"", the inference endpoints accept the parameter as ""Temperature"" with a capital T."
Jonathanroiz,2023-09-18 19:18:25+00:00,comment,tiiuae/falcon-40b,I'm getting the same error on a different model
ericCWY,2023-10-15 07:03:27+00:00,comment,tiiuae/falcon-40b,"I success to run the model when I change bfloat16 to float32, it seems that the MPS doesn't support some dtype and some dtype doesn't compatible to each other."
WajihaIlyas,2023-07-24 09:44:36+00:00,comment,tiiuae/falcon-40b,I am facing same issue. 
armx40,2023-07-26 21:09:51+00:00,comment,tiiuae/falcon-40b,"Able to use all cores on an Intel CPU. Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz. These are the flags from /proc/cpuinfo :
fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_vnni arch_capabilities.

Can you please compare it with yours."
mdabbah,2023-08-09 19:11:35+00:00,comment,tiiuae/falcon-40b,"checkout 

https://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only"
cmp-nct,2023-07-14 13:10:23+00:00,comment,tiiuae/falcon-40b,"I suppose no one tried, doesn't mean it does not work.
Personally I'd try careful fine tuning of the embeddings using dictionaries of that particular language in combination with all languages Falcon knows well, so it can find connections of the new words with existing words.
Then the same on sentences with a large corpus of untrained examples to regularly test the progress."
AshBam,2023-07-19 05:54:51+00:00,comment,tiiuae/falcon-40b,Thanks will try something out.
borzunov,2023-07-17 05:31:39+00:00,comment,tiiuae/falcon-40b,"Hi @Rocketknight1 @sgugger,

Thanks for your work on Falcon support in transformers!

Could you please share if this model is going to be converted to the new in-library format (`model_type=""falcon""`) soon? If yes, when do you expect this to happen?

I'm asking because I'm working on Falcon support in Petals (https://github.com/bigscience-workshop/petals) and would like to understand **(1)** which format we need to support
and **(2)** if we can make a release with Falcon this week or should wait until the format changes.

I'd really appreciate you sharing your plans on this matter!

"
sgugger,2023-07-17 15:44:14+00:00,comment,tiiuae/falcon-40b,The model will be converted back to the new in-library format soon. The problem is that the library test-generation-inference added support for Falcon by checking its model_type and we need to wait for a new release of TGI that supports both model types (`falcon` and the current `RefinedWeb`) to avoid breaking many users deployment of Falcon through TGI.
borzunov,2023-07-17 22:52:57+00:00,comment,tiiuae/falcon-40b,"Got it, thanks!"
cmp-nct,2023-07-13 12:57:55+00:00,comment,tiiuae/falcon-40b,"```
falcon_print_timings:        load time =  1950.86 ms
falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23 ms per token,  4365.33 tokens per second)
falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96 ms per token,   337.94 tokens per second)
falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14 ms per token,    47.30 tokens per second)
falcon_print_timings:       total time =  3142.62 ms
```
As a 7B reference, that's on a 4090 (closer to H100) but also on 3090 (that's A100 speed) the speed is about the same
409 tokens prompt and 90 tokens response take around 3.1 seconds. That's ~6 bit, It's 20% slower on full precision.
Maybe you have a problem on the CPU side, some swapping while loading or loading from a hdd etc ?

Here a 40B reference:
```
falcon_print_timings:        load time =  5666.06 ms
falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23 ms per token,  4408.47 tokens per second)
falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06 ms per token,    99.36 tokens per second)
falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36 ms per token,    16.85 tokens per second)
falcon_print_timings:       total time =  7720.06 ms
```"
kartik99,2023-07-14 09:03:23+00:00,comment,tiiuae/falcon-40b,"> ```
> falcon_print_timings:        load time =  1950.86 ms
> falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23 ms per token,  4365.33 tokens per second)
> falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96 ms per token,   337.94 tokens per second)
> falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14 ms per token,    47.30 tokens per second)
> falcon_print_timings:       total time =  3142.62 ms
> ```
> As a 7B reference, that's on a 4090 (closer to H100) but also on 3090 (that's A100 speed) the speed is about the same
> 409 tokens prompt and 90 tokens response take around 3.1 seconds. That's ~6 bit, It's 20% slower on full precision.
> Maybe you have a problem on the CPU side, some swapping while loading or loading from a hdd etc ?
> 
> Here a 40B reference:
> ```
> falcon_print_timings:        load time =  5666.06 ms
> falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23 ms per token,  4408.47 tokens per second)
> falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06 ms per token,    99.36 tokens per second)
> falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36 ms per token,    16.85 tokens per second)
> falcon_print_timings:       total time =  7720.06 ms
> ```

for smaller tokens the average speed for the 7B is around 3-4 seconds now running on 2 A100 GPUs, but for 40B its still really slow (and failes to answer properly), checked the GPU and CPU usage and it seems to be fine
Running it on 24 vCPUs VM
"
cmp-nct,2023-07-14 13:04:56+00:00,comment,tiiuae/falcon-40b,"> > ```
> > falcon_print_timings:        load time =  1950.86 ms
> > falcon_print_timings:      sample time =    20.62 ms /    90 runs   (    0.23 ms per token,  4365.33 tokens per second)
> > falcon_print_timings: batch eval time =  1210.28 ms /   409 tokens (    2.96 ms per token,   337.94 tokens per second)
> > falcon_print_timings:        eval time =  1881.62 ms /    89 runs   (   21.14 ms per token,    47.30 tokens per second)
> > falcon_print_timings:       total time =  3142.62 ms
> > ```
> > As a 7B reference, that's on a 4090 (closer to H100) but also on 3090 (that's A100 speed) the speed is about the same
> > 409 tokens prompt and 90 tokens response take around 3.1 seconds. That's ~6 bit, It's 20% slower on full precision.
> > Maybe you have a problem on the CPU side, some swapping while loading or loading from a hdd etc ?
> > 
> > Here a 40B reference:
> > ```
> > falcon_print_timings:        load time =  5666.06 ms
> > falcon_print_timings:      sample time =    13.84 ms /    61 runs   (    0.23 ms per token,  4408.47 tokens per second)
> > falcon_print_timings: batch eval time =  4116.55 ms /   409 tokens (   10.06 ms per token,    99.36 tokens per second)
> > falcon_print_timings:        eval time =  3561.89 ms /    60 runs   (   59.36 ms per token,    16.85 tokens per second)
> > falcon_print_timings:       total time =  7720.06 ms
> > ```
> 
> for smaller tokens the average speed for the 7B is around 3-4 seconds now running on 2 A100 GPUs, but for 40B its still really slow (and failes to answer properly), checked the GPU and CPU usage and it seems to be fine
> Running it on 24 vCPUs VM

I'm not sure what you mean by average speed, you'd need to measure speed similar as I quoted. As in tokens/second generation and tokens/sec prompt processing.
Try the ggllm.cpp project if you don't get proper speed using python, it's quite simple to get running and has far more flexibility in terms of configuration and quantization.
2xA100 would be quite an overkill for it, you don't need that much vram. There is no benefit I'd know to inference it at 16 bit precision, you get the same responses at 6K which is a fraction in size."
kiranr,2023-07-18 10:22:16+00:00,comment,tiiuae/falcon-40b,There is an issue with not using `past_key_values` in the 7 and 40b model code. as mentioned here https://huggingface.co/tiiuae/falcon-40b/discussions/48. The issue is fixed in this pr in transformers https://github.com/huggingface/transformers/pull/24523. you can try using that. 
kartik99,2023-07-18 12:32:27+00:00,comment,tiiuae/falcon-40b,"> There is an issue with not using `past_key_values` in the 7 and 40b model code. as mentioned here https://huggingface.co/tiiuae/falcon-40b/discussions/48. The issue is fixed in this pr in transformers https://github.com/huggingface/transformers/pull/24523. you can try using that.

Thanks"
Tron2060,2023-08-03 09:46:38+00:00,comment,tiiuae/falcon-40b,"@kartik99 I think there also has some issue with LayerNorm,  it outputs float32 but the input is bfloat16.  It can influence memory consumption and inference speed."
FelixMildon,2023-07-16 15:26:53+00:00,comment,tiiuae/falcon-40b,bump
FelixMildon,2023-07-19 12:42:35+00:00,comment,tiiuae/falcon-40b,bump
sfd,2023-08-13 11:42:37+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
YoYo1234Qwerty,2023-08-16 17:29:41+00:00,comment,tiiuae/falcon-40b,"Ya, Falcon seemed slow af, and this seemed to help, thanks!

Is this why Llama is beating Falcon, because they're overlooking simple things? I'm quite surprised since this has far superior licensing to Llama."
puru22,2023-08-17 02:59:14+00:00,comment,tiiuae/falcon-40b,"Dont know why this was overlooked. This updated code runs at around 2.5x speed on CPU and I have not been able to measure the speed up on GPU of fp16 model. The quantized version of the model did not seem to give any speed up after this improvement on GPU. What precision did you use @YoYo1234Qwerty and how much speed up did you get in case it was GPU ?

Also Falcon Models by default run with Flash Attention algo so its heavily I/O optimized already. So any change in the algorithm has to keep up with the I/O optimization to get the speed up on GPU. I am guessing they might have chosen to not reuse the past key values because it might have not led to any difference in inference speed and would have led to more complicate code. Not sure. "
mshabeeb,2023-09-05 08:46:10+00:00,comment,tiiuae/falcon-40b,Is there a similar fix for Falcon-7b?
puru22,2023-09-07 03:49:55+00:00,comment,tiiuae/falcon-40b,"Yeah, here is the one for falcon 7b instruct, https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c. Note that I did not get speed up on gpu, just on cpu with all this change for falcon-7b-instruct
"
mshabeeb,2023-09-07 06:44:38+00:00,comment,tiiuae/falcon-40b,"> Yeah, here is the one for falcon 7b instruct, https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/60#64ad2eae4beffa272de2610c. Note that I did not get speed up on gpu, just on cpu with all this change for falcon-7b-instruct

Thanks for your response! Just to make sure: How can I find the exact file path for modelling_RW.py which is used by the falcon model? When I search on my system I find multiple copies of this file (at least one in hub directory and one in modules directory) so how to determine which one should be replaced with the new version?"
puru22,2023-09-09 16:50:48+00:00,comment,tiiuae/falcon-40b,"Did not understand what you mean here, so the directory which you are using to load the model using AutoModelForCausalLM.from_pretrained() should have one modelling_RW.py file which should be replaced by the one in the above pull request. "
puru22,2023-09-10 05:37:07+00:00,comment,tiiuae/falcon-40b,"I understand what you mean, I guess you are using model hub directly to download the huggingface model, which is ending up creating two locations for modelling_RW.py in huggingface cache modules directory. I would recommend just directly cloning the repo (with git lfs installed in your system) and use that directory for everything. "
nshalon,2023-07-10 22:52:41+00:00,comment,tiiuae/falcon-40b,i have the same issue...
Kartik08,2023-08-24 11:34:57+00:00,comment,tiiuae/falcon-40b,I do have the same issue . How to solve this ?
Rocketknight1,2023-07-07 15:00:49+00:00,comment,tiiuae/falcon-40b,"Hey, this PR will convert the checkpoint to an in-library checkpoint. Don't merge it yet, since the Falcon code still hasn't been merged to `main`!"
inspectablock,2023-06-30 00:50:10+00:00,comment,tiiuae/falcon-40b,"I have found that by appending the sentence ""Only respond if the answer is contained in the text above."" to the question it will sometimes respect this directive, but not consistently enough for this to be a viable option.

"
gabriead,2023-07-06 11:39:00+00:00,comment,tiiuae/falcon-40b,We where trying to deploy it as a model endpoint but the inference was a catastrophe even using 2XA100 (80GB). We are now trying to set it up as VM.
amitgurintecom,2023-06-25 20:10:49+00:00,comment,tiiuae/falcon-40b,"Problem solved.
Tried 4 X A100 GPU and the problem didn't happen. 
It could be that Quadro-8000 does not support well BF16
Or, the model was corrupted when downloaded on the first GPU.
"
mindplay,2023-06-26 09:13:36+00:00,comment,tiiuae/falcon-40b,Is this the wrong place to ask questions about Falcon?
andrearked,2023-08-15 08:28:27+00:00,comment,tiiuae/falcon-40b,I would like to know the same answer! Id love to fine tune with danish as well but i havent find anything about that so far
mubarak-alketbi,2023-07-02 08:28:39+00:00,comment,tiiuae/falcon-40b,"80+GB of Vram is required for 16bit model.

try 48GB with 8bit."
cmp-nct,2023-07-12 01:23:38+00:00,comment,tiiuae/falcon-40b,"You can run it on a single 3090 at 17tokens/sec (up to 25 on a 4090) when using the Q2_K variant with ggllm.cpp
Output quality is great, it probably loses a bit in precision but writes flawless poems and summaries in multiple languages."
MetaReps,2023-07-12 02:21:34+00:00,comment,tiiuae/falcon-40b,"Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB Disk  - 
Ubuntu 22.10 x64"
geoffreyporto,2023-07-12 03:04:32+00:00,comment,tiiuae/falcon-40b,"> Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB Disk  - 
> Ubuntu 22.10 x64
No. 
This could work:
Buy for $79USD x month a Skylake SSD 3XL9 100
* 64GB DDR 4
* intel Core i7-6700
* Software-Raid 1
* 2x 500GB SSD
https://buy.server4you.com/en/customer/create/skylake-ssd-3xl9-100/e66fcda9-7b1e-463a-8747-bce776de4ac0
"
cmp-nct,2023-07-13 12:40:03+00:00,comment,tiiuae/falcon-40b,"> Could I run the 16bit on this?  8 GB Memory / 4 Intel vCPUs / 160 GB Disk  - 
> Ubuntu 22.10 x64
No clearly not, also the affiliate link posted below is not suitable.

1) If you want to run the 16 bit version you'll need around 85-90GB of RAM/VRAM. Your server has 8GB and the affiliate link has 64GB.
2) Even if you'd use a server like that with enough RAM your speed would be like 1 token processed every 15-30 seconds. Literally not useable for anything but as a room heater.
3) There are only academic reasons that would come to my mind why you'd want to run a 16 bit version of Falcon on a CPU, it's hard to find a good reason why you'd want to inference that on GPU either.
There are no quality benefits over a high quality quantized version, the RAM requirements are extreme and the processing speed slow.

On your server you can not expect to run Falcon 40B, the smallest version of 40B using the cmp-nct repository is around 13GB with processing buffers included. And that's at 2.5bit quantization. 
15GB on 3.5 bit, almost 24 at 4.5 bit
However, you can run Falcon 7B on that machine, in 5 bit quanitzation you'll have about the same quality as 16 bit and need roughly 7GB RAM for processing it. Speed a couple tokens per second."
Monomial,2023-07-23 04:08:03+00:00,comment,tiiuae/falcon-40b,"Quick question for the community

I have an opportunity to pick up 4 second hand 24GB 3090's at a reasonable discount.

The only reason I would do this is if I could run Falcon-40B on them.  I understand from above the performance on a single 3090 would be around 17 tokens/sec.  Would a (x4) configuration be a viable to run the 40B parameter model?  Would there be a faster configuration to run Falcon-40B in the ~$3k price range?  As a bonus question, can anyone speculate on the kind of performance I might realistically expect to achieve?  This is a training system for me, but I'd also like to actually try and actually use if for real world problems if possible.

Thank you for any assistance and advice. I don't want to spend the money if this is not going to work.
"
cmp-nct,2023-07-23 22:34:28+00:00,comment,tiiuae/falcon-40b,"> Quick question for the community
> 
> I have an opportunity to pick up 4 second hand 24GB 3090's at a reasonable discount.
> 
> The only reason I would do this is if I could run Falcon-40B on them.  I understand from above the performance on a single 3090 would be around 17 tokens/sec.  Would a (x4) configuration be a viable to run the 40B parameter model?  Would there be a faster configuration to run Falcon-40B in the ~$3k price range?  As a bonus question, can anyone speculate on the kind of performance I might realistically expect to achieve?  This is a training system for me, but I'd also like to actually try and actually use if for real world problems if possible.
> 
> Thank you for any assistance and advice. I don't want to spend the money if this is not going to work.

If you run it using ggllm (https://github.com/cmp-nct/ggllm.cpp) a single 3090 can run 3 bit, on windows also 4 bit with a bit squeezing.
Performance on a single 3090 is probably around 15tk/sec, on two 3090 (for larger variants) I'm not sure about performance. Likely about the same. 
More than 2 would never be needed as long as no larger Falcon is released.
The upcoming release will have special support for 4090ies that use the better compute capability, mostly relevant for prompt processing speed.

On python I don't know, I'd assume you can run it on 4x 3090 using pytorch or 2x 3090 using gptq."
ibrim,2023-06-21 06:49:04+00:00,comment,tiiuae/falcon-40b,"+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Graphics Device     On   | 00000000:01:00.0 Off |                    0 |
| N/A   50C    P0    68W / 275W |   7095MiB / 81252MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  Graphics Device     On   | 00000000:47:00.0 Off |                    0 |
| N/A   51C    P0    72W / 275W |    535MiB / 81252MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  Graphics Device     On   | 00000000:81:00.0 Off |                    0 |
| N/A   50C    P0    69W / 275W |   2131MiB / 81252MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  DGX Display         On   | 00000000:C1:00.0 Off |                  N/A |
| 35%   47C    P8    N/A /  50W |      1MiB /  3911MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  Graphics Device     On   | 00000000:C2:00.0 Off |                    0 |
| N/A   49C    P0    65W / 275W |      3MiB / 81252MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+

GPU details"
mubarak-alketbi,2023-07-02 08:22:19+00:00,comment,tiiuae/falcon-40b,"test with batch size 1
make sure your max sequence length is 2048"
ahughes,2023-07-15 05:35:22+00:00,comment,tiiuae/falcon-40b,"@yiz4869  I was able to successfully finetune the model without passing in the `token_type_ids`. If using an `AutoModelForCausalLM` model class and a `DataCollatorForLanguageModeling` being passed to the trainer then you should be able to train successfully! 

My example is available here https://github.com/anthonyhughes/pico-evidence-training-data/blob/main/falcon_main.py in the train section."
muhtasham,2023-07-17 16:43:04+00:00,comment,tiiuae/falcon-40b,"Hey did you try this?

https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/studio-notebook-fine-tuning/falcon-40b-qlora-finetune-summarize.ipynb"
Gage888,2023-06-18 20:26:45+00:00,comment,tiiuae/falcon-40b,
charlesniswander,2023-06-16 17:29:22+00:00,comment,tiiuae/falcon-40b,Is there even any real documentation available yet?
beejay,2023-06-22 18:47:55+00:00,comment,tiiuae/falcon-40b,"AFAIK, the falcon models are text gen models and not embedding models. While somewhere in the network, you could get the embeddings at least conceptually, these models don't give a handle to get to those.
"
Andycarketa,2023-07-17 23:26:17+00:00,comment,tiiuae/falcon-40b,which models would support embeddings?
beejay,2023-07-17 23:30:56+00:00,comment,tiiuae/falcon-40b,There are embedding-specific models such as instruct-xl (https://huggingface.co/hkunlp/instructor-xl) that one can use. 
NajiAboo,2023-06-15 13:17:24+00:00,comment,tiiuae/falcon-40b,"Hi, 

 Did you able to figure out it. I am also facing same issue"
mikeytrw,2023-06-15 18:58:50+00:00,comment,tiiuae/falcon-40b,"No, I'm currently running the H2o OpenAssist fine tuned model on 8xA100 80GB and it's still sloooooow, well, at least compared to Llama."
ajmalsiddiqui,2023-06-15 19:06:17+00:00,comment,tiiuae/falcon-40b,"Please run inference using safetensor, I would suggest to use hugging text generation inference with falcon model. It will be faster. Additionallly, number of token will affect the speed. Please try and share your feedback. Thanks "
NajiAboo,2023-06-16 10:39:02+00:00,comment,tiiuae/falcon-40b,Thanks for the reply. Can you please share some samples or collab or github repo. Thanks for your time.
ajmalsiddiqui,2023-06-16 11:32:56+00:00,comment,tiiuae/falcon-40b,"Please try
https://github.com/huggingface/text-generation-inference"
mikeytrw,2023-06-17 20:41:28+00:00,comment,tiiuae/falcon-40b,I don't want to use text generation interface because I already have a server codebase this is part of. Can you explain a bit more about using safetensor? Or link to a code example. Thanks
ajmalsiddiqui,2023-06-18 18:14:20+00:00,comment,tiiuae/falcon-40b,"Dear Mike, the same thing i need in my server codebase and could not explore  yet the safetensors implenentation that exists in huggging face text generation. I will do but at later stage. In your case, i would suggest to implement your own using the logic and implementation already done in text generation by hugging face."
Someshfengde,2023-06-19 13:09:25+00:00,comment,tiiuae/falcon-40b,I'm also inferencing on the same 2 x A100 80 gb GPUs the inference time is high.
sekharvth,2023-06-21 11:28:18+00:00,comment,tiiuae/falcon-40b,"Inference time for out of the box falcon models is directly proportional to max_new_tokens being generated. This is because of a faulty incorporation of the `past_key_values`  and rotary embeddings , former is used to cache the transformer keys and values as each token gets generated so that it's not recomputed at every timestep, latter is responsible for the embeddings. There's also a bug in the causal masking section of the attention mechanism being called. All this has been mentioned in [this](https://huggingface.co/tiiuae/falcon-40b/discussions/48) thread"
puru22,2023-07-12 07:15:44+00:00,comment,tiiuae/falcon-40b,"I have raised a pull request today fixing the slowness. Just change the modelling_RW.py with the one in this pull request. Let me know if you find any issue with this. Note that pretty much all falcon family models involve the same changes for speeding up the generation. As some people in this thread mention, its mainly because falcon model is recomputing everything from beginning for every next token generation. 
This is the pull request. 

https://huggingface.co/tiiuae/falcon-40b/discussions/85"
xsa-dev,2023-06-15 22:18:29+00:00,comment,tiiuae/falcon-40b,i want too =)
theqwertyhog,2023-06-16 05:17:12+00:00,comment,tiiuae/falcon-40b,Deploy it on SageMaker
samge,2023-06-16 16:28:26+00:00,comment,tiiuae/falcon-40b,i want too ~~~
xsa-dev,2023-06-18 02:17:50+00:00,comment,tiiuae/falcon-40b,"> i want too ~~~

Try to deploy on SageMaker.
today i am make [Space with Falcon 7B Instruct](https://huggingface.co/spaces/xsa-face/falcon-7b-instruct-chat), after implementing [LangChain with another model](https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi/discussions/8#648e0b6080bf2c95993e1c1f)"
alexatallah,2023-07-10 04:54:58+00:00,comment,tiiuae/falcon-40b,Just launched an API for Falcon 40B Instruct (in beta) at https://openrouter.ai. It's free for now. Let me know what you think!
alexatallah,2023-07-10 06:39:56+00:00,comment,tiiuae/falcon-40b,(Uses hugging face ^)
dzekoh,2023-07-18 07:58:56+00:00,comment,tiiuae/falcon-40b,"> Just launched an API for Falcon 40B Instruct (in beta) at https://openrouter.ai. It's free for now. Let me know what you think!

how to use it?
"
alexatallah,2023-07-19 02:36:09+00:00,comment,tiiuae/falcon-40b,"> how to use it?

See openrouter.ai/docs. Or clone the demo streamlit here https://twitter.com/xanderatallah/status/1678170539913801730?s=46"
gaodrew,2023-06-09 22:52:41+00:00,comment,tiiuae/falcon-40b,Please see the Github repo: https://github.com/andrewgcodes/FalconStreaming
gaodrew,2023-06-09 22:53:34+00:00,comment,tiiuae/falcon-40b,"You will need Colab Pro+ using the A100 GPU for 40B.
For 7B, you can likely get away with Colab FREE if you can snag one of the GPUs. Otherwise, you can upgrade to the $9.99/mo options.
I think there is also a new Pay as You Go option."
posdocit01,2024-01-16 07:55:30+00:00,comment,tiiuae/falcon-40b,Qual é a capital da Italia?
beejay,2023-06-17 01:45:18+00:00,comment,tiiuae/falcon-40b,"If you are willing to share a bigger part of the code, I can take a look."
mcapizzi,2023-06-21 21:19:40+00:00,comment,tiiuae/falcon-40b,"I am doing a completely different task but only getting a single token of generated text as well, which strikes me as odd."
FalconLLM,2023-06-09 14:46:24+00:00,comment,tiiuae/falcon-40b,This [blogpost](https://huggingface.co/blog/falcon) from HuggingFace contains a bunch of pointers on using the model in the ecosystem :).
FalconLLM,2023-06-09 14:45:22+00:00,comment,tiiuae/falcon-40b,"We would recommend using [Text Generation Inference](https://github.com/huggingface/text-generation-inference) for optimal performance. On AWS SageMaker, have a look also at this [blog](https://www.philschmid.de/sagemaker-falcon-llm). "
FalconLLM,2023-06-09 14:44:27+00:00,comment,tiiuae/falcon-40b,"We would recommend using [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) instead. 

Falcon-40B is just a based pretrained model, and should be further finetuned for improved usability. "
g-ronimo,2023-09-25 14:55:13+00:00,comment,tiiuae/falcon-40b,"That's the time it takes to process the entire batch and increases with the number of samples. What you expect to decrease is the time per sample, which indeed decreases when I look at your numbers "
aligokalppeker,2024-04-24 18:27:54+00:00,comment,tiiuae/falcon-40b,"> That's the time it takes to process the entire batch and increases with the number of samples. What you expect to decrease is the time per sample, which indeed decreases when I look at your numbers

This is bullshit g-ronimo"
g-ronimo,2024-04-25 05:18:39+00:00,comment,tiiuae/falcon-40b,"be nice.

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch
import time

model = ""tiiuae/falcon-7b""
tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model, 
    trust_remote_code=True, 
    device_map=""auto"", 
    torch_dtype=torch.bfloat16
)

for batch_size in [2, 4, 8]:
    input_prompt = [
        ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:""
    ] * batch_size
    input_tokens = tokenizer.batch_encode_plus(
        input_prompt,
        return_tensors=""pt"",
    ).to(""cuda"")
    input_tokens_cnt = sum([len(t) for t in input_tokens[""input_ids""]])

    # Warmup
    sequences = model.generate(
        **input_tokens, min_length=512, max_length=512, do_sample=True
    )
    
    torch.cuda.synchronize()
    st = time.monotonic()
    generated_tokens_count = []
    num_trials = 2
    for i in range(num_trials):
        torch.cuda.synchronize()
        sequences = model.generate(
            **input_tokens, 
            min_length=512, 
            max_length=512, 
            do_sample=True
        )
        torch.cuda.synchronize()
        sequences_tokens_cnt = sum([len(t) for t in sequences])
        generated_tokens_count.append(sequences_tokens_cnt - input_tokens_cnt)

    tt = time.monotonic() - st
    print(f""batch_size {batch_size}: Avg. time taken {tt/num_trials}, avg. time per token {tt/sum(generated_tokens_count)}"")
```

output

```
batch_size 2: Avg. time taken 17.3977282285, avg. time per token 0.019287947038248338
batch_size 4: Avg. time taken 17.282605756000066, avg. time per token 0.009580158401330413
batch_size 8: Avg. time taken 18.542240016500045, avg. time per token 0.005139201778409103
```"
cchudant,2023-06-07 12:34:49+00:00,comment,tiiuae/falcon-40b,"> I tried to get an answer in falcon-7b discussions, but no one answers, maybe this community can help me.
> 
> Writing my own inference loop, I get error when using `past_key_values`, tensor dimension error or complete nonsense in generation. I debugged default examples from model description with standard `model.generate` or `pipeline`, and saw that they don't use `past_key_values` at all in generation loop.
> 
> I would be very glad if you can tell me if the use `past_key_values` is implemented with a bug or is not supported or I do not understand something? After all, it speeds up inference by several times.
> 
> More details in falcon-7b discussion https://huggingface.co/tiiuae/falcon-7b/discussions/17

Hi!
I had the exact same problem, and found this: https://huggingface.co/tiiuae/falcon-40b/discussions/47

Hope this helps!"
dimaischenko,2023-06-07 12:40:13+00:00,comment,tiiuae/falcon-40b,@cchudant thank you! I'm going to test it now and I'll be sure to write here about the results!
cchudant,2023-06-07 12:55:15+00:00,comment,tiiuae/falcon-40b,"@dimaischenko 
just to be clear, I have not tested it :)
it's one of:

1) Either, it is a bug as I pointed out, and the line I referenced is wrong & other changes elsewhere
I would guess that `_, _, kv_length = key_layer.shape` would also need to change to `_, kv_length, _ = key_layer.shape`. Maybe some other stuff.

2) Or, departure from bloom kv-cache shape is intended, meaning `_convert_to_rw_cache`  and `_convert_to_standard_cache` need to be changed to swap the key dimensions.
The comment
```
# concatenate along seq_length dimension:
#  - key: [batch_size * self.num_heads, head_dim, kv_length]
#  - value: [batch_size * self.num_heads, kv_length, head_dim]
```
would need to change to
```
# concatenate along seq_length dimension:
#  - key: [batch_size * self.num_heads, kv_length, head_dim]
#  - value: [batch_size * self.num_heads, kv_length, head_dim]
```

I have not tested it :) I am currently only interested in benchmarking the inference of the models, and I found this out because my bench broke with kv-cache.
I have not actually checked if it gives me valid outputs."
dimaischenko,2023-06-07 13:01:51+00:00,comment,tiiuae/falcon-40b,"@cchudant  I will carefully sort it out and report back, I also came to similar thoughts and began to swap the dimensions of the tensors manually, this removed the errors of dimensions but it led to ""nonsense"" in the output of the model. Now I will carefully investigate everything"
dimaischenko,2023-06-07 13:51:15+00:00,comment,tiiuae/falcon-40b,"@cchudant Unfortunately, I still get ""nonsense"" when generating if I change the dimensions ... something is not right."
dimaischenko,2023-06-07 13:52:23+00:00,comment,tiiuae/falcon-40b,"Do you happen to know which of the developers can ask a question directly? I can't figure out who to mention to help figure it out. Using `past_key_values` in decoding speeds up the generation by 2-3 times, and it seems that it is very important to make it work"
dimaischenko,2023-06-07 16:00:20+00:00,comment,tiiuae/falcon-40b,"@cchudant  I actually tested on the code from the `falcon-7b` model, it looks like the code is slightly different between `7b` and `40b`. I don't have a video card on which I could test `40b` model, if you can test this code on it (with corrections on tensor dimensions) would be cool!

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import random

device = torch.device(""cuda"")
model_id = ""tiiuae/falcon-40b""

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
).to(device)

tokenizer = AutoTokenizer.from_pretrained(model_id)

text = ""We are in the dark forest and want to find some mushrooms. We go to the nearest tree and""

inputs = tokenizer(text, return_tensors=""pt"").to(device)
input_ids = inputs[""input_ids""]

output = None
step = 0

# generation cycle with 20 steps
while step < 20:
    attention_mask = input_ids.new_ones(input_ids.shape)
    past_key_values = None
    
    if output is not None:
        past_key_values = output[""past_key_values""]

    ids = model.prepare_inputs_for_generation(input_ids,
                                              past=past_key_values,
                                              attention_mask=attention_mask,
                                              use_cache=True)
                                 
    output = model(**ids)
    
    # get random of 3 most probable tokens and add to input_ids
    top_k = 3
    next_token = random.choice(torch.topk(output.logits[:, -1, :], top_k, dim=-1).indices[0])
    
    input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(device)], dim=-1)
    
    step += 1

print(tokenizer.decode(input_ids[0]))
```"
cchudant,2023-06-08 14:33:14+00:00,comment,tiiuae/falcon-40b,"Hi, I am sorry - I don't have the machine anymore :("
dimaischenko,2023-06-08 15:29:42+00:00,comment,tiiuae/falcon-40b,"No problem, we'll try to wait for answers from someone from the @FalconLLM team. Already many people write that they have the same problems and questions with both `7b` ( https://huggingface.co/tiiuae/falcon-7b/discussions/17 ) and `40b` models, it seems that this is an important thing"
ColmanTT,2023-06-08 17:28:15+00:00,comment,tiiuae/falcon-40b,"It doesn't look right to me that your decode loop concats the new token onto the previous input_ids. For KV cache inference, you should only pass in the new token"
dimaischenko,2023-06-08 17:35:29+00:00,comment,tiiuae/falcon-40b,"@ColmanTT The thing is that in `prepare_inputs_for_generation` we get only last token

```python
...
   input_ids = input_ids[:, -1].unsqueeze(-1)
...
```

This loop works fine with any other model such as `gpt2`, `gptj-6b`, `bloom`, etc."
ColmanTT,2023-06-08 18:24:45+00:00,comment,tiiuae/falcon-40b,Appears that we're victims of this issue https://stackoverflow.com/questions/76177216/how-is-scaled-dot-product-attention-meant-to-be-used-with-cached-keys-values-in
ColmanTT,2023-06-08 19:04:16+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
dimaischenko,2023-06-08 19:08:47+00:00,comment,tiiuae/falcon-40b,@ColmanTT Wow! I will test tomorrow and will be sure to report on the results!
ColmanTT,2023-06-08 19:44:03+00:00,comment,tiiuae/falcon-40b,"Apologies for hiding this, I realize I haven't tested well enough. Let me know how yours goes as well"
siemon1996,2023-06-12 03:29:41+00:00,comment,tiiuae/falcon-40b,"@dimaischenko I think I solved this problem by passing 'is_causal' of 'F.scaled_dot_product_attention' different value due to 'layer_past'.
```
if layer_past is not None:
    attn_output = F.scaled_dot_product_attention(
        query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=False
    )
else:
    attn_output = F.scaled_dot_product_attention(
        query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True
    )
```
The generated text turned out to be just fine."
dimaischenko,2023-06-12 04:46:21+00:00,comment,tiiuae/falcon-40b,"@siemon1996 I tested it, feeling that it has become much better, but it seems that there is still a lot of ""nonsense"" 🤔 It feels like it's not the `7b` model, but the very first small `gpt2`. I will be testing more, thank you for sharing!"
Tron2060,2023-06-13 09:46:55+00:00,comment,tiiuae/falcon-40b,@siemon1996  I think you did not really solved it.  This problem maybe caused by a bug in scaled_dot_product_attention: https://github.com/pytorch/pytorch/issues/103082
Tron2060,2023-06-13 10:22:03+00:00,comment,tiiuae/falcon-40b,"I change like this
 ```
   if layer_past is not None:
                L = query_layer_.shape[-2]
                S = key_layer_.shape[-2]
                attn_mask = torch.ones(L, S, dtype=torch.bool, device=query_layer_.device)
                attn_output = F.scaled_dot_product_attention(
                    query_layer_, key_layer_, value_layer_, attn_mask, 0.0, is_causal=False
                )
            else:
                attn_output = F.scaled_dot_product_attention(
                    query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True
                )
 ```"
siemon1996,2023-06-14 09:13:32+00:00,comment,tiiuae/falcon-40b,"> @siemon1996 I tested it, feeling that it has become much better, but it seems that there is still a lot of ""nonsense"" 🤔 It feels like it's not the `7b` model, but the very first small `gpt2`. I will be testing more, thank you for sharing!

It occurs to me that the first several output tokens are correct, compared with run falcon-40b without using past_key_values. Then the 'nonsense' begins,  I`ll try to figure out the reason,  just letting you know.
"
danieldk-explosion,2023-06-16 15:41:02+00:00,comment,tiiuae/falcon-40b,"There is another issue besides the incorrect causal mask. The rotary embeddings are not position-indexed. Suppose that you are generating the next piece, then the new (non-cache) query/value representations of the last generated piece have shape `[batch_size, n_heads, 1, head_dim`]. The rotary embeddings are applied like this:

https://huggingface.co/tiiuae/falcon-7b/blob/2f5c3cd4eace6be6c0f12981f377fb35e5bf6ee5/modelling_RW.py#L257

However, that means that the query/key representations for this new piece get the rotary embedding applied for position 0, whereas it needs the rotary embedding for position n+1 (where n is the number of preceding pieces, which may be different per batch item in case that there are padding pieces). Here is a correct use of rotary embeddings in this model when a cache is used:

https://github.com/explosion/curated-transformers/blob/b44a0fa24c64844909656a0fa9eb4d5acc6af142/curated_transformers/models/attention.py#L315

Example of text generated with the 7b instruction tuned model, with correct causal masks, correct indexing, caching, and deterministic decoding:

Prompt: _What is the Rust programming language?_

 Answer: _Rust is a programming language that is designed to be a safe, concurrent, and efficient replacement for C++. It is a statically-typed language that is designed to be memory-safe and thread-safe, making it a good choice for developing high-performance applications._"
sekharvth,2023-06-21 08:43:09+00:00,comment,tiiuae/falcon-40b,"@siemon1996 @Tron2060 @dimaischenko @danieldk-explosion 

I managed to compile a quick script incorporating all the changes mentioned on this thread - use of the correct `past_key_values` variable instead of `past`, adding the correct parameters in the `scaled_dot_product_attention` call,  addition of the right implementation of rotary embeddings from curated-transformers, and minor fixes for contrastive search. You can find the gist [here](https://gist.github.com/sekharvth/200719858f13e6cf6f64946ef3c20a5c). I've only tested for base 10000 and fraction 1 as the rotary embeddings params, please feel free to experiment with different values. Additionally, as the modifications were done rather hastily, kindly overlook any hard-coded/non-parametrised values that I might have added.

This seems to be generating expected results, and the speed of generation is much higher with the adoption of `past_key_values`. As a result, the speed is not overly dependent on the value of `max_new_tokens` now, as was the case with the default model. I've done these experiments on the 7b variant

Hope this helps."
dimaischenko,2023-06-21 11:25:42+00:00,comment,tiiuae/falcon-40b,"@sekharvth Trying your `modelling_RW.py` I get an error with types

```
--> 509    attn_output = F.scaled_dot_product_attention(
    510        query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True
    511    )
    512 #attn_output = F.scaled_dot_product_attention(
    513 #    query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True
    514 #)
    516 x = attn_output.view(batch_size, self.num_heads, q_length, self.head_dim)

RuntimeError: Expected query, key, and value to have the same dtype, but got query.dtype: float key.dtype: float and value.dtype: c10::BFloat16 instead.
```"
sekharvth,2023-06-21 18:01:30+00:00,comment,tiiuae/falcon-40b,"> @sekharvth Trying your `modelling_RW.py` I get an error with types
> 
> ```
> --> 509    attn_output = F.scaled_dot_product_attention(
>     510        query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True
>     511    )
>     512 #attn_output = F.scaled_dot_product_attention(
>     513 #    query_layer_, key_layer_, value_layer_, None, 0.0, is_causal=True
>     514 #)
>     516 x = attn_output.view(batch_size, self.num_heads, q_length, self.head_dim)
> 
> RuntimeError: Expected query, key, and value to have the same dtype, but got query.dtype: float key.dtype: float and value.dtype: c10::BFloat16 instead.
> ```

Ahh apologies, forgot to mention that this doesn't work on fp16. When you're loading the model, ensure that you're *not* using `torch_dtype=torch.float16` or `torch_dtype=torch.bfloat16`

"
dimaischenko,2023-06-25 13:06:47+00:00,comment,tiiuae/falcon-40b,@sekharvth is this some kind of fundamental limitation on fp16? Or do I just need to do something else to make it work?
sekharvth,2023-07-03 04:31:17+00:00,comment,tiiuae/falcon-40b,"@dimaischenko From what I remember, the precision errors were raised from the rotary embeddings implementation, specifically the sine-cosine transformation parts. The transformations didn't support half precision then, and I didn't experiment with it further to get it to work for fp16, as my immediate goal was to get the whole thing working any way it could. 

You may try out some data transformations internally in the script and check if it works for fp16."
cchudant,2023-06-07 12:00:31+00:00,comment,tiiuae/falcon-40b,Upstream modeling_bloom.py: https://github.com/huggingface/transformers/blob/fabe17a726bbf6081cfbcc975d8ac451a81f3e2d/src/transformers/models/bloom/modeling_bloom.py#L305
FalconLLM,2023-06-09 14:42:15+00:00,comment,tiiuae/falcon-40b,All model-related code is in the [modelling_RW.py](https://huggingface.co/tiiuae/falcon-40b/blob/main/modelling_RW.py) file. 
FalconLLM,2023-06-09 14:41:23+00:00,comment,tiiuae/falcon-40b,"Hey @ChangranHuuu, at this point we have no plans to release the other components of the dataset. "
leoapolonio,2023-06-07 02:43:14+00:00,comment,tiiuae/falcon-40b,157G
Toaster496,2023-06-07 07:19:57+00:00,comment,tiiuae/falcon-40b,? what cards? how is that Vram or RAM?
serin32,2023-06-07 14:18:13+00:00,comment,tiiuae/falcon-40b,"Depends on if you want to do inference in 32, 16, 8 or 4 bit, but at full 32 bit I think it's about 80GB of VRAM.
Correction: 16 bit is 80ish GB and 32 bit would be around 160ish GB I believe.  Thanks Mikael110 was thinking about 16 bit and not 32 when I wrote this."
Mikael110,2023-06-08 04:43:14+00:00,comment,tiiuae/falcon-40b,"With 8bit loading it consumes ~46GB of VRAM, and with 4bit loading it takes ~24GB VRAM. Those numbers exclude OS headroom, so don't expect 4bit to fit on actual 24GB cards, and 8bit will be a tight squeeze on 48GB cards, you will probably OOM once the context gets even remotely long. I can't give numbers for 16bit and 32bit since they OOM on the A100 80GB which I was testing on. But given that even 16bit is too big for the card I'm quite confident that 32bit is quite a bit larger than 80GB. Maybe that's the number leoapolonio was referencing? I could definitively see it actually being that high for full 32bit inference."
Toaster496,2023-06-08 06:09:06+00:00,comment,tiiuae/falcon-40b,THANK YOU!
cchudant,2023-06-08 14:35:33+00:00,comment,tiiuae/falcon-40b,"Hi, the model is trained in bfloat16, not float32 - you need 40B x 2 byte per param = ~80Go to run it"
FalconLLM,2023-06-09 14:40:58+00:00,comment,tiiuae/falcon-40b,We recommend 80-100GB to run inference on Falcon-40B comfortably. 
OmarDrame,2023-06-09 10:31:37+00:00,comment,tiiuae/falcon-40b,"I think TII developed another model call noor, that is based on arabic."
FalconLLM,2023-06-09 14:40:22+00:00,comment,tiiuae/falcon-40b,"Yes, we have another project called [Noor](https://noor.tii.ae/) dedicated to Arabic.

For Falcon, constraints on data availability, target scale, and tokenization pushed us to build an English model instead of an Arabic model. "
Free-Radical,2023-06-11 02:47:46+00:00,comment,tiiuae/falcon-40b,"Hi, regarding translation to Arabic:
1. what is the license expected for NOOR? Apache 2.0?
2. Also, when are you releasing it? 

Thx
PS great job on Falcon, BTW"
awyshen,2023-06-12 11:18:48+00:00,comment,tiiuae/falcon-40b,"> Yes, we have another project called [Noor](https://noor.tii.ae/) dedicated to Arabic.
> 
> For Falcon, constraints on data availability, target scale, and tokenization pushed us to build an English model instead of an Arabic model.

Thanks, Where to experience Noor demo or use the Noor model, [Noor](https://noor.tii.ae/) there is no entry."
FalconLLM,2023-06-09 14:39:10+00:00,comment,tiiuae/falcon-40b,"We have merged this PR for the Instruct models, but encourage the base models to be further finetuned. "
dongZheX,2023-06-05 05:41:32+00:00,comment,tiiuae/falcon-40b,"From the model card, the answer is 2048."
S2023,2023-06-03 22:40:07+00:00,comment,tiiuae/falcon-40b,。。。
Raulyp01,2023-06-03 23:21:40+00:00,comment,tiiuae/falcon-40b,Dime como pruebo la IA Falcon?
leoapolonio,2023-06-07 17:35:36+00:00,comment,tiiuae/falcon-40b,"I had issues running it turns out you need over 250GB of Disk Volume first for weights, packages, and os dependencies"
FalconLLM,2023-06-09 14:37:42+00:00,comment,tiiuae/falcon-40b,"The way HuggingFace implements these tasks does not allow for Falcon to be used for them, as they require seq2seq models while Falcon is a causal decoder-only. "
lucadiliello,2023-06-16 09:54:16+00:00,comment,tiiuae/falcon-40b,"You can do summarization by using the `text-generation` pipeline and providing a prompt like:
```
Document: ""{the text to summarize}""

Summary:
```"
Yassin-sameh,2023-06-25 09:22:09+00:00,comment,tiiuae/falcon-40b,"I have to say from my experience: 
1) It performs poorly in Text Summarisation especially in other languages.
2) It is very difficult to give it context as to what kind of results you want. Example: You are an expert {X} return notes as bullet points. Or ""Expert {X} Notes:"""
Ichsan2895,2023-06-04 16:44:27+00:00,comment,tiiuae/falcon-40b,"**I never use it**, but it seems it was possible to fine tune it with new file. Please see https://github.com/rmihaylov/falcontune

As the website said, the format of the text is simply same as `alpaca-finetune.json`.

```
[
    {
        ""instruction"": ""Give three tips for staying healthy."",
        ""input"": """",
        ""output"": ""1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.""
    },
    {
        ""instruction"": ""What are the three primary colors?"",
        ""input"": """",
        ""output"": ""The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).""
    }
]
```

How to run it.
```
falcontune finetune \
    --model=falcon-40b \
    --weights=tiiuae/falcon-40b \
    --dataset=./alpaca-finetune.json \
    --data_type=alpaca \
    --lora_out_dir=./falcon-40b-alpaca/ \
    --mbatch_size=1 \
    --batch_size=2 \
    --epochs=3 \
    --lr=3e-4 \
    --cutoff_len=256 \
    --lora_r=8 \
    --lora_alpha=16 \
    --lora_dropout=0.05 \
    --warmup_steps=5 \
    --save_steps=50 \
    --save_total_limit=3 \
    --logging_steps=5 \
    --target_modules='[""query_key_value""]'
```"
FalconLLM,2023-06-09 14:30:38+00:00,comment,tiiuae/falcon-40b,"You could also have a look at this [blogpost](https://huggingface.co/blog/falcon) regarding finetuning. 

Finetuning to an European language should be enough, but any languages with a different character set (e.g., Chinese, Arabic, etc.) could be difficult."
guilleguells,2023-06-14 08:43:15+00:00,comment,tiiuae/falcon-40b,What about programming languages?
shaileshp,2023-07-25 09:06:49+00:00,comment,tiiuae/falcon-40b,Same as i need to fine tune new language(Gujarati) can you help me or please give sample code
Jezzarax,2023-06-02 10:20:04+00:00,comment,tiiuae/falcon-40b,"You can find the model code using [scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html), check [here](https://huggingface.co/tiiuae/falcon-40b/blob/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py#L289). It is expected to work in flash attention mode, but due to an [issue](https://github.com/pytorch/pytorch/issues/97514) we got to wait for PyTorch 2.1 to benefit from flash or memory_efficient attention."
utensil,2023-06-02 10:55:54+00:00,comment,tiiuae/falcon-40b,"Thank you for the reply and the pointers, and the great work in general!

As for xformer attention mentioned in the issue, my test shows that falcon can work with it already and saves ~ 15% VRAM (exact number might vary in different setting).

May I also assume that with pytorch 2.1, falcon will work with better transformer (which includes flash attention to my knowledge ) ? Link: https://huggingface.co/docs/optimum/bettertransformer/overview"
joorei,2023-06-01 08:52:52+00:00,comment,tiiuae/falcon-40b,"That would be amazing!

The censored models are not only biased, but as a result less useful."
nudelbrot,2023-05-31 19:13:14+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
coderisnotexist,2023-05-31 17:49:35+00:00,comment,tiiuae/falcon-40b,Yusuf sen ünimi okuyorsun
ravu3718,2023-07-03 09:12:33+00:00,comment,tiiuae/falcon-40b,"same question as above
"
FalconLLM,2023-06-09 14:28:43+00:00,comment,tiiuae/falcon-40b,"Hey @aa33, we have no definitive plan at this time but we definitely took note of the need for an intermediary model! "
tinkertank,2023-05-31 15:15:11+00:00,comment,tiiuae/falcon-40b,"See here: https://huggingface.co/tiiuae/falcon-40b/discussions/18

Contains code & requirements, and runs on any A100 80G instance.
(I'm personally using Datacrunch.io spot instances, but any A100 80G instance should do)"
FalconLLM,2023-06-09 14:27:52+00:00,comment,tiiuae/falcon-40b,"You could have a look to this [blogpost](https://huggingface.co/blog/falcon) from HuggingFace.

Specific to AWS, there is also a [tutorial](https://www.philschmid.de/sagemaker-falcon-llm) for deploying on SageMaker."
FalconLLM,2023-05-31 13:16:36+00:00,comment,tiiuae/falcon-40b,Thanks for the pr!
Nolyzlel,2023-06-07 09:56:56+00:00,comment,tiiuae/falcon-40b,"Yes please, can someone share a guide to finetune the model with individual data?"
RichardN,2023-06-08 21:17:03+00:00,comment,tiiuae/falcon-40b,"Yes - we need an instruction model on how to set up the model on a local environment, then run it using our individual and confidential data. Presumably there is a way to query or use our own data with Falcon?"
FalconLLM,2023-06-09 14:26:38+00:00,comment,tiiuae/falcon-40b,This [blogpost](https://huggingface.co/blog/falcon) from HuggingFace contains useful pointers regarding finetuning the model with PEFT and in lower precision. 
shaileshp,2023-07-25 09:10:59+00:00,comment,tiiuae/falcon-40b,"Yes please, can someone share a guide to finetune the model with individual data?"
BatuhanECB,2023-05-31 18:33:15+00:00,comment,tiiuae/falcon-40b,I am 2x3090 User. I can run Lama 30b model. ın fact ı can open Lama 65b but cant run cuz of memory (system memory not cuz of vram). That means we can use the falcon 40b model ı guess
znsoft,2023-06-05 23:02:36+00:00,comment,tiiuae/falcon-40b,"you can setup a swap file to expand virtual memoery. refere to ""swapon"""
teticio,2023-06-06 10:50:52+00:00,comment,tiiuae/falcon-40b,"Hi @znsoft - thanks for this. Were you able to run it yourself on 2x4090? I have this set up and I got an error running this which appears to be linked with running out of VRAM. I ran exactly the same code with the 7b model, and it works. The exact log and error I got was:

```
Overriding torch_dtype=torch.bfloat16 with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.9
CUDA SETUP: Detected CUDA version 121
CUDA SETUP: Loading binary ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...
The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].

~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[4], line 1
----> 1 sequences = pipeline(
      2    ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
      3     max_length=200,
      4     do_sample=True,
      5     top_k=10,
      6     num_return_sequences=1,
      7     eos_token_id=tokenizer.eos_token_id,
      8 )
      9 for seq in sequences:
     10     print(f""Result: {seq['generated_text']}"")

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:201, in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)
    160 def __call__(self, text_inputs, **kwargs):
    161     """"""
    162     Complete the prompt(s) given as inputs.
    163 
   (...)
    199           ids of the generated text.
    200     """"""
--> 201     return super().__call__(text_inputs, **kwargs)

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1119, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1111     return next(
   1112         iter(
   1113             self.get_iterator(
   (...)
   1116         )
   1117     )
   1118 else:
-> 1119     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1126, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1124 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
   1125     model_inputs = self.preprocess(inputs, **preprocess_params)
-> 1126     model_outputs = self.forward(model_inputs, **forward_params)
   1127     outputs = self.postprocess(model_outputs, **postprocess_params)
   1128     return outputs

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/base.py:1025, in Pipeline.forward(self, model_inputs, **forward_params)
   1023     with inference_context():
   1024         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
-> 1025         model_outputs = self._forward(model_inputs, **forward_params)
   1026         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(""cpu""))
   1027 else:

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263, in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)
    260         generate_kwargs[""min_length""] += prefix_length
    262 # BS x SL
--> 263 generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
    264 out_b = generated_sequence.shape[0]
    265 if self.framework == ""pt"":

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)
    112 @functools.wraps(func)
    113 def decorate_context(*args, **kwargs):
    114     with ctx_factory():
--> 115         return func(*args, **kwargs)

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:1568, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)
   1560     input_ids, model_kwargs = self._expand_inputs_for_generation(
   1561         input_ids=input_ids,
   1562         expand_size=generation_config.num_return_sequences,
   1563         is_encoder_decoder=self.config.is_encoder_decoder,
   1564         **model_kwargs,
   1565     )
   1567     # 13. run sample
-> 1568     return self.sample(
   1569         input_ids,
   1570         logits_processor=logits_processor,
   1571         logits_warper=logits_warper,
   1572         stopping_criteria=stopping_criteria,
   1573         pad_token_id=generation_config.pad_token_id,
   1574         eos_token_id=generation_config.eos_token_id,
   1575         output_scores=generation_config.output_scores,
   1576         return_dict_in_generate=generation_config.return_dict_in_generate,
   1577         synced_gpus=synced_gpus,
   1578         streamer=streamer,
   1579         **model_kwargs,
   1580     )
   1582 elif is_beam_gen_mode:
   1583     if generation_config.num_return_sequences > generation_config.num_beams:

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/transformers/generation/utils.py:2615, in GenerationMixin.sample(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)
   2612 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
   2614 # forward pass to get next token
-> 2615 outputs = self(
   2616     **model_inputs,
   2617     return_dict=True,
   2618     output_attentions=output_attentions,
   2619     output_hidden_states=output_hidden_states,
   2620 )
   2622 if synced_gpus and this_peer_finished:
   2623     continue  # don't waste resources running the code we don't need

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:759, in RWForCausalLM.forward(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)
    755     raise ValueError(f""Got unexpected arguments: {deprecated_arguments}"")
    757 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
--> 759 transformer_outputs = self.transformer(
    760     input_ids,
    761     past_key_values=past_key_values,
    762     attention_mask=attention_mask,
    763     head_mask=head_mask,
    764     inputs_embeds=inputs_embeds,
    765     use_cache=use_cache,
    766     output_attentions=output_attentions,
    767     output_hidden_states=output_hidden_states,
    768     return_dict=return_dict,
    769 )
    770 hidden_states = transformer_outputs[0]
    772 lm_logits = self.lm_head(hidden_states)

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:654, in RWModel.forward(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)
    646     outputs = torch.utils.checkpoint.checkpoint(
    647         create_custom_forward(block),
    648         hidden_states,
   (...)
    651         head_mask[i],
    652     )
    653 else:
--> 654     outputs = block(
    655         hidden_states,
    656         layer_past=layer_past,
    657         attention_mask=causal_mask,
    658         head_mask=head_mask[i],
    659         use_cache=use_cache,
    660         output_attentions=output_attentions,
    661         alibi=alibi,
    662     )
    664 hidden_states = outputs[0]
    665 if use_cache is True:

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:411, in DecoderLayer.forward(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)
    408 outputs = attn_outputs[1:]
    410 # MLP.
--> 411 mlp_output = self.mlp(ln_mlp)
    413 output = dropout_add(
    414     mlp_output + attention_output, residual, self.config.hidden_dropout, training=self.training
    415 )
    417 if use_cache:

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File ~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b-instruct/4e8f82c2d7468e3d9c88be4f38f531449141b52b/modelling_RW.py:356, in MLP.forward(self, x)
    355 def forward(self, x: torch.Tensor) -> torch.Tensor:
--> 356     x = self.act(self.dense_h_to_4h(x))
    357     x = self.dense_4h_to_h(x)
    358     return x

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/accelerate/hooks.py:165, in add_hook_to_module.<locals>.new_forward(*args, **kwargs)
    163         output = old_forward(*args, **kwargs)
    164 else:
--> 165     output = old_forward(*args, **kwargs)
    166 return module._hf_hook.post_forward(module, output)

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:320, in Linear8bitLt.forward(self, x)
    317 if self.bias is not None and self.bias.dtype != x.dtype:
    318     self.bias.data = self.bias.data.to(x.dtype)
--> 320 out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
    322 if not self.state.has_fp16_weights:
    323     if self.state.CB is not None and self.state.CxB is not None:
    324         # we converted 8-bit row major to turing/ampere format in the first inference pass
    325         # we no longer need the row-major weight

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:500, in matmul(A, B, out, state, threshold, bias)
    498 if threshold > 0.0:
    499     state.threshold = threshold
--> 500 return MatMul8bitLt.apply(A, B, out, bias, state)

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/torch/autograd/function.py:506, in Function.apply(cls, *args, **kwargs)
    503 if not torch._C._are_functorch_transforms_active():
    504     # See NOTE: [functorch vjp and autograd interaction]
    505     args = _functorch.utils.unwrap_dead_wrappers(args)
--> 506     return super().apply(*args, **kwargs)  # type: ignore[misc]
    508 if cls.setup_context == _SingleLevelFunction.setup_context:
    509     raise RuntimeError(
    510         'In order to use an autograd.Function with functorch transforms '
    511         '(vmap, grad, jvp, jacrev, ...), it must override the setup_context '
    512         'staticmethod. For more details, please see '
    513         'https://pytorch.org/docs/master/notes/extending.func.html')

File ~/.local/share/virtualenvs/huggingface-OfWfm_Zx/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:417, in MatMul8bitLt.forward(ctx, A, B, out, bias, state)
    415 # 4. Mixed-precision decomposition matmul
    416 if coo_tensorA is not None and subA is not None:
--> 417     output += torch.matmul(subA, state.subB)
    419 # 5. Save state
    420 ctx.state = state

RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
```

I'm very interested in being able to run this model, so any help would be greatly appreciated."
teticio,2023-06-06 11:09:45+00:00,comment,tiiuae/falcon-40b,"For anyone else landing here with the same problem I had ^, I just recompiled pytorch with CUDA=12.1 and it worked fine. Thanks!"
mishaml77,2023-06-18 08:31:43+00:00,comment,tiiuae/falcon-40b,You need to quantize the model to 8bits
DanielHesslow,2023-05-30 10:07:39+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
FalconLLM,2023-05-30 10:42:25+00:00,comment,tiiuae/falcon-40b,"Unfortunately, I do not currently have access to an H100, so it will be hard to debug issues there specifically. Some people do seem to be able to run on H100: https://www.youtube.com/watch?v=iEuf1PrmZ0Q, maybe seeing what they do might be of some help?

80GB is going to be very tight though, so will require some cpu offloading with accelerate. If I understand things correctly accelerate is able to automatically offload to cpu memory, but I am not too familiar with this process.

The smallest we've run it on is 4xA10(4x24GB=96GB).
Sorry to not be of more help, hopefully some other people that has managed to make it run can chime in"
dsatch,2023-05-31 07:55:36+00:00,comment,tiiuae/falcon-40b,"I have not been able to run at all, even on massive deployment of 240 VGPU. I used the code from the main page. It is clearly a memory issue, because 7B runs (but event that takes up more than 50% of VGPU on 240 GB setup). Any ideas, can you help?"
FalconLLM,2023-05-31 08:03:25+00:00,comment,tiiuae/falcon-40b,"@dstatch, Which/How many GPUs were you trying to run it on?"
dsatch,2023-05-31 08:08:11+00:00,comment,tiiuae/falcon-40b,"4 X 80 GB   I tried to use Runpod and Datacrunch, fails in both places. It seems that it is not even a VRAM issue, but in inter-GPU communication. Really excited about the potential of this, but as it stands even throwing very large resources at it does not help."
dsatch,2023-05-31 08:24:26+00:00,comment,tiiuae/falcon-40b,"but I am not sure what the issue, just positive that I am not the only one experiencing it, since I have tried in multiple places"
tinkertank,2023-05-31 14:01:13+00:00,comment,tiiuae/falcon-40b,"I'm running it with the following code on A datacrunch 80G A100 (using 8bit mode).
Credit where credit is due, I basically lifted this code from Sam Witteveen's excellent youtube video & colab:
https://www.youtube.com/watch?v=5M1ZpG2Zz90

Should work on H100 as well.

```
import torch
import transformers
from transformers import GenerationConfig, pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig
import bitsandbytes as bnb
from torch.cuda.amp import autocast

model = ""tiiuae/falcon-40b""

tokenizer = AutoTokenizer.from_pretrained(model)

model = AutoModelForCausalLM.from_pretrained(model,
        load_in_8bit=True,
        trust_remote_code=True,
        device_map='auto',
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
)



pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)

with autocast(dtype=torch.float16):
    sequences = pipeline(
       ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
        max_length=200,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
    )
    for seq in sequences:
        print(f""Result: {seq['generated_text']}"")
```

I'm running the following conda env.
(kind of a mess, but seems to work)
```
conda create --name llm python=3.10
conda activate llm
conda install pytorch==2.0.0 pytorch-cuda=11.8 transformers -c pytorch -c nvidia
pip install einops accelerate
pip install -q -U bitsandbytes
pip install -q -U git+https://github.com/huggingface/transformers.git
pip install -q -U git+https://github.com/huggingface/peft.git
pip install -q -U git+https://github.com/huggingface/accelerate.git
pip -q install sentencepiece Xformers einops
pip -q install langchain
```"
cian0,2023-06-01 02:28:23+00:00,comment,tiiuae/falcon-40b,Would this run with 5 12GB VRAM (3060) gpus? I run a mining rig at home..
Adrians,2023-06-01 18:29:44+00:00,comment,tiiuae/falcon-40b,"Thanks @tinkertank! I tried your install + run on H100 (lamnbda labs) but I'm getting cublas errors...

```
File ""/home/ubuntu/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/b0462812b2f53caab9ccc64051635a74662fc73b/modelling_RW.py"", line 252, in forward
    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
  File ""/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/hooks.py"", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File ""/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py"", line 388, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File ""/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py"", line 559, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File ""/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/function.py"", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File ""/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py"", line 397, in forward
    out32, Sout32 = F.igemmlt(C32A, state.CxB, SA, state.SB)
  File ""/home/ubuntu/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/functional.py"", line 1781, in igemmlt
    raise Exception('cublasLt ran into an error!')
Exception: cublasLt ran into an error!
```

Any ideas?"
nateraw,2023-06-02 00:37:22+00:00,comment,tiiuae/falcon-40b,"Also was getting error @Adrians was getting. Looked to me like some issue in 8-bit, probably because some wrong operation is being called. So, I skipped it, and the below worked for me on H100 from Lambda.

Just checked, and the below worked on a fresh instance (I ran no other commands).

## Install miniconda

We only do this because the install for torch/cuda works smoothly. 

```bash
# Download latest miniconda.
wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# Install. -b is used to skip prompt
bash Miniconda3-latest-Linux-x86_64.sh -b

# Activate.
eval ""$(/home/ubuntu/miniconda3/bin/conda shell.bash hook)""

# (optional) Add activation cmd to bashrc so you don't have to run the above every time.
printf '\neval ""$(/home/ubuntu/miniconda3/bin/conda shell.bash hook)""' >> ~/.bashrc
```

## Setup env

Note: I don't think you *need* to install transformers from github if you do `device_map={"""": 0}` later instead of `device_map=0`, but I haven't checked.

```bash
# Create and activate env. -y skips confirmation prompt.
conda create -n falcon-env python=3.9 -y
conda activate falcon-env

# newest torch with cuda 11.8
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# For transformers, the commit I installed was f49a3453caa6fe606bb31c571423f72264152fce
pip install -U accelerate einops sentencepiece git+https://github.com/huggingface/transformers.git
```

## Run it

This will use up basically all the memory, but it works.

```python
import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer


model = ""tiiuae/falcon-40b""
tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=0)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=0,
)
sequences = pipeline(
    ""To make the perfect chocolate chip cookies,"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    pad_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")
```"
airtable,2023-06-09 10:56:32+00:00,comment,tiiuae/falcon-40b,"Hi guys

I am back, [the code](https://huggingface.co/tiiuae/falcon-40b/discussions/18#647939c2c68a021fbba88182) from @nateraw worked on my Lambda H100 instance, only needed to upgrade Transformers to 4.30.0 from 4.29.2, without that it was giving a `device_map` `int type doesn't have .values()` error and took me a while to figure out.

But looks the model tightly fits, here's the GPU usage at 99.1%

![image.png](https://cdn-uploads.huggingface.co/production/uploads/6466d68a14e059dde8bbab65/_AAZsfFUPljPaUolS2CJR.png)

Next up, load in `langchain`
"
vinwizard,2023-07-06 14:01:25+00:00,comment,tiiuae/falcon-40b,"> Also was getting error @Adrians was getting. Looked to me like some issue in 8-bit, probably because some wrong operation is being called. So, I skipped it, and the below worked for me on H100 from Lambda.
> 
> Just checked, and the below worked on a fresh instance (I ran no other commands).
> 
> ## Install miniconda
> 
> We only do this because the install for torch/cuda works smoothly. 
> 
> ```bash
> # Download latest miniconda.
> wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
> 
> # Install. -b is used to skip prompt
> bash Miniconda3-latest-Linux-x86_64.sh -b
> 
> # Activate.
> eval ""$(/home/ubuntu/miniconda3/bin/conda shell.bash hook)""
> 
> # (optional) Add activation cmd to bashrc so you don't have to run the above every time.
> printf '\neval ""$(/home/ubuntu/miniconda3/bin/conda shell.bash hook)""' >> ~/.bashrc
> ```
> 
> ## Setup env
> 
> Note: I don't think you *need* to install transformers from github if you do `device_map={"""": 0}` later instead of `device_map=0`, but I haven't checked.
> 
> ```bash
> # Create and activate env. -y skips confirmation prompt.
> conda create -n falcon-env python=3.9 -y
> conda activate falcon-env
> 
> # newest torch with cuda 11.8
> conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
> 
> # For transformers, the commit I installed was f49a3453caa6fe606bb31c571423f72264152fce
> pip install -U accelerate einops sentencepiece git+https://github.com/huggingface/transformers.git
> ```
> 
> ## Run it
> 
> This will use up basically all the memory, but it works.
> 
> ```python
> import torch
> import transformers
> from transformers import AutoModelForCausalLM, AutoTokenizer
> 
> 
> model = ""tiiuae/falcon-40b""
> tokenizer = AutoTokenizer.from_pretrained(model)
> model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map=0)
> pipeline = transformers.pipeline(
>     ""text-generation"",
>     model=model,
>     tokenizer=tokenizer,
>     torch_dtype=torch.bfloat16,
>     trust_remote_code=True,
>     device_map=0,
> )
> sequences = pipeline(
>     ""To make the perfect chocolate chip cookies,"",
>     max_length=200,
>     do_sample=True,
>     top_k=10,
>     num_return_sequences=1,
>     pad_token_id=tokenizer.eos_token_id,
> )
> for seq in sequences:
>     print(f""Result: {seq['generated_text']}"")
> ```

How much was the inference time on this? @nateraw"
FalconLLM,2023-05-30 07:25:07+00:00,comment,tiiuae/falcon-40b,"Hey @hankcs, 

This is another one of these somewhat arbitrary decisions :).

* We experimented with both rotary and ALiBi early on in the project (that's why the RW models use it!);
* We found rotary to consistently improve downstream zero-shot performance + autoregressive loss, but it was a bit slower than ALiBi;
* However, with a custom Triton kernel for FlashAttention+Rotary, we were able to close much of that gap;
* We also did extrapolation experiments, and with a sliding window we found Rotary & ALiBi to perform similarly -- however, we did not had good long-dependence tasks, so we suspect more work is needed here;
* We experimented with finetuning to longer sequence lengths with Rotary and it worked fine.

Stay tuned, we are actually interested in sharing more thoughts and principled experiments about positional embeddings in the future."
jangtu052,2023-07-18 07:11:36+00:00,comment,tiiuae/falcon-40b,Any updates on this?
yuuhan,2023-05-29 07:51:16+00:00,comment,tiiuae/falcon-40b,"Of course, even 13b model need v100_32g to run, the 40b model must need more!"
FalconLLM,2023-05-30 07:09:43+00:00,comment,tiiuae/falcon-40b,"Model weights alone are ~= 80GB, so fast inference would require at least 90-100GB.
You can try to see if you can get accelerate with cpu offloading to work: https://huggingface.co/docs/accelerate/package_reference/big_modeling 

The community has also created a 4bit quantised version of the model: https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ, which should only require 20GB for the model weights. 

Otherwise the best bet would be to work with the smaller models: https://huggingface.co/tiiuae/falcon-7b
"
Ichsan2895,2023-06-05 14:32:27+00:00,comment,tiiuae/falcon-40b,"I successfully quantizing it with Qlora with using bitsandbytes package.

Activated it with  bitsandbytes config which select ""nf4"" Quant type + load_in_4bit + bfloat16. It allows to run on single A100 40 GB VRAM. The quantized version is run on the fly in jupyter lab without manually exported/saved it the new model."
max-fry,2023-06-07 13:55:06+00:00,comment,tiiuae/falcon-40b,@Ichsan2895 How is its performance?
pb68,2023-06-08 00:05:20+00:00,comment,tiiuae/falcon-40b,@Ichsan2895 : Please share the finetuning and evaluation code if possible.
jimsrc,2023-06-08 04:44:34+00:00,comment,tiiuae/falcon-40b,how many tokens/sec? aprox
FalconLLM,2023-06-09 14:25:17+00:00,comment,tiiuae/falcon-40b,You can check out the [FalconTune](https://github.com/rmihaylov/falcontune) package from the community as well :).
Ichsan2895,2023-06-10 03:32:08+00:00,comment,tiiuae/falcon-40b,"> how many tokens/sec? aprox

I Ran it from cloud environment with Single A6000 48 GB VRAM. Falcon-40B ran with 1-2 tokens/sec

> @Ichsan2895 : Please share the finetuning and evaluation code if possible.

Sorry, I never do fine tuning with new dataset. Just interference it with question to see the answer :)"
Ichsan2895,2023-06-12 07:06:33+00:00,comment,tiiuae/falcon-40b,"> Quantizing the large (40 or even 7b) model on 4bit will help community a lot. And please fine tune it with large code database and on Wizard-Vicuna, Mega and other big chat databases as well so it can produce code during chat even.

Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA
https://huggingface.co/blog/falcon

Colab Falcon fine tuning with QLoRA and Guanaco Dataset
https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"
lewtun,2023-05-28 15:48:41+00:00,comment,tiiuae/falcon-40b,"I can confirm this fix works for inference - however, it seems you have introduced a duplicate `forward()` method instead of updating the existing one?"
SpiridonSunRotator,2023-05-28 19:47:30+00:00,comment,tiiuae/falcon-40b,"Sorry, actually I wanted is to slightly update forward to output the same type given in query and key. Fixed."
akashcollectiv,2023-05-28 20:28:32+00:00,comment,tiiuae/falcon-40b,"it worked yesterday for me , clone new one  getting allocation exceed issues , while running it on 80GB a100


    return forward_call(*args, **kwargs)
  File ""/root/.cache/huggingface/modules/transformers_modules/falcon-40b/modelling_RW.py"", line 93, in forward
    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.32 GiB total capacity; 77.15 GiB already allocated; 832.00 KiB free; 78.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF



my bad forgot to install 
pip install xformers :P
still same error : ("
SpiridonSunRotator,2023-05-29 04:29:00+00:00,comment,tiiuae/falcon-40b,@akashcollectiv  this model occupies almost all A100 capacity if loaded on a single GPU. I think there is not enough memory for all but very short sequences.
lewtun,2023-05-29 21:12:13+00:00,comment,tiiuae/falcon-40b,"> Sorry, actually I wanted is to slightly update forward to output the same type given in query and key. Fixed.

Thanks! I can confirm this works as intended and it also enable loading the model in 8-bit which is great for inference :)"
FalconLLM,2023-05-30 06:10:39+00:00,comment,tiiuae/falcon-40b,"This change will unfortunately change things for bfloat16, due to numerical precision. Which is the only dtype we have properly validated the performance with, we should expect some degradation of model quality in fp16.

In particular the cos/sin needs to be applied in full precision, as was done in the original code.  I believe only adding the q.dtype to the rotary forward should be sufficient, I'll take care of it today."
FalconLLM,2023-05-30 06:18:15+00:00,comment,tiiuae/falcon-40b,"Should be fixed now, though as previously mentioned, inference with dtypes other than bfloat16 may incur model degradation."
lewtun,2023-05-28 15:18:57+00:00,comment,tiiuae/falcon-40b,I encountered a similar problem & the solution is to bump up to `torch` v2.0.1
WajihUllahBaig,2023-05-29 06:15:44+00:00,comment,tiiuae/falcon-40b,"Correct, I just bumped to 2.0.0 and it worked"
Minami-su,2023-05-28 22:49:38+00:00,comment,tiiuae/falcon-40b,问题解决了，3090x2 -->  a40x2
Bastao,2023-06-09 15:48:12+00:00,comment,tiiuae/falcon-40b,what is the origin of the bug?
max-fry,2023-05-27 13:09:18+00:00,comment,tiiuae/falcon-40b,Most likely it's because training a 40B model is significantly more expensive than training a 7B model.
hankcs,2023-05-29 17:07:10+00:00,comment,tiiuae/falcon-40b,"I'm interested in this question too. Looking forward to an official explanation from the authors. 

BTW, according to the [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), falcon-7b outperforms mpt-7b by 0.2, which could also be attributed to the fact that falcon-7b is only trained on 1T tokens of *unrefined* web data."
FalconLLM,2023-05-30 07:17:40+00:00,comment,tiiuae/falcon-40b,"Hey! 

This is a purely arbitrary decision :). We iterate a lot on internal models, and Falcon-40B was our first serious foray into this scale--so we wanted to validate infra, codebase, data, etc. That's why we stuck to 1T.

The 7B came later, when we had 384 GPUs unscheduled for two weeks, so 1.5T was a good match. 

Regarding the different with MPT-7B being smaller, we believe this is due to a combination of three factors: (1) we are approaching the limits of what can be done with a 7B pretrained model; (2) multiquery with 64 attention head size improves inference scalability, but that's at the cost of some task performance; (3) we experimented for the 7B with a very large batch size."
Sloba,2023-05-28 00:36:28+00:00,comment,tiiuae/falcon-40b,"qa = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=""auto"",
    **trust_remote_code=True,**
    device_map=""auto"",
)"
mvoisin,2023-05-28 15:07:34+00:00,comment,tiiuae/falcon-40b,"what if we run it through AWS Sagemaker? 

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
    initial_instance_count=1, # number of instances
    instance_type='ml.m5.xlarge', # ec2 instance type
    trust_remote_code=True,
)"
kennethleungty,2023-05-29 06:11:13+00:00,comment,tiiuae/falcon-40b,@mvoisin - setting `trust_remote_code=True` still gave me the same error
sunsetsobserver,2023-06-01 07:47:01+00:00,comment,tiiuae/falcon-40b,"This worked for me:

   from transformers import AutoTokenizer, AutoModelForCausalLM
   import transformers
   import torch

   model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b"", trust_remote_code=True)

   tokenizer = AutoTokenizer.from_pretrained(""tiiuae/falcon-40b"")''"
mahmoudajawad,2023-06-05 07:39:01+00:00,comment,tiiuae/falcon-40b,"I failed with all possible combinations to overcome this issue, so I ended up changing the raising this exception. For those using SageMaker Jupyeter:
```bash
!sed -i 's/if not trust_remote_code:/if False: # Manually replace in-line to avoid #9/g' ~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py
```
You can now confirm it has been changed with:
```bash
!cat ~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py
```"
ashfakh,2023-06-05 10:10:29+00:00,comment,tiiuae/falcon-40b,"Facing the same issue, following this tutorial https://huggingface.co/blog/sagemaker-huggingface-llm to deploy falcon 7b on aws via sagemaker, getting the same error and don't know where to set trust_remote_code. please help"
cw-franabellan,2023-06-05 11:29:47+00:00,comment,tiiuae/falcon-40b,Same here!!
kieryn,2023-06-06 12:56:11+00:00,comment,tiiuae/falcon-40b,Same for me too - from the tutorial there's no mention of the trust setting for falcon and have not figured it out yet either.
JoseadClutch,2023-06-06 14:25:58+00:00,comment,tiiuae/falcon-40b,"Hey Everyone, I've been trying with https://gist.github.com/timesler/4b244a6b73d6e02d17fd220fd92dfaec as well, I'm having issues with the tar.gz file but please try and see what happens."
jweissenberger,2023-06-06 15:50:29+00:00,comment,tiiuae/falcon-40b,"Looks like they're is working on releasing the new 0.8.2 version of the huggingface llm inference docker image to sagemaker based on this thread: https://github.com/huggingface/text-generation-inference/issues/390

That should fix this issue based on this blog: https://huggingface.co/blog/falcon"
FalconLLM,2023-06-09 14:24:44+00:00,comment,tiiuae/falcon-40b,"For deploying on SageMaker, we recommend having a look [here](https://www.philschmid.de/sagemaker-falcon-llm).

Otherwise, the HF [blogpost](https://huggingface.co/blog/falcon) contains useful pointers."
JayalekshmiGopakumar,2023-06-25 22:33:04+00:00,comment,tiiuae/falcon-40b,"Hi I aml using Hugginggface estimator for the training job and I get this same error.I am using the transformers gitrepo tokenclassification ,run_ner.py as the entry point.But the it throws the following error.

ValueError: Loading tiiuae/falcon-40b requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."" Command ""/opt/conda/bin/python3.9 run_ner.py --dataset_name JayalekshmiGopakumar/falcon_doclayn, exit code: 1"
aovalle,2023-10-18 12:50:56+00:00,comment,tiiuae/falcon-40b,"I had the same issue,  resolved it with updating to transformers==4.34.0"
airtable,2023-05-27 09:33:27+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
beothorn,2023-05-27 21:33:28+00:00,comment,tiiuae/falcon-40b,"I get the same error only by running the ""How to Get Started with the Model"""
bharven,2023-05-27 23:46:02+00:00,comment,tiiuae/falcon-40b,+1 I also get this error
Minami-su,2023-05-28 04:39:19+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
Talha,2023-05-28 06:35:21+00:00,comment,tiiuae/falcon-40b,me too
beothorn,2023-05-28 06:52:24+00:00,comment,tiiuae/falcon-40b,@Seledorn :)
Minami-su,2023-05-28 07:19:07+00:00,comment,tiiuae/falcon-40b,"and this：
File ~/.cache/huggingface/modules/transformers_modules/falcon40b/modelling_RW.py:32, in Linear.forward(self, input)
     31 def forward(self, input: torch.Tensor) -> torch.Tensor:
---> 32     ret = input @ self.weight.T
     33     if self.bias is None:
     34         return ret

RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
WajihUllahBaig,2023-05-28 12:00:11+00:00,comment,tiiuae/falcon-40b,This comment has been hidden
isrouush,2023-05-28 18:45:46+00:00,comment,tiiuae/falcon-40b,same problem here!
manuelrech,2023-05-29 16:25:51+00:00,comment,tiiuae/falcon-40b,"same here with the how to get started
model: tiiuae/falcon-7b-instruct"
FalconLLM,2023-05-30 05:59:17+00:00,comment,tiiuae/falcon-40b,"Sorry about the delay, the `The model 'RWForCausalLM' is not supported for text-generation` comes from the model not being integrated into the core part of the transformers library yet. It's just a warning, and generation should follow afterwards.  See for example: https://twitter.com/camenduru/status/1662225039352283137?s=20 of a video where it is working correctly. 

It will take a little bit of time to integrate the model fully into the transformers library, but hopefully in a couple of weeks this warning will go away."
airtable,2023-05-30 09:40:19+00:00,comment,tiiuae/falcon-40b,"@FalconLLM Thanks, Falcon-7B is generating data but I am unable to load Falcon-40B on a 1xNvidia H100 GPU with 80 VRAM, opening a separate issue"
YannDubs,2023-07-05 15:43:56+00:00,comment,tiiuae/falcon-40b,"@FalconLLM any updates on this issue?
"
neuralworm,2023-07-18 10:42:36+00:00,comment,tiiuae/falcon-40b,For me it was resolved with <code>pip install git+https://github.com/huggingface/transformers</code>
matheusalb,2023-08-04 06:07:43+00:00,comment,tiiuae/falcon-40b,"> For me it was resolved with <code>pip install git+https://github.com/huggingface/transformers</code>

It worked for me as well. Thanks!!"
audioscavenger,2023-08-07 02:52:07+00:00,comment,tiiuae/falcon-40b,"same here. thanks
> For me it was resolved with <code>pip install git+https://github.com/huggingface/transformers</code>

"
Dehmax,2023-08-08 08:17:38+00:00,comment,tiiuae/falcon-40b,"> For me it was resolved with <code>pip install git+https://github.com/huggingface/transformers</code>

Same for me. It also speed up inference drastically for the 7b-instruct model. Thanks a lot! "
k3ybladewielder,2023-08-14 23:46:30+00:00,comment,tiiuae/falcon-40b,Still getting this issue
amarahiqbal,2023-09-28 09:44:27+00:00,comment,tiiuae/falcon-40b,"Its not working for text generation. It says AttributeError: module transformers has no attribute RWForCausalLM
"
zqbok,2023-05-28 14:13:34+00:00,comment,tiiuae/falcon-40b,Set 'return_token_type_ids=False' in tokenizer()
Imran1,2023-05-28 14:14:35+00:00,comment,tiiuae/falcon-40b,"> Set 'return_token_type_ids=False' in tokenizer()

Thank you ."
AryanShiv46,2023-08-12 05:28:17+00:00,comment,tiiuae/falcon-40b,did it solve for you?
hanfang,2023-08-19 10:13:54+00:00,comment,tiiuae/falcon-40b,"Thank you！
> Set 'return_token_type_ids=False' in tokenizer()

"
FalconLLM,2023-05-30 15:01:21+00:00,comment,tiiuae/falcon-40b,There is an upcoming integration in `text-generation-inference` that should be lightning fast: https://github.com/huggingface/text-generation-inference/pull/379  :)
FalconLLM,2023-05-30 07:13:24+00:00,comment,tiiuae/falcon-40b,"Hi Emmanuel,

We have experimented internally with finetuning to longer sequence lengths, and it works quite well. Would be happy to see some community versions of the models exploring this idea 🤗. "
Chr-Hau,2023-06-19 07:15:14+00:00,comment,tiiuae/falcon-40b,"Are there any plans to release some of the experiments?
Is it possible to use AliBI for longer sequences or is this a dead end?"
abhinavg86,2023-07-14 02:17:10+00:00,comment,tiiuae/falcon-40b,"Yeah, the 2048 limit is too short, we are working with 30k tokens and need help to move ahead on that"
Chr-Hau,2023-07-21 06:30:47+00:00,comment,tiiuae/falcon-40b,is there anyway I could contribute to that?
maccam912,2023-05-26 21:01:17+00:00,comment,tiiuae/falcon-40b,"Possibly related, I get `The model 'RWForCausalLM' is not supported for text-generation`

I do see that this warning pops up on 7b, which goes on to work fine, so might be a misleading warning here, just thought I'd share it."
leonlahoud,2023-05-26 21:20:51+00:00,comment,tiiuae/falcon-40b,"the Model doesn't work. I get the same error on 40B

ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following classes: (<class 
'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 
'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
catid,2023-05-26 21:35:33+00:00,comment,tiiuae/falcon-40b,Oh good thought I was just doing something dumb
Ichigo2899,2023-05-26 22:36:59+00:00,comment,tiiuae/falcon-40b,"I am able to run the model on my end but the answer just keeps going and does not end. Also pretty slow in streaming response. Running on 96gb 4 A10G's. 

model = AutoModelForCausalLM.from_pretrained(mname,trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')

loading like this and im getting error after one answer:
RuntimeError: The size of tensor a (9) must match the size of tensor b (488) at non-singleton dimension 1"
Themis,2023-05-27 07:14:37+00:00,comment,tiiuae/falcon-40b,Have we solved the problem?
vs4vijay,2023-05-27 08:00:58+00:00,comment,tiiuae/falcon-40b,"Facing the same issue, how do I solve?

```
ValueError: Could not load model tiiuae/falcon-7b with any of the following classes: (<class
'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).
```"
JBHF,2023-05-27 10:34:11+00:00,comment,tiiuae/falcon-40b,"Whe using this code:

# https://huggingface.co/tiiuae/falcon-40b
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = ""tiiuae/falcon-40b"" # Gebruik evt het kleinere broertje: tiiuae/falcon-7b

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")


I get this output:

Downloading (…)okenizer_config.json: 100%
175/175 [00:00<00:00, 6.61kB/s]
Downloading (…)/main/tokenizer.json: 100%
2.73M/2.73M [00:00<00:00, 5.61MB/s]
Downloading (…)cial_tokens_map.json: 100%
281/281 [00:00<00:00, 1.34kB/s]
Downloading (…)lve/main/config.json: 100%
656/656 [00:00<00:00, 947B/s]
Downloading (…)/configuration_RW.py: 100%
2.51k/2.51k [00:00<00:00, 3.46kB/s]
A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
- configuration_RW.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading (…)main/modelling_RW.py: 100%
47.1k/47.1k [00:00<00:00, 108kB/s]
A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
- modelling_RW.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Downloading (…)model.bin.index.json: 100%
39.3k/39.3k [00:00<00:00, 697kB/s]
Downloading shards: 67%
6/9 [05:54<02:52, 57.54s/it]
Downloading (…)l-00001-of-00009.bin: 100%
9.50G/9.50G [00:46<00:00, 258MB/s]
Downloading (…)l-00002-of-00009.bin: 100%
9.51G/9.51G [01:14<00:00, 257MB/s]
Downloading (…)l-00003-of-00009.bin: 100%
9.51G/9.51G [00:50<00:00, 262MB/s]
Downloading (…)l-00004-of-00009.bin: 100%
9.51G/9.51G [00:55<00:00, 246MB/s]
Downloading (…)l-00005-of-00009.bin: 100%
9.51G/9.51G [00:57<00:00, 224MB/s]
Downloading (…)l-00006-of-00009.bin: 100%
9.51G/9.51G [00:58<00:00, 170MB/s]
Downloading (…)l-00007-of-00009.bin: 18%
1.74G/9.51G [00:12<00:44, 174MB/s]
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ in <cell line: 13>:13                                                                            │
│                                                                                                  │
│ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788 in pipeline       │
│                                                                                                  │
│   785 │   # Forced if framework already defined, inferred if it's None                           │
│   786 │   # Will load the correct model if possible                                              │
│   787 │   model_classes = {""tf"": targeted_task[""tf""], ""pt"": targeted_task[""pt""]}                 │
│ ❱ 788 │   framework, model = infer_framework_load_model(                                         │
│   789 │   │   model,                                                                             │
│   790 │   │   model_classes=model_classes,                                                       │
│   791 │   │   config=config,                                                                     │
│                                                                                                  │
│ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279 in                    │
│ infer_framework_load_model                                                                       │
│                                                                                                  │
│    276 │   │   │   │   continue                                                                  │
│    277 │   │                                                                                     │
│    278 │   │   if isinstance(model, str):                                                        │
│ ❱  279 │   │   │   raise ValueError(f""Could not load model {model} with any of the following cl  │
│    280 │                                                                                         │
│    281 │   framework = ""tf"" if ""keras.engine.training.Model"" in str(inspect.getmro(model.__clas  │
│    282 │   return framework, model                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: Could not load model tiiuae/falcon-40b with any of the following classes: (<class 
'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 
'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
oystini,2023-05-27 19:17:44+00:00,comment,tiiuae/falcon-40b,"> Whe using this code:
> 
> # https://huggingface.co/tiiuae/falcon-40b
> from transformers import AutoTokenizer, AutoModelForCausalLM
> import transformers
> import torch
> 
> model = ""tiiuae/falcon-40b"" # Gebruik evt het kleinere broertje: tiiuae/falcon-7b
> 
> tokenizer = AutoTokenizer.from_pretrained(model)
> pipeline = transformers.pipeline(
>     ""text-generation"",
>     model=model,
>     tokenizer=tokenizer,
>     torch_dtype=torch.bfloat16,
>     trust_remote_code=True,
>     device_map=""auto"",
> )
> sequences = pipeline(
>    ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
>     max_length=200,
>     do_sample=True,
>     top_k=10,
>     num_return_sequences=1,
>     eos_token_id=tokenizer.eos_token_id,
> )
> for seq in sequences:
>     print(f""Result: {seq['generated_text']}"")
> 
> 
> I get this output:
> 
> Downloading (…)okenizer_config.json: 100%
> 175/175 [00:00<00:00, 6.61kB/s]
> Downloading (…)/main/tokenizer.json: 100%
> 2.73M/2.73M [00:00<00:00, 5.61MB/s]
> Downloading (…)cial_tokens_map.json: 100%
> 281/281 [00:00<00:00, 1.34kB/s]
> Downloading (…)lve/main/config.json: 100%
> 656/656 [00:00<00:00, 947B/s]
> Downloading (…)/configuration_RW.py: 100%
> 2.51k/2.51k [00:00<00:00, 3.46kB/s]
> A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
> - configuration_RW.py
> . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
> Downloading (…)main/modelling_RW.py: 100%
> 47.1k/47.1k [00:00<00:00, 108kB/s]
> A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
> - modelling_RW.py
> . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
> Downloading (…)model.bin.index.json: 100%
> 39.3k/39.3k [00:00<00:00, 697kB/s]
> Downloading shards: 67%
> 6/9 [05:54<02:52, 57.54s/it]
> Downloading (…)l-00001-of-00009.bin: 100%
> 9.50G/9.50G [00:46<00:00, 258MB/s]
> Downloading (…)l-00002-of-00009.bin: 100%
> 9.51G/9.51G [01:14<00:00, 257MB/s]
> Downloading (…)l-00003-of-00009.bin: 100%
> 9.51G/9.51G [00:50<00:00, 262MB/s]
> Downloading (…)l-00004-of-00009.bin: 100%
> 9.51G/9.51G [00:55<00:00, 246MB/s]
> Downloading (…)l-00005-of-00009.bin: 100%
> 9.51G/9.51G [00:57<00:00, 224MB/s]
> Downloading (…)l-00006-of-00009.bin: 100%
> 9.51G/9.51G [00:58<00:00, 170MB/s]
> Downloading (…)l-00007-of-00009.bin: 18%
> 1.74G/9.51G [00:12<00:44, 174MB/s]
> ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
> │ in <cell line: 13>:13                                                                            │
> │                                                                                                  │
> │ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788 in pipeline       │
> │                                                                                                  │
> │   785 │   # Forced if framework already defined, inferred if it's None                           │
> │   786 │   # Will load the correct model if possible                                              │
> │   787 │   model_classes = {""tf"": targeted_task[""tf""], ""pt"": targeted_task[""pt""]}                 │
> │ ❱ 788 │   framework, model = infer_framework_load_model(                                         │
> │   789 │   │   model,                                                                             │
> │   790 │   │   model_classes=model_classes,                                                       │
> │   791 │   │   config=config,                                                                     │
> │                                                                                                  │
> │ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279 in                    │
> │ infer_framework_load_model                                                                       │
> │                                                                                                  │
> │    276 │   │   │   │   continue                                                                  │
> │    277 │   │                                                                                     │
> │    278 │   │   if isinstance(model, str):                                                        │
> │ ❱  279 │   │   │   raise ValueError(f""Could not load model {model} with any of the following cl  │
> │    280 │                                                                                         │
> │    281 │   framework = ""tf"" if ""keras.engine.training.Model"" in str(inspect.getmro(model.__clas  │
> │    282 │   return framework, model                                                               │
> ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
> ValueError: Could not load model tiiuae/falcon-40b with any of the following classes: (<class 
> 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 
> 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).

I got the same bug in google colab. Switched to using GPU and then it worked fine."
leonlahoud,2023-05-27 20:21:37+00:00,comment,tiiuae/falcon-40b,"now I see this using the GPT V100 on colab:

The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
Setting `pad_token_id` to `eos_token_id`:11 for open-end generation."
Sloba,2023-05-28 12:11:09+00:00,comment,tiiuae/falcon-40b,"Same problem (ValueError: Could not load model tiiuae/falcon-40b with any of the following classes: (<class
'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class
'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>))

 happens as well when running with downloaded model using code below:

```
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = ""X:\\ai\\falcon-40b""

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    offload_folder=""N:\AI\offload_folder"",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")
```


Following code gets it to run, but it never (5+ min) outputs result (on 3090 24Gb):
```

model = ""X:\\ai\\falcon-40b""
rrmodel = AutoModelForCausalLM.from_pretrained(model, 
    offload_folder=""N:\AI\offload_folder"",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",)

tokenizer = AutoTokenizer.from_pretrained(model)

# Define the input text
input_text = ""What is girrafe?""
input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

attention_mask = torch.ones(input_ids.shape).to(device)

# Generate text
output = rrmodel.generate(input_ids,      
            attention_mask=attention_mask,
            max_length=200,
            do_sample=True,
            top_k=10,
            pad_token_id=tokenizer.pad_token_id,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,)

# Decode the output
output_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(output_text)

```"
leonlahoud,2023-05-28 13:20:08+00:00,comment,tiiuae/falcon-40b,Now it is taking my entire space in my colab pro tier. anyone knows how much space it is supposed to take? And how long does it take to run it on colab pro with v100?
maccam912,2023-05-28 14:41:44+00:00,comment,tiiuae/falcon-40b,"> Now it is taking my entire space in my colab pro tier. anyone knows how much space it is supposed to take? And how long does it take to run it on colab pro with v100?

It will take 80 GB of VRAM or so, plus some extra for overhead (it doesn't fit in a single A100, with 80 GB memory). But you can try https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ which should take only 20 GB of VRAM."
leonlahoud,2023-05-28 15:55:31+00:00,comment,tiiuae/falcon-40b,@maccam912  how much space does it take on the colab?
JBHF,2023-05-29 06:09:11+00:00,comment,tiiuae/falcon-40b,"> > Whe using this code:
> > 
> > # https://huggingface.co/tiiuae/falcon-40b
> > from transformers import AutoTokenizer, AutoModelForCausalLM
> > import transformers
> > import torch
> > 
> > model = ""tiiuae/falcon-40b"" # Gebruik evt het kleinere broertje: tiiuae/falcon-7b
> > 
> > tokenizer = AutoTokenizer.from_pretrained(model)
> > pipeline = transformers.pipeline(
> >     ""text-generation"",
> >     model=model,
> >     tokenizer=tokenizer,
> >     torch_dtype=torch.bfloat16,
> >     trust_remote_code=True,
> >     device_map=""auto"",
> > )
> > sequences = pipeline(
> >    ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
> >     max_length=200,
> >     do_sample=True,
> >     top_k=10,
> >     num_return_sequences=1,
> >     eos_token_id=tokenizer.eos_token_id,
> > )
> > for seq in sequences:
> >     print(f""Result: {seq['generated_text']}"")
> > 
> > 
> > I get this output:
> > 
> > Downloading (…)okenizer_config.json: 100%
> > 175/175 [00:00<00:00, 6.61kB/s]
> > Downloading (…)/main/tokenizer.json: 100%
> > 2.73M/2.73M [00:00<00:00, 5.61MB/s]
> > Downloading (…)cial_tokens_map.json: 100%
> > 281/281 [00:00<00:00, 1.34kB/s]
> > Downloading (…)lve/main/config.json: 100%
> > 656/656 [00:00<00:00, 947B/s]
> > Downloading (…)/configuration_RW.py: 100%
> > 2.51k/2.51k [00:00<00:00, 3.46kB/s]
> > A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
> > - configuration_RW.py
> > . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
> > Downloading (…)main/modelling_RW.py: 100%
> > 47.1k/47.1k [00:00<00:00, 108kB/s]
> > A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
> > - modelling_RW.py
> > . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
> > Downloading (…)model.bin.index.json: 100%
> > 39.3k/39.3k [00:00<00:00, 697kB/s]
> > Downloading shards: 67%
> > 6/9 [05:54<02:52, 57.54s/it]
> > Downloading (…)l-00001-of-00009.bin: 100%
> > 9.50G/9.50G [00:46<00:00, 258MB/s]
> > Downloading (…)l-00002-of-00009.bin: 100%
> > 9.51G/9.51G [01:14<00:00, 257MB/s]
> > Downloading (…)l-00003-of-00009.bin: 100%
> > 9.51G/9.51G [00:50<00:00, 262MB/s]
> > Downloading (…)l-00004-of-00009.bin: 100%
> > 9.51G/9.51G [00:55<00:00, 246MB/s]
> > Downloading (…)l-00005-of-00009.bin: 100%
> > 9.51G/9.51G [00:57<00:00, 224MB/s]
> > Downloading (…)l-00006-of-00009.bin: 100%
> > 9.51G/9.51G [00:58<00:00, 170MB/s]
> > Downloading (…)l-00007-of-00009.bin: 18%
> > 1.74G/9.51G [00:12<00:44, 174MB/s]
> > ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
> > │ in <cell line: 13>:13                                                                            │
> > │                                                                                                  │
> > │ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788 in pipeline       │
> > │                                                                                                  │
> > │   785 │   # Forced if framework already defined, inferred if it's None                           │
> > │   786 │   # Will load the correct model if possible                                              │
> > │   787 │   model_classes = {""tf"": targeted_task[""tf""], ""pt"": targeted_task[""pt""]}                 │
> > │ ❱ 788 │   framework, model = infer_framework_load_model(                                         │
> > │   789 │   │   model,                                                                             │
> > │   790 │   │   model_classes=model_classes,                                                       │
> > │   791 │   │   config=config,                                                                     │
> > │                                                                                                  │
> > │ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279 in                    │
> > │ infer_framework_load_model                                                                       │
> > │                                                                                                  │
> > │    276 │   │   │   │   continue                                                                  │
> > │    277 │   │                                                                                     │
> > │    278 │   │   if isinstance(model, str):                                                        │
> > │ ❱  279 │   │   │   raise ValueError(f""Could not load model {model} with any of the following cl  │
> > │    280 │                                                                                         │
> > │    281 │   framework = ""tf"" if ""keras.engine.training.Model"" in str(inspect.getmro(model.__clas  │
> > │    282 │   return framework, model                                                               │
> > ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
> > ValueError: Could not load model tiiuae/falcon-40b with any of the following classes: (<class 
> > 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 
> > 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).
> 
> I got the same bug in google colab. Switched to using GPU and then it worked fine.

I am running this on Google COLAB (free version).
When I switch to GPU (GPU T4 Runtime in COLAB) I still get this error.
Also I tried switching to TPU on COLAB (which is possible because of the use of the accelerate lib !), I still get the same error."
airtable,2023-05-30 09:44:43+00:00,comment,tiiuae/falcon-40b,"> > Now it is taking my entire space in my colab pro tier. anyone knows how much space it is supposed to take? And how long does it take to run it on colab pro with v100?
> 
> It will take 80 GB of VRAM or so, plus some extra for overhead (it doesn't fit in a single A100, with 80 GB memory). But you can try https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ which should take only 20 GB of VRAM.

@leonlahoud @maccam912 Falcon-40B doesn't load on 1xNvidia H100 GPU with 80 GB VRAM, Falcon-7B works though I don't like answer repetition."
eek,2023-05-30 13:31:56+00:00,comment,tiiuae/falcon-40b,"I also have issues with running Falcon-40B on 1xH100 GPU with 80GB of VRAM using 8-bit quantization. It fails with `Exception: cublasLt ran into an error!` tried with both everything built for CUDA 11.8 and CUDA 12.1, and still fails, even though bitsandbytes says everything is ok."
sumitagrawal,2023-05-30 18:41:05+00:00,comment,tiiuae/falcon-40b,"Same error at my end also.
```
 % python falcon-demo.py
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Downloading (…)l-00007-of-00009.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 9.51G/9.51G [20:08<00:00, 7.87MB/s]
Downloading (…)l-00008-of-00009.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 9.51G/9.51G [18:28<00:00, 8.58MB/s]
Downloading (…)l-00009-of-00009.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 7.58G/7.58G [18:49<00:00, 6.71MB/s]
Downloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [57:32<00:00, 383.65s/it]
Traceback (most recent call last):
  File ""/LLM/falcon-demo.py"", line 10, in <module>
    pipeline = transformers.pipeline(
  File ""/Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py"", line 779, in pipeline
    framework, model = infer_framework_load_model(
  File ""/Users/sumitagrawal/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py"", line 271, in infer_framework_load_model
    raise ValueError(f""Could not load model {model} with any of the following classes: {class_tuple}."")
ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).
```
Reran it, this is the result again.
```
% python falcon-demo.py
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Traceback (most recent call last):
  File ""/LLM/falcon-demo.py"", line 10, in <module>
    pipeline = transformers.pipeline(
  File ""/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py"", line 779, in pipeline
    framework, model = infer_framework_load_model(
  File ""/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py"", line 271, in infer_framework_load_model
    raise ValueError(f""Could not load model {model} with any of the following classes: {class_tuple}."")
ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).
```"
alexwall77,2023-05-31 01:36:31+00:00,comment,tiiuae/falcon-40b,"I can't run this on my machine because I don't have the hardware, but I was able to get past the above errors by adjusting the code as follows, specifically the model = line:
```
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b"", trust_remote_code=True)



pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")
```"
Kitachan,2023-05-31 02:57:27+00:00,comment,tiiuae/falcon-40b,Does anyone know how to move the .cache file generated by the model data download from C drive to another drive? My C drive doesn't have that much space =)
alexwall77,2023-05-31 03:03:35+00:00,comment,tiiuae/falcon-40b,"Possibly by adding this to your python script that runs the transformer 
```
cache_dir = ""/path/to/new/cache_directory""
CacheConfig().cache_dir = cache_dir
```"
Kitachan,2023-05-31 03:10:42+00:00,comment,tiiuae/falcon-40b,"> Possibly by adding this to your python script that runs the transformer 
> ```
> cache_dir = ""/path/to/new/cache_directory""
> CacheConfig().cache_dir = cache_dir
> ```
I tried doing this but it doesn't seem to work"
FalconLLM,2023-05-31 04:03:56+00:00,comment,tiiuae/falcon-40b,"@Kitachan You should be able to set environment variables `HUGGINGFACE_HUB_CACHE` or `HF_HOME` to where you want the cache to be. 

See https://huggingface.co/docs/huggingface_hub/guides/manage-cache"
Kitachan,2023-05-31 06:28:09+00:00,comment,tiiuae/falcon-40b,"> @Kitachan You should be able to set environment variables `HUGGINGFACE_HUB_CACHE` or `HF_HOME` to where you want the cache to be. 
> 
> See https://huggingface.co/docs/huggingface_hub/guides/manage-cache

This did work for my problem, thanks"
phdykd,2023-05-31 14:27:42+00:00,comment,tiiuae/falcon-40b,"I am having the same issue for Falcon 40b instruct. See the error below. Any solution?

ValueError                                Traceback (most recent call last)
Cell In[2], line 8
      5 model = ""tiiuae/falcon-40b-instruct""
      7 tokenizer = AutoTokenizer.from_pretrained(model)
----> 8 pipeline = transformers.pipeline(
      9     ""text-generation"",
     10     model=model,
     11     tokenizer=tokenizer,
     12     torch_dtype=torch.bfloat16,
     13     trust_remote_code=True,
     14     device_map=""auto"",
     15 )

File ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/__init__.py:788, in pipeline(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    784 # Infer the framework from the model
    785 # Forced if framework already defined, inferred if it's None
    786 # Will load the correct model if possible
    787 model_classes = {""tf"": targeted_task[""tf""], ""pt"": targeted_task[""pt""]}
--> 788 framework, model = infer_framework_load_model(
    789     model,
    790     model_classes=model_classes,
    791     config=config,
    792     framework=framework,
    793     task=task,
    794     **hub_kwargs,
    795     **model_kwargs,
    796 )
    798 model_config = model.config
    799 hub_kwargs[""_commit_hash""] = model.config._commit_hash

File ~/anaconda3/envs/Falcon/lib/python3.10/site-packages/transformers/pipelines/base.py:279, in infer_framework_load_model(model, config, model_classes, task, framework, **model_kwargs)
    276             continue
    278     if isinstance(model, str):
--> 279         raise ValueError(f""Could not load model {model} with any of the following classes: {class_tuple}."")
    281 framework = ""tf"" if ""keras.engine.training.Model"" in str(inspect.getmro(model.__class__)) else ""pt""
    282 return framework, model

ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
Kitachan,2023-05-31 15:00:13+00:00,comment,tiiuae/falcon-40b,"hummm I was wondering how much VM I needed to run it, and my computer didn't seem to have enough RAM =( Maybe I should try google colab, but it feels cumbersome to have to re-download the model every time"
phdykd,2023-05-31 15:05:07+00:00,comment,tiiuae/falcon-40b,"I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was downloading .bin, but then stopped and gave the error mentioned above. Still waiting someone to answer it."
philmui,2023-06-01 01:53:26+00:00,comment,tiiuae/falcon-40b,"M2max with 96GB -- definitely not a machine / hardware issue!

Same error as many of you:

> raise ValueError(f""Could not load model {model} with any of the following classes: {class_tuple}."")
ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
ashreim,2023-06-01 05:38:59+00:00,comment,tiiuae/falcon-40b,"> Does anyone know how to move the .cache file generated by the model data download from C drive to another drive? My C drive doesn't have that much space =)

move your cache to new directory  
and use this:
import os
os.environ['HF_HOME'] = '<drive>:\\<xxxxxxxxx>/HuggingFace'"
Kitachan,2023-06-01 07:26:27+00:00,comment,tiiuae/falcon-40b,"> > Does anyone know how to move the .cache file generated by the model data download from C drive to another drive? My C drive doesn't have that much space =)
> 
> move your cache to new directory  
> and use this:
> import os
> os.environ['HF_HOME'] = '<drive>:\\<xxxxxxxxx>/HuggingFace'

It really works, thx =)"
joorei,2023-06-01 08:53:22+00:00,comment,tiiuae/falcon-40b,Same here. :(
alephpt,2023-06-01 15:03:19+00:00,comment,tiiuae/falcon-40b,"Was this problem fixed?

I have a Ryzen 9 7900X 12 core, and 64GB of Ram with 2TB of disk space.. everything downloaded but doesn't run and gets the same error as OP."
phdykd,2023-06-01 15:05:04+00:00,comment,tiiuae/falcon-40b,"I am on M2 max chip, 12 CPU, 38 GPU, 96GB PROCESSOR, 2 TB. It was downloading .bin, but then stopped and gave the error mentioned above. Still waiting someone to answer it. 

It has not been resolved yet..."
Sloba,2023-06-01 16:53:23+00:00,comment,tiiuae/falcon-40b,"On AWS g5.12xlarge with 96Gb of VRAM and Deep Learning AMI GPU PyTorch 2.0.0 (Ubuntu 20.04) 20230530 it works. ~5$/hour. storage should be 100gb.

Here is hopefully complete steps from 0 to running Falcon-40 (it was tested on Falcon-40-instruct):
On AWS g5.12xlarge with 96Gb of VRAM and Deep Learning AMI GPU PyTorch 2.0.0 (Ubuntu 20.04) 20230530 it worked using following code:
1. Assuming you know how to create new EC2 instance and set it up. Lets say you name your EC2 instance ""GpuTest"".
2. Open port 8888 toward your EC instance (i.e. ""GpuTest"")(this is how you will access the Jupyter notebook).
3. You need to get PEM key. Watch out for chmod and file privileges, it may complain. If complain, I think ""chmod 400 your_key.pem"" will fix it (group and others should be no access).
4. ssh -i ""name of the pem key"" ubuntu@dns-address-of-your-ec2-instance.com (please use the correct one).
5. conda init
6. ""restart bash"" or ""ctrl-c"" or ""exit"" and reenter instance (using step 4.)
7. conda activate pytorch
8. jupyter notebook --no-browser
9. open second terminal and execute: ssh -i ""name of the pem key"" -L localhost:8888:localhost:8888 ubuntu@dns-address-of-your-ec2-instance.com (this creates SSH tunnel from your computer to running instance). It will not work without this.
10. At this point you should be able to go to your local browser and type: http://127.0.0.1:8888 (or http://localhost:8888) and connect to jupyter (it will ask for token, this token should be visible as a result of step 8.) on ""GpuTest"" instance.
11. Now comes the fun part:
12. pip install transformers
13. pip install einops
14. pip install accelerate
15. pip install xformers

At this point provided sample worked perfectly.

There will be a big business in AI cards with 100Gb on them."
phdykd,2023-06-01 17:09:03+00:00,comment,tiiuae/falcon-40b,"Hi @Sloba ,
I am not using AWS, but M2 max chip MacBook. Do you have any recommendation for MacOS systems that does not have NVIDIA, but has mps (CPU AND GPU as platform computer)?
Thanks"
Sloba,2023-06-01 17:19:00+00:00,comment,tiiuae/falcon-40b,"Hi @phdykd , unfortunately no. I think quantization will be needed due to size."
chnlyi,2023-06-02 02:02:54+00:00,comment,tiiuae/falcon-40b,"> I can't run this on my machine because I don't have the hardware, but I was able to get past the above errors by adjusting the code as follows, specifically the model = line:
> ```
> from transformers import AutoTokenizer, AutoModelForCausalLM
> import transformers
> import torch
> 
> model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b"", trust_remote_code=True)
> 
> 
> 
> pipeline = transformers.pipeline(
>     ""text-generation"",
>     model=model,
>     tokenizer=tokenizer,
>     torch_dtype=torch.bfloat16,
>     trust_remote_code=True,
>     device_map=""auto"",
> )
> sequences = pipeline(
>    ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
>     max_length=200,
>     do_sample=True,
>     top_k=10,
>     num_return_sequences=1,
>     eos_token_id=tokenizer.eos_token_id,
> )
> for seq in sequences:
>     print(f""Result: {seq['generated_text']}"")
> ```

I tried your method and the code worked. However, same as you my hardware could not support (only 8gb GPU).
Here is my modification:
```
model = AutoModelForCausalLM.from_pretrained(
    ""tiiuae/falcon-40b-instruct"", 
    trust_remote_code=True,
    )
tokenizer = AutoTokenizer.from_pretrained(model)
```"
DrNicefellow,2023-06-04 01:50:41+00:00,comment,tiiuae/falcon-40b,"To those who have questions not related to this, please go start another discussion. Here it is to discuss ValueError: Could not load model tiiuae/falcon-40b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). Anyone knows how to solve this problem?"
chrishare,2023-06-04 07:40:08+00:00,comment,tiiuae/falcon-40b,"Newbie here, so take this with a grain of salt. In my case, I was running out of GPU memory. Remember, even 'small' 7B models can require 14Gb memory during inference without optimization (e.g. quantization) and 16bit floats - so it's real easy to max out even modern GPUs. To confirm that is the case for you, you want to track memory usage during training (or inference). There are a bunch of easy ways to do that and rule that in or out that depend on your setup:

```
# Baseline GPU memory
nvidia-smi
nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu
# Watch GPU memory interactively
watch nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu
nvidia-smi -l 1
# Use HF trainer callbacks to integration with WanDB etc - https://huggingface.co/transformers/v3.4.0/main_classes/callback.html#available-callbacks
# Use Colab's Runtime > Manage sessions UI to watch what the usage is, interactively
```

There could be other reasons to hit this error, I am sure - for instance, I tried 7B on a TPU and that seemed to fail as well. But experimenting with memory usage was how I got through this on a 16Gb GPU (Colab T4 Tesla) for a 7B model."
mahmoudajawad,2023-06-05 07:41:35+00:00,comment,tiiuae/falcon-40b,"Agree @Mlemoyne, this topic has pivoted. 
I'm having the same issue. I'm running this on SageMaker but doesn't seem to work. I'll consider giving Lambda Labs a shot later on to see whether this error is related or not."
cw-franabellan,2023-06-05 20:04:02+00:00,comment,tiiuae/falcon-40b,"Just for the record, the same happens with model falcon-7b-instruct in Sagemaker:

ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
Nolyzlel,2023-06-07 10:13:50+00:00,comment,tiiuae/falcon-40b,"Can someone give a final solution to the error which is 

ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).

Thanks :'')"
4639-94d6,2023-06-07 13:26:11+00:00,comment,tiiuae/falcon-40b,"I tried to run the 7B model on CPU only and got this message, removing device_map=""auto"" from the pipeline seemed to solve it."
sehadnassim,2023-06-07 14:12:04+00:00,comment,tiiuae/falcon-40b,"> I tried to run the 7B model on CPU only and got this message, removing device_map=""auto"" from the pipeline seemed to solve it.
This removes the error but the process gets killed after a while. Do you know what are the minimum CPU requirements ?"
lawrence-algocat,2023-06-07 16:18:07+00:00,comment,tiiuae/falcon-40b,"removing device_map=""auto"" worked for me, the model loads on CPU.
still though, my colab VM crashes because of lack of RAM..."
4639-94d6,2023-06-08 07:45:39+00:00,comment,tiiuae/falcon-40b,"You are correct, I was a bit too quick with my answer, I do not recommend removing device_map due to the amount of ram it requires (it crashed for me as well after 20Gb allocated ram). Instead i found a solution by simply upgrading torch to 2.0.1, xformers to 0.0.20 and accelerate to 0.20.1
Now it runs fine for me with device_map=auto"
carlosmtoro,2023-06-08 20:56:59+00:00,comment,tiiuae/falcon-40b,"Hello, I have a computer with 64 GB of ram, two Xeon processors with 32 cores each. I did NOT enable graphics card (it is very small), I downloaded the falcon-40b model files and installed in Dockerfile:

ENV TRANSFORMERS_CACHE=/app/transformercache

RUN mkdir -p $TRANSFORMERS_CACHE

RUN pip3 install transformers

RUN pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cpu

RUN pip3 install einops
RUN pip3 install accelerate
RUN pip3 install xformers

When trying to test the script:

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

# This instruction loads the pre-trained model
model=""./model/falcon-40b""

rmodel = AutoModelForCausalLM.from_pretrained(
         models,
         offload_folder='/app/model/falcon-40b',
         trust_remote_code=True,
         device_map='auto',
         torch_dtype=torch.float16,
         low_cpu_mem_usage=True,
         chunk_size_feed_forward=512000,
         cache_dir = ""./transformercache""
)

Just exit python:

>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> import transformers
>>> import torch
>>> model=""./model/falcon-40b""
>>> rmodel = AutoModelForCausalLM.from_pretrained(
... models,
...offload_folder='/app/model/falcon-40b',
...trust_remote_code=True,
... device_map='auto',
...torch_dtype=torch.float16,
... low_cpu_mem_usage=True,
... chunk_size_feed_forward=512000,
... cache_dir = ""./transformercache""
... )
Loading checkpoint shards: 67%|███████████████████████████████████ ████████ ████████████████████████████████████████ ████ | 6/9 [01:04<00:32, 10.75s/it] Killed
root@75b049882bd4:/app#

I guess it's because of memory, does anyone have any idea what I can do?"
FalconLLM,2023-06-09 14:23:16+00:00,comment,tiiuae/falcon-40b,"We recommend having at least 80-100GB to fit the 40B model comfortably. 

If you do not have that much memory available, you can have a look at [FalconTune](https://github.com/rmihaylov/falcontune) to run the model in 4-bit, or at this [blogpost](https://huggingface.co/blog/falcon) from HuggingFace."
sehadnassim,2023-06-12 20:41:05+00:00,comment,tiiuae/falcon-40b,"still getting this error for 7b : ─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│                                                                                                  │
│ /home/sehadn1/falcon7b-modif.py:14 in <module>                                                   │
│                                                                                                  │
│   11 model = ""tiiuae/falcon-7b-instruct""                                                         │
│   12                                                                                             │
│   13 tokenizer = AutoTokenizer.from_pretrained(model)                                            │
│ ❱ 14 pipeline = transformers.pipeline(                                                           │
│   15 │   ""text-generation"",                                                                      │
│   16 │   model=model,                                                                            │
│   17 │   tokenizer=tokenizer,                                                                    │
│ /home/sehadn1/transformers/src/transformers/pipelines/__init__.py:788 in pipeline                │
│                                                                                                  │
│   785 │   # Forced if framework already defined, inferred if it's None                           │
│   786 │   # Will load the correct model if possible                                              │
│   787 │   model_classes = {""tf"": targeted_task[""tf""], ""pt"": targeted_task[""pt""]}                 │
│ ❱ 788 │   framework, model = infer_framework_load_model(                                         │
│   789 │   │   model,                                                                             │
│   790 │   │   model_classes=model_classes,                                                       │
│   791 │   │   config=config,                                                                     │
│                                                                                                  │
│ /home/sehadn1/transformers/src/transformers/pipelines/base.py:278 in infer_framework_load_model  │
│                                                                                                  │
│    275 │   │   │   │   continue                                                                  │
│    276 │   │                                                                                     │
│    277 │   │   if isinstance(model, str):                                                        │
│ ❱  278 │   │   │   raise ValueError(f""Could not load model {model} with any of the following cl  │
│    279 │                                                                                         │
│    280 │   framework = infer_framework(model.__class__)                                          │
│    281 │   return framework, model                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 
'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>)."
lucky,2023-06-14 03:07:24+00:00,comment,tiiuae/falcon-40b,"> Possibly related, I get `The model 'RWForCausalLM' is not supported for text-generation`
> 
> I do see that this warning pops up on 7b, which goes on to work fine, so might be a misleading warning here, just thought I'd share it.

"
lucky,2023-06-14 03:08:07+00:00,comment,tiiuae/falcon-40b,"pip install transformers
pip install einops
pip install accelerate
pip install xformers

if you pip this package , it maybe ok"
lucky,2023-06-14 03:10:48+00:00,comment,tiiuae/falcon-40b,"pip install transformers
pip install einops
pip install accelerate
pip install xformers

if you pip this package , it maybe ok , the problem of  "" ValueError: Could not load model tiiuae/falcon-7b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class
'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>""    maybe solved
"
tommykoctur,2023-06-14 14:07:20+00:00,comment,tiiuae/falcon-40b,"I am loading model to A6000 GPU with 48GB ram. with torch.int8 . I am getting the same error.:
ValueError: Could not load model tiiuae/falcon-40b-instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,)."
phdykd,2023-06-14 19:42:22+00:00,comment,tiiuae/falcon-40b,"Kernel Restarting
The kernel for Desktop/LLM/Falcon/Fl.ipynb appears to have died. It will restart automatically.
it does not work on M2 Apple MacBook.

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

# model = ""tiiuae/falcon-40b""
model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b"", trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    # torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    # device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")"
patti-j,2023-06-18 22:44:03+00:00,comment,tiiuae/falcon-40b,"I was able to get past the AutoModelForCausalLM error in falcon-7b-instruct by using the line @alexwall77 provided below: 

model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b"", trust_remote_code=True)

Thank you, Alex!

> ```
> from transformers import AutoTokenizer, AutoModelForCausalLM
> import transformers
> import torch
> 
> model = AutoModelForCausalLM.from_pretrained(""tiiuae/falcon-40b"", trust_remote_code=True)
> 
> 
> 
> pipeline = transformers.pipeline(
>     ""text-generation"",
>     model=model,
>     tokenizer=tokenizer,
>     torch_dtype=torch.bfloat16,
>     trust_remote_code=True,
>     device_map=""auto"",
> )
> sequences = pipeline(
>    ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
>     max_length=200,
>     do_sample=True,
>     top_k=10,
>     num_return_sequences=1,
>     eos_token_id=tokenizer.eos_token_id,
> )
> for seq in sequences:
>     print(f""Result: {seq['generated_text']}"")
> ```

"
Andcircle,2023-06-19 20:02:13+00:00,comment,tiiuae/falcon-40b,"> > > Whe using this code:
> > > 
> > > # https://huggingface.co/tiiuae/falcon-40b
> > > from transformers import AutoTokenizer, AutoModelForCausalLM
> > > import transformers
> > > import torch
> > > 
> > > model = ""tiiuae/falcon-40b"" # Gebruik evt het kleinere broertje: tiiuae/falcon-7b
> > > 
> > > tokenizer = AutoTokenizer.from_pretrained(model)
> > > pipeline = transformers.pipeline(
> > >     ""text-generation"",
> > >     model=model,
> > >     tokenizer=tokenizer,
> > >     torch_dtype=torch.bfloat16,
> > >     trust_remote_code=True,
> > >     device_map=""auto"",
> > > )
> > > sequences = pipeline(
> > >    ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
> > >     max_length=200,
> > >     do_sample=True,
> > >     top_k=10,
> > >     num_return_sequences=1,
> > >     eos_token_id=tokenizer.eos_token_id,
> > > )
> > > for seq in sequences:
> > >     print(f""Result: {seq['generated_text']}"")
> > > 
> > > 
> > > I get this output:
> > > 
> > > Downloading (…)okenizer_config.json: 100%
> > > 175/175 [00:00<00:00, 6.61kB/s]
> > > Downloading (…)/main/tokenizer.json: 100%
> > > 2.73M/2.73M [00:00<00:00, 5.61MB/s]
> > > Downloading (…)cial_tokens_map.json: 100%
> > > 281/281 [00:00<00:00, 1.34kB/s]
> > > Downloading (…)lve/main/config.json: 100%
> > > 656/656 [00:00<00:00, 947B/s]
> > > Downloading (…)/configuration_RW.py: 100%
> > > 2.51k/2.51k [00:00<00:00, 3.46kB/s]
> > > A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
> > > - configuration_RW.py
> > > . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
> > > Downloading (…)main/modelling_RW.py: 100%
> > > 47.1k/47.1k [00:00<00:00, 108kB/s]
> > > A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-40b:
> > > - modelling_RW.py
> > > . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
> > > Downloading (…)model.bin.index.json: 100%
> > > 39.3k/39.3k [00:00<00:00, 697kB/s]
> > > Downloading shards: 67%
> > > 6/9 [05:54<02:52, 57.54s/it]
> > > Downloading (…)l-00001-of-00009.bin: 100%
> > > 9.50G/9.50G [00:46<00:00, 258MB/s]
> > > Downloading (…)l-00002-of-00009.bin: 100%
> > > 9.51G/9.51G [01:14<00:00, 257MB/s]
> > > Downloading (…)l-00003-of-00009.bin: 100%
> > > 9.51G/9.51G [00:50<00:00, 262MB/s]
> > > Downloading (…)l-00004-of-00009.bin: 100%
> > > 9.51G/9.51G [00:55<00:00, 246MB/s]
> > > Downloading (…)l-00005-of-00009.bin: 100%
> > > 9.51G/9.51G [00:57<00:00, 224MB/s]
> > > Downloading (…)l-00006-of-00009.bin: 100%
> > > 9.51G/9.51G [00:58<00:00, 170MB/s]
> > > Downloading (…)l-00007-of-00009.bin: 18%
> > > 1.74G/9.51G [00:12<00:44, 174MB/s]
> > > ╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
> > > │ in <cell line: 13>:13                                                                            │
> > > │                                                                                                  │
> > > │ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:788 in pipeline       │
> > > │                                                                                                  │
> > > │   785 │   # Forced if framework already defined, inferred if it's None                           │
> > > │   786 │   # Will load the correct model if possible                                              │
> > > │   787 │   model_classes = {""tf"": targeted_task[""tf""], ""pt"": targeted_task[""pt""]}                 │
> > > │ ❱ 788 │   framework, model = infer_framework_load_model(                                         │
> > > │   789 │   │   model,                                                                             │
> > > │   790 │   │   model_classes=model_classes,                                                       │
> > > │   791 │   │   config=config,                                                                     │
> > > │                                                                                                  │
> > > │ /usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:279 in                    │
> > > │ infer_framework_load_model                                                                       │
> > > │                                                                                                  │
> > > │    276 │   │   │   │   continue                                                                  │
> > > │    277 │   │                                                                                     │
> > > │    278 │   │   if isinstance(model, str):                                                        │
> > > │ ❱  279 │   │   │   raise ValueError(f""Could not load model {model} with any of the following cl  │
> > > │    280 │                                                                                         │
> > > │    281 │   framework = ""tf"" if ""keras.engine.training.Model"" in str(inspect.getmro(model.__clas  │
> > > │    282 │   return framework, model                                                               │
> > > ╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
> > > ValueError: Could not load model tiiuae/falcon-40b with any of the following classes: (<class 
> > > 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 
> > > 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>).
> > 
> > I got the same bug in google colab. Switched to using GPU and then it worked fine.
> 
> I am running this on Google COLAB (free version).
> When I switch to GPU (GPU T4 Runtime in COLAB) I still get this error.
> Also I tried switching to TPU on COLAB (which is possible because of the use of the accelerate lib !), I still get the same error.

How did you solve the problem? I run it on an EC2 with 8 A100, also got the same problem."
sumedh7-11,2023-07-02 06:22:43+00:00,comment,tiiuae/falcon-40b,"I am still getting the same error. Unable to load Falcon 40b instruct or Falcon 40b. This is the error
ValueError: Could not load model tiiuae/falcon-40b with any of the following classes: (<class 
'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,).

Also, I have enough space in RAM. It could be an issue with text generation. Any help on this."
RobeFranWBA,2023-07-19 10:35:58+00:00,comment,tiiuae/falcon-40b,"I found the solution to this.

I had to create a folder to offload the existing weights to to get it to work though which i named `""device_map_weights""`.

```from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = ""tiiuae/falcon-40b-instruct""

tokenizer = AutoTokenizer.from_pretrained(
    model,
    device_map=""auto"",
    trust_remote_code=True,
    offload_folder=""device_map_weights""
    )
model = AutoModelForCausalLM.from_pretrained(
    model,
    device_map=""auto"",
    trust_remote_code=True,
    offload_folder=""device_map_weights""
    )

pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")
```"
surya-narayanan,2023-08-23 20:37:15+00:00,comment,tiiuae/falcon-40b,have we solved the original issue? 
MaziyarPanahi,2023-05-26 17:59:36+00:00,comment,tiiuae/falcon-40b,"This would be very useful. If there are any special tokens to end the text, how to incorporate an external `context` that LLM should answer from, any good pre-prompt, etc."
FalconLLM,2023-05-30 07:07:03+00:00,comment,tiiuae/falcon-40b,"Hi Joshua and Maziyar,
This being a pretrained model, it does not support taking in instructions/conversations out-of-the-box. I would recommend you check-out  [Falcon-Instruct-40B](https://huggingface.co/tiiuae/falcon-40b-instruct) for that. Even in the case of Falcon-40B-Instruct, it supports a wide range of prompt formats, but you can see [this discussion](https://huggingface.co/tiiuae/falcon-7b-instruct/discussions/1) for more information."
DanielHesslow,2023-05-26 16:24:58+00:00,comment,tiiuae/falcon-40b,"Not at this time, this is a primarily an English only model. We've added a some European languages which are related and should not incur too much of a  performance penalty. 

High quality multilingual models are an interesting topic which I'm sure we will get back to at some point though."
HatemH,2023-05-26 18:30:51+00:00,comment,tiiuae/falcon-40b,"I was excited to hear that there was a model coming from an institution based in the UAE. I came here racing, expecting it to be versatile with Arabic, but was quite disappointed to find that it wasn't trained on it at all. Should we expect an upcoming version - in the near future - trained extensively on Arabic sources?"
hassanback,2023-05-27 10:28:27+00:00,comment,tiiuae/falcon-40b,Does it support swedish language as good as open AI does?
FalconLLM,2023-05-30 07:03:29+00:00,comment,tiiuae/falcon-40b,"Hi Kemal and Hatem, 

For this model, we focused on English first and foremost, and added European languages for which we could gather enough data in our web crawl. To avoid issues with tokenization, we only included European languages using the latin alphabet. 
We have also been working on state-of-the-art Arabic language models, and hopefully you get to hear about them soon 🤞.

@hassanback, we do not have good evaluation coverage in Swedish, so this is difficult to answer. Happy to hear back from you if you end up testing this!  "
TeaCult,2023-07-03 20:14:00+00:00,comment,tiiuae/falcon-40b,"First of all thank you very much for this model. 
Turkish is a European language with latin alphabet, Turkey and its culture is very different than Arabic countries (by far) .Secular, latin alphabet , no islamic rule , totally free and governed by law. And far more democratic than most of the western countries however which is not enough for citizen thats why people find it anti-democratic (it is not relatively). 

So I ve been giving a try to fine tune it with %15 of Stanford-alpaca instruction set translated to Turkish. It seems promising.  Would it differ to fine tune it afterwards using QLORA orther than pretrain it ? 
I am using instruction based json dataset. Would it be logical to give simple text such as wikipedia in Turkish, before giving instruction based data ? 

Btw it is going like this: 

Saving model checkpoint to ./falcon-40b-instruct-4bit-alpaca/checkpoint-5300
Trainer.model is not a `PreTrainedModel`, only saving its state dict.
Deleting older checkpoint [falcon-40b-instruct-4bit-alpaca/checkpoint-5150] due to args.save_total_limit
{'loss': 0.7811, 'learning_rate': 9.39177797950979e-05, 'epoch': 2.06}                                                                                                                                                                                                              
{'loss': 0.9781, 'learning_rate': 9.372325249643366e-05, 'epoch': 2.06}                                                                                                                                                                                                             
{'loss': 0.9802, 'learning_rate': 9.35287251977694e-05, 'epoch': 2.07}                                                                                                                                                                                                              
{'loss': 0.7647, 'learning_rate': 9.333419789910517e-05, 'epoch': 2.07}                                                                                                                                                                                                             
{'loss': 0.8621, 'learning_rate': 9.313967060044092e-05, 'epoch': 2.07}                                                                                                                                                                                                             
{'loss': 1.0175, 'learning_rate': 9.294514330177668e-05, 'epoch': 2.07}                                                                                                                                                                                                             
{'loss': 0.8003, 'learning_rate': 9.275061600311242e-05, 'epoch': 2.07}                                                                                                                                                                                                             
{'loss': 0.9179, 'learning_rate': 9.255608870444818e-05, 'epoch': 2.08}                                                                                                                                                                                                             
{'loss': 0.9157, 'learning_rate': 9.236156140578393e-05, 'epoch': 2.08}                                                                                                                                                                                                             
{'loss': 0.9958, 'learning_rate': 9.216703410711969e-05, 'epoch': 2.08}                                                                                                                                                                                                             
 69%|███████████████ ...

Questions reminder :  Pretrain vs QLORA  ?  First Simple Text than Instruction based using QLORA ? "
TeaCult,2023-07-06 08:46:16+00:00,comment,tiiuae/falcon-40b,"I have finished that. Results are promising entire Stanford alpaca dataset should take a day using A100 40GB with falcon 40B.
"
kemalcankara,2023-07-06 09:21:38+00:00,comment,tiiuae/falcon-40b,"Great job, can we try it somewhere if you do the entire dataset?"
TeaCult,2023-07-27 14:35:59+00:00,comment,tiiuae/falcon-40b,"I dont plan to train entire dataset. However it would be very wise to make the model generate most probable answers to instructions (top_p top_k temperature) and then using gpt3.5-turbo api to translate them turkish and feeding them into model. Such very smaller dataset gave a lot better output than standford aplaca. Only 4k instructions exceeded my preivious 12k Stf-Alpc finetuning. Answered coherently to many questions. So If I try this again, I will try this that way with 20K instruction or so. Then I will share."
alayaran,2023-09-22 10:07:30+00:00,comment,tiiuae/falcon-40b,Bard says yes via fine tuning. Can LLMs be fine-tuned to add new languages?
YukiTomita-CC,2023-12-27 09:22:51+00:00,comment,augmxnt/shisa-gamma-7b-v1,"I encountered the same error as @kik0220 and was also able to run the model using the tokenizer in `stabilityai/japanese-stablelm-base-gamma-7b`.
I would like to know if this is intentional or not, as the performance of the model certainly seems good."
leonardlin,2023-12-27 15:51:49+00:00,comment,augmxnt/shisa-gamma-7b-v1,"Hey guys, wrong tokenizer might have gotten copied in (this was more of an test model w/ good results I just threw up). Traveling atm but when I get a chance I'll take a look and update if necessary."
leonardlin,2024-01-02 16:00:58+00:00,comment,augmxnt/shisa-gamma-7b-v1,"Sorry for the delay w/ holiday travels (and not having access to my dev box!). I've uploaded the tokenizer fix to the repo (but using the original Gamma tokenizer was of course the solution). Thanks @kik0220 for the report, and happy new year everyone!"
zekozeko,2023-12-06 16:04:24+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"I don't have a precise knowledge on how this model was tuned, but multi-round conversation is a general property of large language models, and this property is maintained when fine-tuning on single-round instructions"
winglian,2023-11-30 15:34:32+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,The axolotl configuration is already shared in this repo. https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca/blob/main/configs/mistral-7b-oo-phase1.yml
timlim123,2023-11-30 23:57:19+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"@winglian can we directly use that config as the dataset set up is different?

Did you make edits to the code to run: 
alpaca_w_system.load_open_orca_chatml"
timlim123,2023-12-01 00:07:15+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"All good, saw that you have added it into prompt strategies. Thanks for your work 👍💪"
unaidedelf87777,2023-11-29 01:23:29+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"@dim Please look in the ``/configs/``  folder of the repository. Note that the configs are setup for a cluster of 8 Nvidia A6000 GPUs, so adjust Batch size and micro_batch size according to what you have at hand."
smrazaabbas,2023-11-29 09:48:38+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"Facing the same error, wondering if you've found out a solution yet?"
jlzhou,2023-11-10 12:22:07+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"You need to upgrade the transformers version, mistral support was introduced in 4.34.0, TGI 1.1.0 depends on transformers 4.33.3. After upgrading transformers my TGI can stop without generating '<|im_end|>'.

We build a docker image if you want to use, `zjuici/mirror.huggingface.text-generation-inference:1.1.0-transformers-4.34.1`"
mattma1970,2023-11-13 01:30:51+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"Thanks for helping. I haven't tried yet. I am running TGI for the official docker image so I'm try yours instead. Cheers
Matt
"
mattma1970,2023-11-13 01:38:43+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"I'm curious about where the change in the transformer version is set in the image? (docker novice). 
"
jlzhou,2023-11-13 02:31:16+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"I could not find the Dockerfile right now but it should be as simple as (IIRC):

```dockerfile
FROM ghcr.io/huggingface/text-generation-inference:1.1.0

RUN python -m pip install transformers==4.34.1
```"
mattma1970,2023-11-13 03:18:40+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,Thanks!
deleted,2023-11-09 17:07:54+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"You see, it is not a rlhf/rlft model and the OpenOrca dataset doesn't have any samples to guide the model to do so. And adding such instances might cause a lot of degradation in performance, i.e. Rejection of known content too. But their are methods to prevent hallucination, like apologising for not having the knowledge about something, can be done by training a model which just looking at the main model's states would tell whether hallucination is occurring or not, thus provide rejection. Hope it helps "
deleted,2023-11-09 17:51:08+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"Thanks. I don't know a lot about the inner working of LLMs, but that makes sense. And the issue of universally and confidently stating something isn't true, even though it is, plagues most other LLMs, including Zephyr and Open Hermes.

It's just from a human perspective this seems like a needless mistake, perhaps because we have an instinctual self-RAG-like mechanism. Confidence levels are INVALUABLE when conveying information. Any human that always confidently says things are either true or false would be avoided like the plague.  'I don't know', 'I don't think so', 'most likely' are crucial modifiers. For example, saying 'Jenny is pregnant' vs 'I think Jenny is pregnant' helps prevent incredibly awkward situations.

Perhaps in time someone will find a way to train LLMs in less black and white terms without significantly degrading performance."
unaidedelf87777,2023-10-30 16:20:07+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"@jiaxiangc  

For the compute config, it is 8x a6000 gpus, rented from runpod.io. To prevent overfitting we use packing, which also will speed up training a considerable amount. As far as that trainer we use, it is called axolotl, and you can find it here https://github.com/OpenAccess-AI-Collective/axolotl.  for learning rate and all other config options, in the configs folder on each model there is a yaml file which details all the options which axolotl uses. 

Hope that helps!"
jwkirchenbauer,2023-11-30 21:55:33+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,Thanks for storing the axolotl config! I suggest you add this to the model card so that people know where to find it :] just my 2c
mattma1970,2023-10-17 02:59:55+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"More details after debugging in IDE

Exception has occurred: DatasetGenerationError
An error occurred while generating the dataset
pyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.

The above exception was the direct cause of the following exception:

  File ""/home/mtman/Documents/Repos/llama2-4int/data.py"", line 6, in <module>
    dataset = load_dataset(""./data/OpenOrca/"")
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset"
mattma1970,2023-10-17 03:00:26+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"UPDATE: I upgraded datasets to v 2.14.5 and it solved the problem. I was using 2.13. 
"
unaidedelf87777,2023-10-17 03:35:59+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"> UPDATE: I upgraded datasets to v 2.14.5 and it solved the problem. I was using 2.13.

@mattma1970  FYI, I recommend using the SlimOrca subset of our data. It is verified answers, and is smaller. it will cost hundreds to train on all 4.5m entries, and to be frank some are little to no learning value. https://huggingface.co/Open-Orca/SlimOrca/"
mattma1970,2023-10-17 04:59:25+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"Thanks for the feedback. I'll check it out. 
Cheers
Matt
"
Rocketknight1,2023-10-04 14:29:51+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"This PR adds a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) following the ChatML format, which means users can just use `tokenizer.apply_chat_template` to handle chat formatting for them."
Tom9000,2023-10-06 10:44:01+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"Some temporary compatibility issues, which will be ironed out eventually, is not a good excuse to stick with old and inherently flawed formats.
ChatML is inherently better than alternatives, but it will take some time for the community and for all our toolkits and frameworks to support it properly."
SerialKicked,2023-10-10 14:04:24+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"Inherently better how, exactly?"
Sujan42024,2024-02-08 07:27:21+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"hello
"
YaTharThShaRma999,2023-10-05 23:09:24+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,Roughly 14gb vram
aborgohain,2023-10-06 11:51:34+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,Use quantized GGUF/GGML/AWQ models if you want to run on machines with lower computational resources.
YaTharThShaRma999,2023-10-06 12:58:08+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,Yeah then it will be roughly 6gb vram. 
smrazaabbas,2023-11-29 09:47:24+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,Can you suggest the smallest SageMaker instance I can use to deploy? For some reason loading the model via sample notebook given fails on the ml.g5.12xlarge instance even though the VRAM should be enough based on your suggestion?
YaTharThShaRma999,2023-12-06 22:33:21+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,@smrazaabbas you have to use the quantized version with 4 bit. It should work then
mirek190,2023-10-03 20:22:14+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,32k as guff shows 
hiiamsid,2023-10-04 06:15:07+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"hello @mirek190 where did you find this information about 32K? I was also looking for mistral model with longer sequence length and found that it was actually trained on 8K token also on this specific model card it has mentioned 7B-8K.  However mistral model has been implemented using sliding window approach due to which it considers tokens outside of the window as well while predicting next word but i was not able to find any evaluation or how to use it for 16k or 32K tokens
"
mirek190,2023-10-04 06:28:59+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"I'm using llamacpp where models are using newest binary implementation called gguf. 
As gguf has baked in model parameters during start you can check what parameters are loaded .
So after loaded the model I see ctx 32k."
bleysg,2023-10-05 06:42:47+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,It was trained with an 8192 token context window: https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions/4
mirek190,2023-10-03 20:22:57+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,use llamaccp and guff format ?
bleysg,2023-10-03 20:25:08+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"You'll need to get into whatever environment you have setup there (e.g. conda) and do:
```
pip install git+https://github.com/huggingface/transformers
```

This is because support for Mistral in Transformers is not merged to PyPI yet, so you need to install from the development snapshot."
writerflether,2023-10-03 07:20:58+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"I concur.  I'm trying to load this model in text-generation-inference.  Here's the stack:

```
2023-10-03T07:17:50.796666Z  INFO download: text_generation_launcher: Successfully downloaded weights.
2023-10-03T07:17:50.796964Z  INFO shard-manager: text_generation_launcher: Starting shard rank=0
2023-10-03T07:18:00.806182Z  INFO shard-manager: text_generation_launcher: Waiting for shard to be ready... rank=0
2023-10-03T07:18:05.539569Z ERROR text_generation_launcher: Error when initializing model
Traceback (most recent call last):
  File ""/opt/conda/bin/text-generation-server"", line 8, in <module>
    sys.exit(app())
  File ""/opt/conda/lib/python3.9/site-packages/typer/main.py"", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/typer/core.py"", line 778, in main
    return _main(
  File ""/opt/conda/lib/python3.9/site-packages/typer/core.py"", line 216, in _main
    rv = self.invoke(ctx)
  File ""/opt/conda/lib/python3.9/site-packages/click/core.py"", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/opt/conda/lib/python3.9/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/opt/conda/lib/python3.9/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/typer/main.py"", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/cli.py"", line 83, in serve
    server.serve(
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py"", line 207, in serve
    asyncio.run(
  File ""/opt/conda/lib/python3.9/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/opt/conda/lib/python3.9/asyncio/base_events.py"", line 634, in run_until_complete
    self.run_forever()
  File ""/opt/conda/lib/python3.9/asyncio/base_events.py"", line 601, in run_forever
    self._run_once()
  File ""/opt/conda/lib/python3.9/asyncio/base_events.py"", line 1905, in _run_once
    handle._run()
  File ""/opt/conda/lib/python3.9/asyncio/events.py"", line 80, in _run
    self._context.run(self._callback, *self._args)
> File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py"", line 159, in serve_inner
    model = get_model(
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py"", line 252, in get_model
    return FlashMistral(
  File ""/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_mistral.py"", line 297, in __init__
    tokenizer = LlamaTokenizerFast.from_pretrained(
  File ""/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py"", line 1854, in from_pretrained
    return cls._from_pretrained(
  File ""/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py"", line 1886, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File ""/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py"", line 2073, in _from_pretrained
    raise ValueError(
ValueError: Non-consecutive added token '<unk>' found. Should have index 32000 but has index 0 in saved vocabulary.
```"
bleysg,2023-10-03 07:26:42+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"You'll need to get into whatever environment you have setup for ooba (e.g. conda) and do:
```
pip install git+https://github.com/huggingface/transformers
```

This is because support for Mistral in Transformers is not merged to PyPI yet, so you need to install from the development snapshot."
writerflether,2023-10-03 08:43:54+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"Thanks, that worked for me.  

I assumed that since text-generation-inference:1.1.0 has support for Mistral, that it would work out of the box.  Instead I had to create a new image.  eg:

```
FROM ghcr.io/huggingface/text-generation-inference:1.1.0

RUN apt-get update -y && \
 DEBIAN_FRONTEND=noninteractive apt-get install -y git && \
 apt-get clean && \
 rm -rf /var/lib/apt/lists/*

RUN pip3 install --no-cache-dir \
    ""git+https://github.com/huggingface/transformers""
```"
Tonic,2023-10-03 09:26:27+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"is there a way to do this programmatically - yet ? (i'm trying to host it here, on hugging face)"
gilnore,2023-10-05 07:37:26+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"I seem to be still getting:
""""""raise TypeError(f""{config.model_type} isn't supported yet."")

TypeError: mistral isn't supported yet.""""""
even after updating with the given command.

I'm just loading it through AutoTokenizer.from_pretrained"
Tonic,2023-10-06 21:25:44+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"normally it's been fixed, you have to set the max token lengths when you deploy"
Dampfinchen,2023-10-02 21:33:16+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,"I have the same issue unfortunately. Otherwise, the model is really good. "
bleysg,2023-10-02 22:22:41+00:00,comment,Open-Orca/Mistral-7B-OpenOrca,We just pushed updates to tokenizer config which should resolve this. Please report if not.
irenedea,2024-03-26 23:51:06+00:00,comment,mosaicml/mpt-30b,
nudelbrot,2023-07-17 09:00:23+00:00,comment,mosaicml/mpt-30b,"Good luck getting anyone on HF or the companies support this. In their view (for sure I cannot know their view) the discussion seems to be too harmful for their business. The guess is ""not a single"" dataset or model here would fulfill this requirement."
danbider,2023-07-04 13:51:12+00:00,comment,mosaicml/mpt-30b,"Yes. Make sure to target the “Wqkv” parameters.
Could do something like this

lora:
  args:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ['Wqkv']

And

LoraConfig(**lora_cfg.args)
"
316usman,2023-08-22 07:04:29+00:00,comment,mosaicml/mpt-30b,@Sven00 Can you please link those instructions (single gpu) here
matorus,2023-06-24 14:01:02+00:00,comment,mosaicml/mpt-30b,Adding `load_in_8bit=True` solves the problem.
sam-mosaic,2023-06-23 20:31:25+00:00,comment,mosaicml/mpt-30b,Thank you for catching this. it is fixed
sam-mosaic,2023-06-23 20:33:12+00:00,comment,mosaicml/mpt-30b,"This model has not been fine-tuned. There is no particular format that is required for prompting. There likely are strategies that work better than others, but they are yet to be discovered!"
kartikmosaicml,2023-06-21 06:24:02+00:00,comment,mosaicml/mpt-30b,Creating a draft readme. Need to update it further.
andromeda01111,2023-09-19 09:53:37+00:00,comment,EleutherAI/pythia-1b,can i solve this issue by changing max_length. I tried it but it takes way too much time to do each prediction.
young-geng,2023-07-10 04:48:49+00:00,comment,openlm-research/open_llama_7b,This is done by design. You can also turn off the BOS token during tokenization if you want.
Pytagora,2024-05-07 20:17:28+00:00,comment,allenai/OLMo-7B-Instruct,Didactic textual tomography
lsetiawan,2024-05-22 18:56:23+00:00,comment,allenai/OLMo-7B-Instruct,"We have created an HF Spaces to start playing around with this model, check it out: https://huggingface.co/spaces/ssec-uw/olmo-chat"
julien-c,2024-04-22 16:44:13+00:00,comment,allenai/OLMo-7B-Instruct,inspiring work AllenAI 🔥
julien-c,2024-04-22 16:44:44+00:00,comment,allenai/OLMo-7B-Instruct,and very cool table @markding 
SFconvertbot,2024-03-29 18:16:11+00:00,comment,allenai/OLMo-7B-Instruct,"This is an automated PR created with https://huggingface.co/spaces/safetensors/convert

This new file is equivalent to `pytorch_model.bin` but safe in the sense that
no arbitrary code can be put into it.

These files also happen to load much faster than their pytorch counterpart:
https://colab.research.google.com/github/huggingface/notebooks/blob/main/safetensors_doc/en/speed.ipynb

The widgets on your model page will run using this model even if this is not merged
making sure the file actually works.

If you find any issues: please report here: https://huggingface.co/spaces/safetensors/convert/discussions

Feel free to ignore this PR."
dirkgr,2024-05-29 19:52:14+00:00,comment,allenai/OLMo-7B-Instruct,:+1:
randhash,2023-12-29 14:15:53+00:00,comment,THUDM/cogvlm-chat-hf,"I think FlashAttn is already used under specific conditions. The [attention implementation](https://huggingface.co/THUDM/cogvlm-chat-hf/blob/main/modeling_cogvlm.py#L129) calls PyTorch's `scaled_dot_product_attention` function which calls into a FlashAttn kernel if some conditions are met. You can actually enforce the use of this kernel for debugging purposes with an appropriate context manager:

```python
with torch.backends.cuda.sdp_kernel(
    enable_flash=True, 
    enable_math=False, 
    enable_mem_efficient=False
):
    model.generate(**input)
```

Note, that in the referenced code there is a branch which may execute a naive attention implementation so even though you are using enforcing FA use in PyTorch, you would still make sure that the if-statement runs into the first branch."
zRzRzRzRzRzRzR,2024-02-14 01:22:02+00:00,comment,THUDM/cogvlm-chat-hf,"If you upgrade to PyTorch 2.2.0, you should be able to directly use PyTorch integrated with Flash Attention 2.0. Try to follow the instructions in the previous comment, but there's no need to pull a separate branch."
neil651,2023-12-04 21:07:07+00:00,comment,THUDM/cogvlm-chat-hf,"for the issue mentioned, just use the BitsAndBytesConfig to set the compute type

```

from transformers import AutoModelForCausalLM, LlamaTokenizer, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_use_double_quant=True,
bnb_4bit_quant_type=""nf4"",
bnb_4bit_compute_dtype=torch.bfloat16
)

max_memory_mapping = {0: ""23GB"", 1: ""23GB""}

model = AutoModelForCausalLM.from_pretrained(
    'THUDM/cogvlm-chat-hf',
    low_cpu_mem_usage=True,
    trust_remote_code=True,
    quantization_config=bnb_config,
    max_memory=max_memory_mapping
).eval()
```

using 4bit quantisation with batch size 16 processing, on an L4 was able to achieve inference time of 4 sec/image. compared to 15s per image without quantisation and batch"
Minami-su,2023-12-05 05:41:26+00:00,comment,THUDM/cogvlm-chat-hf,"I attempted to quantize CogVLM using AutoGPTQ. Quantization was successful, but inference failed."
z3ugma,2023-12-07 00:03:45+00:00,comment,THUDM/cogvlm-chat-hf,"@neil651  I'm following your BitsAndBytes quantization strategy. It loads fine onto the GPU, but when trying inference I get datatype errors for input vs inference. 

```
  File ""/home/user/mambaforge/envs/tensorml/lib/python3.11/site-packages/torch/nn/modules/conv.py"", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Input type (c10::BFloat16) and bias type (c10::Half) should be the same

```

Can you post a whole example of your bitsandbytes 4bit quant with inference? "
chenkq,2023-12-07 03:36:54+00:00,comment,THUDM/cogvlm-chat-hf,@z3ugma change the `bnb_4bit_compute_dtype=torch.bfloat16` to `bnb_4bit_compute_dtype=torch.float16` may help
qingsonglv,2023-12-07 04:20:20+00:00,comment,THUDM/cogvlm-chat-hf,"Just change your image type from bf16 to fp16 is ok.

```python
'images': [[inputs['images'][0].to('cuda').to(torch.float16)]],
```"
z3ugma,2023-12-07 04:54:10+00:00,comment,THUDM/cogvlm-chat-hf,"@qingsonglv that was the solution to making 4-bit quantized work. With that in place, the model takes up only 12GiB on a 16GB CUDA Nvidia GPU 
"
z3ugma,2023-12-08 04:44:07+00:00,comment,THUDM/cogvlm-chat-hf,"Repo owners updated quantization to be easier at https://github.com/THUDM/CogVLM/commit/2f293fc6b4063791e80e2bf7e1a045c758352d7e 
"
panopstor,2024-03-07 23:57:24+00:00,comment,THUDM/cogvlm-chat-hf,I'm wondering if KEG would consider committing the modeling_cogvlm.py into the core transformers package?  It would be more likely to be maintained there.  The licenses appear to be compatible (Apache) but it is not my place to do so.
jrtayloriv,2024-04-24 23:19:38+00:00,comment,THUDM/cogvlm-chat-hf,Did you ever find instructions for fine-tuning? I am trying to create a dataset for fine-tuning CogVLM and have not been able to find much information. Thanks!
zRzRzRzRzRzRzR,2024-02-07 14:09:36+00:00,comment,THUDM/cogvlm-chat-hf,"We haven't tried much with it yet, but I think it's a good approach, and more CogVLM/CogAgent models are really difficult.
It should be noted that the token length supported by this model is very short (2048, which is caused by the training set), which will cause RAG to easily exceed the token limit, making the effect very poor.

"
zRzRzRzRzRzRzR,2024-02-07 14:11:25+00:00,comment,THUDM/cogvlm-chat-hf,"You can use FP16 for inference, and that's also possible .You can view our github repository

It should be noted that the video memory required for reasoning this model, if using FP16, requires at least 36G of video memory."
chenkq,2023-12-06 13:04:30+00:00,comment,THUDM/cogvlm-chat-hf,try fp16 maybe? https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/5#6567f0b792d9319907436b10
sidnb13,2023-12-03 18:56:51+00:00,comment,THUDM/cogvlm-chat-hf,"I went into `<venv-path>/site-packages/triton/common/build.py` and changed `subprocess.check_output([""ldconfig"", ""-p""]).decode()` to `subprocess.check_output([""/sbin/ldconfig"", ""-p""]).decode()`.  Even after adding `ldconfig` to path since `check_output` isn't called with `shell=True`, you don't have access to env variables. I ran into more issues installing the latest version of triton so the hotfix was the best solution."
chenkq,2023-12-05 07:49:03+00:00,comment,THUDM/cogvlm-chat-hf,"the modeling_cogvlm.py is updated. the latest version of the model now includes a rotary embedding, which has no dependency on triton.
This update should address the issue you were experiencing. Please check and let me know if your problem has been resolved."
chenkq,2023-12-01 13:20:22+00:00,comment,THUDM/cogvlm-chat-hf," try modifying `no_split_module_classes=['CogVLMDecoderLayer']` to `no_split_module_classes=['CogVLMDecoderLayer', 'TransformerLayer']`"
chenkq,2023-11-30 02:50:21+00:00,comment,THUDM/cogvlm-chat-hf,"yes, try sth like this.

```python
import torch
import requests
from PIL import Image
from transformers import AutoModelForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')
model = AutoModelForCausalLM.from_pretrained(
    'THUDM/cogvlm-chat-hf',
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True
).to('cuda').eval()

input_sample1 = model.build_conversation_input_ids(
    tokenizer,
    images=[Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/openai_demo/demo.jpg?raw=true', stream=True).raw).convert('RGB'),],
    query='Do you think this is a spring or winter photo?',  # Q2
    history=[
        (
            ""What's in this image?"",   # Q1
            'The image displays a wooden boardwalk extending through a vibrant green grassy wetland.'  # A1
         )
        ], 
    )
input_sample2 = model.build_conversation_input_ids(
    tokenizer,
    images=[Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB'),],
    query='Describe this image',  # Q1
    history=[], 
    )

def recur_move_to(item, tgt, criterion_func):
    if criterion_func(item):
        device_copy = item.to(tgt)
        return device_copy
    elif isinstance(item, list):
        return [recur_move_to(v, tgt, criterion_func) for v in item]
    elif isinstance(item, tuple):
        return tuple([recur_move_to(v, tgt, criterion_func) for v in item])
    elif isinstance(item, dict):
        return {k: recur_move_to(v, tgt, criterion_func) for k, v in item.items()}
    else:
        return item

def collate_fn(features, tokenizer) -> dict:
    images = [feature.pop('images') for feature in features]
    tokenizer.padding_side = 'left'
    padded_features = tokenizer.pad(features)
    inputs = {**padded_features, 'images': images}
    return inputs

input_batch = collate_fn([input_sample1, input_sample2], tokenizer)
input_batch = recur_move_to(input_batch, 'cuda', lambda x: isinstance(x, torch.Tensor))
input_batch = recur_move_to(input_batch, torch.bfloat16, lambda x: isinstance(x, torch.Tensor) and torch.is_floating_point(x))

gen_kwargs = {""max_length"": 2048, ""do_sample"": False}

with torch.no_grad():
    outputs = model.generate(**input_batch, **gen_kwargs)
    outputs = outputs[:, input_batch['input_ids'].shape[1]:]
    print(tokenizer.batch_decode(outputs))
```"
chenkq,2023-11-30 02:59:04+00:00,comment,THUDM/cogvlm-chat-hf,"sry, I'm not familiar with SageMaker. However, if you're interested in using it as a api, you might find [this resource](https://github.com/THUDM/CogVLM/tree/main#openai-vision-formate) helpful"
sidnb13,2023-12-03 19:00:30+00:00,comment,THUDM/cogvlm-chat-hf,"Curious if you were able to set up a finetuning pipeline using HF? I'm trying to finetune CogVLM using LoRA on my custom image-text datasets and couldn't find any info on what linear layers to target, etc. from the original repository. The implementation using SAT is a bit obscure so ideally I'd like to use PEFT."
mohammednuruddin,2023-12-18 22:31:42+00:00,comment,THUDM/cogvlm-chat-hf,"> Curious if you were able to set up a finetuning pipeline using HF? I'm trying to finetune CogVLM using LoRA on my custom image-text datasets and couldn't find any info on what linear layers to target, etc. from the original repository. The implementation using SAT is a bit obscure so ideally I'd like to use PEFT.

Were you able to do it?
"
mnbucher,2023-12-18 23:20:21+00:00,comment,THUDM/cogvlm-chat-hf,"hi both @sidnb13 @mohammednuruddin ,
i haven't continued working on this since then, so if you have any running code snippet that might be very valuable.
— cheers, martin"
z3ugma,2023-12-22 23:55:12+00:00,comment,THUDM/cogvlm-chat-hf,"@sidnb13 I would like to do the same as you are doing, did you succeed at finetuning using PEFT? 
"
ayensujeremiah,2024-01-22 17:07:18+00:00,comment,THUDM/cogvlm-chat-hf,"Hello Everyone. I am also trying to finetune the chat version of CogVLM too for image-text dataset. My text set are QA pairs regarding the image in a json format. any idea how I might preprocess the data for the finetuning?
"
expert78,2024-01-26 13:45:33+00:00,comment,THUDM/cogvlm-chat-hf,"can i Finetuning with --quant 4
so it fit on 16gb vram even if it slow a bit?"
ayensujeremiah,2024-01-28 22:49:44+00:00,comment,THUDM/cogvlm-chat-hf,"@expert78 I am working with 8*A100 80GB GPU environment and I still get Out of Memory issues. Maybe the 4 bit quantized version might help, special when you are not making all layers trainable.
"
KIK99,2024-01-30 10:37:13+00:00,comment,THUDM/cogvlm-chat-hf,"I am getting an error at this point.

Could find a solution 
File ""/home/ec2-user/trail_ver/llava_train.py"", line 121, in <module>
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ec2-user/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py"", line 562, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ec2-user/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 3504, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ec2-user/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 3919, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ec2-user/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py"", line 802, in _load_state_dict_into_meta_model
    or (not hf_quantizer.check_quantized_param(model, param, param_name, state_dict))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ec2-user/anaconda3/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py"", line 124, in check_quantized_param
    if isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):
                  ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
KeyError: 'inv_freq'

Can anyone help me out 
"
KIK99,2024-02-13 16:19:57+00:00,comment,THUDM/cogvlm-chat-hf,"Hi @mnbucher ,

I am trying to finetune the model with LORA, I just want to know how should I input the data from tokenizer like image and text. pretty confused with that part. 

processor_data = model.build_conversation_input_ids(tokenizer, query=text, history=[], images=[image])
is it possible to do it with this. i get some input_ids and padded_masks, attention masks and images. Also I add labels as well to it but I get an error 
Error : pyarrow.lib.ArrowInvalid: Column 7 named input_ids expected length 10 but got length 1290

Could you please guide me through this step?


"
zRzRzRzRzRzRzR,2024-02-14 01:20:13+00:00,comment,THUDM/cogvlm-chat-hf,"I'm sorry for the inconvenience. In our demo, we indeed do not have example code for finetuning based on the huggingface model. In our finetune demo on GitHub, we have released code based on the SAT framework, which can be used to finetune the cogvlm_224 and cogvlm_490 SAT models. You might want to check our GitHub: https://github.com/THUDM/CogVLM

Regarding the input id issue, it is foreseeable that it's caused by the lack of padding. You need to pad the inputs for better training."
crux82,2024-04-05 10:37:13+00:00,comment,THUDM/cogvlm-chat-hf,"Dear all, 

is there any news about examples of the fine-tuning process?

TNX!!!
"
mohammednuruddin,2024-04-19 15:20:02+00:00,comment,THUDM/cogvlm-chat-hf,"> Dear all, 
> 
> is there any news about examples of the fine-tuning process?
> 
> TNX!!!

No"
chenkq,2023-11-30 02:17:27+00:00,comment,THUDM/cogvlm-chat-hf,"just change all `torch.bfloat16` to `torch.float16` in example.

```
import torch
import requests
from PIL import Image
from transformers import AutoModelForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')
model = AutoModelForCausalLM.from_pretrained(
    'THUDM/cogvlm-chat-hf',
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    trust_remote_code=True
).to('cuda').eval()


# chat example
query = 'Describe this image'
image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')
inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode
inputs = {
    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),
    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),
    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),
    'images': [[inputs['images'][0].to('cuda').to(torch.float16)]],
}
gen_kwargs = {""max_length"": 2048, ""do_sample"": False}

with torch.no_grad():
    outputs = model.generate(**inputs, **gen_kwargs)
    outputs = outputs[:, inputs['input_ids'].shape[1]:]
    print(tokenizer.decode(outputs[0]))
```"
justinwickett,2023-11-25 16:46:21+00:00,comment,THUDM/cogvlm-chat-hf,"Just discovered and read https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/2#655ca4de93ac084b7d802358. @chenkq where can I find the model ckpt? I don't see it in this local path where I thought it would be:

```
$ pwd
/home/ec2-user/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/8abca878c4257412c4c38eeafaed3fe27a036730

$ ls -l
total 0
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 24 23:11 config.json -> ../../blobs/8b5a3c26659a087d6f41701ef63c8b094d36cad0
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 24 23:11 configuration_cogvlm.py -> ../../blobs/60d487ac530bb1aa6e13828e2a17ae2f521b02d1
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 24 23:31 generation_config.json -> ../../blobs/a4fee64c3bea2cc2488999c0611a0602930b7e93
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:21 model-00001-of-00008.safetensors -> ../../blobs/e29f6ec471ca55789ab14947b527729b9c30313ceb1e7726590b85f9f6406cca
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:22 model-00002-of-00008.safetensors -> ../../blobs/e82356882701b1a778408f31e676d17c2aff799c543e8596ed74bc805b4a1213
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:22 model-00003-of-00008.safetensors -> ../../blobs/04096f84f42798d0c89319ff8254995a2a3512c16ec88dfd078ce421867d92ec
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:22 model-00004-of-00008.safetensors -> ../../blobs/2b42af0bb16647959b3e55def4b3c66ab8c3a25fd948a5245c81d070f2b4313d
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:23 model-00005-of-00008.safetensors -> ../../blobs/38c07825790e055dd169376479994a58a4f59775ba7cf31d5ca25d8a465e7b0c
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:23 model-00006-of-00008.safetensors -> ../../blobs/d01880ca5677e69a5f8632f9dda62814f0c549b5a40d4f7e136065e5d64c1a7d
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:31 model-00007-of-00008.safetensors -> ../../blobs/e70b0e10d2ac8800e69e514b6a9b04ac28cd7db43985ce62daa4e0e639b4e5ba
lrwxrwxrwx 1 ec2-user ec2-user 76 Nov 24 23:31 model-00008-of-00008.safetensors -> ../../blobs/a756381ef65b92af7f1fb97da3d59cb04586080982de86d76805299898223294
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 24 23:11 modeling_cogvlm.py -> ../../blobs/4a7a748e2dbf349e260cd50771197cefc99c68b0
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 24 23:21 model.safetensors.index.json -> ../../blobs/197b878b0e9f9b1ffb97a7c4da1fb834fffcaf97
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 25 15:30 README.md -> ../../blobs/77cb0e1cf17f1a8f70e26cb86af0ed7f44ec1952
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 24 23:12 util.py -> ../../blobs/1dccacad2ded4c357ab9cb23d04027500256e281
lrwxrwxrwx 1 ec2-user ec2-user 52 Nov 24 23:12 visual.py -> ../../blobs/367e2fb1d19ff3b5c43fb898b7d15e436bbbf7cc
```"
chenkq,2023-11-26 03:33:33+00:00,comment,THUDM/cogvlm-chat-hf,"Based on the call stack information you provided, I suspect there might be an issue with the dispatch of the visual model. You could try modifying `no_split_module_classes=['CogVLMDecoderLayer']` to `no_split_module_classes=['CogVLMDecoderLayer', 'TransformerLayer']`. This adjustment should prevent the model from being fragmented excessively, thereby reducing the need to wrap sth like xformers.ops (I guess

Regarding your question, ""Where can I find the model checkpoint (ckpt)?"", in your case, the local path is `models--THUDM--cogvlm-chat-hf/snapshots/8abca878c4257412c4c38eeafaed3fe27a036730`. The files `model-0000x-of-00008.safetensors` and `model.safetensors.index.json` constitute the model checkpoint. Here, these are symbolic links pointing to files in blobs."
justinwickett,2023-11-26 03:50:53+00:00,comment,THUDM/cogvlm-chat-hf,"@chenkq I made the change you suggested, but I am still encountering the error reported above. Any other ideas?

Here is the output when I run `model.hf_device_map` after modifying `no_split_module_classes=['CogVLMDecoderLayer', 'TransformerLayer']`:

```
{'model.embed_tokens': 0,
 'model.layers.0': 0,
 'model.layers.1': 0,
 'model.layers.2': 0,
 'model.layers.3': 0,
 'model.layers.4': 0,
 'model.layers.5': 0,
 'model.layers.6': 0,
 'model.layers.7': 0,
 'model.layers.8': 0,
 'model.layers.9': 0,
 'model.layers.10': 1,
 'model.layers.11': 1,
 'model.layers.12': 1,
 'model.layers.13': 1,
 'model.layers.14': 1,
 'model.layers.15': 1,
 'model.layers.16': 1,
 'model.layers.17': 1,
 'model.layers.18': 1,
 'model.layers.19': 1,
 'model.layers.20': 1,
 'model.layers.21': 1,
 'model.layers.22': 2,
 'model.layers.23': 2,
 'model.layers.24': 2,
 'model.layers.25': 2,
 'model.layers.26': 2,
 'model.layers.27': 2,
 'model.layers.28': 2,
 'model.layers.29': 2,
 'model.layers.30': 2,
 'model.layers.31': 2,
 'model.norm': 2,
 'model.vision.boi': 2,
 'model.vision.eoi': 2,
 'model.vision.patch_embedding': 2,
 'model.vision.transformer.layers.0': 2,
 'model.vision.transformer.layers.1': 2,
 'model.vision.transformer.layers.2': 2,
 'model.vision.transformer.layers.3': 2,
 'model.vision.transformer.layers.4': 2,
 'model.vision.transformer.layers.5': 2,
 'model.vision.transformer.layers.6': 2,
 'model.vision.transformer.layers.7': 2,
 'model.vision.transformer.layers.8': 2,
 'model.vision.transformer.layers.9': 2,
 'model.vision.transformer.layers.10': 2,
 'model.vision.transformer.layers.11': 2,
 'model.vision.transformer.layers.12': 3,
 'model.vision.transformer.layers.13': 3,
 'model.vision.transformer.layers.14': 3,
 'model.vision.transformer.layers.15': 3,
 'model.vision.transformer.layers.16': 3,
 'model.vision.transformer.layers.17': 3,
 'model.vision.transformer.layers.18': 3,
 'model.vision.transformer.layers.19': 3,
 'model.vision.transformer.layers.20': 3,
 'model.vision.transformer.layers.21': 3,
 'model.vision.transformer.layers.22': 3,
 'model.vision.transformer.layers.23': 3,
 'model.vision.transformer.layers.24': 3,
 'model.vision.transformer.layers.25': 3,
 'model.vision.transformer.layers.26': 3,
 'model.vision.transformer.layers.27': 3,
 'model.vision.transformer.layers.28': 3,
 'model.vision.transformer.layers.29': 3,
 'model.vision.transformer.layers.30': 3,
 'model.vision.transformer.layers.31': 3,
 'model.vision.transformer.layers.32': 3,
 'model.vision.transformer.layers.33': 3,
 'model.vision.transformer.layers.34': 3,
 'model.vision.transformer.layers.35': 3,
 'model.vision.transformer.layers.36': 3,
 'model.vision.transformer.layers.37': 3,
 'model.vision.transformer.layers.38': 3,
 'model.vision.transformer.layers.39': 3,
 'model.vision.transformer.layers.40': 3,
 'model.vision.transformer.layers.41': 3,
 'model.vision.transformer.layers.42': 3,
 'model.vision.transformer.layers.43': 3,
 'model.vision.transformer.layers.44': 3,
 'model.vision.transformer.layers.45': 3,
 'model.vision.transformer.layers.46': 3,
 'model.vision.transformer.layers.47': 3,
 'model.vision.transformer.layers.48': 3,
 'model.vision.transformer.layers.49': 3,
 'model.vision.transformer.layers.50': 3,
 'model.vision.transformer.layers.51': 3,
 'model.vision.transformer.layers.52': 3,
 'model.vision.transformer.layers.53': 3,
 'model.vision.transformer.layers.54': 3,
 'model.vision.transformer.layers.55': 3,
 'model.vision.transformer.layers.56': 3,
 'model.vision.transformer.layers.57': 3,
 'model.vision.transformer.layers.58': 3,
 'model.vision.transformer.layers.59': 3,
 'model.vision.transformer.layers.60': 3,
 'model.vision.transformer.layers.61': 3,
 'model.vision.transformer.layers.62': 3,
 'model.vision.linear_proj': 3,
 'lm_head': 3}
```

"
justinwickett,2023-11-26 04:12:01+00:00,comment,THUDM/cogvlm-chat-hf,"@chenkq here is the full stack trace that occurs when running `outputs = model.generate(**inputs, **gen_kwargs)`. Thank you 🙏

```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[5], line 2
      1 with torch.no_grad():
----> 2     outputs = model.generate(**inputs, **gen_kwargs)
      3     outputs = outputs[:, inputs['input_ids'].shape[1]:]
      4     print(tokenizer.decode(outputs[0]))

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/utils/_contextlib.py:115, in context_decorator.<locals>.decorate_context(*args, **kwargs)
    112 @functools.wraps(func)
    113 def decorate_context(*args, **kwargs):
    114     with ctx_factory():
--> 115         return func(*args, **kwargs)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/transformers/generation/utils.py:1673, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)
   1656     return self.assisted_decoding(
   1657         input_ids,
   1658         assistant_model=assistant_model,
   (...)
   1669         **model_kwargs,
   1670     )
   1671 if generation_mode == GenerationMode.GREEDY_SEARCH:
   1672     # 11. run greedy search
-> 1673     return self.greedy_search(
   1674         input_ids,
   1675         logits_processor=logits_processor,
   1676         stopping_criteria=stopping_criteria,
   1677         pad_token_id=generation_config.pad_token_id,
   1678         eos_token_id=generation_config.eos_token_id,
   1679         output_scores=generation_config.output_scores,
   1680         return_dict_in_generate=generation_config.return_dict_in_generate,
   1681         synced_gpus=synced_gpus,
   1682         streamer=streamer,
   1683         **model_kwargs,
   1684     )
   1686 elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:
   1687     if not model_kwargs[""use_cache""]:

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/transformers/generation/utils.py:2521, in GenerationMixin.greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)
   2518 model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
   2520 # forward pass to get next token
-> 2521 outputs = self(
   2522     **model_inputs,
   2523     return_dict=True,
   2524     output_attentions=output_attentions,
   2525     output_hidden_states=output_hidden_states,
   2526 )
   2528 if synced_gpus and this_peer_finished:
   2529     continue  # don't waste resources running the code we don't need

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/accelerate/hooks.py:164, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    162         output = module._old_forward(*args, **kwargs)
    163 else:
--> 164     output = module._old_forward(*args, **kwargs)
    165 return module._hf_hook.post_forward(module, output)

File ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/8abca878c4257412c4c38eeafaed3fe27a036730/modeling_cogvlm.py:610, in CogVLMForCausalLM.forward(self, input_ids, images, token_type_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels)
    607 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    609 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
--> 610 outputs = self.model(
    611     input_ids=input_ids,
    612     images=images,
    613     token_type_ids=token_type_ids,
    614     attention_mask=attention_mask,
    615     position_ids=position_ids,
    616     past_key_values=past_key_values,
    617     inputs_embeds=inputs_embeds,
    618     use_cache=use_cache,
    619     output_attentions=output_attentions,
    620     output_hidden_states=output_hidden_states,
    621     return_dict=return_dict,
    622 )
    624 hidden_states = outputs[0]
    625 logits = self.lm_head(hidden_states)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/8abca878c4257412c4c38eeafaed3fe27a036730/modeling_cogvlm.py:378, in CogVLMModel.forward(self, input_ids, images, token_type_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)
    376 assert len(input_ids) == len(images), f""{len(input_ids)} {len(images)}""
    377 inputs_embeds = self.embed_tokens(input_ids)
--> 378 images_features = self.encode_images(images)
    379 images_features = rearrange(images_features, 'b n d -> (b n) d')
    380 images_features = images_features.to(dtype=inputs_embeds.dtype, device=inputs_embeds.device)

File ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/8abca878c4257412c4c38eeafaed3fe27a036730/modeling_cogvlm.py:350, in CogVLMModel.encode_images(self, images)
    347         images.append(image)
    349 images = torch.stack(images)
--> 350 images_features = self.vision(images)
    351 return images_features

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/8abca878c4257412c4c38eeafaed3fe27a036730/visual.py:129, in EVA2CLIPModel.forward(self, images)
    127 def forward(self, images: ""tensor(B, C, H, W)"") -> ""tensor(B, L, D)"":
    128     x = self.patch_embedding(images)
--> 129     x = self.transformer(x)
    130     x = x[:, 1:]
    131     x = self.linear_proj(x)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/8abca878c4257412c4c38eeafaed3fe27a036730/visual.py:94, in Transformer.forward(self, hidden_states)
     92 def forward(self, hidden_states):
     93     for layer_module in self.layers:
---> 94         hidden_states = layer_module(hidden_states)
     95     return hidden_states

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/8abca878c4257412c4c38eeafaed3fe27a036730/visual.py:79, in TransformerLayer.forward(self, hidden_states)
     77 def forward(self, hidden_states):
     78     attention_input = hidden_states
---> 79     attention_output = self.input_layernorm(self.attention(attention_input))
     80     hidden_states = attention_input + attention_output
     81     mlp_input = hidden_states

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1518, in Module._wrapped_call_impl(self, *args, **kwargs)
   1516     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1517 else:
-> 1518     return self._call_impl(*args, **kwargs)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1527, in Module._call_impl(self, *args, **kwargs)
   1522 # If we don't have any hooks, we want to skip the rest of the logic in
   1523 # this function, and just call forward.
   1524 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1525         or _global_backward_pre_hooks or _global_backward_hooks
   1526         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1527     return forward_call(*args, **kwargs)
   1529 try:
   1530     result = None

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/accelerate/hooks.py:164, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)
    162         output = module._old_forward(*args, **kwargs)
    163 else:
--> 164     output = module._old_forward(*args, **kwargs)
    165 return module._hf_hook.post_forward(module, output)

File ~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/8abca878c4257412c4c38eeafaed3fe27a036730/visual.py:40, in Attention.forward(self, x)
     37 qkv = qkv.reshape(B, L, 3, self.num_heads, -1).permute(2, 0, 1, 3, 4)  # 3, B, L, H, D
     38 q, k, v = qkv[0], qkv[1], qkv[2]
---> 40 out = xops.memory_efficient_attention(
     41     q, k, v, scale=self.scale,
     42 )
     43 output = self.dense(out.view(B, L, -1))
     44 output = self.output_dropout(output)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:223, in memory_efficient_attention(query, key, value, attn_bias, p, scale, op)
    116 def memory_efficient_attention(
    117     query: torch.Tensor,
    118     key: torch.Tensor,
   (...)
    124     op: Optional[AttentionOp] = None,
    125 ) -> torch.Tensor:
    126     """"""Implements the memory-efficient attention mechanism following
    127     `""Self-Attention Does Not Need O(n^2) Memory"" <http://arxiv.org/abs/2112.05682>`_.
    128 
   (...)
    221     :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``
    222     """"""
--> 223     return _memory_efficient_attention(
    224         Inputs(
    225             query=query, key=key, value=value, p=p, attn_bias=attn_bias, scale=scale
    226         ),
    227         op=op,
    228     )

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:321, in _memory_efficient_attention(inp, op)
    316 def _memory_efficient_attention(
    317     inp: Inputs, op: Optional[AttentionOp] = None
    318 ) -> torch.Tensor:
    319     # fast-path that doesn't require computing the logsumexp for backward computation
    320     if all(x.requires_grad is False for x in [inp.query, inp.key, inp.value]):
--> 321         return _memory_efficient_attention_forward(
    322             inp, op=op[0] if op is not None else None
    323         )
    325     output_shape = inp.normalize_bmhk()
    326     return _fMHA.apply(
    327         op, inp.query, inp.key, inp.value, inp.attn_bias, inp.p, inp.scale
    328     ).reshape(output_shape)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:337, in _memory_efficient_attention_forward(inp, op)
    335 output_shape = inp.normalize_bmhk()
    336 if op is None:
--> 337     op = _dispatch_fw(inp, False)
    338 else:
    339     _ensure_op_supports_or_raise(ValueError, ""memory_efficient_attention"", op, inp)

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:120, in _dispatch_fw(inp, needs_gradient)
    111 def _dispatch_fw(inp: Inputs, needs_gradient: bool) -> Type[AttentionFwOpBase]:
    112     """"""Computes the best operator for forward
    113 
    114     Raises:
   (...)
    118         AttentionOp: The best operator for the configuration
    119     """"""
--> 120     return _run_priority_list(
    121         ""memory_efficient_attention_forward"",
    122         _dispatch_fw_priority_list(inp, needs_gradient),
    123         inp,
    124     )

File ~/jw-cogvlm-grounding-generalist-hf/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:63, in _run_priority_list(name, priority_list, inp)
     61 for op, not_supported in zip(priority_list, not_supported_reasons):
     62     msg += ""\n"" + _format_not_supported_reasons(op, not_supported)
---> 63 raise NotImplementedError(msg)

NotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:
     query       : shape=(1, 1226, 16, 112) (torch.bfloat16)
     key         : shape=(1, 1226, 16, 112) (torch.bfloat16)
     value       : shape=(1, 1226, 16, 112) (torch.bfloat16)
     attn_bias   : <class 'NoneType'>
     p           : 0.0
`decoderF` is not supported because:
    attn_bias type is <class 'NoneType'>
    bf16 is only supported on A100+ GPUs
`flshattF@v2.3.2` is not supported because:
    requires device with capability > (8, 0) but your GPU has capability (7, 0) (too old)
    bf16 is only supported on A100+ GPUs
`tritonflashattF` is not supported because:
    requires device with capability > (8, 0) but your GPU has capability (7, 0) (too old)
    bf16 is only supported on A100+ GPUs
    operator wasn't built - see `python -m xformers.info` for more info
    triton is not available
    requires GPU with sm80 minimum compute capacity, e.g., A100/H100/L4
    Only work on pre-MLIR triton for now
`cutlassF` is not supported because:
    bf16 is only supported on A100+ GPUs
`smallkF` is not supported because:
    max(query.shape[-1] != value.shape[-1]) > 32
    dtype=torch.bfloat16 (supported: {torch.float32})
    has custom scale
    bf16 is only supported on A100+ GPUs
    unsupported embed per head: 112
```"
chenkq,2023-11-27 02:15:59+00:00,comment,THUDM/cogvlm-chat-hf,"oops, V100 does not support `bf16`, try `fp16` instead."
justinwickett,2023-11-27 05:56:30+00:00,comment,THUDM/cogvlm-chat-hf,"Thanks @chenkq, that seems to have been my issue! I switched to NVIDIA A10G GPUs, and I am able to now use `bf16`. "
qingsonglv,2023-11-21 11:24:29+00:00,comment,THUDM/cogvlm-chat-hf,"Yes, you are right. The huggingface version does not support model parallel, and we suggest use the official sat version: https://github.com/THUDM/CogVLM"
lzxcgnkhnrlnto,2023-11-21 11:27:33+00:00,comment,THUDM/cogvlm-chat-hf,"> Yes, you are right. The huggingface version does not support model parallel, and we suggest use the official sat version: https://github.com/THUDM/CogVLM

If you have the time, consider checking this issue as it is the primary one keeping dual-gpu users from using CogVLM on WSL2: https://github.com/THUDM/CogVLM/issues/56"
qingsonglv,2023-11-21 11:40:48+00:00,comment,THUDM/cogvlm-chat-hf,It seems like a problem of WSL2 and torch multi-gpu support... I have no idea... sorry
chenkq,2023-11-21 12:38:54+00:00,comment,THUDM/cogvlm-chat-hf,"if u have two 24GB devices, u can use `accelerate` to dispatch model as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch` function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf', the local path for the model ckpt is needed. I have personally tested this code on my own device, and observed that the peak GPU usage reached approximately 22GB.

```python
import torch
import requests
from PIL import Image
from transformers import AutoModelForCausalLM, LlamaTokenizer
from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch

tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')
with init_empty_weights():
    model = AutoModelForCausalLM.from_pretrained(
        'THUDM/cogvlm-chat-hf',
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
    )
device_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'}, no_split_module_classes='CogVLMDecoderLayer')
model = load_checkpoint_and_dispatch(
    model,
    'local/path/to/hf/version/chat/model',   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'
    device_map=device_map,
)
model = model.eval()

# check device for weights if u want to
for n, p in model.named_parameters():
    print(f""{n}: {p.device}"")

# chat example
query = 'Describe this image'
image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')
inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode
inputs = {
    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),
    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),
    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),
    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],
}
gen_kwargs = {""max_length"": 2048, ""do_sample"": False}

with torch.no_grad():
    outputs = model.generate(**inputs, **gen_kwargs)
    outputs = outputs[:, inputs['input_ids'].shape[1]:]
    print(tokenizer.decode(outputs[0]))
```"
chenkq,2023-11-21 13:25:11+00:00,comment,THUDM/cogvlm-chat-hf,"also, thanks for the reminder. the requirement is added in README"
lzxcgnkhnrlnto,2023-11-21 15:30:30+00:00,comment,THUDM/cogvlm-chat-hf,"> if u have two 24GB devices, u can use `accelerate` to dispatch model as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch` function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf', the local path for the model ckpt is needed. I have personally tested this code on my own device, and observed that the peak GPU usage reached approximately 22GB.
 > ```

This works in WSL2 with two gpus, thank you!
CogVLM is the best captioner out there and to finally get this to run is a great relief.
(And, I see you've already added this as an example, great work ^^ )"
2thousand,2023-12-10 13:21:50+00:00,comment,THUDM/cogvlm-chat-hf,"> if u have two 24GB devices, u can use `accelerate` to dispatch model as demonstrated in the following. it seems that the `load_checkpoint_and_dispatch` function does not support remote Hugging Face model paths  like 'THUDM/cogvlm-chat-hf', the local path for the model ckpt is needed. I have personally tested this code on my own device, and observed that the peak GPU usage reached approximately 22GB.
> 
> ```python
> import torch
> import requests
> from PIL import Image
> from transformers import AutoModelForCausalLM, LlamaTokenizer
> from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch
> 
> tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')
> with init_empty_weights():
>     model = AutoModelForCausalLM.from_pretrained(
>         'THUDM/cogvlm-chat-hf',
>         torch_dtype=torch.bfloat16,
>         low_cpu_mem_usage=True,
>         trust_remote_code=True,
>     )
> device_map = infer_auto_device_map(model, max_memory={0:'20GiB',1:'20GiB','cpu':'16GiB'}, no_split_module_classes='CogVLMDecoderLayer')
> model = load_checkpoint_and_dispatch(
>     model,
>     'local/path/to/hf/version/chat/model',   # typical, '~/.cache/huggingface/hub/models--THUDM--cogvlm-chat-hf/snapshots/balabala'
>     device_map=device_map,
> )
> model = model.eval()
> 
> # check device for weights if u want to
> for n, p in model.named_parameters():
>     print(f""{n}: {p.device}"")
> 
> # chat example
> query = 'Describe this image'
> image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')
> inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode
> inputs = {
>     'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),
>     'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),
>     'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),
>     'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],
> }
> gen_kwargs = {""max_length"": 2048, ""do_sample"": False}
> 
> with torch.no_grad():
>     outputs = model.generate(**inputs, **gen_kwargs)
>     outputs = outputs[:, inputs['input_ids'].shape[1]:]
>     print(tokenizer.decode(outputs[0]))
> ```

Anyone tried to deploy cogvlm (4bit quantization) on multiple GPUs with accelerate? "
chenkq,2023-12-11 04:00:44+00:00,comment,THUDM/cogvlm-chat-hf,@2thousand see if [this](https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362) can help
2thousand,2023-12-11 06:04:55+00:00,comment,THUDM/cogvlm-chat-hf,"> @2thousand see if [this](https://huggingface.co/THUDM/cogvlm-chat-hf/discussions/11#656e3f7b4390ade20ebae362) can help

Thanks, I just figured it out. we can directly add device_map=""auto"" in AutoModelForCausalLM.from_pretrained()

```python
tokenizer = LlamaTokenizer.from_pretrained('vicuna-7b-v1.5')
model = AutoModelForCausalLM.from_pretrained(
        'THUDM/cogvlm-chat-hf',
        load_in_4bit=True,
        trust_remote_code=True,
        device_map=""auto""
    ).eval()
query = 'Describe this image in details.'
image = Image.open('image-path').convert('RGB')
inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode
inputs = {
    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),
    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),
    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),
    'images': [[inputs['images'][0].to('cuda').to(torch.float16)]],
}
gen_kwargs = {""max_length"": 2048, ""do_sample"": False}

with torch.no_grad():
    outputs = model.generate(**inputs, **gen_kwargs)
    outputs = outputs[:, inputs['input_ids'].shape[1]:]
    print(tokenizer.decode(outputs[0]))
```"
Hisma,2023-12-16 15:35:47+00:00,comment,THUDM/cogvlm-chat-hf,"can someone create a web demo version of this?  I tried adapting the cogvlm web demo using the accelerate code above to allow multi-gpu support in wsl2, but couldn't get it to work.  
has anyone gotten a gradio UI version of cogvlm working in wsl2? "
qingsonglv,2023-11-21 11:25:24+00:00,comment,THUDM/cogvlm-chat-hf,"No, it's an English version. The bilingual version will release recently."
jaywanghz,2023-11-21 14:06:58+00:00,comment,THUDM/cogvlm-chat-hf,Thanks
nkasmanoff,2023-09-19 11:49:41+00:00,comment,tiiuae/falcon-rw-1b,"

Update generation config to correct token ids for BOS and EOS
"
FalconLLM,2023-05-30 14:54:05+00:00,comment,tiiuae/falcon-rw-1b,This discussion on Falcon-RW-1B might contain some pointers: https://huggingface.co/tiiuae/falcon-40b-instruct/discussions/4#6475abcb09e773226333c550 
YaTharThShaRma999,2024-03-08 15:59:52+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GPTQ,"@JoaoCP what library are you using? Also are you using the correct prompt format? 
Things like llama cpp python and llama cpp should automatically get the correct prompt template i believe. 
"
JoaoCP,2024-03-09 07:57:34+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GPTQ,"@YaTharThShaRma999 Thank you for your reply. I’m using transformers (HF) having more control of what gets into the model. For the prompt template I am using the same format indicated in the model’s card that is ```<s>[INST] (user’s input) [INST] ```

I am using the same code I use to do inference with other auto recursive models such as Llama 2 7B Chat (The Bloke’s GPTQ version) for example. I am using the auto class AutoModelForCausalLM to load the model and the generate method for inference.

Yesterday I tried the same model using the text web ui with the same prompt and It worked well. I am doing something wrong."
JoaoCP,2024-03-18 11:55:05+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GPTQ,"This is my code:

```import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

model_dir = "".../TheBloke/Mistral-7B-Instruct-v0.2-GPTQ""
model_name = ""TheBloke/Mistral-7B-Instruct-v0.2-GPTQ""

if not os.path.exists(model_dir):
    os.makedirs(model_dir)  

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map = ""auto"")
        
    model.save_pretrained(save_directory=model_dir)
    tokenizer.save_pretrained(save_directory=model_dir)

else:
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=""auto"", local_files_only = True)

generation_params = {
    ""do_sample"": True,
    ""max_new_tokens"": 500,
    ""temperature"": 0.1,
    ""top_p"": 0.95,
    ""top_k"": 40
}
generation_cfg = GenerationConfig.from_dict(config_dict=generation_params)

prompt = ""What is your favourite condiment?""
prompt_template=f""<s>[INST] {prompt} [/INST]""

print(""\n\n*** Generate:"")

inputs = tokenizer(prompt, return_tensors=""pt"").to('cuda')
pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id
generate_ids = model.generate(inputs.input_ids, pad_token_id=pad_token_id, generation_config=generation_cfg)
generate_ids = generate_ids[:, inputs.input_ids.shape[1]:]

output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
print(output)
```

I am saving the model locally and loading it from local folder. I've noticed that it works well the first time (when loading from the HF hub), but when it loads from the local files the generated response is gibberish.

Any help? 

Thank you"
DayiTokat,2024-02-02 20:54:05+00:00,comment,TheBloke/Mistral-7B-Instruct-v0.2-GPTQ,"Sorry for my rush, I just saw :/"
YaTharThShaRma999,2024-03-17 19:08:33+00:00,comment,TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ,@tunggad huggingface transformers is not that fast so speed will be pretty slow. Use exllama or exllama v2 for faster inference. I would actally reccomend you use 3bpw exl2 mixtral and load it with exllama v2 and you will get much faster speed.(50 tokens per second around?)
danielhanchen,2024-01-29 08:47:17+00:00,comment,unsloth/tinyllama-bnb-4bit,@gardner Yep
heegyu,2023-10-06 14:02:44+00:00,comment,EleutherAI/gpt-neox-20b,"This PR contains the Flax weights of GPT-NeoX 20B
FlaxGPTNeoXForCausalLM is not yet included in transformers release. This work is progressing in https://github.com/huggingface/transformers/pull/25334

I uploaded this weights for passing tests_pr_documentation_tests
https://github.com/huggingface/transformers/pull/25334#issuecomment-1733859618"
adwaita28,2023-06-16 18:48:21+00:00,comment,EleutherAI/gpt-neox-20b,How do I convert this model to a fast transformer model? Has anoyno
sam-mosaic,2023-04-18 18:28:23+00:00,comment,EleutherAI/gpt-neox-20b,"you probably do not have enough memory to train a model this big, with optimizer state, I'd expect 8x 80GB A100s to be the minimum. You can probably do PEFT though https://github.com/huggingface/peft"
gsaivinay,2023-03-02 11:09:16+00:00,comment,EleutherAI/gpt-neox-20b,"As far as I know, you need atleast 42GB free memory to load the model checkpoint with `low_cpu_mem_usage=True` argument. For finetuning you might need even more."
vmajor,2023-02-27 05:26:18+00:00,comment,EleutherAI/gpt-neox-20b,"There is something wrong with the model. Here is it's response to a stacked query:

Query: ""What is the highest mountain in the world? Tell me the height in meters.""

Response: ""This is my code:
import java.io.*;
import java.util.*;

public class Main {

    public static void main(String[]"""
stellaathena,2023-02-07 06:46:59+00:00,comment,EleutherAI/gpt-neox-20b,Rejecting as we are preparing our own model card.
RagequitInc,2023-01-19 13:49:50+00:00,comment,EleutherAI/gpt-neox-20b,"i am not sure but i guess for this to run locally you need at least 45GB of GPU VRAM better 48+ GB.
i am refering to this information but it might be some other ""fork"" of GPT. im also new to this.
https://github.com/zphang/minimal-gpt-neox-20b
but since Apple will launch a 5k Euro Beast of a System this month... with up to 96GB of VRAM... it might get possible."
hiddenchamp,2023-01-19 14:19:01+00:00,comment,EleutherAI/gpt-neox-20b,This info helps me a lot. Thanks for taking the time :)
panrodrigo,2022-12-29 17:08:21+00:00,comment,EleutherAI/gpt-neox-20b,Same issue here.
kesar,2022-12-29 19:28:34+00:00,comment,EleutherAI/gpt-neox-20b,Same issue here. Any help?
Justlearn,2023-01-02 08:47:07+00:00,comment,EleutherAI/gpt-neox-20b,Same issue on my sagemaker
ml-workshops,2023-01-11 22:33:46+00:00,comment,EleutherAI/gpt-neox-20b,Hey folks I just saw this as well. Will triage.
ml-workshops,2023-01-12 23:06:31+00:00,comment,EleutherAI/gpt-neox-20b,"I am pretty sure the core issue here is transformers version. GPT-Neo was released in a later version of the transformers sdk. When you use the boilerplate provided above, which is 4.17, this doesn't have neo. When I use a more recent version locally on my notebook, 4.25, I can use neo without an issue. 

To solve this on the hosting side of things, bring a requirements.txt file and point to a more recent version of the transformers sdk. 

You can build a custom image and point to it, or you can pass an entry point with a requirements.txt.
https://sagemaker.readthedocs.io/en/stable/api/inference/model.html"
deleted,2022-12-03 03:23:48+00:00,comment,EleutherAI/gpt-neox-20b,This comment has been hidden
stellaathena,2022-08-20 14:38:49+00:00,comment,EleutherAI/gpt-neox-20b,"This model does not fit on one GPU unless you have an 80 GB A100, an A40, or an A6000. It will just barely fit for running inference in 48 GB and does not fit in 40 GB."
FrostAura,2022-08-19 23:44:01+00:00,comment,EleutherAI/gpt-neox-20b,All ready to be merged.
stellaathena,2022-08-27 12:21:13+00:00,comment,EleutherAI/gpt-neox-20b,Yes
nielsr,2022-06-18 08:52:56+00:00,comment,EleutherAI/gpt-neox-20b,"Hi,

Looking at the [docs](https://huggingface.co/docs/transformers/model_doc/gpt_neox), the weights are in float16 format, meaning that 16 bits or 2 bytes are used to store each parameter. 

That means that, for a 20 billion parameter model, you need 20 billion parameters * 2 bytes / parameter = 40 billion bytes, also known as 40 GB. That's the amount of RAM required to load the model."
stellaathena,2022-06-18 12:52:17+00:00,comment,EleutherAI/gpt-neox-20b,"That’s not quite correct. GPT-NeoX-20B was trained using mixed precision (fp32/fp16). These weights are in fp32, which is why the docs mention using `.half()` before loading the model onto GPU.

I’m not sure what GPUs you are able to get via Colab but inference with this model typically requires more then 40 GB of VRAM."
nielsr,2022-06-20 08:20:12+00:00,comment,EleutherAI/gpt-neox-20b,"Oh my apologies. I read from the docs ""GPT-NeoX-20B was trained with fp16"", I guess this can be corrected.

Also, I think it may be beneficial to add the RAM requirements to the docs as well, similar to the [""tips"" section](https://huggingface.co/docs/transformers/model_doc/gptj#overview) of GPT-J. 

Do you think it would be beneficial to have a separate branch on this repo with float16 weights?"
nielsr,2022-06-21 15:46:24+00:00,comment,EleutherAI/gpt-neox-20b,cc'ing @sgugger regarding whether or not this model can be loaded into Google Colab using Accelerate's big model inference feature.
sgugger,2022-06-21 21:42:28+00:00,comment,EleutherAI/gpt-neox-20b,"Not on Colab free no, they don't provide enough disk space to even download the weights.

@stellaathena I'm surprised to learn the model was trained in fp16 (not bfloat16?) as we get crappy generations in FP16 but decent ones in bfloat16 in our tests.

**Edit:** Looks like it was only a bug in the Transformers implementation. https://github.com/huggingface/transformers/pull/17811 should fix the float16 generations."
ghpkishore,2022-06-25 05:31:00+00:00,comment,EleutherAI/gpt-neox-20b,Thanks for letting me know and fixing the issue @sgugger. I will upgrade to Colab Pro and see if can run their.
keithhon,2023-02-10 13:02:36+00:00,comment,EleutherAI/gpt-neox-20b,"I have tried running the model in Colab Pro but failed, as it only has 39GB~40GB gpu ram"
HexDev,2023-04-02 00:01:48+00:00,comment,EleutherAI/gpt-neox-20b,github codespaces
teknium,2024-05-01 11:45:31+00:00,comment,NousResearch/Nous-Hermes-2-Mistral-7B-DPO,"> Will there be a NousResearch/Nous-Hermes-2-Llama-3-8B-DPO or NousResearch/Hermes-2-Pro-Llama-3-8B?

:]"
supercharge19,2024-05-01 17:57:38+00:00,comment,NousResearch/Nous-Hermes-2-Mistral-7B-DPO,"@teknium thanks, following now."
mlabonne,2024-02-22 14:34:36+00:00,comment,NousResearch/Nous-Hermes-2-Mistral-7B-DPO,"Shouldn't be too different but this model is probably slightly better than NeuralHermes (haven't tested it, but I would choose this one by default)."
Tim1785,2024-03-21 09:48:13+00:00,comment,NousResearch/Nous-Hermes-2-Mistral-7B-DPO,Is it multi-lingual as it is finetuned from Mistral ?
edisonzf2020,2024-03-02 06:06:45+00:00,comment,NousResearch/Nous-Hermes-2-Mistral-7B-DPO,"Yes, I have the same doubts."
dvilasuero,2024-03-05 23:30:27+00:00,comment,NousResearch/Nous-Hermes-2-Mistral-7B-DPO,I'd also love to know about the datasets used for DPO in case we can help improving them!
chrisammon3000,2024-04-15 16:01:13+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"I would try this:
```
TEMPLATE """"""{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
""""""
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>
```

I am using this for Hermes-2-Pro-Mistral-7B:
```
FROM ./models/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf
TEMPLATE """"""{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
""""""
PARAMETER stop <|im_start|>
PARAMETER stop <|im_end|>
```

Chat works in Ollama using the ChatML template and parameters, but I haven't tested this for function calling or structured output yet."
Ziizu,2024-04-28 23:47:30+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"Thanks,  up and running now :)
"
teknium,2024-01-07 10:35:19+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"> Overall solid model with good inelegance and prose, but (like felled 80% of Yi fine-tunes) it quickly devolves into word vomit about 400 to 500 tokens in.
> 
> Useing TheBlokes gguf 4_k_M version on Koboldcpp with default launcher options and Mirostat 2.

I haven't experienced this yet in gguf 4_k_m on lmstudio"
teknium,2024-01-07 10:35:44+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"> The 4k context window is quite limiting these days.  Any chance of a version trained on Yi 200k context?

Maybe but probably not for a while"
anon22342134,2023-12-29 17:42:23+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"same here, i think it's all yi model's actually :/"
teknium,2024-01-02 06:56:20+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"> this model seems to repeat easily.

Can you share examples"
brucethemoose,2024-01-05 19:57:38+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"Yeah.

Mergekit gives this warning:

```
WARNING:root:Token '<|startoftext|>' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B tokenizer but >= vocab_size
WARNING:root:Token '<|endoftext|>' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B tokenizer but >= vocab_size
```"
brucethemoose,2024-01-06 07:29:59+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"Correcting the vocab size to 64002 doesn't seem to work either.

"
KeithCu,2023-12-31 21:43:22+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"Capybara is a small dataset of just 20K examples. This is 1,000,000 GPT4 + other stuff. "
pseudotensor,2024-01-24 09:36:14+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"Ya, would be good to have 200k context version with the 1M GPT4.  Instead we  have good data or small context."
dillfrescott,2023-12-26 02:21:09+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,Would love to see this
Yhyu13,2023-12-27 13:39:44+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"Yeah, 4096 is too short for most task that requires text summerization"
brucethemoose,2024-01-06 07:29:09+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"> It doesn't have to be trained at the full 200K CTX size. Finetunes trained at lesser context sizes still seem to hold the long context performance.

I may have spoke too soon on this. It seems that Bagel's 4K training actually hurt Yi 200K's context considerably. 

I suspect its because it was partially a full finetune, not a lora?"
N8Programs,2023-12-26 02:50:14+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,While we wait for the bloke: https://huggingface.co/N8Programs/Nous-Hermes-2-Yi-34B-GGUF/tree/main
teknium,2023-12-27 04:26:22+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B-GGUF
dillfrescott,2023-12-27 05:05:49+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,Excellent. Thanks
HDiffusion,2023-12-26 20:25:32+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,Shouldn't it keep the original Yi34B license? 
amgadhasan,2023-12-27 06:08:17+00:00,comment,NousResearch/Nous-Hermes-2-Yi-34B,"@teknium I'm not sure why this was closed, maybe I should ask  different questions.

So:
1. What's the license for this model?
2. Can we use this model commercially? "
djstrong,2024-04-18 17:27:53+00:00,comment,speakleash/Bielik-7B-v0.1,I guess it is not recommended.
akshitab,2024-03-04 22:26:43+00:00,comment,allenai/OLMo-7B,"Hi @borgr , for the revisions from step0 to step556, we follow a linear LR schedule, and then in the last 1000 steps, we anneal the LR to 0. We found this to be better for the performance of the final model."
borgr,2024-03-04 22:33:07+00:00,comment,allenai/OLMo-7B,"I think I didn't put the question well

I find the differences between those checkpoints unclear, specifically the ones that are part of allenai/OLMo-7B, how can the not annealed one be the one with more tokens,batches and steps?
![image.png](https://cdn-uploads.huggingface.co/production/uploads/61bf40824b4300d0fb0acf59/m-8h-F5N1W8sg2B98Eozu.png)
"
akshitab,2024-03-07 23:54:19+00:00,comment,allenai/OLMo-7B,"@borgr This might make it clearer:

OLMo-7B step452k | 2T tokens | following linear schedule (not annealed)
OLMo-7B step 556k | 2.460T tokens | still following linear schedule (not annealed)
OLMo-7B step 557k (main) | 2.464T tokens | LR annealed to 0 
"
borgr,2024-03-08 16:39:01+00:00,comment,allenai/OLMo-7B,Maybe write in the NAME and Note something comparable between  the second and third row then?
dirkgr,2024-02-23 00:08:17+00:00,comment,allenai/OLMo-7B,"I think the plan is to just add it fully into Huggingface transformers, and remove all the custom executable code. I assume at this point it would run get added automatically?"
clefourrier,2024-02-23 08:25:42+00:00,comment,allenai/OLMo-7B,"Hi!
We almost never run models which require `trust_remote_code=True` on our cluster for safety reasons - it might happen exceptionally, but then it means my co-maintainer or myself has to read the full code of the model + dependencies before running evaluations, and we don't have the bandwidth at the moment.
However, if the model becomes natively integrated into `transformers`, then it will become instantly submittable on the leaderboard :)"
kno10,2024-03-12 19:53:17+00:00,comment,allenai/OLMo-7B,"@clefourrier CohereForAI/c4ai-command-r-v01 is another example of a hyped model that currently is not (yet) evaluated in the dashboard.
Both are probably worth the effort of checking the ""remote code"" (or integrating it into transformers... vLLM seems to have merged OLMo support: https://github.com/vllm-project/vllm/pull/2832 ).
On lmsys.org OLMo currently ranks on the level of Mistral-7B-Instruct-v0.1 there, but my subjective impression was that OLMo is substantially worse.

Either way, both aren't the common case of yet-another-merge, but in both cases substantial effort went into the training, and hence their performance on the leaderboard is of much higher interest than the thousand LoRA overfitting models we see these days. Hence, I'd really like to see them included in the evaluation.

@dirkgr I did not see a pull request for OLMo in the transformers library yet. Why not? It would help promoting your model if it is easier to run."
dirkgr,2024-03-12 20:01:20+00:00,comment,allenai/OLMo-7B,"Mate, we have lots of priorities, and getting OLMo into the transformers library is just one of them. We recently cleaned up the naming conventions we have in the training code to make that easier. Meanwhile, the entire stack is open source, and the HF version of the code is already in the OLMo repo. If you need this right now, you can make the PR today!"
clefourrier,2024-03-13 06:28:49+00:00,comment,allenai/OLMo-7B,"Hi @kno10 ,
We actually discussed with Cohere before the release, and since it's not a base pretrained model (it's a pretrained + SFT model), we followed our usual guidelines and did not evaluate it. We however explained how they could report numbers equivalent to the Open LLM Leaderboard's result, and as far as I know, the code is currently being integrated in transformers.

But I also agree with @dirkgr , that if you think things are missing, you should feel free to contribute! After all, that's also one of the cool things about open source :)"
shanearora,2024-02-09 18:05:09+00:00,comment,allenai/OLMo-7B,"@dirkgr Can correct me but I am not aware of such plans. You should be able to load the model and then call, say,`model = model.bfloat16()` to convert the weights to 16 bits. You may need to load the model on the CPU, downcast to 16 bits, and then move the model to GPU. An alternative with a higher memory requirements (that we used while training the model) is to use `torch.autocast` with a 16 bit type."
saattrupdan,2024-02-09 19:48:20+00:00,comment,allenai/OLMo-7B,"@shanearora I completely get that, but if I’m loading in the model with vLLM then I get OOM errors before any conversion can happen. I guess I could convert it and upload it myself, but it would just be a bit more official if you all had a 16bit version uploaded. Same thing with quantised and GGUF versions for that matter, as these are required by other applications like llama.cpp and LM Studio. But it’s up to you - feel free to close this issue if you’re not planning on it 🙂"
shanearora,2024-02-09 21:21:36+00:00,comment,allenai/OLMo-7B,@akshitab Do you know about OLMo plans in relation to vLLM?
akshitab,2024-02-10 23:05:54+00:00,comment,allenai/OLMo-7B,vLLM integration for OLMo is currently in progress here: https://github.com/vllm-project/vllm/issues/2763 
shanearora,2024-02-06 18:57:39+00:00,comment,allenai/OLMo-7B,@akshitab 
epwalsh,2024-02-06 03:07:40+00:00,comment,allenai/OLMo-7B,"Hopefully within the next month but it depends on queue times... lots of other groups using the LUMI supercomputer right now as well, and the job time limit is 48 hours. So at most we can only  run for 48 hours at a time before sitting in the queue again."
DrNicefellow,2024-02-06 05:23:50+00:00,comment,allenai/OLMo-7B,@epwalsh Thank you very much for the information! Hope everything will go well!
DrNicefellow,2024-02-14 02:53:14+00:00,comment,allenai/OLMo-7B,"@epwalsh BTW, it may be beneficial to the community if you release the intermediate checkpoints before the model is fully trained."
shanearora,2024-02-06 18:55:12+00:00,comment,allenai/OLMo-7B,"@akshitab Quick review, maybe you'd prefer to change the name in the OLMo repo instead?."
akshitab,2024-02-09 07:06:20+00:00,comment,allenai/OLMo-7B,The config in the main branch has been updated. Other revisions will be updated soon.
dirkgr,2024-02-04 07:52:46+00:00,comment,allenai/OLMo-7B,Did you mean to add this PR here? Looks like wrong repo?
fodibaj1971,2024-02-08 17:17:23+00:00,comment,allenai/OLMo-7B,"> Did you mean to add this PR here? Looks like wrong repo?

"
fodibaj1971,2024-02-08 17:18:25+00:00,comment,allenai/OLMo-7B,"
![IMG_20190612_150018_1.jpg](https://cdn-uploads.huggingface.co/production/uploads/6575f0f0839aa08899357478/yrChIPJzsyz2qa0zFCCu_.jpeg)
"
fodibaj1971,2024-02-08 17:20:45+00:00,comment,allenai/OLMo-7B,"proceedpush
![IMG_20190612_150018_1.jpg](https://cdn-uploads.huggingface.co/production/uploads/6575f0f0839aa08899357478/buLYVnZWwg3Sreb6kqtZ5.jpeg)
"
shanearora,2024-02-09 23:43:32+00:00,comment,allenai/OLMo-7B,"Fixing this appears to require changes in both the OLMo repo and this repo. The OLMo repo change was completed today (https://github.com/allenai/OLMo/pull/442), so once a new minor release of that repo is done, this repo will be fixed."
DrNicefellow,2024-02-10 06:24:23+00:00,comment,allenai/OLMo-7B,@shanearora Thank you very much for the information! Please close this issue when both the new release of the repos are ready.
natolambert,2024-02-03 21:00:04+00:00,comment,allenai/OLMo-7B,They're being finalized and will be released (and added to the readme soon)!
natolambert,2024-02-05 18:38:00+00:00,comment,allenai/OLMo-7B,@DrNicefellow  https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5
natolambert,2024-02-02 16:36:56+00:00,comment,allenai/OLMo-7B,Yeah 7B will be hard with 12-16 GB of memory. Maybe try the 1B model? https://huggingface.co/allenai/OLMo-1B
Kre8tiveAi,2024-02-02 15:17:59+00:00,comment,allenai/OLMo-7B,Did you check visual STudio for ai extention/2convert ? 
natolambert,2024-02-02 16:33:27+00:00,comment,allenai/OLMo-7B,"@TheBlocki plz thx
we'll work on more code integrations if anything specific is wrong."
jbkcrash,2024-02-02 18:53:39+00:00,comment,allenai/OLMo-7B,"I have been trying to hack it to work this morning. I added the new arch ""OlmoModelForCausalLM,""  but I'm not sure if there is an existing compatible one like MODEL_ARCH.LLAMA. 

As a likely result, I am running into deeper model issues with llama.cpp, for example.

```
Loading model: OLMo-7B
gguf: This GGUF file is for Little Endian only
Set model parameters
Set model tokenizer
The repository for /backup_disks/OLMo-7B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co//backup_disks/OLMo-7B.
You can avoid this prompt in future by passing the argument `trust_remote_code=True`.

Do you wish to run the custom code? [y/N] y
gguf: Adding 50009 merge(s).
gguf: Setting special token type eos to 50279
gguf: Setting special token type pad to 1
Exporting model to 'olmo.gguf'
gguf: loading model part 'pytorch_model.bin'
Can not map tensor 'model.transformer.wte.weight'
```

Transformers needs to be updated to the latest version from Github, but ai2-olmo seems to need a version of the torch that is hard to resolve. I will give it one last attempt to try with this version torch-2.3.0a0+git52b679d. But I fear a proper arch needs to be added to llama.ccp, and all my attempts are to no avail. In that regard, I am trying to use the llama.cpp convert_hf_to_gguf.py, just too early, I think, at this point.
"
MaziyarPanahi,2024-02-05 12:46:24+00:00,comment,allenai/OLMo-7B,Interested to see if anyone can make GGUF work for Olmo arch.
HR1777,2024-02-08 08:07:42+00:00,comment,allenai/OLMo-7B,Any news about the GGUF versions? Could someone finally make them?
MaziyarPanahi,2024-02-08 08:12:02+00:00,comment,allenai/OLMo-7B,"I tried, but I couldn't add this architect into llama.cpp and make the required changes."
HR1777,2024-02-08 08:34:13+00:00,comment,allenai/OLMo-7B,I hope they add this feature faster to llama.cpp
eleius,2024-04-19 15:01:10+00:00,comment,allenai/OLMo-7B,@MaziyarPanahi finally implemented! https://github.com/ggerganov/llama.cpp/pull/6741
MaziyarPanahi,2024-04-19 17:48:24+00:00,comment,allenai/OLMo-7B,"Awesome! Thanks @eleius, shall we do it or it's been done already?"
eleius,2024-04-20 07:38:53+00:00,comment,allenai/OLMo-7B,"It seems @nopperl just did the GGUFs, I'll have to try them"
taufiqdp,2024-02-02 12:12:14+00:00,comment,allenai/OLMo-7B,"to fix this, you need to run: ```pip install ai2-olmo```

you can read how to use it in the [Uses](https://huggingface.co/allenai/OLMo-7B#uses) section"
natolambert,2024-02-02 16:34:26+00:00,comment,allenai/OLMo-7B,"Yeah @taufiqdp is right.
It's a default python error we can't overwrite (mentioned in the model card I think)."
Sakura77,2024-02-02 08:14:43+00:00,comment,allenai/OLMo-7B,"same error when running it locally

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Path to the locally saved model
local_model_path = ""G:/LLM/olmo""

# Check for CUDA availability, and use CPU as fallback
device = ""cuda"" if torch.cuda.is_available() else ""cpu""
if device == ""cpu"":
    print(""CUDA not available, using CPU instead."")
else:
    print(""Using CUDA."")

# Load the model and tokenizer from the local path with trust_remote_code=True
model = AutoModelForCausalLM.from_pretrained(local_model_path, trust_remote_code=True).to(device)
tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)

message = [""Language modeling is ""]
inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False).to(device)
response = model.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])"
natolambert,2024-02-02 16:35:53+00:00,comment,allenai/OLMo-7B,What is your compute environment? Is it MPS?
epwalsh,2024-02-02 16:39:47+00:00,comment,allenai/OLMo-7B,Fix is here: https://github.com/allenai/OLMo/pull/426
natolambert,2024-02-02 16:39:50+00:00,comment,allenai/OLMo-7B,This comment has been hidden
NickyNicky,2024-02-03 06:10:19+00:00,comment,allenai/OLMo-7B,"Play:

https://colab.research.google.com/drive/1pEM_03NIopVwc629heD4XVhdZwHd2k1s?usp=sharing"
Sakura77,2024-02-03 09:11:47+00:00,comment,allenai/OLMo-7B,"thx for the above link, works perfectly,  I initially tried from Anaconda 3 and Python 3.9 (win env), and looks like there is an issue with my current env ; running now on ubuntu, works great :) "
natolambert,2024-02-01 21:04:40+00:00,comment,allenai/OLMo-7B,Which link? Likely a typo.
natolambert,2024-02-01 21:05:30+00:00,comment,allenai/OLMo-7B,ah done
